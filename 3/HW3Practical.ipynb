{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISKf-ajkZKU6"
      },
      "source": [
        "# Homework 3\n",
        "\n",
        "## Course Name: Deep Learning\n",
        "#### Lecturer: Dr. Beigy\n",
        "\n",
        "---\n",
        "\n",
        "#### Notebooks Supervised By: Zeinab Sadat Taghavi\n",
        "#### Notebooks Prepared By: Zahra Rahimi, Zahra Khoramnejad, Mehran Sarmadi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7FFJaRgaqC1"
      },
      "source": [
        "---\n",
        "---\n",
        "## 1 Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q8G6YosbKrX"
      },
      "source": [
        "In this notebook you have to design and train models for a time series prediction task on the provided dataset using these three different architectures:\n",
        "\n",
        "- Simple RNN\n",
        "\n",
        "- GRU\n",
        "\n",
        "- LSTM\n",
        "\n",
        "You will compare and rank them at the end of the notebook and explain why they were ranked that way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FANbmLJ2d2dp"
      },
      "source": [
        "---\n",
        "### 1.1 Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WdzOu4UGgDFQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prWan6z7eKEp"
      },
      "source": [
        "---\n",
        "---\n",
        "## 2 Dataset\n",
        "Electric Production IP Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rtouQ7MVomh0"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./data/Electric_Production.csv', index_col='Date', parse_dates=True, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "-m3r2Dw_os75",
        "outputId": "98d8b0b6-6d12-4696-c609-bf24c550ba04"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Value</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1985-01-01</th>\n",
              "      <td>72.505203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985-02-01</th>\n",
              "      <td>70.671997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985-03-01</th>\n",
              "      <td>62.450199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985-04-01</th>\n",
              "      <td>57.471401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985-05-01</th>\n",
              "      <td>55.315102</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Value\n",
              "Date                 \n",
              "1985-01-01  72.505203\n",
              "1985-02-01  70.671997\n",
              "1985-03-01  62.450199\n",
              "1985-04-01  57.471401\n",
              "1985-05-01  55.315102"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oc6T4bkqqVQX",
        "outputId": "e0f489e9-cdd4-48f2-8d47-ad9c83b7fe90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 397 entries, 1985-01-01 to 2018-01-01\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   Value   397 non-null    float32\n",
            "dtypes: float32(1)\n",
            "memory usage: 4.7 KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "o_WhChFOqYb4",
        "outputId": "6eb1b565-c322-4aa9-81fb-8a1520de9938"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGwCAYAAACD0J42AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAADS70lEQVR4nOy9eZxkZXk9fm7tvff0bD0DAwybA8q+IyrKhEUgEPmKKIoEBDVogvwUJQFUoqJEEwIS0WgQVBIlKi5JICMYQBx2RpAdWWaGYWaYpfeu/f7+uPW893mXW1W39+55zufDZ7q76q5V1HvqPOc5j+f7vg+BQCAQCASCGYTEdJ+AQCAQCAQCgQkhKAKBQCAQCGYchKAIBAKBQCCYcRCCIhAIBAKBYMZBCIpAIBAIBIIZByEoAoFAIBAIZhyEoAgEAoFAIJhxSE33CYwF1WoVGzZsQEdHBzzPm+7TEQgEAoFA0AR838fg4CCWLl2KRKK+RjIrCcqGDRuwbNmy6T4NgUAgEAgEY8C6deuw8847133OrCQoHR0dAIIL7OzsnOazEQgEAoFA0AwGBgawbNkytY7Xw6wkKFTW6ezsFIIiEAgEAsEsQzP2DDHJCgQCgUAgmHEQgiIQCAQCgWDGQQiKQCAQCASCGYdZ6UERCAQCgWAiUKlUUCqVpvs05gzS6TSSyeSE7EsIikAgEAh2OPi+j40bN6Kvr2+6T2XOobu7G729vePOKROCIhAIBIIdDkROFi1ahNbWVgn9nAD4vo+RkRFs3rwZALBkyZJx7U8IikAgEAh2KFQqFUVO5s+fP92nM6fQ0tICANi8eTMWLVo0rnKPmGQFAoFAsEOBPCetra3TfCZzE3Rfx+vtiU1Q7r33Xpx66qlYunQpPM/D7bffrj3+hS98AStWrEBbWxvmzZuHlStX4sEHH9Ses23bNpx99tno7OxEd3c3zj//fAwNDY3rQgQCgUAgiAMp60wOJuq+xiYow8PDOOCAA3DDDTc4H997773xzW9+E08++SR+97vfYbfddsPxxx+PN954Qz3n7LPPxlNPPYVVq1bh17/+Ne69915ceOGFY78KgUAgEAgEcwqe7/v+mDf2PPz85z/H6aefHvmcgYEBdHV14Te/+Q2OO+44PPPMM9h3333x8MMP49BDDwUA3HHHHXj3u9+N9evXY+nSpdY+CoUCCoWCts9ly5ahv79fou4FAoFAEAv5fB4vv/wyli9fjlwuN92nM+dQ7/4SJ2hm/Z5UD0qxWMR3vvMddHV14YADDgAArF69Gt3d3YqcAMDKlSuRSCSsUhDh6quvRldXl/pPJhkLBAKBQBAfxx57LC6++OLpPo2mMCkE5de//jXa29uRy+XwT//0T1i1ahUWLFgAIGjtWrRokfb8VCqFnp4ebNy40bm/yy67DP39/eq/devWTcZpCwQCgUAwY3HqqafixBNPdD523333wfM8PPHEE1N8VvGwsX+06edOCkF55zvfiTVr1uD3v/89TjzxRJx55pmqL3osyGazanKxTDAWCAQCwY6I888/H6tWrcL69eutx2666SYceuih2H///afhzJrHX/3osaafOykEpa2tDXvuuSeOPPJIfO9730MqlcL3vvc9AEBvb69FVsrlMrZt24be3t7JOB2BQCAQCOrC932MFMtT/l8cG+gpp5yChQsX4vvf/77296GhIdx22204/fTT8f73vx877bQTWltbsd9+++Hf//3f6+7T1Y3b3d2tHWPdunU488wz0d3djZ6eHpx22ml45ZVXmj5vjnK1+eudkqC2arWqTK5HHXUU+vr68Oijj+KQQw4BANx9992oVqs44ogjpuJ0BAKBQCDQMFqqYN8r75zy4z591QlozTS3FKdSKZxzzjn4/ve/j7/7u79T7by33XYbKpUKPvjBD+K2227DZz/7WXR2duK//uu/8KEPfQh77LEHDj/88DGdX6lUwgknnICjjjoK9913H1KpFL70pS/hxBNPxBNPPIFMJhNrf+VKtennxlZQhoaGsGbNGqxZswYA8PLLL2PNmjVYu3YthoeH8bd/+7d44IEH8Oqrr+LRRx/Feeedh9deew3vfe97AQD77LMPTjzxRFxwwQV46KGHcP/99+MTn/gEzjrrLGcHj0AgEAgEggDnnXce/vSnP+Gee+5Rf7vppptwxhlnYNddd8WnP/1pHHjggdh9993xyU9+EieeeCJ+8pOfjPl4P/7xj1GtVvHd734X++23H/bZZx/cdNNNWLt2Lf7v//4v9v4mVUF55JFH8M53vlP9fskllwAAPvzhD+PGG2/Es88+i5tvvhlbtmzB/Pnzcdhhh+G+++7Dm9/8ZrXNj370I3ziE5/Acccdh0QigTPOOAPXXXdd3FMRCAQCgWBC0JJO4umrTpiW48bBihUrcPTRR+Pf/u3fcOyxx+LFF1/Efffdh6uuugqVSgVf+cpX8JOf/ASvvfYaisUiCoXCuBJz//CHP+DFF19ER0eH9vd8Po8//elPsfdXrkwiQTn22GPr1sx+9rOfNdxHT08Pbr311riHFggEAoFgUuB5XtOllunG+eefj09+8pO44YYbcNNNN2GPPfbAO97xDnzta1/DP//zP+Paa6/Ffvvth7a2Nlx88cUoFouR+/I8z1rTeUT90NAQDjnkEPzoRz+ytl24cGHsc69Umy/xzI5XQyAQCAQCAQDgzDPPxN/8zd/g1ltvxS233IKPf/zj8DwP999/P0477TR88IMfBBD4P59//nnsu+++kftauHAhXn/9dfX7Cy+8gJGREfX7wQcfjB//+MdYtGjRhHTQlmIoKDIsUCAQCASCWYT29na8733vw2WXXYbXX38d5557LgBgr732wqpVq/D73/8ezzzzDD760Y9i06ZNdff1rne9C9/85jfx+OOP45FHHsHHPvYxpNNp9fjZZ5+NBQsW4LTTTsN9992Hl19+Gf/3f/+Hv/7rv3a2OzdCOYaCIgRFIBAIBIJZhvPPPx/bt2/HCSecoBpMLr/8chx88ME44YQTcOyxx6K3t7fuKBoA+MY3voFly5bhbW97Gz7wgQ/g05/+tOZZaW1txb333otddtkF73nPe7DPPvvg/PPPRz6fH5OiUm6en4xvFs90IU6Wv0AgEAgEHDKLZ3JR7/4u///+E6/843unfxaPQCAQCAQCARCE4VVitBkLQREIBAKBQDDpiGOQBYSgCAQCgUAgmALEMcgCQlAEAoFAsINiFlowZwWi7mucFFlACIpAIBAIdjBQGy3P+xBMHOi+8nZlIF6KLCBBbQKBQCDYwZBMJtHd3Y3NmzcDCFppafCeYOzwfR8jIyPYvHkzuru7kUzqMf5xBgUCQlAEAoFAsAOit7cXABRJEUwcuru71f3lKMUs8QhBEQgEAsEOB8/zsGTJEixatEibPSMYH9LptKWcEERBEQgEAoGgSSSTycgFVTCxEJOsQCAQCASCGYe4JlkhKAKBQCAQCCYdpZglHiEoAoFAIBAIJh1S4hEIBAKBQDDjENckKwRFIBAIBIJpwvfvfxlf+OVTO0SqrSgoAoFAIBDMEnxj1fP4/u9fwbpto9N9KpMOMckKBAKBQDBLkC9Vgn/LlWk+k8lHSYYFCgQCgUAw8+H7Pko1VaFYjrd4z0aIgiIQCAQCwSxAkZlGizENpLMRYpIVCAQCgWAWgKsmpR1AQYk7i0cIikAgEAgE04ASK3mUYpY/ZiMq4kERCAQCgWDmQ1NQdoAST1wSJgRFIBAIBIJpACclhR2gxCMmWYFAIBAIZgEKO5iCUpYSj0AgEAgEMx+clOwIBEVKPAKBQCAQzALsaB4UMckKBAKBQDALwEnJjhDUJgqKQCAQCASzAJyUFHeANmMxyQoEAoFAMAtQ3ME8KGKSFQgEAoFgFmCHS5IVBUUgEAgEgpmPHW0Wz6SbZO+9916ceuqpWLp0KTzPw+23364eK5VK+OxnP4v99tsPbW1tWLp0Kc455xxs2LBB28e2bdtw9tlno7OzE93d3Tj//PMxNDQU91QEAoFAIJi1KO1gBGXSFZTh4WEccMABuOGGG6zHRkZG8Nhjj+GKK67AY489hp/97Gd47rnn8Od//ufa884++2w89dRTWLVqFX7961/j3nvvxYUXXhj3VAQCgUAgmLXQSzzxTbK/WPMavn7nc/D92WGwjetBScU9wEknnYSTTjrJ+VhXVxdWrVql/e2b3/wmDj/8cKxduxa77LILnnnmGdxxxx14+OGHceihhwIArr/+erz73e/G17/+dSxdujTuKQkEAoFAMOtQ1IYFxldQ/uY/1gAADl/eg7fvvXCiTmvSMOO6ePr7++F5Hrq7uwEAq1evRnd3tyInALBy5UokEgk8+OCDzn0UCgUMDAxo/wkEAoFAMJsxUUFt67ePTsTpTDpmlEk2n8/js5/9LN7//vejs7MTALBx40YsWrRIe14qlUJPTw82btzo3M/VV1+Nrq4u9d+yZcsm87QFAoFAMAtRKFeQL1Wm+zSaxkQFtY0UyxNxOpOOGZMkWyqVcOaZZ8L3fXzrW98a174uu+wy9Pf3q//WrVs3QWcpEAgEgrmAStXHUVffjaO/ejfKs8Rwqge1xTtn7jsZLswOUlaqxlNQYntQmjqJGjl59dVXcffddyv1BAB6e3uxefNm7fnlchnbtm1Db2+vc3/ZbBbZbHYyTlUgEAgEMwi+78PzvNjbbRsuYttwEQDQN1rCgvaZv2aMZ1hgmS32I6XZoaDEJY4TrqAQOXnhhRfwm9/8BvPnz9ceP+qoo9DX14dHH31U/e3uu+9GtVrFEUccMdGnIxAIBIJZgmrVx/u+/QDO+beHYnemzKbSDkH3oMS7Xk5oRmaJghLXJBtbQRkaGsKLL76ofn/55ZexZs0a9PT0YMmSJfh//+//4bHHHsOvf/1rVCoV5Svp6elBJpPBPvvsgxNPPBEXXHABbrzxRpRKJXziE5/AWWedJR08AoFAsANj+0gRD72yDQBQKFeRSyeb3naY+TBmS2x8cRweFP784VniQZn0Es8jjzyCd77zner3Sy65BADw4Q9/GF/4whfwy1/+EgBw4IEHatv99re/xbHHHgsA+NGPfoRPfOITOO6445BIJHDGGWfguuuui3sqAoFAIJhDKLBFt1CKR1CG8oygjCFTZDowHg8Kf/5gfnYQlLgm2dgE5dhjj60rvTUjy/X09ODWW2+Ne2iBQCAQzGHwMk2+XEEX0k1vO1QIF+nZkso6njZjvm3/aGnCzmkyMaPajAUCgUAgaBamghIHnKDETSydLozHJMsX+/6R2UFQpt0kKxAIBALBWMAVlEI5nvFzVpZ4OEGJec5cQekbLU7YOU0myjE9KEJQBAKBQDAjwBWU/DgUlNlT4gkX7LjnzBWXvlmioEiJRyAQCASzEuNSUAo7VhePVg4rVzFanPmtxjMmSVYgEAgEgjgwF9040Eo8s4SglMZhkjWfH7fMM1Is4/X+qZ3hM+OGBQoEAoFgx0KpUkUlpt8AMLp4YgavzXYFZTxdPED8Ms9f3vQw3n7Nb7F5MB9ru/GgJAqKQCAQCKYLxXIVb7/mt/iLf7k/9rbjUlC4B2WWmGTHMyzQUlBiEpSXtwyjVPGndBLypCfJCgQCgUAQhec2DuL1/jxe78+jWvWRSDQ/V6cwQR6U2dJmPJ6oe5PQ9Mcs8VBHzXimKMeFmGQFAoFAMG3g5CCupD+uLp5Z6EHRTLKVaqz5Q2bXz/aYCgrdo6m8V2KSFQgEAsG0gXtP4kr6WhfPeDwos6TEY6oXcXJCzG1HYnbx0GszlQqKmGQFAoFAMG3gBCXut/MJ86DMFgXFuMY498ssl8S916R0TWmJRxQUgUAgEEwX+MIZlyhMVFDbVJd4fN8fU9eSeZ5xyIK5bSnGtr7vq9ep3mtUKFcmlMCIgiIQCASCaQMv04yrxBPDJOv7/rR6UM75t4fwZ/90T+zF3Hx+HEI3HvWFk6moc+4fLeGtX70b7/326qb3Ww++78eOupcuHoFAIBBMGEYZyYhd4imNrcRTKFe1xS9ut8h4cf+LW1D1gU0DeSzraW16O7tMM3aTbDHGts3cq589th5bhorYMjQxc37ikhNAFBSBQCAQOBCno4QjPw6Cki+PLaiNl3fGctzxoFL1QWtvXNXHJBlxyjTj86/w/BX3Of/+T1vVz1WDXPi+HztIbywlMCEoAoFAINBQLFdxwrX34oJbHom9rU5Q4i1KY1VQeHknOK697VChjP98dD36Ryd2sF5xjL4Zfm9StayYOCUe8xrLYzTYul4j3/fx+xe3hM8xzK0f++GjOOxLv8Ebg4UYx4xPGoWgCAQCgUDDa32jeH7TEO5+dnNsJYUv0uNRUGIRFEtBsc/52lXP49O3/QEfufnhWOfUCMUxdh5xMtKWTVn7inPcYH8xSjxG/oqJ5zcNYbjoJpq+7+POpzZhsFDGqqc3xTimKCgCgUAgGCeoVFGp+rFVkInyoIynxONa6Fc9EyymD7+yPdY5NQJf4OOUeHg5p71GUMZSpskkE/G3bWCSfWpDv/Y7JzRcgVrUkW36mOJBEQgEAkHT2D5cxKdv+wMeenmb9neugozGDADjxCLuTJyxKijmOboW64OWdauftw41X5poBJ2gxFdQkgkP2TSRjPgm2dZssrZt88dupKCYpSr+nLXbRtTPqWTzYwzIg5KOsY0QFIFAINhBseqZTfjPR9fjX+97Sfs7JxkjpbK5WV1wBSXuTBzNgxJDQTG/nbsW6/Zc2LRqErLxQCvxxPCg0HaZZGJMKgiRv7bMWNQX5kFxkCrTOMvLM5ygxDG+0nshzmwmISgCgUDQAP0jJbzjH36Lq//7mek+lQnFYM1capZT+O/jUVDG1cUTQ40wF0qX34H/7cEJJCilMZZ4SJVIJz2kawQllgeFFJRMsrZtfLLA98NRL6WWE5Q4ig+9RikhKAKBQDBxeGpDP17dOoI7nto43acyoRgtBgTFJBK8VBF3xgsvD8Qt8YxVQakaRl7Xosv/9tjaifOhjNkkSwpKKqnKHrG6eGrbt43Bv1LWunjq36vgOeHz141ZQQmem/SEoAgEAsGEoVD7wI6b/TDTQeTD/CasKSgxr5krLrFLPGXuX4mx4DZR4uHXOFyIV7aqh7HODyKSkE56yKTGUOKpEEGJ70EpNfDNmH+LUlDivL6ioAgEAsEkgBbLuPNhZjqIfJgZGoXxmGTLk9vFU6n6+ONr/Vrnjhkk5io98GuM21Hy1IZ+nHbD/bifZYOExxqb6kPZIilW4hkLyRiLB6VRkqw1ZTnCgxKnxEP7SApBEQgEgolDSFDmloJC5MPM0OAkI26JhxOa0gR38Tz66nYc8qVVOOX63+GSH69RfzdLDW4FhRGUmK3Tf/Mfa/CHdX04+7sPWo+NtcQTKgqhSTaOalQwSjxxclC4MdaVJGsNMaz9XqpUsaEvr/5eGYOCIgRFIBAIJhC0cBTKVevb+mxGWOKpo6DE7OLh5lYzgbQezMwV12L/X0+8jr6RIIdj/fZRbVsO10LPF/C4pafBfHT6rN7FE6PzqBKWPKjEE4fglMwST5xtYyoodKxtw0XtXsdSUGr3PCltxgKBQDBxGGvWxUwHERSzxKO1Gcct8WgKytiTUV1qFVdY+EJZMUyyrhIOP5e4CkpnLh352FjfG2rBTnjIpQOSMZY25TGVeLRZPK42Y3eJxzxGHJMsV4yahRAUgUAgaAB93srsKfPc8/wbeHJ9f+TjpI5YJllGBMbnQWl+ATPva7nq1/XG8MWSFr96eSJcNXE97vt+ZKx/V0s0QWlkOI1CWQWXJZCrBbXFeW+FCsr4clAadTzxfZvCk+uYmwfzOONbv8dtj6zT/l6WEo9AIBBMPDhBidvVMl3YNJDHuTc9hFO/+Tt81whiI4xGlHjGkySreVBilFJci7u5UHLywx+jNmNa6BuXeOzpvOf820M47Yb7napAJyMoJonRu3iav1cVZhrNpgIFJR8nR6Ws56CMpdzC96PtO4KgmKUxl1J1w90v4tFXt+Mz//mE9nfp4hEIBIJJAP/Ani0KytahImgt/fJ/P4PX+kat50R6ULhJNm6bcWlsJlm6r7TgBn+LVlB4mYZ+plKJ0yRbp8RTKFdx3wtb8MT6fry6ddjatpOl0A7ko+f+xCnR0GKfYlH3Y5mG3K5Msvq2o8UKbrr/ZazdOmJt2zAHxfKgBM8382ZcZC5KRaLjSJKsQCAQTCAKWolnZnlQHl+7HUdffRf+89H12t85yfB99/wZIhN2DsrYFZSoMkzD7ZgiQMFlpiJRiGhhpoWz3kwb/nxT2RlgJlhXCSLBwsW2GPdxvCWeVNJDrqagxFFg1P2KKPHc+tBafPFXT+Pt//Bba9tSXA9KlRSUxt1SHYzMcSgFRYLaBAKBYOIw3hLPQy9vcyoY44Xv+/iLf/k9NvTn8cVfPaU9Vi9sixCloIw16r5S9bVv8nFKPHTMbCqpSh6mItHIg0ILvduDEi6uvq9/+x8YDVWRRsmqW4eK+mNjLPGEXTwJpfzEU1DIJOvu4nlx81Dkeek5KI0JCv1uKk8uBaWDGYpHiuF9FQ+KQCAQTALG2koKAE+s78OZ316Nt3717ok+LW2mzPIFbdpjNkGxF5N8Ex6UOCUes/wVp8RD55tNM9NoXQXF7uJpqePHiGqdBfQ2YhdJKGkERVdQxpqDwhfssZhkVRcPKSgGWdh1fqv6+bFX+/RjN1BQzPcDnatZ4nHdZ2qZBnQypxSUpHTxCAQCwYShWOFD7OIRlAde2jrRp6PwE9Yp0dOW0R6rtyADgfpC5KNqKAqFBl0867eP4Jt3v4C+EV1NMNWlOCWeZhQUTh64YZMMp/UUlKhFFwiHJgJuFYQvxFuGi8ZjY/OgUMhZOhmaZMeUg8LajLmBl7/+ZgKu3sXjIHO1fefSeleUWeIxu6zM425l96o8FSbZe++9F6eeeiqWLl0Kz/Nw++23a4//7Gc/w/HHH4/58+fD8zysWbPG2kc+n8dFF12E+fPno729HWeccQY2bdoU91QEAoFgSqCVeIrxPChxuiviYmA0/OZvyu2WrG+cR7FSNUK33AutK6jtw//2EL7+v8/jUqNTw24Vju9ByaYSzDRaX0GhBblieVCaICjsd+5BcZEMvu2WQV1BKfD7FoO8lipjV1CqVV8t+K21oDazbMXP5T6LoHAFxT6mnbHiq+NyuLp4NILC1CYiZJNqkh0eHsYBBxyAG264IfLxY445Bl/72tci9/GpT30Kv/rVr3DbbbfhnnvuwYYNG/Ce97wn7qkIBALBlGA8OShx59HEAf/GXS8NFnB3eXDw7RtF3f/pjaDT5X+f1r9YmvcmzjRjIgz1FAXzd1V6qP3bUqeLxyRonDjqCkojVWBiSjw8uCxuUBt/LYlEAPo18X29uGlQ277hLJ4KGXD1+2kpKA4Cqvl1uIKiPDfWJpFw223r4KSTTsJJJ50U+fiHPvQhAMArr7zifLy/vx/f+973cOutt+Jd73oXAOCmm27CPvvsgwceeABHHnlk3FMSCASChugbKeID//og/vzApfjYO/aItW0xYvFuBpNJUPjiaCsopmKgP24SD/74WE2ypn8jzrWTCpJMeMjUunjqmXfp8XQyoRbOsM04UFc81jFiEjS+uOoelPqzaUyTbKMunrVbR3DVr5/GR9+xOw7brYcdP+ziyabcnpsoaAQlm9T+3gKb3NUzTDuD2gwFhcijqaC4TLK6gmJ7UJIzOUn20UcfRalUwsqVK9XfVqxYgV122QWrV692blMoFDAwMKD9JxAIBHHw+Lo+PP36AH722PrGTzagl3jiERQ9r2NiyYrWOmtle9T3g9Tzi/AFLU7X0ng8KHyYXDjdtz7posdViYd9PTe3tUs8zSsofF91u3gc9+rzv/wjfvPMJrz3Rn19o/fCWKLuecdOq6aguMtNZiovv/ZK1Y+cZWQOIrTbjOvnoPASz5R4UMaLjRs3IpPJoLu7W/v74sWLsXHjRuc2V199Nbq6utR/y5Ytm4IzFQgEcwn04R93tgxQ/9toIzSKFR8P+P4aKSiNSjxRYXRx7petcMSf1ZLwOEGpX+IpGd/saaE3t61UfZhf9vnj3MvjNsmOvcSzzTDVElTUfSK6aykKpUq42AeEzlac6r3+Zvt31PRiCs0jcmPNPIphkp2z04wvu+wy9Pf3q//WrVvXeCOBQCBgoIUnrgICTJwHxdXSCQDrto3g/O8/jIdY23Dc82rkQYlX4qmvGEVlbdXztTQCj0JPp2yCUjJMvfyc6WktLIU2Ki2V2mCju3jq+ypMotdoWGBvV079zLts6PySzHPT7HuLXncicorQMc9P3Q4oh2Gao2SUeOj+VYztnCZZbih2KCgzmqD09vaiWCyir69P+/umTZvQ29vr3CabzaKzs1P7TyAQzF5MdKmjGdDiMRYFhS9wsYfnsUUnSn358E0P4a5nN+OCWx6Jte/6HpT6ZIGHaJmPax6UUsWaP8ONmfy4+SbC4aKglXgS9RUBIkhq4awpAplkQj1WNMgNIZxdwxQUTlAadPGY2S7cCOxSXxZ3hgRli+bJCPaZ0rp4mjXJBsch5YQIik6WojugzP//rGA2amHO6l08dpuxy4MSHmer43pnNEE55JBDkE6ncdddd6m/Pffcc1i7di2OOuqoqT4dgUAwxfjTG0M48KpV+Mb/Pjelx6UP4dFSxTL7Ndx2HCbZ4WL9xQ8AXqp1xfSzUkMz4CUUc/FolIMSVY7xfV8jA2Y6LBDmYwDA9hHeqVFftakHKh/wEk9R60oJz7c9o8+fUdsm3OUhfp+o00f3oDQwyZb5fa5TOqnYfg7O7ficn9CTkWBdS829t+j1ydS2c12z+ZppKbx1IuurVV/drzaji8cMamvYZjw8Pg9K7C6eoaEhvPjii+r3l19+GWvWrEFPTw922WUXbNu2DWvXrsWGDRsABOQDCJST3t5edHV14fzzz8cll1yCnp4edHZ24pOf/CSOOuoo6eARCHYAfO1/nsVQoYzr734R/9/xb5qy45rGT/p22Az0Ek889WeowCfw2gvQcCEkMCt6O2Ltm1+TuXA2SpI1lSRahFwqz2ixohZR8zlbh4pY0J4NzqFB6aAeiDSmkp7q9OBm0LxalBNBmaZgl3iSXqC+FKEvutyQGpZ4IhSUBsmq5n00c0QK5YpmXOWk45WtIzi01smjzeJhUfdm95EL9H4kU7Cr68kaE1C274e5P0B/zVqNEk/sNuOhorqecHrzJHbxPPLIIzjooINw0EEHAQAuueQSHHTQQbjyyisBAL/85S9x0EEH4eSTTwYAnHXWWTjooINw4403qn380z/9E0455RScccYZePvb347e3l787Gc/i3sqAoFgFmIy227rgX8Ixy3zjGcWDycgLnLz+No+9fPCjmzM8+KyfdwSTwRBYedIX3bNa+Yqg6tTI+PwkDRCWTPJerW/8QWXkmYTlmLAyY3Lv0KLZjrpqW/wpQgFxaViRJWLzP0E5xm9+GsKCgtqyzJFqqC9T8v4t9+9jPXb9YnEJkFxXbOdIeMeE2Bux39uy+hqkxXU1mCkQJkFypWmQkE59thjrXokx7nnnotzzz237j5yuRxuuOGGyLA3gUAwd+GShacCjeLb62E8s3g4QXEpCg++HEbhW1NkK9W6s0v4/sz7aueg6L/bhtZgeyphJRMe2jJJDOTLGpkpVaqR0e9EKFozSRTL1XglHuZByTjajIncZVNJpAzFQCc3tfIQK8vQftKJkNzEazOuYzBtECbHX6NXtoZEQ0XdJ8JpxrQ9KSq//sPruOrXT+OqXz+NP1x5PLpa09oxiAi6rtn2oESTLP4Yv57WrF5Ka0pBcQwaTCcTs8ODIhAIdmzEWbQmEpqC4ohvr7utZiCNW+Kp70F55vUw14kf55FXtmG/L/wvbv79K5H71jwoEZ4DWsTMmStRCkqeKRXUFcPJjKmmaApKpXGiaxSqLKgtXHBt02cunbAITLgt1GN88aTzSKcSIbmpPe77fhNBbXqreNTMG36e6nf2mnMFpaQIWQLppKfUKk6AuXJy+S/+aB0zaxCUugpKnS4eVyt8OsnKYTEUFIuglXVyIwRFIBDMWLjSJ6cChXGUeMYaXAY0VlD4/vhid9nPnsRoqYLP//Ip537NgC1bQQn225HV00Bdxw221z0ouXRSEQ3+3Lxx7/RW0mBb2i6OB0UtYJ5nKST8vLKMZKh8DmOxN7ctaSUefdEdLla0jBRzgXUFmUXNMHJtz+/B5gE2m4ayTJIePE/3oRD4Ue974Q1rn5kmPChUUtGD2+p4UEidSSas/YZKFbTfo66X/x5e7wxOkhUIBDs2zJCoqcJ40mDHk4MyzEyyrvIQ/2bLj8MzPRqdE+DyoASPt+d0oyNh1GgzpvIAXV8ulWCLZnRwG28lNSPn46hlVWeJx77v2VTSaqtVBMVzlztUiScZEhgiN1w9AWyVyznXRxvKF61UmL+7wtKIQLji7vm9zjuILBmXU04FJXh+Z0u6tj0nbI09KOlUQpE5UnvCxN7a69tkiSd4bo2gNDAAcwhBEQgEU4rpKvHwb5Bc1WgE3/eNEk/zBKVYrtYN+QrOy+0N6MjVtwiaC0FUkqwK2zIejy7xhApK1vGt3lRetrhKPI6skUagpyYiou5DZSehFuSwi8cuD7kVFHvR5f6T4DiNw+bqvaZWiUdTNcLHzGRVV9y9TlDC0hIdI/SgeLVzse9XZ+191KyCUmAKijLflkkFqRGjtO3jce2L72/OJskKBIK5g+kyyY61E6dc9bUsizgeFJMIuTwofOHg59iRTdfdt5X+GbHoKAXFWDhGIko8RMAyqQRy9K3eCG7j4AoRqRIUhhanxMODy+p5KrKppFV60HNQaos1J34qedWzykOmgmK+vq5F2JVSG5ZSGi/+AIu6r12rUqvKbtWEb8/LMHwfvIxHzyEFRSfC0e3gat+phArMo/cGnXPWkcbrusbgWOJBEQgEswTTkSILjN2DYn4jjKOgDBkEpZGCUoxQUFydk+a+fF83MRLxac+6Szyml4TCyLgHxVXiqTfDZzwlHi2oLWV7KlSJJ22rIKrNOBHGxuvlFF7i0b/9m++FKAUlmQiNrK7xBe0OpcL8nQ/t49ksACvxaOU0N8FVZC1NHhSd0JUrVfVadOZqBKVkE0nzGvjPGdbOXTTMyOHr23yJR3Xx1K9cahCCIhAIphTT12Y89QRl2Fpg6ntQ+Dl25EIFZdBRknLN9eEqivKgZJss8RgKSi7Nh9hF+3fMBRgYWxePCluLaDNWxCmVtEoPvM2YFm0t94PNrlE5KKQKNJiYzLtanImttecToayXg8L3x2cPAVDltIJWTnOXjwoRCgoRCU7OuhwKij2V2FZXMknbjNyMgkLHpveO2aI8o6cZCwSCHRu8/l0vU2mioZtkm/egWFOAHbNpomCWeNwKirvEQ6UKAOgbtiPw6bktbIIv96HQQqfmqUSUeGghoce5GTX0RVSs7VznbLYZl6t+02MFeImHFjFttgxTUMzSA29RDgfvuTwonqWgNJ6YHK2+VFkQGZXkzFEIliejZCzYVOJxmGTN9yldE1c5AFiEjZMcRZwcKgkRDT1JNizxmcpMVREUt0LGO57aa/cjVFDCTqtmIQRFIBBMKfiH2lS2HBciuiMawVxgqr5dw48Cj7kHojwoeomHyA//dto3WrS2o0WjlXX76IoDlXhCssBBix+VAEpqwa59g04lVIBY3tFmTAufnhxaI038nJrs2tJMssaCG1xPuKiaigE3YCoFhXe9cJNsROssEcKoEk8mabcwcwI1vz0DABgYNU23bgJE/x+kDJNsPUMykRc6blbN4tHPi46RSnjqtXApXa7BidzfYpqRGykomofKeH+IgiIQCGY8SprBcHoUlDgEpeBQKpodGNicgsIVpXCx5eRt+4itoKjzykQoKKrEQwTEUFCKehuqKnlQwmkyXNxci2Z3q/4NOdhW7+IBmvehKBXEizDJKsWHJck6OkQUqXIoO7yLR0WwV/RSmGmSpetLswW75Cil0NRiPjyRb6+uo/beoftM1+L2oLhHDJghfKbSQcfIphJORSk0M9vqSoFdb9gdZCgo1MVTpxvIVG7CWTxCUAQCwQxFhX2oxenyGC/MGSfNgj5027IpUIRDsz4U0yTrip83VaSwZh8+t2/EVlC4TK+CsxxdL+ZEWgJdA3kUyCTLF2RaiFyLZndLRjtffnxO5pr1oZRZFHqYBhvemzy7XjMtNsxB4QqKq8Rj56AQ2VAmV2vKc23blGeVlviCTHOU+gwyabch11dQtFBAi6CQcmO2GeuKUmiiTSri45rFEyoo9pcGbpI11aZcRImnwIZhmp1cPIivWQhBEQgEUwr9w3DqCMpYFZRQTg8XRpdB1QVLQYmQ+13P4bdm+7CDoKg00aSlCnDi06GC2twmWUVQLDMjK/FwX4ShoGgR6qwEQOuQi4RuHy7iQ997EL9Y85r6GzfJOqPumTcmLNPoAWIJTUHR5wcBepsxbUtEpc2hJvDnpXkuiCI34X57WgPCxhUU/jq0GCbYsuHJyDnIIN1ret/RY42i7ukY2VTCSdjUzKSsXaZzdfGUjfvcSEEJJk4ntb/JLB6BQDDjwT+Ap5Kg1PtmWg/ah66ju6QeGrWw1ico4WOuEk+JnVfYbWGXHlwlHt/31eJHQV5U4gn3645fV94V6g7RFJTQ+Jk2IuU5vvzfz+C+F7bgb/5jjfobT5J1xdXzoLbIacYJt4JSdJZ4aterTJ0hQeEmaO5BMSchc78GETb+Wrnaxuk9oLp4kmZQm0OtaqU0WL2LJ5xmrJe8SGHhJR6ti6d2/m1OD0qozpglHirTRJlkFXFK2gbbsnG9zUAIikAgmDIUy1VNtp/KVNnxthlnkolw8F6TCoqV7hrRNppOekyK1z/QgYgSTyVcDJJG6YEfJyzxsHJJqarC5zqNEk+JKyh1vtWT8lJkCzpvnXWRDMKT6/utv5U1guKaZmxH3VszYiIUlLJSOniJx1BQsmHujCubhnfxKJMsI4nzagoKf63qddOYAW/m61+p+mr/tG8qc5ldPJYHhU1+dpd4dA+KlhnD2oxNBcVlktUGJ1bC8zI7hMrKgyJdPAKBYAaiXsjXZEPr4okTV1/h3yht82Y9WBOGzd/ZQpIxygeNTLLKK5JirbNV3YMQEB/7WzLvDqEuHqUosAXZHdQWPN7dEua0hB1AtUU36VnlEA4+pZfQ0CTLwslMkuHyoLiUuqDEo5MfZZLNuQkKT6E1F2xOXua1kYISEhR6POGFBMhUUMyoezpv/hrRvhuVeGwPSsLpbTG7eIoOQpZxELKqrxMUfh18P5r6YhiZpYtHIBDMSJhtk9PmQYkxi0evq+vfchshTFa1yw58P1n+jdNQBQC7M0Q7L66gqAUq9C6YplAgNAnz4/IUUiBYkEMFhZObYFsqO/BzpkUonUg4VRDCsEPBqjAVxCwtBOdQ6+JJJRXJKBoLZyrhqTwRXQVhPhJDbVKG0XRS+WZchlJXizJ/b3STgsIya/hQP6VklIx7ZUbd1x4nMu95LGzNKPGYJlmziyeTTFjHBViJJ2un7vJ9q9JhTSlRCgozQZcjCErGej/XyJoQFIFAMBNhds+UyvbiNRnwfb/pEo8ZwlZkkvdYSzxtDikd0NtBTQNuhS3s/aMOBYXJ6eaiy7s4XESBFr/WTJjKStN/+YLsykEZNcy1/JxLvEyTcJd4+DduvlZVGMmgc+IlQF1BMcybvMTj8HJowwKN+6FakFPuxbzE77PZZuwo8QwWyjZRcHhBSoZp1JxmTPe5JZ1UBlsrqC1ZPwclm05Y+/V9Xx27JW2H+LlMsnTNVaPEA0QQlKRN5kNCJgRFIBDMQJjEYKpKPK40WBfuef4NHPDF/8V/P/l6uK3jA7tZ5Ud9O8/aiyagEwmT/DStoKSSSLJvusFxQmUm5VAjRtjil7IUhbBM4xpgR9u2ZVNsQB6VLRqXeF7bPqp+XtCeVT8TIUtoUfeOEg8jZGY5TAtq00yhYTnMjm+vPcZSaLV0X+YVUWqUob5kkgl0taSVAkOtxlyNML0gdL1kJrZmBNWUqhbHTCQqFVolHiNJNptKWhH6FTb80qWguFqy6ZrNWUv8PvLzCgzlevnI7FpqBkJQBALBlMHMD5mqEo+peETloHz43x7CQL6Mv/rRY9a2mmzdtIISPC9SQWFEwtw37+LZOhRtkk0nPatjxqXMlF0EJWN7X/gClXWUeOg1bHGQKlqwU4nwuGZZ609bhtTPnIRVGnpQmEk2pSsZfNCgS/UpMTKQtoLaws6jMCwtasHWr5f7k5IJT/l5yCjLvSJ0L11qU3B8nXSNstfIJIpEdOn+p0yCqgW16cSI3/OWekmypoJSDhUUnbg0KPGYHhTJQREIBDMR1oC6KSIopmckX6o2PSOGt11m6nSmuGCaEaO6eHh8e0F9sw/Pb6RYsUgVX/zMLh7ufajXEdOaSYWTgc2SB1dQHF08LoJSZuoLLX6mWvXyG8PqZ15a4K3CKcNgGTyX+UisNmOobV0KSr2oexchc5pkU3YKLS9pAMA8o9VYV1D0Eo/ZZmwSJ17iMYkiL+9p25olHo2g6NcLMOLsKPFkUwnN0FpiCkoyET7Gy3CuEo/ZaSU5KAKBYEZiugiKS/Go18kzvy0TbsvaeU3jXyOEZsQoD0pIJNQHurGAEbYMuiPU3SbZ0INgLsiArqBEzZfRPShcQQnLUq7WWKA2A6ZGbszXfO22sINHy1BhPhJX3kyJl4+MUgtPoXUpKHqJx2id5aZgR4lHb7vVy0M8Fh6AMspuNxQUzaxqkrkoBUWRyKR1TWYXj62gsBKPQYw4oQjTXu3cl3TSg+fp7eLcJ+R6XxU1QuZWUJIxWIcQFIFAMGUwSzzFKTLJqth3NiPG9INwcyzFlgNuybvZEo+loFhBbTUFJc1Msg4FBQDeGCpov7vMmxWHxO/yc5Aaw1WQsIsnLHnkHIP36HlaWUqpEeG2dM1mazlP1+UZKuFEYrsrhf+sl1rIJBs8J+G5FRSNZJieG5b74g55Y6U0o52Xx8IDoYJCJR5n5HxJL7WQKmOSTCJ2Oc2DoitkYYnH7OIJCar5GhLR87zQS1LkhMw4L+6N4fN06HGtzZi/N6z3c0gim4UQFIFAMGWYbgUllw4VA1PN4J0yGkFhXS3mh24jlE0PikFs6ntQDIIyqBMU/u09aZhG9VbRYL9VP9wn/3ZulnjC5FQ2LNAR8pVKepbPhCsoFAJmlnhM5YoW6gorH6SZIqCmO7PSU8q4XtVmzHJfOBnmRle7i6dad1u9A0hXUEyiEIa1lbTHs8lEaFY1TaNU4jFIpqagUImHPChmDkpEm3ngfXErKOkEL8OE7zWuKNE9o3uofEJMQeFx93VzUCo68WkGQlAEAsGUYbw5KL7v45FXtjln09SDa7qr6QdZz7pLeO2dL2BxTbJKQcnqi0R4XlyKd3fxEPnYYigoWptxbTFQCgrr4uCGRpcB0yzxaEmyKuTNV/vmwwTVvBW1bUgEiNyYpNQK6zOv1ws7gOjY/N8UW1jNYYEJL8xuifKR1OvEMbNo+H3hBDXMQdFn5YQlnhpBqYQKWVQ5jMiFIl1V/TVqzaRY67ROjOi9HHqQ9Ndf7x4K1KpQIfOs1nbzemkfQE1BYe9J04/D95Nh7w2LkImCIhAIZiJGzRyUmARlzbo+/L8bV+PSnz4Ra7t6AVKE1/pCgqJ9o2SBWq5yST3Qt8ZIBcVhkg1Dz4J/eztzABwEhV2TUlCMEg9PA+XnPeLIQVEEhS3mvJ00b5Qm+Ddwq1MjGXpQzNfcJCjmPJ1kIlzw+eOqHTgZLo4lVeJhbcYuDwp7DaMUI05AneWhFPfzuEs83WaJpxQu2Lybxvf1xZ7ODYgq8eidOJZJNqkTNhfxpb/zziOzvMeviZSm8H5VNRLpNMlWov8/M03BzUAIikAgmDLYJZ54HpQNfXkAepZGM3B9YNdTUFzfoFOJsSgowfNaIyblKqUjbS/29MG/uDMoN5klHj7ELiwP1L5hs4WTExTaJ+/EMVuU9VyQcFta8NU37JRnqz5M5WiNUFDMEo8iN6p8YHSPOLwxSvUxFJSk4UGh8pA28M/MQWH7ra8ocNWA7nO4X7qfdGztcUO546oD7bNZkyyfkEzHNYdFuhRDOq9C2X7f8OvlRBAIBxEWK2HnWyrpNskWXF8EjPZmSZIVCAQzEuMt8dC3v7xhNiWsWdeHG+/5kzUDxx1Xr++Dkx7+ga0PwNNNko1AH8oUiFWp+nqwFe/isRSUYNverhgKiiPhNJnwVGJrqKDUTLKZlOXJ4Z6bBCNl+dqCz0stYUmEFqHQCBnVZpyPCOvjXR7JhKdCz8LzChdO00SryA1rjfZ9tm05VEm4v4WfczqCgGolHsOzYUfOG/fSkYNSKFc11SGlvB5mmzG9RrpJlpNn2idXOfhxqfzHI/wbqYm8nZv/WypXw7h6pqA4Z/Ekk1Ynltm11AyEoAgEOyhKlSp+9YcN2DyYj73txv48zvjW7/GLNa/F2i5vqBaxCYqRlGniA//6AL76P8/iH/73Oe3vruAqUwXhA+yiJO/YCkpt2xbWPaTPPWHdNhHlkt7OFgDAlqHoNmOzoyKyy0MtfqTsJK1Yee65AaBm24wWK9o3/4xDcSgzchPVxUMJqeZ1cB+J59kTjfWJxPo5V7mCYpQ0gn2E29olHlbCc5V42IJtmnNNv0Z4L/VSi5kky42lYYlHV3a4ysUHIPL3XiapE6MwfC48rsfuSaFU1d43ppEV0H1E/BhB1D3UY6bZmO/HOc2YvUbNQgiKQLCDYtXTm/DJf38c19zxXOMnG7j3hTfw6Kvbcdsj62NtZyobcUs89GFntisTqJzw7Xte0v6uzyZxG1a3MuMtJyh8wY49zdjo4uHXoJ2Xg6CUlYISlHjqmWRDP4C+cNLikmHfgoFw4F9rJizxmIsubcPD2sxv/uaCzo2QUTkotgfF9pHw45drpQXiRlqQm9GSnaj5V2gNNH0zrhIP96DQe8OloLim+5pE0CRVUUFtXHWgbcwFn/uEeJIs7TPhhduYLcqcdAHQJhrrZSf7/Vw2tuU+Fa6QuRSUZgi3dPEIBIKGIE+DufA1A/rgjyIKUTCzPZpVIgiFBgQlw749c9Mrr7tHTSR2qSaAXuKJUlB+8sg6nHzdfdjQp3tjaNtcOqHKLPy4BRZ6FvWBvrhmkjU9KHxxNIO6bBOl3rnCZ/GQx8Dl9QjOPcxw4eoPVxzMJNl00kNLjZSZnpOoLh7bNBqeV4kpDilDQeGJwKmErRjwY2jqC5V42IJslqz4vUzz8pCjlGaeM3/c9D7x9xe9L8yZSGqkQCbcNs8UEO4tMU2yZsIt/csVmKhcH0sVYqVHOm3eZlyKbDMOt+OTkKWLRyAQNEQ4PyUeSQAYQYnwgkTBPFZ8D0rtwzuC2PAE2P96YoP62WmSLbsXSvNnLcgrgqBc+p9P4KkNA/i7nz+p/Z0WomQiodJkB/NhicM5zdjoWlnSFZR4zLh7Lb494V44VZaFEWzGk2TNkgdfkAGEGRylqqaApZOMCDgUlLDEE56z7/uKsJDCojwobBYPXRedM1/QeWmCp5vybTmpoucF98HOUOEljboelJQ9CZlnxujnTCUeNs2YdeJw0ut5epIsPZZnGTlcxTJ9L3zbsvUa6ipIsaKXeFw5KPxeBfsIy0AVTUHRy2z8uKaCwr+XCEERCAQNYRr54oA+PE1PSSPQh1mrY0hZM+Dftl3b8muhjh/+93oD/1ydO8E529+go877jxsGtN/5QkThb9zzo5V4TONnbdvOlpQiCTzuXldQdFUg7C5J1v7V9z3KygdRnThmeSBfqjCDbLCwRpWl0smE0yQbdNYEP3e1pLVtq5aCwkoL7H7rpTZfKzFQ9cAc+sdLHqZ/hV+vu4vH3rZxicdUUMLpvgVmNuUttxZRZKSa56Bw0mNvq3/poPcrH6Og5+foKgfAX0P7daCXQS/xuBUUFeLHrte85kYQgiIQ7KAIWyHj+UCA8Zd4QoIyNg9K1LG18km5Yv3sGp5GcMWq83PkbaiFCIIS1QqcSnpY1GG3C7tm8ZhqRCqRwIL22rasHMdTSk0Pim2S1csHoQEzZeVzmBI/n8djhXglQxWEZ3ukEh5aHR4UXt4hghI1TE4r8XDvS8KYD8MICm1rKihlRdgS1r3QhgU6c1DCbc1SWYE9xv91elAcXTzcj2GaZPlrmGMKCFdWzG3pXnDVh59XsVLVzomTHKUKlfXXmPtUNAWldsx/f2gd1tXmK7nUmSJTjMxrbgQhKALBDgo1Mn4qFZTaB1xLRpf3m4VOUOxtNVLC56loH8q2ERIIuzUA04MSfiin2TfRKHBPBPdVLOoIvCSbBzhBYbN4TA8KUysWOsgN79QwPSjmN2hzdg1PkiUvQ9UPtjclft5BYj6mSJUj26PF0cVD5Z1MMoFcRn8dohSUIjNnknITqiBGiYcIijHksKhIpufo4gmvyaWu8ZEC5n1UpbQGCooW1FaqOP0YZqcVJz88MG8gH6TUcnJhBvXRezljkIxi2SjxsIycMA1YJ0+8bKWl/db+fs/zb+B9316t7UM34Prae0NKPAKBoCFMOTgO1EyQuAoKlXjSgR+j3kLvAic0poJSrepehXxZLy0AdvR31L75z1zyjprF05ELu3S4ObfCSh6LXCWeOrN4+CJGCsoWh4LCW2dNk2TWWDgtAyZrYaV7wqPf6TlAcD/Nx0Jvg93h4wpq48QoY5TLiGgkDA9KueJbZSfuBeGEkDwoWSN5VR80aAS18deX3hu8FbxEia4Jy8hqdjxF5qCk9anCFaOMAvB5OlXtuJmUQVBqM6M4ueCETTuvlPE6cYJihPjR/4vmLB6e/cOD2jjR2NCfV9dG++a+l4qhgDWL2ATl3nvvxamnnoqlS5fC8zzcfvvt2uO+7+PKK6/EkiVL0NLSgpUrV+KFF17QnrNt2zacffbZ6OzsRHd3N84//3wMDQ3FPRWBQDAOjEtBKY7PJNsyTg8K4DC5GvviCkrJ9cFpKSi64mKmkKYc0e6u83pu42C4T6a+LOokghJR4okIakslIwiKq83YUlDc3SXc+2KmxZr+CB4SZqkr7H6Y2R4tjmGB3PvSKAqdR/AXjeNyIuD6dm4qKHqSrK5U8HJLPQUlmGuk+1csD4rRtqsUMqN8RI+7FJSqH5BtbjhNsrLWQM1knWWkhV5/c1uXSZaTl2QiJBquidR8HzzqPghqs+mDS52psPPxvElOkh0eHsYBBxyAG264wfn4Nddcg+uuuw433ngjHnzwQbS1teGEE05APh9+azj77LPx1FNPYdWqVfj1r3+Ne++9FxdeeGHcUxEIBOPAuLp41DdT3aTYCJYHJca2QP0SjxnexskTz8GI6uIxfSXmlN1UwrPMpvQ4V2Oe2xQSFF6mqVvicSoo4SK2sD3oTuIEhX9bTSpVwPY+AHaJh5eWuNmVG2FDk6xd4jH9CcWyno6aTiaUB6XIFAOVYJtOaqUDuo9AqIJwhcVUUNJscaTtEx5UR4ypoKjtU56lVLiGAbq8Trm0PS8pqp2XSixaycPZxcOMrnyoY7VqkR8iXaSgZJN824S2bVSJr1CuWuqaec31DLZVdt4uJURTZxjxJRUtjnoCAKnGT9Fx0kkn4aSTTnI+5vs+rr32Wlx++eU47bTTAAC33HILFi9ejNtvvx1nnXUWnnnmGdxxxx14+OGHceihhwIArr/+erz73e/G17/+dSxdutTab6FQQKEQ/o85MDAQ97QFAoGBYmUcBIWRgUK5ombNNAItCoqgxFRvOIkwI9QLFeN37kHhi5DjWzI3hxJKlSrSyYT2jZI+X/m25nms3coSadkHOpV4Nrm6eNJ6F48eTJao70FJJtSCHZkky0oTvBRGZYdsKoFiuaqVY2ifGU0l0YkC/2bOlYyEp6fnjhTL6MilQ3NuJloxom/YtHgXK8wXk9DVFSAkEFyNyDIFxffDb/CpRDgWgNQGviBnDXNt8HOooESFvEUN7dOi7tlkaFMR4tcW7N+3XsNsOonBQjks8ThMsnQfrVIcIyHm/KB00sNoibe3G6+xQ61KJjwMs/bxnbpb1HPo3HgJiohpHP8JMMEelJdffhkbN27EypUr1d+6urpwxBFHYPXqwESzevVqdHd3K3ICACtXrkQikcCDDz7o3O/VV1+Nrq4u9d+yZcsm8rQFgh0SYVbDWAhKhf3c/Pb0wUnyf1xyxEmH6UGpp6AQEeJpofy6K1UfviHm0LdgXvJwzeIZMab18g9uXragEs8bXEEphYsfJwKm8TMs8bjbjMNZPLYqQM+hv2uzXOjbeW1h5hktFODG71fY4aGnvfISTzD7JchIoURXKu2MagFxuuGYJ8UGxw8fNxdNviDT+y/BItSVWlGy4/lNpaLRsECuoNTr0gn2oUfHu7xPADBcsBdsrqaUmSoXvkbBv2SS5fvj25YYuXG+/sY5Z1JhubVa1Tux+D6KlSqqfkhQ/rQ5tGXQ/eaEjL9Go0pBiUc5JpSgbNy4EQCwePFi7e+LFy9Wj23cuBGLFi3SHk+lUujp6VHPMXHZZZehv79f/bdu3bqJPG2BYIeEOackDnSC0rwPRU33TddvM35u4yD+5j8ex8tbhrW/6yZZo8RjEC3Ng8K+NbrC1lzdROobJZUHEnrrJMFMRuW/8wFpi2qJsIOFsnqOq8RjRqGnEh4WdOgelHIlDL/i7a9KQYn4Zl+q+Np9oXtB/w4VQoKScigoZocHP2e6T7Toep7daswVlKyhoPBBdMF18TZjo2TBFrpRh4KiPChm6SnladuWK74W1JZN6edE1xbco1BBUV06Vtu1TmBcSbL8fqQdbcbBedmJsUQi+x0Kiq6+hCRUtRmz19AkVRlGqngqrLltqaynwZIxVrsfquSV1EqHdL3TqqBMFrLZLDo7O7X/BALB+GB+qMRBvo6SQfi/5zbjpH++D398rV/9zRyeF9Vm/J5/uR+/WLMBf/Wjx7S/c2JgHte8Dk1BccSZFzSFJVzE6Iu4K5/DnLUC2LNmuILCs0w6smHgGnXyaF0PWlutrqAsJAWlVuLh9y1QUPTzMttMeXoqXXeSDXxTBIUrKIYRtlCuhApKykFQqiGRI5hGWT5fxryXtDZaQW1a63PwtwQzjRZcJR6loOiKUdpUUJi/JcpAzctw5sThqBKPK8gtlQyVriGHguJ5oWG1zIylyoNSu6a+kVLt97CElmATq8tG2Qow2owrIYng++f3AghfRz2oLfx/4c1Lw3XY5bkBQp/MWD0oE0pQent7AQCbNm3S/r5p0yb1WG9vLzZv3qw9Xi6XsW3bNvUcgWA2Im5o2XRD5U/4iGV0BZor8Zx708N45vUBfOLWkGTQN7Rcun4Xz3DtA+2lN/TuvnpBbZbp1aGgaB6UiLbiFuPceNulS32pNwxPkZta2UMZZQdJCXEvjmZbJikow7W4e06oMimuoOjf7K3uEv4NmnkE6PWg0gOPYNcUFLW46eUfLQadEQCz1ZiXeMxgOiprmaWFUpkHm7G23Nrj9RWUivYeS9USULmXqMxKGubryxflXCqJjDG3yDTJ8qnCVVamUQs2KQp0n41UVbq+Qik8rhraWLsmF0EJ9hU8L1+qqHKl5UFxlHi4gdZsFefP42bnVMLDv5x9MN75poW167UJGf+XhlNOq4KyfPly9Pb24q677lJ/GxgYwIMPPoijjjoKAHDUUUehr68Pjz76qHrO3XffjWq1iiOOOGIiT0cgmDLc/ewmvOXzd+LfH1o73afSNLQFOqaKohGUBq3GQwVe8jC6eBqUl3i+CGCUeMySjqmglLiCYpMMl4mWE4XQz8E8Cg5yY5Z4RhwlHuocUVkoAwXNvGmad82W3bZMUou754bgFGsVpRKM6UHg5RTewUOgezJYWzh5PobmQYko8RRZfgr3GRDZG7VKPCntmzlPoU2YBKXC/S28JGIQFKcHRS8PeZ4e0a+VtJIJizTx95SmoFglHJ0IAgEZt30kNSIYoSjQNXEVLmNs2zcS+JBaDIJC7zFu2rbMzGWXByVUsooGmQv2wbqp2Gu06/w2XH7Kvtp9iNr3lCkoQ0NDWLNmDdasWQMgMMauWbMGa9euhed5uPjii/GlL30Jv/zlL/Hkk0/inHPOwdKlS3H66acDAPbZZx+ceOKJuOCCC/DQQw/h/vvvxyc+8QmcddZZzg4egWA2YM3aPpSrPh55Zft0n0rTaOTBqAdODhopR23Z8IPUajMu11du2rMGQeHSu6WgBI911kiNHlcelnhcJKPEvgmbLbn8WyP/Vk+gxYQ+e+nDmHfiEIGgePehQkkr46T5IEJDSqdFVXXyDBW0hcDz2FwUY1icShJlC7YrKp1ICJV4+Dd7jYSYJZ6kfc58EWpRCkpZuzdcQbGGyXn2whi+fvZ50fXwb+fheVWsFmV+vcMFvpjbSbL8vZ1NJSwPitmSzVWpUsW35uZQZD2ZkU3TKO2fG69Nk2zfaEn7PdxWJwP8uK42Y7sF3TY682sqVnibse4FKld9jXCbQX5EUJPJSSYojzzyCA466CAcdNBBAIBLLrkEBx10EK688koAwKWXXopPfvKTuPDCC3HYYYdhaGgId9xxB3K5nNrHj370I6xYsQLHHXcc3v3ud+OYY47Bd77znbinIhDMGNCCbXZ0NIN120ZwwS2P4JFXtk30adWFRlDGoaCY3TMmeAsyKQrNBrW1mwpKHWJEj3XWSEAwmE5fsAMPSjh4jcC/ZZtZJ7yLx5U0Sh++82s+EfoGyztxaPEIFzi9rTmd8rTzckWhq3k8gyFByar90kLh/iabZSUPbvokEIEZKtgppdyDohawhC7/F8p2DD4QElG6J/SatRpJsto8HUeCqelB4efobDN2eFDSjmviSkUqYXfxFAwiaL43ohZ7ICCxpsJC8f50n+0SD70OwTUlvPCas4bh2FRQiDRwRS9lvE5FZqDNGiSCe1C0GUGONnPlE2IlL9MXBYSvERHBuF08sXNQjj32WPU/vQue5+Gqq67CVVddFfmcnp4e3HrrrXEPLRDMWBQME2Ac/NeTr2PV05vQkUvh0N16JvrUIhE1vbcRfN+P1cXTxvIwzBKPS7nhny/1FBS7iyc4j66WNNZvH639rYpcOqn5SKhLxJV1YUZ0A9BKF/xbve/78DxPveYL2rN4Y7CgSGrZ8JEExw8XA64emcqOS42Y3xYQlG3DxTDnorYNN1fS/gG3gsI7hwjkbxhyeCOy2uKmKwbcs+E651ZjHo8KasskQ/JodC2FCoq9cGrR8LWf6f3H24z5t37TMMrvx3BBNwWrHJSKrqCQ8qGIYMVQDNjrkPACXxcnA2bYGilVpieD7h15VDKO14jAc2aCcyP1JZx3ZPqIOGkicuHKSNHvs172Alyvka/9v0nvi7ba/7/UGj2tJlmBYEcFLZZjUVDoA2WqTbZjVVBKFV+T5F05Kvw+tDGSQSbZlnR0DsoA6yRpz6b1c9bajN2m2M5cuE3YSk1JovpU2fCawm/ZioSUHSbZWueD74dkgK51QS3tNV8zOJo+EgDaN3De0skNmkEyrcP4qbImKo5v7hQg5g5qy7kUFO5BSeulB5faUChVw7ZpR3eIGZEenLP+rZ9/++f5Gq6Bf7SAlhnJ4N/ArfJBwkGqyiERTDvUlyHDFMwXa98PW7KJuPCIfd7J5fTGFEOzKr1vWjIGETQJSm3/5FFxqVwEyySboBIPvYZuwsZbgQEjbM9ZDqtdD/v/Talc7PXQSku17clDtr3mm5mTbcYCwUwHGUV5TbtZkPrSqFQy0RirgmImp7qI1VYWKMY/lGyTrK3GbhsuWn8jcFJhmnPpetqySeUHoXurCEgioXWeEHjyphlnzmVtfTx98Lgq8bRl1GOjJX14Xjjkzi7x0Dddvm+XqVCX6SvafpPKCxD4BPgAPMBQUFg4HEGVPBwmWeccF4f5sq6C4irxOLYNrsUkc/x6bCJA7wPXvSqwbA+XB8VUjPhrUKxU1b6JHIZJsbqhlKtRaYP8AOH9z6X1xziZ49uOFhsrKCZBodea3o/csMsJW6HiJre8SyvlUlA4QfF0EgnoHWBkdCYFtH/EXdJqBCEoAsEEIF/SJeyxbDuWRNfxoJFJdt22ETz0su2LMc2pLoLCSQapAb7v2yZZx3H5tubj9WfxUHeKPjkWgCZdm50afL98wiuRFr448m+ltM1I7bjdrRmVoTJSLFvR77QPui76Zq8WR7ZYEdFNcsWAh62ZCkoiVFBcXoAcux/OEk9aX7C1cgjzxoQqielBCYkAX4TCQYOGgqLloNQv8USVaRRBoSRZR9S9Zux1lHhGCnpgGr8nxbJN5pQZuepr/x/oxKmmgjCCYrYKRyooKielom0H2KZYy4NimGSdJNPZxRP+v+hSqsxuKSAkkfx59J7l95AUFDL2JqczSVYg2FFBC8ZYPCj0AWvmeEw2GpV4Trvhfpz57dWWedckBma7L6CTDHo+X7DrmWS3D9tx7gC0mn+wX3cXTzaZ0Abc8eOkU/pUWYJrYJzZ/pqqBZuZ83joG2tbNqm11VaYiuE5fBWmeZMHeVFuRKQqYMxacQV88W2y7H6Yra/8Z1d3iUtBoXOm6y2zBZtva+aKqDbjdLSCombxsFKbq3wUZmzYbcau9mdX2UIRMsPUSdvmDTLnGoDHJwLz4ww5FAVlko3yoCiSUdaOC9iKSUtGX76VSbZkkxtNfSvrj3MFRQ3V1BQhu305DNNjCopD9aESLbVGiwdFIJgGmN8QY21bniYFRSvxRJdabn1wrfZ3s7TiLPFoBCV4nJc8WtUsnvolHk5gysa8nKgunmw6YSkoZbagu+LquZmRdyZwUpUykjVpG1pMWjMpdV0jxbCrRV+8QqXDle0RllpsX4VGbiK6R8rVqtYCHSoDXEFxlXgMBcVBXgqOacbcqDmQr0+qABbUxoYFBp4muzzEiaKZJ8Mfzzs8KPRYwUGqgvOyg+kAPaG2WAkVFCIH3HOhFuSIMg3dS5cZ2eX1CX4n9UVvTw7OQX+uWfIxW5SjjK5W2ivr4jGHMvLnjRZtkyxvcR92GHupC48+D9JS4hEIph6kEgwXy3W73NzbTr1JtsKGggH1TbKPrtWzXczzdCXJbhsusMdrKgYzhVJtulL1rf1tG3ETFCvKPmIWTzaVjFZQkm4FJQw2YwMBm0jWBHTjJ09ODZUXWy4vshJPRiun0GJQ0Y5pHtdMME06SjzpJPvmzhUUVQqzFz/lQYnwvpjdNBmmKA26MlSSOlGkf3OGgsIDwMx7VapGeVBqXTwOQ3HGSars+zzk8twoz4VdDtMUAweJ4L8PORZsUj3INGp14hiLfT0FJWdtW6fEk7TfO1mD3EYZnc12bs9zv06u6+2o/X9O6bfdLaFPqxkIQREIJgD0wev78ab78m2nUkExF/t6JtlXt45oj1slngYKiqliAEB3a1r5NfgEXcAo8VSiSVRU1H0wOdbwoJRDfwRfcIlMuks8vt5pY3hF6MOcyERrhhOUspafQnCVeFxli2HHeHp9W31xpGNUqvYkWyBKQbEfdy3Y3INilqU8z1Oq0aBqJWXbGlNuC6z8oE3JJSOy57jeCPOm2cWje1B4WcpedMMclDpE0KGg8NcjVCqiVBAiGSGRINWDvhuYbfR0jq5yidVmHGGSpf8v+LZ1pxmzvBlSqjKO9+yoKuFFdB45fDNmjtG8Nr0rrxGEoAgEEwBumItrlFUm2Sns4jFNsSY5MlWgJ9b3q5/NLh6Xd2bbkMODUjum5wUfevTtiqazErZqHhS7DKP2a3bxsIXXVlDCEg8tGL7vmjqbCOfWVNzD06IUlNZsiiWnVrQZL2ofrMTj/mavt+WmXIsua51VBIUlepoD7ABDQakT1GaaYPl+CjzIy9FK7FZQ9HKYCglLG7OHHMF0epKsTbrsqHv1UMT8IJt01e1achiKPS9sB6frzaZMgkKKgk0UTMWkLWMQFMpBKdrbNt9m3KRJljwoDVrFaT+u14jvJyzxhOdlErDuVlFQBIIpR14jKPFKNdNhkm2koJjekDXr+tTPzZV4mIJCHhRj2i0lvlKIE2G75kGpp6BElXhsBcVV4gF4GJutoPDoby5rh9+wa74jap1lJZ7RYsWdyskWbFfmRDi7hBQUu6RRrLBWUaN9uVx1DwPkXS3mN2i6ZxyuJFl9cbNbiQcdYVy81AJAOzfX9GZXmaZU8Z3DAsPSg915oi3IdYLaXN00/L2TNxQUfr1UtjBLPIqg5B0qiEEq+BgIgCkoLvXFNMlaXTwGuXG9ho7wOM3rQ6qf43UgcJWLX6/TJGsqKK2ioAgEUw6uQAyPVUGZyhJPnfZd1+MjrGXSJigNTLJl3SRLH6QUqDZgKChRHpR6wwD54xkWxhZMdtVzQfgHNy2cPCGVEwFFIlwko6Zi0L1pzSRVAB1XUNxlGrfSYZpkXQsyJwpppaBwZcYmPrqCEt1mTHApKEBIxvg37BZTQXF18ZCCwl4jPhmYTLL8XtF+eJmGm3fDWTxU4gnPn/uMnETQIBHNKihAaPDuG3UbP01FwdUtRTAVhrSpoDiUKkKjqHueUaK8QKxV3Bzop6lNKfteEezOI6OkxZ7fmRMFRSCYdoxHQZmJHhTzcX5uZinKdd59I7oKUqmGfg76IO1SCopO6HjJp14rdFTUfTal56BwFSaVTFidGnzfeg5K1SJVgF224Nke9I14pFhGxelBCVtnlQfF2THh6OJx+QhIQWElHtMESfeE7oe7i8etApiPuUoiLRmdoCRdfo5yLZmVHZsWTV7iSXiOe8W+2acdZM85i4fnoDjKQyrqvhhdliqUK04Fha63P0pBqRGDIYeiYJV4LA+Kp52X7kHRj2OWfMwWZRfp0rJZLA8Kn0jN33c6IYks8TjKUmYS9DwhKALB1INngYzETJOlD9hKNTSpTTYaKSZmuYn/3kybsWl8LZTtqbKdLW4PCr9//Lysc4xqMzY8KJx8qdk0xsBAPerelrydRMHI9gjajMMSj/PDnrcDuxZOwyTbSEFxpYG6TLJ6F0/oAwmPaxou3d+gXUFuoYISvI5ph2/GJIpmO7crhZbuM/fruLwRyoPSMLvFJjCkVMVTUGoEZdQerBhcg+HJ4K+DcZ+jTLIjzjbjcFvPc5FKXX1xdTwNOsLjXIqhq4uHYIatmQqKlHgEghmEspGEGbfEo8e3TxNBifE7ERL6cHUSlIJ+D/IluxMjqsTDv+W52oxpgTCJklbi0RSUcB+qPdYoPYRGyrDLJypiPWspKHaJZ7hhm3Hoq9C7LfThelGeDDpfWqRas1RaKkeUjuonyZrfxvlizmP4XQpKq6mgJOzjci8IHTtUKtwmWW1uUR3zLqkcmkmWGTvp/enOm3HMrWHvjXxJv89ASMjIg2J38RglHof6Qmg1fidyN9wgqK0lnVThfwR6n9ULauMIxxW41CabKBKMy7WuV+viEZOsQDC9MEnFWLt4AFsVmCxYHpQYJR4KbOqufRtylVrs7XlXCykotknW932N4LlMskRsShVdceKx5FlNQQn24XnhAsgH4PF9ByWeUI2oF1xVKFdQrfrq+lsy3CQbRt3zRTXlKPHUVVAc7Z4u42c7C75zx9WHJSD6hu0KagvP0ygnGIsQvx+UxzGQr7PYl6vae5u3GZcq4bBA/uVcNxRTiYd/s695LlwlHka4qJvGFT4Xts7aj2kKisskW/Og2DkonnbcenH1donHWOwdnVjBz/rrFVyDqaBEG115Ro6eUGy/J6MUIvucXSUeUVAEgmmFqSDE8aD4vh5UNhYfitkF0wwsD0pZ79oxz8M1pE8RFEPJGGLlHaV2sEm49MGpPChMQcmXqtqkZB4oR4tFT1tGfRi/MRQGwhWYqpBzKCjphD1+njpxOFkgbwT3oLhIRLFc1VquWzNJvc3YRW4cJZ76HhR7odAm0taez7tBKAQsqjxA97ueedNclMLwMXsRajVKPC7Vh3eP0OLI72PVqTaxNmNj+GHwc7g9oPtX9LlGdvhcVHmEP1aMyIxpVe3xjdqMa/eZE4sGJR46D/p/IOo1NA2yQEhmRx0lHpqmTOD3p81Fqh2qH8H2oET7ZpIJT1OJ6P/5ZiEERSAYJ8zFPI4HJYj5jt5XI/z22c3Y/wv/i+vueiHWdqZiYppkzfNwlXjow8Y0zdI3+LZMUnU8cCWDvnGRw39gNCQ0rvIYnRsv8ew8rwUA8MqWkfCcWUBV1uFBcS1CTg+KI+qefyjTQlEoVzUymkuxoLZS/S6eYsUPh9i5ungcOShpx8JJZCrF2qe3D5N5022CJUKbrfON3AzjMksifAEjUkbn5E5lDVU1cxJy1Q+35YcNg9pCpUzzoBjEQFeqwoRbV1nKUowc/pWgzTh4HbQ2Y1XicSsoaUNR4F0tZvqrqaCYi7+2rdZybC/ddG+INLs6nly/c1LtGqxozhoyz9Eq8UR0KnXmUpYy1whCUASCccJUUOJ4UGwfRbwSz6U/fQIA8I+rno+1XSOTrKtEQ6CSBsVWm9dPXoT2XEotbPlSxcpYcJV4XJ4AM+Qrk0pg1/ltAIBXtw5b5xypoDg+sAsG+cmk9CTZcGF0kxs1WyadRCLhqeAtPiyQfyjzEo8azOZQZ5weFM2TYftMaCFQCgo750QibK/uVwpKnS6eiAVNLX7cJGssuq5Si0v1aWUhZS71RZv8XLEVFFPlSRieDDNyXiOCpufG0VYbqaAYOShRHhRnDkojk6xRPuGlpXqEEggVoorjfVWPoLRlw9b4ksNzFfweTVCsEo+xLU00ntcWz38CCEERzEG8MVho/KQJxHhKPFaWR8w0WbMDplk0MsXaXTxV6zEiGOY10ILQnk2pb3o8jyJVp8RDH3JdbGaHiklnC9zyBQFBeZkRFLWYpHUPCuWVRMW3A3rSrDaLxwiXC/Yf7nukFBpkAf5ttOwu4bASjzNALBVub22bCrstio5vurTQUEieuSjRIjvgKE1YC7aloBhmTq3E426V5dfram/mfh86J1cwnT7ELlpBsdQH6tRxtd2aC7CrRbniVlBaiIQ6zKjB73oLe9ahVgCBWmQqIVGZKtZ+nCWeaBJiqiAZB+EaLpSdhDw4L7YvkwgaJR4r66XmGYtrkAWEoAjmGP7jobU47Mu/wU8eWTdlxzRNonFMss1kitRDvSF/dberGC26MUyyRDSoTXi0FoZGCBWUtPpg56UW0yTLSRbdu/ZsUlug+DllkgnsOr8VAPAqL/GwxykzosDSYKOSNfm/PAclKuGUd8TwDBQA7mGBjrJEqeIr349rRsyIU0GpeWPK9sC34J7pCor5LZi+jdPC6spJUedZZ7EDdMJGA/Bcj/FyScFQUICQVA0oBcU+B54kqysdDfI5DN9M2qHsEJzR/g0UFPMazPM29wfohKQtm7I7cepsy2P2TdXKvIbgPKJ/17uw7Anc1jVwgmIqKGZbtfFeoZEWcQ2ygBAUwRzDsxsHAQDPvD4wZcc01YY4HpR6eSMTge/e9xI+cetjltJimmJLpqJSx5NCH2KkgHAPARCaAzu1Ek9VS3MNHreD2pR/JZvSPAiAXobZrVbieUVTUKjjIqEWYz7N1rkIUZIs809oAWH1/CvliirFUGlHN8na2R70c9SMGL44mtumuYLSVInHraCo39m3cPObvNXFU8dU2pKJ9lHwNmM+zJFA943eA/zbOR9w5wpqaxTBbia6pusQMtfAxmK5ftQ9wTR+RpXHAF35MOfwAC7lSt8XnYd5/oDtG6pHMlwm2UD1s0tp5jVYHhTmJTL3DYTvy7ghbYAQFMEcA32wu7I5Ju2YhgoSy4NiTQYemyIShS/91zP49ROv45Tr79O7hRq0GdM10Wc+bxGlhZUIBhB6JoCw9h6UeEhtsNuMeYmHFJgRtuCbWSW6B6WmoGwdUdu6gtoK5WpEiUcnAtxIyxfGepNw3QpK6EGpZ5LVg9qiu0sSjvJQVCQ5dfKEJllzcYsmHeaisvvCNu13c1+uqHu137S92ANujwKdM5X53PcqTMd1BbUR7BKPQVAiyiXm77yN3JUZY6oX3YYyUK88xomOOYcHqK+gBNvXUVAM/4qt5Lg9R9SVVPX5bKLofVkEJUK5IlBYm3mfmoEQFMGcAn2QjcaMmx8PJtSDEkNB4Z035geuiXXbRvFfT7yufm8YdV/7neRZLXK+tmi3pJNqsRlh1zHgIiiszVgFtdVKRDybI1RQklYbKScgO89rRTLhYbRUwebBghajzoPa9C4evggltX2WHCWeUtl3Dk/LsmviIW38Xx5178qjKEXMzLGIgLMDKCw9pLXFXvegWCUe41u3WT7geNteC4xto7+Nm4rCgvascztX260q8TgICle81HTniDIFoJM5wDbJZhyvA6GHfbvnBDRM3Y1WUCyCUodkuEpyHGaZxiYoNa+To4unnn/FPHaUqhMaf6PLQ1FdPFHnvMfCdgDA3os7rHNuBPsOCQSzGLSwjk6lgmIFtcUhKG7lohlwM7DrG1W1qpdxNg3m1c/NJsd25NIYyJcND0rYFdOSSaI4WsVo0S7TtOdSSk3KlyvIIThH+kBrSSeRSngoV30M5Etoy6bCwXvZlNa1ws8pm0oik0pgp+4WrN02gle2DGuLRDaVZNHu3IPiKtPY/hZOBCqOsDVe4qHXWplk06xls0IKSviBTYSjFDUjxiARSUeAWKnia+dLoAXP5TEB6isoHLvNb9U6bACXgsJKPIaCsqgjJCj8/Mib5DpnIrVRWSbNpKM2bI2uc595hwnvxKIvDPzemfemu0UvXZglnqxBBHPpBPKlquVdAXT/DmCTDOoCcga11em8AfT7ZZZs6JzIC1RPfYkaFkgw7+2Fb98dx75pId40BoIiCopgTqFY+0AZneBSST3kjQVhuBCnxBPdLdMImwZCwuEiNua+uKpEC1xOtYGaQW3Bc6lF0OVBySQ9zRRKoBJPRy6tKxmGGuF5XthqXOviGFYlnqRWagnOUS9r9NQWlf7RknZ+QYknNIQWHWZU7jPg1x/koDhKPFoXj8skG9wnWnQKZfcwQKXORJR4mlFQeHy7y3BqPj+8L8m6vxPesfdC62/1TLQmOV7UkVM/88GMg462W3r/kIISpZCMOnJhmm0zJp5er8QznxGU1mzY1ZJn6cTmORO6YigoQEguXATFKlMZRESVeOokyarzqKN6ma+n8gKp18Hc1vZRqeM0uN5kwsM+SzothasZCEERzCkoD8o0lHhowYyloIzDJLtpIFRQzE4afl6EYW0Inz5Pp2gc14yV5+dVZIt2i4ug1AhaB28zLrlTWbuMTp5hzSSrt2uaqkELIyF0rTREjXtB6ga1lXWTLG99DVqjo7fNlypKOaLwLr54UbaHKym2VPHrRt0TolpDXamd5oJntRmbCorx+5mH7oyd57Xgb1buDROuqHSCpaB0ZrXf6fUigsIXR7OLh5OMZMJTYWthiad5D0q9czavnSsonHS7FRSjxGOYZM3uIpNI0f1ylXgalmnSdRSUGMRhl55W7TEiZfT/oV3iib7v9TJWxgsp8QjmFJQHZQwlnu3DRfz0sfU47cCdsLAj23iDGogUzWvN4PX+/JS1GW9mJRsgKGfwb7PmPRhxzLhpz6awZaiozbwBeInH9qDwBFQ+vZdAC3MQ1FZTUMoVtFQo9TT88ONtyAAjKJmwi8eVgxJsG5KfcA5PEGff6sgjiZpYG9yP8Dkt7JpKzjbjsDxkmmSD4wO+H6pCKce3zwozfsb1oAT3yS55tBumS+ubu6Wg6I9/7Yz9Adh+FNdzXcMC6Xx7jG6NbDqJ4WIl9KA4fBjhoEF7YSywkQL11Ca7hBWt+piPaQpKJiRNJea3IrQYuS9mvkdUe3d4nqSgjMUk23yJp17n1b5LOrXHSEEhD0o9k+xei9r141rR9xNHUERBEcwp0Id+3IF9APD/3fYHfOm/nsFHbn441nYkA89vnwAFJUZpamO/TlBMQmIn3NolnjaHCRYIyQCVYArlqlJoysz8SSFdIxpBCU2yWc0LYpdLTKKgSjy8zbhitxkDISngCgotPLoXxJWDEpZpgmPQvkNyM1qqoOKIWHfloNA2nudZs2miwsXUzJQ67a9JreQR/uxKKW3P1i81mKqBuZB4nuckJ+ZxzH3zxXJhR9Y2qxoKil7iobk2ttrkOkf9m7z+XLONtd625vW4PCibmULJ1SlTQaGRDa7juM6DiIOrxBPloyG0qBKPvXTXUz0A/Zr3XaoTFP7/kmtffNu37NRV9ziNDPtxIAqKYE6BZPOxtOve/exmAMAf1vfH2i5ftks81arfVM3VNQm4WfASD2ATFEtBYd6YgkFQombxdLAP32KlimwqqXWf8ORUgirx5FLqW3u+VHGXS1jSKMBLPHU8KLVt+L7DvIrgMVpEeNhW1MA//m8mmWTkpsySZO3yAM9B4YtWSyaF4WLFqQpoQ+wo4bROtgdfsDwviKuPykExv5HPN6LFc0abaRQZccFWI8Jt+bXzDh4CnWNY4rFVH3rPmD6SeuFj5sJvKhl1SzzGY7xMQ+WOLbVBlFnW2QXo19uetefLWCZZg0zQ/zPOHJQGCsqZhy5D30gJx+2z2Nq2UZvx1qGi+nmvxboKYp5LvfLZW3bqNJ5bv7Q2HghBEcwpjKfEM1aQ6sG/wY2WKs5vSCZMlSMOsTLD18zWajvh1lZQlAclos2Y18mL5YCg8NJEq/HNCzAICjOUhl0t9oJMnpjQJBt28YQkQjeG5riCorwCtgHRZcA0PSjhvJ5QQcmXQnLDFyE156dUVe3VPKxMGT8dHhT+s3PqrOVBsRcs/lq5OmIIlBWj9s0Wys6YU2XrKSi8rEit49pxqd3XoaA0GpZnLpQu9Y1gekHqtUabSkXKQUCInHYYCgm/XlepxfSgZI1roPePU0ExtjVJ5nH7LHaSE9e2JnF4YfNQeE4G4TRVIXNfr20fVT8vX6CTG/M1ct2TsUJKPII5hfHkoJgfjs2CFsfu1rQKNms2rK1gdfE0f94mqbDJTrQHxSQgVomnFLYZh+dmh5q5TLJhiSfN2n0rrDQUXeIZ0XJQIkyyRFBSjEiQgpLSvSBASORcLarmvgPSxYfY2TNxsiwEbtTIQeE/hx6U8LjJhKfOa8RR4mnUOltPxjcXPBqoSOALxwE76zJ9I5iLveYjSumKggm6JiJseoqp/nxTdbQISp0cFDOPpF64nBfRzuw6J/O+8veHq5xhKhlW5kqNdCxot5NV+bZdLWn13GZgHTeGF6RRB9hzmwbVz1HDAgljibSPghAUwZxCgSkoZldLI5iLQbMgItCSTir/QbMEKV9n5k0jmJ03VonHOAfdgxJ6PYDooLZsKpzuS+fGp/DWazPWTLKsi8fVKkoEgdSX1gz3oOglnnAeSUh+wqFuwd+4FyTsTLCVCiI2vHyUS4fkRvlIIrp4TA8K/9mloHiep7IuaFtXPguhUWdKVPBXRy5lLRT8uQftMg9xYBku2WLISQUntOY5uzwo5sJoeVDqKDc2QWm+xMNhlsZMNcEkXVGdVeo41muk7++S4/fG3757BU54c6+9LTvH5QvaYpXhLAXFOI93rVgEAPjIMcutbc1WcfNe0TWbRmTAVozGEmkfBSnxCOYU9Km71VhyYyrhYSxzkPlAsdZs4D8YbnIeDy2sHdkUBgvlmAQlOucECJWdlnQSo6WK5kGhcourSyf4PSynZGtlBdXxwsoeYTtvsO9CuRKm0Ob4NGMWde9QMui6R5hJVnlQHGFqgOFBUXN4bC+IIigpV2mJVCFf/d3zPLSkkxgpVlSAmJaD4hoWqCWN6p0pLum9WAlVNte+CXFaR/liv9t8e3Hj/y8cPA6Ckkx4kf4qp4JSO2cin3qbsWEK9ppXjKwSTwMFxbx3BDN4zSRN9Uq1LpWikSdjj4XtKl3VBCc/uy9ocz4nCo1Msv945gG4/8Wt+LN97RJRm1niMdSYfz3nEFz938/iG2ce4DguV6bilw/rQRQUwZwCVxXilnlMqbJZFNS396TW3trctka3TAzvjFniMbuH6PpVd1GJKyjBtku6ckh4wPaREu55/o3wvBjp4qZQftx00tO6ZYBwUQYCqdwVde8yK9I+NZNsUn8sqotHN8naSkbYIeIgGaUgLbZS1TNarAAxZ5sxGxbIFjE6r3CuifubravEE1dBiTLJmjkXQBiBDwAHLItX4uHHOWy3aHJjdrQAIWkccky7NRf/OCUekxyYQ/vqRftzmKSKlweBcNyDC5z0Rp1XHNMov97lMQlKoxJPd2sGJ++/xHk+Jkkz7/u7VizGqkvegf137raPa+QajbVU7oIQFMGcAl+0R2IaZaMk4EZQaZPpUFEYbrrEEzyvi7XzNgtasOnzwPKg1B4nox2fskzbLu7M4dyjA8n3itv/aHe18BJPyWjJTdo5KHSMlnRSRWjTdZaaajO2c1BI3TBzUGjh423GOUdKaf+ovTCqa6pUNTKpIuvNMg0vDzEPCp1vi6PEQzCNrnT9oe8lusQTp+2WL7SuHB9uqjYXpEZY0duJZMLD4ct78O0PHmo9ftJbepFMeDjr8F2sx+iciQS6phkT6plXAb201EgxaHQv1TkYKo7nedp51VNQXKrMHgvbVXmtLZOMlanEz3H3CJUlclt2PxKebZKuB/MexPks5M+dyPIOMEkEZXBwEBdffDF23XVXtLS04Oijj8bDD4fZEr7v48orr8SSJUvQ0tKClStX4oUXXpiMUxHsYOClitgKCvvwq1Sb96/wGTFtasFuTkGhhTUkKM2fMy3YtK2Vg1LU2595maag1IwELjl+b3RkU1i7bQTP18xwRU1BSartK1Vfiw43PSh0DvT3rKPNWPsWzAhKuRKOt+clnjCOXi/xkHqTL1WZB4WXeHQVxJUkWmA+kmTCU4sa5buQIqSTiGC/vh8u+i4PCsFOB40mHVZMeJ1v456nL2i8zOQyYH7sHbtj+YI2fPU9+1mPNcIBy7rx2BV/hh9feKQV7Q4AN3zgYDzx+eOxtLvFeqze5GBTvbAzVPRFl2/rmjCtb9tcicdFQLRWYocqFHUMIMhUuf9z78LP/+po3PmptzvLXlFITZCCsuei9qa6CAktDcpc9cDv61gmFtfDpBCUj3zkI1i1ahV+8IMf4Mknn8Txxx+PlStX4rXXXgMAXHPNNbjuuutw44034sEHH0RbWxtOOOEE5PP5BnsWCKJRrlTBeYWpKDQClyab7cIBoEWW0+jyZj0oYSCaPfOm4XFrzyVzoEnIiCz0tIXf4Og5XCFpz6ZUUBURpAIjXdxQys20qaSnPthIrSI1Iky8DNWGMOreJiiFclVTvFozbJpx7Zik4JhJsqOlijpfVyT5gMMky8tHoTE3qXwbJrnhH/78GERgWlm6qKlOWK2zdbpL7Hk50YtsJqlnmfCfF3fmYGLPRR347aePdaoczaCrJR1p2kwkvMhFzWr35SpXQw9K+NyOXDrS++I6drMR7GYbsbm/eiWeqC6b1kwKB+0yDzvPa17FAMIyGADstiDetpz4u0ox9cA9KJlkItIj4wJ/jWa8gjI6Ooqf/vSnuOaaa/D2t78de+65J77whS9gzz33xLe+9S34vo9rr70Wl19+OU477TTsv//+uOWWW7BhwwbcfvvtE306gh0I1nC8mASFd/0M5ZsnKGEKaUJ1jsT2oNS6H+LkoKh5OVEKSu33zpZwrg0RL2WCrX24mGZVTmC454ITFC0HpbZfM7hMHxboGLzHunhI8aFvylTfL5kKCnlQlL/FTpINHq+pIAVSQXiZJvSgkO+Ff9O1OnHqeB+A+iUes0xRr3XYXETfttcC7fd6agsAfPDIXfCWnTpxyv5LrcemC812HgH1c1BcRIJg3nPzOOa+AOCvj9sLXS1pfO7EfaxtuRrlIj9f+Yv9sPfidvzdyfa248FurCwTtwzHy1/7x2wj58d6U29HLN8M/39johWUCe/iKZfLqFQqyOV0Bt/S0oLf/e53ePnll7Fx40asXLlSPdbV1YUjjjgCq1evxllnnWXts1AooFAI+ysGBgYm+rQFcwCNuloagbf8DsWYSMy9BPSNsNm4e1IsOsdQ4lEKSguRG7eC0pJOojWTQv9oSZ0X71oBwoVPxcozMsBLLXxmj54kW9H+pQUjx6Pu60zvLZarmtGVUlODc9JJU1YpKKEHpZ6CEp6v25xLapcrzpzOyWwVzqYSGiHWk2RjduIYOSmENy/txJIuvWRSLxYfAL50evzyzWSjHkHJphJIJjxVUq1HUDodLcwEVyorP24q4Vnvh0v+bG9cfNxeTlWGezJcJZoPHLELPnDE2JSoeth1fhtuv+itsXwrBE4U9tspLkEJr9dMim2EzGxSUDo6OnDUUUfh7//+77FhwwZUKhX88Ic/xOrVq/H6669j48aNAIDFi/VWp8WLF6vHTFx99dXo6upS/y1btmyiT1swB2B2tcRVUPgCPxihoGwfLuKBl7Zqags3jbbFNMnSOaupwTEUFPKRKA9KRJJsLh16Y0aUgqIv9qbfo6AUiYTWVkskI5nwkEx4Vu6LIkWGghKoL9FtxsVK1UqDDcswxiyeZFJ7XpQHxSYo9nELpQobUKi3KHM0GsSmtRkbre2Np+y6P4bPPmJX6298MVg0hkVsOlCvxMMHOwKue8UzVuooKI7Be/w4K/dZ7IwciCoZcUUhjodkInDgsm7s5PDyNAL3ze2zJB7J4ITszUvjkRtOwCcypA2YJA/KD37wA/i+j5122gnZbBbXXXcd3v/+9yORGNvhLrvsMvT396v/1q1bN8FnLJgLMBWUOB4U3/edce0mTrn+dzjrOw/gjj+GZJqrEUpBaVKBIULSVfOgNHvOvh9OwyWCYnYt8QA5IgykFnDfDJ074DCkGiUe+jt9KNEHuTLJGrkgnESUGLkh8KA2sxMnXeecgn2HHpR6JlmCM6iNdeJoCkq6vgrC81Zy6YS20JnSvLkw2p0q+u9f/ou34IK3LceZh+4ME3zBXtJl+0xmIiwvSDI6CM1KKWVrRr18DZeCwvG+w+N9qdUUlDrEaCZh3yWdOP3Apbj0xDfFjpvnE5rNYYCNwFW9rglWUCblzu+xxx645557MDw8jIGBASxZsgTve9/7sPvuu6O3N0jP27RpE5YsWaK22bRpEw488EDn/rLZLLLZ2fFtQTB9MD0ocaYKFytV8ODZKA/Ka33BTIpVz2zCSfst0Y6bTiaUWTKugkJG12bPmZdaiKDkI3JQcumEWnxNBUW17ColQzfJZpLuEo/qpDGUmREV/Z5SxzbPR++mCdUZM8uEJ8nyrJKMUeLhOSj827qpoERF3btKPFaZxpyvoh0nelYLAOvbcKNpty7lxPXc3tlCUAxCYhIWTlDMwLBmSzwuDwovN7x9r4XNnWwNfMGO09EynUgkPFx71kFj2pb/f7qityPWtnqJZ4Z7UDja2trQ1taG7du3484778Q111yD5cuXo7e3F3fddZciJAMDA3jwwQfx8Y9/fDJPRzDHYfo34nhQTHPqUKEU8cwA3NlfYmoEffNqts1YKSitodHV9/2GEde8nBXZZsxKJmY7sElQzMh51cWTTmpqg/KRUDuuMSxwJKLEA4RmVS0HhXXTKKMrlXhSIUHh6phtko0q8ZhlGnercN9oEGDWVsfomjbUX02pMb6tmpkSZmhasyUeF/hzZ4uCYk7zNUs+H3nbcvz44XV4y05dVodRsyUeF4k4eo/5uOq0N+PgXebFDg9r5EGZa1i+oA0fP3YPLO1uia2+pCYxB2VS7vydd94J3/fxpje9CS+++CI+85nPYMWKFfjLv/xLeJ6Hiy++GF/60pew1157Yfny5bjiiiuwdOlSnH766ZNxOoIdBJZJNkaJx0xwdXlQuO+kzUFQsqn4QW1Fw0cCNBfRz681iqCocksmqSTwkWI5KA8ZmSJcJeHlo0xS96DYJZ7gsVLFR6kSduLQ39NJDwkPqPphSqzbJFtR50/f5igDo1g2CEpSV1CKlTByvh5xiJoavL2WsOoyyRLqKSjz2vRvjfzbd0s6aWWSRCXLNgP+XNNAO1PRKFn1fYftgvcd5jacagpKnRLP4ct7rL95nodzjtotxpmGmE4PynTA8zx89sQVY9p2MnNQJuXO9/f347LLLsP69evR09ODM844A1/+8peRTgcnf+mll2J4eBgXXngh+vr6cMwxx+COO+6wOn8EgjgYjwfFXNxdHhROOnhdmk/CbVNdPPGmGfNR8SPFirbQvrp1GAOjZezHWgfpmKlE2Dlkz+KplUxSSS2fhZeHlILCVBLzcddjaaPEQ+dtzqbxPA+52lybYZeConXxkAdFL/EUK1UUKuG1EcHhBIRUkPpdPLZyAwDbhgO1TC/x1G9/5QTFLOHw4+7S02qpYaZiEie1MzMLFZRm80hc0Es89nL1P3/zNjz8yja879CJbZzgatps8aBMF7i6OCsUlDPPPBNnnnlm5OOe5+Gqq67CVVddNRmHF+ygsLp4xlPicSgo29ksEwqUqlZ9NQSPz6ZpNqiNzrklk0QmlUCxXLXI0lnfeQCv9+fxi4veigOWdQfbsRJNC/NiaNfEFBSez8LvU9Ys8VSqWqksG5GDwktD1CY6WqxYJR7ax0ixEpZ4IoLaCsqDUjPJMg8Kv15a8DlJ6BsJSEaO56AYBIV/u0skPKSTHkoVH9uGgwgDbrQ0yY3pf+Clq5269RIO33aZYyaOqZiY5aN64Iv7rPGg1GkzbgT+XnF5UPZZ0hm7Y6UZ8PfOjqCgjAf882LGtxkLBBOBuCmwwPhKPObxXAoKH7ZGC3Wpqnsj6Ft4M+SoWvWVIpFNJRXRMLd9vT9IWL7+7hfV38jMmknZWSQEXjIhlWW4WNHuk6uLxyynhLHwVea3CRYOz/M08mOWeILjJ9W++TEBI6jNbDNm5MVsiwYCkkG/9xNBqeNB2WuRno5JJGPbCCko0R6UXYy5JtxXsbTbyHzSCIpdhkkzNabeZGAXeClythCUem3GjcAVo3oelIlGWx3jrkDHrvPbsKA9g13nt1pfCsYLISiCGYfbHlmHt3z+Ttz5lDsXJwrjISjmcwcdBGX7SEhQzGwOQE9WbSYqnysZmZQ9eA8ISAzhN89sUsSJd9koYhPRZhzkoITEic6Zskzo+HQ9fFpxQAJCr4fKMmHf+jlBUiUeRg5MP40rj4SbZOn5yjdTCNubzcWOnkuvVzaixNPTlsH8dr0TkI7t9qCEP3ueXcbh57HzPLPEE27ryrPg1x93QOUWRpLjJo1OFw6sqX6EbLL5RaxZD8pEQw2NTCfHPOV8R0EmlcDvPvsu3HXJOyZ833LnBTMOq1/ainLVx0Mvb4u1nRV1H6vEYygorhIPJyhlmuzL/BpJ3s7b+Ng8lC2bchMNs2x11zObtOPTLB0gUBG4kVdLkiUFpVDWDLD83Gm/auaNGYNfqqrYeV6m4J08I+yY/No4uFLhTpJNaM8bijhn8zhAdIlnT0M94efVyCTb25mzSFazJR5XIii/d3E6eABg21Cx8ZNmGHad34bDdpunfh+7B2XqCAqR49nSYjzdyE0SkROCIphxoFIKL6k0g/GVeMw2Y1eJJ2w9ViUe1tWSSHAPSmMFhYyfNJU2x0ol6jnGeW2tLVCcoOw6vw3ZVAKDhTJe3jJsbcsVlJFiRSsPEbiSEcbGU6uww4PCvvm3sH2PqhwURlCMxb0jGy40LpMsLf7tLLvFbIsm5NLRv/NzMMs7fF+DziTZ+j4SXpUxSzyckPU6hvbxEk9cguKj+SnbMwkfPDLMdolHUJprM55o0LFoiKdgeiB3XzDjQN9ot8YkKIVxmGTNDJVGJlmVbmr4KugbV6EchIvVy18osHAxHvnN1RzzvFTQGmsTzqQS2G+nLjzy6nY8vrYPuy9sR6UathK3pJOaOlMs64FntB+6noLhBeFtxqWq3sUDMAWlWLai7oEwGZbAFxpVPnIEtfHOo2iCkoz8nU8YdhEUW9lxKyhmjgmgvzfNibae5+G8ty7H6/2jOGw3u/11PCWez5/6Znzsh4/i4pV7xdpuunHK/kvx8Cvb0NOWjZVJkmAdUFNZ4jl413k467BleOueCxo/WTBpEIIimHHYqhSUQoNn6qBFrD2bwlChHK/NuEZm5rdlsHW4iJGSQ0EZsU2yRcM0yhe2kWIZHXVkaTOLpIWVSghm2WrYCFqjRfagXboDgrJuO844ZGft2nPppPJmjBYr1nEB0wui75v+zZcqqsSTchAU7kHhUfEmieBtm6p8pM3iCf7WXlNmipWquu5GBIV7UPQSj52OWS/RlJMbF0HZwkotrlC9K0/d1/obgZd4etriJWTvs6QT93zmnbG2mQlIJrwxDTLk7/+pVFDSyQS+esb+U3Y8gRtS4hHMOJBSEbfeTos25SWYZZt6oAV9Xu3b8IijTVhTUIwJu5lUWA6hL4iNfChKQUmHhjxzO5Nk0YwfU1E4aJegxv/42j4AOsnh/pZ82a1G8FILKSj0N67sOEs87Lx5OByBl11y6YQzj6RYDkPeQgUl3Af5fxp6UNjvnHAsX9gGE+Y04KiJxKYJFgC2DsUjzxy8xHPK/kvqPFPA38dxy2GC2Q95xQUzCvlSRX1b3jpc1EyfjaAICs2mKcfwoNS2JbneRS62OUo85oLteZ5WtuC4+9lN+OgPHkH/aOBliVRQik0oKBWToHQDAJ7dOIiRYlmbw5Ng/hbexeNUULQ8kmCbHFNIeKw/gXcf0YKieVBSnDToihInSeQFUcMC2RwgIocNPSiGSfazJ67ApSe+ydlNY5Kd9ogSz2KHj+TYNwWzXVylo0bYOJBXP//FQTvF3n5HwljiBgRzB1LiEcwocBJQKAcR5s066embP7n9TYNpPdAH4fwaQRktVVCt+lpGBYWBAbZJNm2oEaOliuYfyZcqOO/7jwAAjli+Hucds1xlWlBZwhW4FuVBKRgkY0lXC7pa0ugfLWHdtlHQ2kskg8hPEFfvMMnymThlMqvWFBRGbswk2WDfoUnW3WYcnQbKfSADo3aWSXs2hW3loiqvZQzVgysdCc/2dHz82D0QBXNGTCtPkmXnsMdCm4Rcfsq+2HdJJ949BgWEd/YsdRAnQYg4PjLB3IMQFEEkhgplPPbqdhy9x/wpywIwO3e2DRebJiihglIr8cRQUOibPzc8jpZ0crTN0WZsEgWAd72EBOm2R9apn4nzRCkoXL0xSdZQwT3sDwgW1f7REorlKsgWQYoC/aspKFElHqP0RHkbo6zEwxM+SW0YKpTUvqM8KGZsOL9vAzVjsh62lsS2YaCv1kFlqh5cMcmlkw2HLEYd2zznRMLDD88/AvlSxRmI1plL49y3Lm/6WBwXvn0PVH3gA8ZgPIGNw5f34AcPvDrdpyGYJghBEUTi2lXP47u/exn/9L4D8BcH7Twlx3QRFFebpwu04JOCEkcepkW5uzUNzwum3A4Xy4qg+L6veVBISXApCnyxJ9x0/yvWtlEelHomWduD4sgUYXNrlDqTqRldyxUVMuciVYEHhWb42MSp5DDYEkHhnS1m1D3BjA1PJDykEh7KVZ8pKPbziRxaQW2OxNpmwduf2zJJK9H1mL0mp4OjqyU95sFsOxpO2X8J0kkPb9mpq/GTBXMOQlAEkdjQPxr825dv8MyJAw9DA+JloVgelFIwmbeZb9V5HmqWTmKYGT6BgCiUWaorkSFXcJlJUEqVKl5i+SR0LCICtOi6ou7NEo/lQXEpN6WqSsvIKfNukwoKS3TNGuWhoAPIVeKpEZSaqdnzdCLBiYOrEyOTSqBcrGAgX9KOC4TkJ8qDwlWPODNezOe3SiDXjITneTjxLWIk3lEhJllBJOgbvvktvlnEMbgSthqdO3GyUMwuHsBOYo0Cj1nn+RsEs9SiclAogt3RmUL3jZQBAikkpHRkDaXCpaCEnTLuLh7+c4EN/AsVlNCDQo/p3TShsdckTkQCipWq8s1oJZ7a40QmW41Si+4psduu6TiDVOJhqlCboaCYZZl31MyqgDu7ph4ydZQdgUAw/RCCIogELVSFMTjpf/74ehzwxf/F6j9tjbWdXeJpvp2Tgtp49kizrcajTDUIo9vDBc/0s9gm2XBBJgWASERfBEEpGHkjzi6ekt5dNGx4ULKNfCS1v3GSQEQgatuQrOnnBUCpHHqJJ1jcqfW2JRNthI1SUDhcJZ4oBeUYFqTlmp9UD/waXFknAoFgeiEERRAJ+qY9FgXlvue3YCBfxgMvxSQoI2NXUGhRbsumlEm0WXJFRKYlnVQLLldQTD9LVJIswCb01shLv0FQ6Fhmq3CrU0GhfJaAdFlJshFpsOZkYJ7mSufjVF80BcXOdqFtXSUeGmRHfhd1P5oo8XC4JhJvH7HPGQhKAN/+0CEAgFMPWGrtux64svW3794n1rYCgWDyIbqmIBJKQYnRDUOgab6umTb1QOFsizuz2DRQiBXWxife5lJJjJYqTSsoXDVodXTTmPsxk2SjMkWAYIif61ihymGYZB3Hndca5rNUq74zy4QrN6WKrqCkkgmkkx5KldCMyrel9twiLw/VtvW8YMbQcLGCgdFy7fm2STbs4NE/VnKNFBSzM0crCenGV9cclxPe3It7PnOsczBfPZx64FL879ObcNm7V+BNvXbSrEAgmF4IQRFEghbQOImsBFIf4voCSEHZa1EHNg0ULNNsPRRZAmoubWeR1EOeqQYhQQnP3Rw8qLp4HCZZPlwPAPpG9WvIKw+KTjJyzi4ed/tzweVBYd6XkKDoXS6lSlm182Yc5+yaiQMEZZvhYkUpKCltWGB0Nom5H5cHxcw20Qb+ZaPbkjl2nW8nxTbCO9+0CE98/nire0cgEMwMSIlnDiNfquD6u17AWd9ZjbVbR2JvH5Z4pk5B6asRkuULggVnLF08AUGh0LPmyBX3dLgVFP0eEEEoOVp2GykooQdFN7KqvBFHkmxXS1qVrYYjpvuGJKPCOnHsbpoBV4mnZpItORSU4Nxq29bxoBDM+PnYHhQteTZV97njhZATgWDmQgjKHMb5Nz+Mb6x6Hg+8tA2/fW5z7O1Dk2x8BYVm2cQlKHTMBe2BXD/smIkTBe7LUAPumiRXXH1pU8mozCRb0hftZko8BcMkS91FREAKZlBbnRyUXDoZnlchVIaiBv6ZPhK+f5ePRJ/FYxtwiaCE29pBbQRzum+9oDZA74Ayn9+WiSY7AoFgbkP+b5/DeHJ9v/q50eA6F2ihipPIShirgkIlk572TOztlacjyRWU5s6dL8quRFdSYihjRREUVeIJF2yzzZgWdUokzRvEzwxT05NkQ2JERCBKQeEm2YIxGZj/rFQQh0m2XPXZHB87BI3UF17SMhWTBe3RBKXD0c5rqiL18kkmWkERCAQzF/J/+xwGDxYzPRTNwDRzxgEtsnE9KBQE1lMzhQ7mS/WebmzLFJTaotjsufMFn7I3RhyBaaSCmMMCtS6etLvEQ0Pn8lbYGgWihZHy4XFDBaWdnZeri4cfN1+KVlCIZLjajIHwnvPyEJEjekulE/ZjBNOsykkSbwFX522cBy+7WCUemWgrEOwwkP/b5zDKlZCgjCXLJOziiU9QSPmIq6AUDVPoUKHcdOBbkZU1cjFLPJqCYoSiAWFZhhbYctXXu2kcfg6zzbi3RlCsHBRjWGCxXEWlxgT4ebXWOlqGC2V3DgopNxE+kqwiKGXt+ebPYU6KnehK4IqR6UGh8pw6bqp+iYffu5yhkJjHFQVFINhxIP+3z2GUqyGxiKuglCvhIhl35HmpUlULaBwFJNi2pqDUCErVb/7cOVnIOkyyvu/joh89ho/94FFUq75z22wqibYaERhx5KBQiQcICIhrNo1pku0zSzwRXTx8MVYkpsxLPGE+i6vNWHlfShGdOGmdOPHFnntKVKIrUz7M8DWuGOXSCfBpAqaCwn2ojUyy5jydyTbJCgSCmQv5v32Oolr1wdfguGPLuWoSV0HhpZE4CggQLp5dLWm1sDVbJtK6eEhBYeRmsFDGfz35Ou54aiNe2DzkPG4mlVCL8TA3yZYppTZcMEuVqnM2jdlmbHpQorp4uNpBrxcv1bRxD4orqM1pkrU9KAR+zp7nqe3DEhAnN/W35T4UU0Hhz23L1M9BMQmKOck6a7QkCwSCuQvJQZmjKFV1UhFXQdEJSrxteWmk6geLrJmN4UKl6ivVhrwgg/kyBgtlLGrmnNmiTQsdv45hVm56duOACufialE2lVBEwNVm3Mk8FKVKRInHVFBG9BJPwUySTeqBaEHAnKGgpBPKMDpSiDDJalH3dpuxaWa1zKnJBIrlqoqM17t4TAVFb89tSSfV/VpgKCi7LWjDBW9bjp62LJKOtl7djKufk0lozJKPQCCYuxAFZY6iYpQw4pZpOCmJG9Q2bPhOBgvNlXmoXAIEC2CHGtrXWEHxfT1dNetQUPh+/vha2OHEBwpmInJQiOC1ZpJI1RZZntjqUlCK5WCacv8opePm1PHKFTYvxzG9l47NZ+qECoq7xEPqQhB1XyvxGEFtHCZBqRc5bxLMtGFW5a+d2cUDAH938r74+LF7WH8HgJPe0qt+fn6Trmxxz8r+O3fhsN16nPsQCARzD0JQ5ihKFZ2gxFZQSmNXUMzskqZLNAZRoMWpme3NbZWCwks8bD9PcoLCVJZMMvR6jDiG9uXSCbU4lzQPCmszZjkoo6WKei2oxAMEJSNXhoqZJsvzTFpZPoszSbaBgmIRlGR9gqIpKOn6BGWY3au4ZZij91yA0w8M5uis3EfXynraMvjgkbvgrMOW4d8vOFI8KALBDgQp8cxRlCtGiWecHhTf9+F5zaVucu8G0HwnT4kdM51IKIOkOaX254+vx7OvD+LgXefh+H0Xw/M8jWRka1H3QOgdAXTi9NRrA6hWfSQSnrrWZMJDKplwRt2rWT2pJNJJD6Ol4L7UT3StqvJOKuGp1mnaHy/fEFqMY3OTbJvq4oloM1b5K2EUfj0FxQw9swlKPQVFfy+Yil1cfOPMA7Fy38U4cFm39diXTt9vXPsWCASzE0JQ5ijsEk+8Mg1XTXw/UCia/WY8Mk4FJZ30kEh4aK/5Pfj2Q4UyPn3bE+r6fnHRW3HAsm5LBXEpKEOs1DRYKOPVbSNYvqDNKpe4FJRwmGCyNjumXDPJ2iUeblYlg2x3axqJhIdcOoF8qYpRVqbhSapEjvKmgpJOaAqKs82YHdcVdd/Ig2KqIjkHcSLMb4s3mK8RkgkPp+wfbxqxQCCY2xC9dI6iNE4Piklo4nTymAqKqYBEoVTWO2LIg8IVmKF8WSNfmwcLAGxyE3pQwvMeMojTi7VOngKLuQeYD6TAFZSaIpFJqnIOL/FoBCUZGnQHVMx9QLZC825FIx8EVeIpGmmzqaTqIBrMl9U94cP3eImHt02H+65PSMyST1QOSks6iV16WiEQCASTCSEocxRWiWccJlkgXpqs6UFpxuQKAMWKThTaHQTFJFp0XaYKoqLu2XWY50G/my25FIg2UqqoFulRVeJJOImAq8RTKFdDIlEjFy2MgITn7DLJ6iWeXDqhSM6mgbwiaZ0toQjKS0thAm20CtLIJKurL+Fx9u7tkCF7AoFg0iEEZY6iXJ04kywQT4EZGaMHpWgoKO1MMVDnYRAnsxRCi6wr6t48DyrhmCSDSim+HyonvMRD5xcEtdWfZkzHpHZZboJ1KSgtab3Ek3coKOu3jwIIyiK8bMPNueFwQ6agGCW6RimtvHzEyc2KxR0wcXitu+Z9hy6zHhMIBIKxQDwocxRls4tnHCZZ1+/1YCoog016UMxU1lBBCb0jZukpbyootUXVFXVvE5QIBYUt+iPFMloyybBtlxOUiDbjkKBULAWFDzF0tQrzica+72smWUqxpbJWZy6lGZf1YYGOoLaMTmZ2m9+m3Q/Tz8L3zcnMiiU2QbnxQ4fgN89swin7L7EeEwgEgrFAFJQ5Clo4KbOjUK5a8e71YJV4YrQaj1lBMTpTFEHJR5d4LDNpTSXIGkoEYJd4bAUl2CaRCFNVw8nDYamFHmsY1FapqmPStVAi60ixEpaNHHkjI8VKMO+n9pJlU0ktJA6wB++5FBQtBI2d4z5LOu0SD293Nh7jSg0F3HH0tGVw5qHLrEA3gUAgGCsmnKBUKhVcccUVWL58OVpaWrDHHnvg7//+77W4c9/3ceWVV2LJkiVoaWnBypUr8cILL0z0qezQII8Cj2ZvdnAeYCsmcbqAyCTrxYyqpzZjamFVOSj1PCg1M2mkgsJLPHk3QXFlitD2YeR88G9LOqkWcr2LJ1QblAelVFXHJIJChGEwX1KvUYthQAX0EhAQlIG43wSA9TuRs3ypokp8UWWaA3buggl9MrJe/kkwNWVFb6e1rUAgEEw0JpygfO1rX8O3vvUtfPOb38QzzzyDr33ta7jmmmtw/fXXq+dcc801uO6663DjjTfiwQcfRFtbG0444QTk8/mJPp0dFjQokM8yiUMyzOnHcRQUKvHQTJZmTbKFCAVF86CYJZ6ye/CeK+qeiM782iBCUnpcLbstRrvvqNZmXL/Ek2UKCnUOtWV1k+y24aJ6vtYhkyETbUV7DTLJhK2gZN0KCr9feox8+PN+O9kEhT9uZqTsvrANS7pyOGTXeWqQo0AgEEwmJlyP/f3vf4/TTjsNJ598MgBgt912w7//+7/joYceAhCoJ9deey0uv/xynHbaaQCAW265BYsXL8btt9+Os846a6JPaYeEMm/Wuk6KtVTTZtGMB+X+F7egM5fGfsa3cSIkizuzeGOwEKPNWF/sSUEZdgSmEUYjjK6uHBTaz8KOLLYOF8MSTyX0eRByEWbVIEm2FnVfqbrzSJJh5Dzdiw7Dg7JtpFi7Vk8jNxpBYR6VRMJDayaJZMJzdvDwczBTdcPzCn8+wBGIxks3ZqhbLp3E/33mWKQTUhUWCARTgwn/tDn66KNx11134fnnnwcA/OEPf8Dvfvc7nHTSSQCAl19+GRs3bsTKlSvVNl1dXTjiiCOwevVq5z4LhQIGBga0/2Yb1m8fwU8fXW+1/04WyCSbTiRYa+s4CIpBDN4YLODs7z6IU7/5O4s00MK/uCOIdo8f1GbkoNTxoJCyY2aZuGbx0H4W1WbijBrzblxmVbOLJ5syTbKOacZpu82Y5ugogjJU1I5jHtfV5eN5HjpZyc5UVOzgNU8bzkdJtACwx8J2mDhieTjnxsxIoWuX9mKBQDBVmHAF5XOf+xwGBgawYsUKJJNJVCoVfPnLX8bZZ58NANi4cSMAYPHixdp2ixcvVo+ZuPrqq/HFL35xok91SnHSP9+HwXwZ20eK+Mjbdp/041GJJ5UMWlH7R0uxWoVtIqATli1DBfXz6pe24p1vCmeocKUCsE2zUaByCZGL5jwouoKSNRSUvKPEs5BKT1TiqehEIPjZ7KYJntOSSTKTbNUZOU9Ep1L1MZAv1a4lXds+eIxKPKaptFVTUOxW4c6WNLbX4vMtk6yZBGu0Fe+5qANf/PM3Y5f5rc6pwm9hZZ+1W4etxwUCgWAqMeEKyk9+8hP86Ec/wq233orHHnsMN998M77+9a/j5ptvHvM+L7vsMvT396v/1q1bN4FnPDUgX8CqpzdNyfFIQUklvLBsMJ4Sj+H94L6Su5/ZrD1GUfeLagSl+Vk8Rg6Ky4MSYd61CQoZVe0240WdRJzcIW9A2G2TN8yqOWaS5UFtrjZjICQi7TX1gkgDlXhas3YpBQheKyJfPGyNqyZWicdIijV/B4APH72bRiY5+DUMx2xLFwgEgonGhBOUz3zmM/jc5z6Hs846C/vttx8+9KEP4VOf+hSuvvpqAEBvbzBafdMmfaHetGmTesxENptFZ2en9t9sxcgUffCHCkpCLdrxSjxGO6/xOycddz+7WevSMhUUMxcl8pgROSgFZkYNSy21a6IcFEPJUB0tjmGBRJxG63XxOIgCoCfJkjoC6EZXN0EhBUU3yZphabzEQ++VNqaycFLSSEGJO1UYAI7eY37sbQQCgWAyMOEEZWRkBAnDSJdMJlGtLZjLly9Hb28v7rrrLvX4wMAAHnzwQRx11FETfTozDuacmskCtZmOWUExZ/EYv3OC8lrfKP70RlgSIBKxwCilNIIyyRplGiAkEaSYzKtNBraC2pK6glKp+ihVggwYizhZQW12u2+hVFHkLFWbdkxKA00q5s+n51EFZWuNiJD/w/SgtKbNEk/w+2ixolQqrrJoCkpO3zaVTIBXblwKSiNce9aBWLnPYtx83uGxtxUIBIKJxIR7UE499VR8+ctfxi677II3v/nNePzxx/GP//iPOO+88wAERr+LL74YX/rSl7DXXnth+fLluOKKK7B06VKcfvrpE306Mw7mpN/JgirxJD0kE3ZoWSPYXTz15+u8MVjAnosC4yUpDgs6wjZj3/e1ZFIXzFZhrggUShW0Z1PqGrpb09g4kI+MuufkJl+qwPM8kMizqMMwyRoGW779aKnCOniS2vOIoLRmdPOo5wVBb/lSWAKiLh418K92/8z5OORRiVRQGEExFRQ6Nx6PHxeLOnL47ocPjb2dQCAQTDQmnKBcf/31uOKKK/BXf/VX2Lx5M5YuXYqPfvSjuPLKK9VzLr30UgwPD+PCCy9EX18fjjnmGNxxxx3I5XITfTozDs0aRseLMEk2odJk4xGU4LnppIdSxbfyR8z4erou3/cxYigoVT8gEGbrqnXOimQE55tIeMgkEyhWwuh2Oq+uWuw7nZc1i4eRjUK5qlpzEx5UjgeRLFercI518ZiprEpBGXUbXYGAXPF7RjkoZueNWeKhY4wUK0rh4c/hJR7Tg2Ie18wyEQgEgtmECScoHR0duPbaa3HttddGPsfzPFx11VW46qqrJvrwMx5xh/aNFbQgp5Ne2JEyhjbjrpY0tgwVHQqK25NSKFeVUsEDvYYK5YYExWwzBoIyRbFStfJIqMRjTzMOjkEqRrEW+07X055NqXKLta1GUEIlIwxpI2UnIFDbhwMFpS1rX1c2nQQYiSM/TYdRljEVFCI7+WJFqW08bE8v8dgKCj9ubgwlHoFAIJgpkE+wKUap0vw8nHEdp0ZQ+MTb0VhJssFzaRE0Sz58gB8QEhau0rRmkurbfzOlLbPEA4RlitCDEuxnXlta+12pIGxR5nH3PHKefB80S8dlkuVThU0FpbVGGN6otVpHKSgcVKahgX+EeiZZt4JSn6C47p1AIBDMRghBmQV4akM/vvo/zzYdGQ9ABcKlkglt0WsWpJh0tuhEgDBkEA4q8dAxKCG1LWtnmURBtRk7lIxQQSEPiqGg1NJgtYF3jGTQvWvLpjTVYrRYYSUeOxY+2FZXMkgFoSyYtoxDQWHX0MY8KnaJx62olKu+8rhoCkqDEg8/rpR4BALBbIaMHp0C8BZcIPi2b6Z+1sP7v/MABvJl9I0U8dUz9m9qG1XiSXjWIt8MSFUggmIrKO6JxWF2R7DQtmWSeAPNeW9cJEMN3jO6eLrpvIwcFJfRtVCuKsNpay1ojbw1I6Wyc1ueJEtqUacyugbHppfVLNOY+4oiGPw4rt+3OlqROcFpzzqUG3bcnea1WI8LBALBbIF8xZoCFI14+76RYsQz3RiolSd+/vhrTW9DpaQki7ofS5IsLcrmtqRIdLemtd9J0aBjRikoX/r107jhty/q51wO5wcRrBJPTdkhD0qxEhhgCw6jazhVmMfGJ7XzG+GJrUmHB6VYsSYSmz6SNkeJh59HO3u+2Xljlnh4PP1WpdBwgpOu/S2JlDOOPvzbgY55OwKBQDBbIARlClA01IftI6WIZ9aHa2BfFMrKcOohx+LT4x6rR03+NUo8tUWb5u1Q8igRGVIVaHHlptrNA3l893cv4x/ufA6bBsIJ1i4Piqn+0DV0tYYLfb5Uqaug5MshCckZxGmkUHFG3fNtqSVYERRDuTDTYM3z4EpHWyapZZWYBMXzPLTWjk0lJL7/ZfNakfCAXee3Wcc0cdCyeQ2fIxAIBDMVQlCmACax2B5TQeHfivmCXg8qqC0ZmmRHIhSUz//ijzjrO6u1Th06Z0pd7R/VSZUZG68UlGJtZo0iAsG/PKyNqyn3PP+G+rnISBXBVFDo325mFh3lBMVJbqpW6y0RqJFi2TksMMc6n5SCYpR4CC4FhXtLOEHxPE8zurY4tiVCubUW5sb339uVwy8/cQxu+svDrO0A4MXNQ+rnZT1S4hEIBLMXQlCmAKaCEqfE4/u+8pMAwGOvbm9qOxV1n0ioBdXMLgGA/3tuM25e/SoeeGkbnt4QTommGTYUahZFUBbXJgNbJZ6M3vHCDb5cjbnnOUZQjCRZINok25JJahOLnQP/FLnhg/eCx2nRHym6t1VlMTaRuCOixONSUN69Xzi2IWEE1PHtXQZbUlVcHhQgGOpH990En6HTKBhPIBAIZjKEoEwBbAWl+RLPcLGi1BAAeGxtkwSFDQsktaHfIEaVqo+v/Pcz6vcBRmDonBd2uhWUYUVQ9Hk7ZIalBb6dEQEC//m+F95Q5ahSjDbjXDqpddqoVuEk78QJFRQzzj5UUCpWhgrtHwjySGwFpbEH5T0H76x+Xr99RHuMG11dBlvTONvmMMNGge7dkbv3NL2NQCAQzEQIQZkCmCFncUo8JjFYu20k4pk6whJPQvk1+ox9vbxlGM9vCksCpOz4vm+VeAZGS6gyojRoKig1YpI3TLKkLgxpCkr480C+jKdqyk2pYvtIssZUYhU7n0pqnTYuD0qWE5iSrpK08hKPq4unFjmve1DIoJqCV8dHAgRhc/927qHoyKbwNyv30h7jBMWVoWIG2rn2H4UffuQInHrAUlz//oOb3kYgEAhmIqTNeApgl3iaV1DMchBNwW0EbpJVCopBUPg0XiAgIYDedUQlnqoPDBXL6MylUaqEhIAIitVmXFtU25UZNSQlpln3jcHADOr0kTAFxfd91cWTSye0tFfnROJUSGCUSTalm3dHtBwUW7lxeVASCQ/tmZQiLi6SAQDvWrEYT3zheKvUwluNXeTD/FscBeXw5T04fLmoJwKBYPZDFJQpgFniieNBMUnF1iYJCk+SJQWl31BBzOA3Ik78fDtbUmrh7q89zrcjhYWSYimtVikoGWozdpd4+DUWKxTP71ZQipUwRj9rlHiKNPDPtW05HPhHf+MlHreCEu7b9KAAho/E4UEhuHwgHQ1KPCbhiaOgCAQCwVyBEJQpQMGImB+OaPf96v88i4tufUwLdiNVg/JIqLOjESpssafBer6vG2VNgkJEgZ9vhm1Pj9M+sqnwsagclPZsWEohmKFtiqA0CFvjw/cCBSVUOYqO8pBLQQlNsrRtue404yCoTVdQAJ1kRCkoUdBLPDb5WNKlG2BdHheBQCCY6xCCMgWghFTCiCP2vVyp4sZ7/oT/euJ1PP162E1DqsbuC9sBBIt5yQh+c6Gkung8ZFPhTByawAvYcfXkUSEfSTaV0NpiqSREfpOOXEqVcIaL5aAEUwoTW4N/7aC2KAWl5BoWyDp1yIeS8ALixDtt3BOJ2baGSbY1Gyo7rm1p38VKVZ0fbxdub9CJUw9aiSdtkw8zAdbVJSQQCARzHUJQpgDNKCi8dJNkSV60OO42v1UZM5sx2ZYrYYkHgKWCAHUUFGPBpm1JzSFPRls2pRb6qh+oDVbUfYM2YyAkPqFJlueghFH3yiCbTsLzWIR/hI+Eqy/mNVGJZjBfcvtXWMsx+X7aI0o8rTE8IsG29Us8S7tDgpJKeNbgQYFAINgRIJ98UwAz6t41l2bzQEH9TJHvQKhqdLdm0FOLd3eVeX777GbcsvoV9buaxZPUSQY36A4ZCan9yoNSU1BqC7xJbvh2razjZKhQtnJQ2rKh14NgDi20Sjyudt9ShRlk9VbhfNmdJMuj7s2JxFRm4cRQGxbomATcEVHiia2g1PaTSnjOmUw7MYLSmklKnolAINghIQRlCkAKSgeLVzfBE2LzrC25XxGUtIqdNzt5CuUK/vL7D+PKXzyFP70RtA2X1DRjT20P6K3GRDSWduthbFEKiklQ2rIpJBKeWqCHC2WloNSbxWNmqAyYJZ5IBYU6cRK1f0MPilsF4VH3+jVR2WpdrW3b83QTbMJBHtomSEGhY7vUEwDYmZV4smkp7wgEgh0TQlCmAKRIzKsRjGGXgjIYKii8JESqRldLSFDMTp5HXwnD26j8onJQaiWe7pZMbX/htkQU6Bs7+VMKrJRCxwZCgjJsKC+tzIdiDQt0BLURientatH2W3C0GWcdJlk6rxzrxKHrjYq6J/8KdfGQikG5Mp25NBIJXanggWm5dELzxnAy0xqTRJB6E9Wds7A9q34ecqT/CgQCwY4AIShTAFp459VUDJeCsnmQKSglt4Iyv72moAwVtG3vfWGL+pnIT0hQEmp7vj+AKygmUdA7XmgxDwmKboRVRtlCxcpBaXMGtQXPWdKpKzeNTLKjJb30RASCX5O7A8g2yZKKoWb7sOGD4fZ84J/+uK6gxCMob+rtQHdrGocvn+98nBMlsxwmEAgEOwqkf3EKoAgKU1B839e8BZuYB4XnkNDi29WSxvy24Ju1qaDc90I4z4bIT9ko8bg8KEpBqZUUgqF60eWQ/lG9lZjUETUQsGArKB21hb1Y67TJpBJqaOESo7RULzBNK/HUiAMRoy2MsPEU1pDc2CbZTmPgHx8+SOAKihlvTx6UsZhYe9oyePBvjxPzq0AgENSBfELGBM8oaRa0OJLJterb4W1vRCgoVHbpask4SzxbhwoqKh5gCkpFV1BccfekhCzuyKlun4HRkqU2RJV4SDmgVuLhYtlqM+YhZqSiUJs15X30j5ZQqfqgDLm0s0xTsWL0iTS83p9Xz007ykO8RVmZZFt0wtFVe204ONlpz5oEJaWucywm1mxKzK8CgUBQD0JQYuD6u17A4V+5SxkrmwUpA12sjGC2+HIFRTPJMg9KWOIJCcq67aPafqh8oqYZJ3UPiquLpyOXUmWcvtGS5dcwCQodwy7xhAoKLe6pZEI9bzCvb08eFB6GBujTjLmCEnXcDX2jtevQVZAwqM2hChnP7XIoKPUICv0eJ4Y+DpYvaJuU/QoEAsFsgRCUGPjGqufxxmAB3/jf52JtR56OlnSSDanTvQXcg8JNsqqckk0yBSUkMybRod/Lqs1Y7+IZcOSgtGdT6K4pCP2jJeQjungGTQUloy/SQ8yD0uJY3CmBlq5pUUdWZbvwMo3L6Foos31naKpwcF7UAWWWYcKoe5YkW/tbNpXQjuMq8ey9uD28BmPfi2r+GXpNJhr/cvbBWNHbgW9/6JBJ2b9AIBDMdOxwBOW3z23G5bc/qZVR4sIkF43AMzp4OYRQqfrYwlQRUlAqVR+lWqkmm0o6PShDFkEJti2poLaaSZY8KKN2F09bNqV8Jn0jTEGhEo9hsB1RHhQ9zn4ob+egADwUrVbiKYbEiLphKAfG80JSxc8hX6oyf0vNg5ILQ+KC40QpKNVwmnHtb0FCbkg6XCbZj75jD/Xzq1uHtccO2LkLV532Znz5L/aztpsI7LOkE3dc/Hac8ObeSdm/QCAQzHTscATl2lXP44cPrMXqP20d8z7M4LVG4J6O0FAakpytwwUVrAaECgqfgszn3gyMRs+1od8rtRJPuuYt6awT1NaWTWkTj+3U1ZqCktc9JNReTI8PFUpuBUU9TgQlLNUQ+Vm/PSibtWdTmjeDDwscVdvRcXVVo9P4Pce2pWvinTm8zOMq8eyxsF2pKEfvsUB7zPM8nHPUbjhwWbe1nUAgEAjGjx2ui2dbLQfEnBIcB8XyWAlKqKBwYsFTZIFQQSkwL0o2ldCm8xLMeTpEApRJNql7Lkg18X1fRe63Z1Oaz0Sdb+14lPNRrFRRrlTVdqYXZNtwSZWWOEHpZLHyvu8zghIcdx1Gsb7mpTG9ITyufqSoqzMdEcZVc9uRUkURQJ4W29FSn6AAwE8/fjRue2Q9Tj1gqfNxgUAgEEwOZjVB4Qt1syAVwCyNNALv3mlmWB9HkU3L5amr5jkRSEEhopBMeEglw+m93KNielBoEadhgdSdQx03I6UKqlUfhXJVLdpt2aQ2M4fuK5VIeLlmhCkZbYaS8QZv983wDJEwTbZYCY/bkkkqQrKupqB0GkSBJ8kSqVOTks3W36x7W65O8RZmrrh0O7p4gmtL47xjljsfEwgEAsHkYVaXeJ7fOBjr+b7vKzJgLuyNwNuCx6WgsFAzglmmIYIQ+iYo2j34t1ipolpbdKncQkSErqtS0U2yRCZ8P1BoOEFry6RC4lQsh8dlhlLa/0ihovwzpKBQiWdzzaxK04YJ3CQ7yvw7rZmkUi7WbyMFxa2CAKHqZR5XHafOtgSNoDAy5PKgCAQCgWD6MKsJyqtb47X7jhRDqT8uQeHGWDPDpBFUAFk6qYgAJyWmmpNXCoqe6MrnstA5DKksk8BAS+ShZCTJ5tIJ1TEzXKiwTpxkME9HU1BsQymVeUaKZRUGR9u42n25j4R7WOg+ZpJBZgkRFIqcN0kHJxQ0xZmIR2s6CR4lElXiIZASReDlJFcXj0AgEAimD7OaoLyyZbjxkxh4KcX0bjRCvZJMI/AZM2EXD1dQKsbzK9p2RBRybLGm59B5Lay1vQ5HJMmaJIMbZIN/Q/OuSYyAsMwzUgwVFPobqR4Dtfsyr9WtbAwVSmGZprYttelurKkvZoBaOhmqN9uHdQUlkfDQnnFPGAYCQsI7gnLG8D9+rC5RUAQCgWBGYXYTlK1xCYqdAdIseFtw30ixzjNt8AwOIgIj7PjmuVgKSq3UkmKLNT1nqHZei2oKyog1iydcoHl5yRz451ZQwrcHPT5SdHlQjNh4w8/RydqMzbA1M0fENMny86AW6VZHC7P5c7gti743FJVGXTwCgUAgmD7MaoLycswSzwAjKEOOicL1wD0jw8VKLB+KKvE0UFCo/BPlQQFCFYByXIjoEEEJFRTyoDCSwcpLw0WDoLB8loIxlA8Ijal9I0VFfsh4a3o/TD+HMskygkIKCqXjEkyTLL/+7bUWaS3hlR3b9K8Ez7UHD5rHakknNSIjEAgEgunHrCYor24djjUbZyAfrVo0gvn8OCoKb9t1eVCILPTUFuu80cWjDcBjbbfBeVEqa07bV9no4gGgkaMh5SOhmTmhuuJWUILn8UC5VmMmDmGeoaBwD8qopaBktee6SUatzbl2Xq0RZR1TyQmuwR4eaB5LDLICgUAw8zCrCUq+VFXehWbginlvFmanzfaR5nNUeIqpq4uHzoUWa1JHXF4QU0EhL8mizlqJhxQUKvEwDwYvL9klnrD92UxdBcJ4eYqkz6QSynBqEhRLQaESTyFUblrTwd/mmyWeOgoKgZd4+IwcV4lHV1B0lWRBe1b7VyAQCAQzB7M6BwUAXn5jGEtqQ+caYTwmWfP522MoKJQ8y3NQOOEhUkGLNSkYpkkW0IPL+H6oxFOsVJEvVUDCUjoRLtBcQRk2TbKZ0GPSmrGJEaklRFDaMlyZSCKTTKjrtBWUMKiNXgMyqFolnjoKiuv39pxbTeHnFm6nE50jlvfg4pV7WSmxAoFAIJh+zGoFBQD+FKOTZ3AcJR5TQemLpaCEC75TQSlS9wuVeKI9KBlLQdFLPICuFCVdCoqziycMUzOTZIHQb/LGYEBQeJkF0NULs4ung+2b7hspJc2YZE1FhisonQ1Msr1d4X0xFZRUMoGLV+6Nw5f3WNsJBAKBYHox4QRlt912g+d51n8XXXQRACCfz+Oiiy7C/Pnz0d7ejjPOOAObNm0a8/Fer2VvNIOBcXTxmFklcTwoDRWUmi+D1AQiCHmjiwewFRS6jq6WtCIvPMbfqaA4u3jCEg+RH65UEClQCkpWX+w5Oeiq40GhTpzuluA52VRSK9O4SjzzDZ9KnBLP3os71M/Z9Kzn4wKBQLDDYMI/sR9++GG8/vrr6r9Vq1YBAN773vcCAD71qU/hV7/6FW677Tbcc8892LBhA97znveM+Xj5UvPdNLzNOG7U/YhV4olWULhxl08kzqWSiiTw4w8pD0qUgmIbPfO1+TI04bctG4bAcYKieVAyDgUloyso5aqvHtNMssqDEhCMFktBCYlFVA5KpepjU3/gGeJtvbzM41JQTJVFK/HU4u0zyYSzE2dFLyMoKSEoAoFAMFsw4R6UhQsXar9/9atfxR577IF3vOMd6O/vx/e+9z3ceuuteNe73gUAuOmmm7DPPvvggQcewJFHHuncZ6FQQKEQznkZGBgIH4sxj4eXeArlYPAdTxath2YUFN/38eGbHsbWoQJ+cdFbkUomFNkAgoWVvv1zYkXkp8fyoDhMsmnavqKpMG3ZFNqyKWwfKekEJSIHxeziaWWL/tbhYu243CRbU1AGbQ8KoCsZpgelLRMkvvo+1FBAXrbpacuoVGAzqI0eJyQ8/X6QauJSTwBDQZFWYoFAIJg1mNSvlMViET/84Q9x3nnnwfM8PProoyiVSli5cqV6zooVK7DLLrtg9erVkfu5+uqr0dXVpf5btmyZeiyOgjJgTDAejmGUNQfVuaYhr9s2inuffwNPbRjA6zWlgMfiZ1MJtdCPlmwPCplki+WqGuhH2/F90H7p/JMJD9lUQqkcdG7JhKdFzms5KEaJJxhGGOybBCDNJFvbdlBF5Ed7UEzPiOd56jhEUDQFhREQTnTU40xhaUkntWtqb0BQ9ljUpn6m8pJAIBAIZj4mlaDcfvvt6Ovrw7nnngsA2LhxIzKZDLq7u7XnLV68GBs3bozcz2WXXYb+/n7137p169RjY1VQgHhhbUQGltRMl9zPQnjk1W3WeZGCkkkmkEh4iuCMOoLa5rGFulCuMrOq3cWTL4WR84FC4SkjKxEUrp4AZg6KbpIN9qMv8ppJ1njM9qDwEo89GZhKN9QWzqPlyWPSlkk6FS2uoJilJdqvy7sC6KrJK1viBfsJBAKBYPowqW3G3/ve93DSSSdh6dKl49pPNptFNuvOqojnQdEJSRyjLJGB3q4cXtoyjIFRe9tHXt3O9q0TFFrsiWCMlirwfR+e56nzmK8RlIrW/UMglSNQUNxpsFEEhUjFKFdQmPLQlk2p8k5wXNskG/U7KRjppGc9BgQk4zVmaObD+SigLopk9DDCY+776D3nY+U+i3HqAUuc23K8FsNQLRAIBILpxaQpKK+++ip+85vf4CMf+Yj6W29vL4rFIvr6+rTnbtq0Cb29vWM6ThwFxVQ94gz9G1EKSotzXwDw6CuMoNQITd4wurawBZZ8MKSUdObS2qwdd4mn1sVTslUQIirUymuqEfW6ePh+wmNFKyhRJZ7u1oxWgiHwdl/AXeJxGWSBkMAAYYmN0JlL47sfPhSnHbiTc1sA+PvT3wIA+If/t3/kcwQCgUAwszBpBOWmm27CokWLcPLJJ6u/HXLIIUin07jrrrvU35577jmsXbsWRx111JiOUxiDgkIqRBwFhcgAlXhMctM/UsLzmwfV71TCIQJFx+QTdUeLFW0mT1s2pR4vlCvOqHtdQanFxteIxby2YIHfVCujpC2Cwrt49IF/wc/64u/yoEQ9lwiK2cFDWGoQFD5QkJJcowb28RKPGbbWDD505K74w5XH472HLmv8ZIFAIBDMCExKiadareKmm27Chz/8YaRSLB+jqwvnn38+LrnkEvT09KCzsxOf/OQncdRRR0V28DRCvkkFpcLaZ5d2teClLcOxCAqZZJd01zwohkn22Y0D4GOBiHiQgkIkI5VMqNTV0VIFJDakkx4yqQRy6SSGi5WagmKXeEhB4V087Vl9MvCLm4cA2GTBlSQbpaAkE56mwFglHkNtoXZfc5IxYUm3nvbLA9beuWIRTt5/Cd5zkFsF4Z6WQowhjRxdMm9HIBAIZhUmhaD85je/wdq1a3HeeedZj/3TP/0TEokEzjjjDBQKBZxwwgn4l3/5lzEfi7fx1gPvmlnYkcVLW4ZjZaEMOUyy5CEJftf3RVOG84aCQj8TQSFSQ+SB55y4clBoP/lS1coyoYX85Vq6rjljhjwog/mSlp9iPs7Pg2CWdHaepxOOw5f3YEF7FsfvuxguLGEKSkc2pZGfrpY0bvjAwc7tAF0JGm3y9RYIBALB7MakEJTjjz8+cspwLpfDDTfcgBtuuGFCjtXsN+oiex4pDWNSUGoelFLFR75UVZ6SoYLRwkwlHufgvSQGapN9K7WhflQy4Umxrsh55UEp2yoItePSoEBzzg2RDIqrB6K7eHabH7bnBtvqCgrPFwGAPRe14+G/O87pPwGgzUuKMsM2g2YJqUAgEAhmN2Z9tGazCxYRlFQizOTg/o96qFZ91Qq8oD2rjKzcKDsUoaCYHhQgNHpqrcK1c+KzdtxBbaGCQj6Y0P+hE5IoBaXGX5Cq5aeEj4cE5Yjd9fk0rUxdSSY8i8AAiCQngK6gjIegjDT5mgkEAoFgdmPWE5RmFZQSn4fDhtfFPUZrJqkIAfehmCWe0INSIyiOicSjpYrqDiJPh1NB0aLuQwWFCAq1Cpsza+a3uRUUQls2pQe5MRJyxPL5kdv2duYUkWoWiztDglKtutW1ZiAKikAgEOwYmPUEpdkFixb7TCoRKihNE5TwGNlUQrXDDtSZ7TNithlzBYXSZIt62BrtP9iu4pxmnGUKCh2fQtJ6jJLO/PboIXuAndrKM2XMCb+8vdf0nzQDTmhGSvHmIHF0tbhNuAKBQCCYW5gDBKUa6XfhIAUlnRy7gkKdLTQvhoe1UYmHyAS1AKsSD/egcAWlprSQQsHVlULdacYVq8TT02oSFP33dDKhEQUzDXY7my9kDuhLstC3ncZAUDhGx1CmufWCI7Dvkk786zmHjOvYAoFAIJgdmPUEBQCKlcZlHvKgZJIJ1ZbbtIJiKBn1FBQqZdgKip3Kmi+FRlciC0Q2BvNlK+SNn0O+VFWkiBSUlkxSUzpMDwqgqyamgvLRt++B3s4cvvjnb466FQCAnbvHR1DG4iM5eo8F+O+/eRsO2mXeuI4tEAgEgtmBOUFQmom7Lzo8KM0OCzTNqiFBCQkOqRmLOwNSMGJ6UBwqyGixos6BzonCyvpHS/WnGZcrGKx1DnUwosGVjwXtdjlkt/mt6mczOfZNvR144G+Pw4eP3s1xF4BlPQExOfWAsY0u+NxJKwAAX3/vAWPaXiAQCAQ7DiZ1Fs9kg/ydwUJevzOkxBSUsZZ4SMkISzxcQQl+XtShKyguo2tY4qmq55EHhQjKwGgpwiRbS5ItVVWLMp/ky2femB4UAHjz0i48trYPgHtycD38+hNvwxtDBey5qD3WdoSPvWMPnHXYssgwN4FAIBAICLOaoGRTCRTRXNx9gTwoKW/MJlnygnTUKfEsqiko5rDAnMsky9qMyYPiJCgRHhTiR3ySMCko2VTCiqMHgH2XdqqfTQWlEbpa0+NOZBVyIhAIBIJmMKsJSi6VQLHaXCdP0aGgjNuDMmqXeHojPCh8no6Wg6JKPKTOBPveOlxUCkkuwoNCxMlUUIDAf+LKJdl3SUhQ4iooAoFAIBBMFWb1CpVJJYFic1koPAeFTLLjLvE4gtrIJKtyUFQXT5QHxa2g8LRXl4LCz73dQVDMDh7Cm3rDBNg405wFAoFAIJhKzGqTbJiq2ryCkk4m1GC74WJFa1GuVn38+OG1eHbjgLZtpEmWeVAGjRKPSpJ1dPHwEg+ZadsNk+xmRlAySZugEDwPaM84CEqbm6Dw7dduG3Y+RyAQCASC6casJihhqmrzCko2lVDllErV17b9v+c347M/fRInXnuftq3pBaEyDHXxFMoVRYBUm3GpgmrVrxt1r3tQaupMjfxsGQoISiaZQIJlkJhD/NozKe3xI5b3IJtK4Ji9Fkbei3etWAQAOOuwXSKfIxAIBALBdGJWl3iCxboaW0HhQ/GGCmWlKqzfPqr+PpgvKfOpOfAvzCoJFBTerrywI1BQfD8o77ii7pUHpRhG3Zttxvo1Rv/O/ScAcOhuPfjjF0/QJgCbuOEDB+PZjQM4cFl35HMEAoFAIJhOzG4FRZlNm+jiYVH3iYSnFAtulOXzZp5c38+21Us8ZhcQ+U9aM0mt3DJcqLg7cViJZ6igKygWQUnrL1EqmUCKKSbtOZtj1iMnQFBiOmiXeXWH+wkEAoFAMJ2Y1QSFYtv5rJwolCqB14T8HK4sFOq8AYDH1/Wpn0OTrL4tKScUmNaeTWnkZ6RYrqugBB6UstoWCBQRzhtMzwmgz9ThLcYCgUAgEMwVzGqCkmOD8xpBlXgsFSQkN/znNU6CEhAD8rAMF8vwfd+aiUNKzEix4oy6b+FdPDSLp3Y+iYSntf8umxcmvxL2Whx24pglHoFAIBAI5gJmNUHJJpvv4lFtxkpBsUs8XEFZs65PdfgUSnpQGxEI3w9ICJV42mtqBu1bU1C0oLbg58F8WREnHqrGyzy7L2yzruUtLGxNFBSBQCAQzEXMboKSbr6Lh8/iAaCMsrzEwxWUNwYL6BspafunEk9LOgmygQwXymofNBOHFBTNg8JKPFS22TocthJz/4tOUOxY+Tfv1KV+lrA1gUAgEMxFzGqCMpYcFFJQXHH3XEEBgjRXwC7xeJ6nERzKQKF9tjEDrlNBMcy96aSniBMQthoDwO4LXApKSFCkxCMQCASCuYhZTVAyMXJQLAXFYZIlPwhh+wgRFHuqMDfKhiWelPbvQD6cp5NzBLWZ+yJw0uEq8ey1OFRV+kdK1uMCgUAgEMx2zGqCkkvFV1DSSXcnDhCmvxK2kYJSsluF21hc/hDr4gGAebWBeJsGWFx9ylZQ1L4yOkEZZkrOzg6TLG8jHmni2gUCgUAgmG2Y1QQlTJK1F+lyparF2BfLuoISzuMJFYjhYgRBcfhIeIloyOji6a5N/N04kFfP5wqK2TrcaigqW4eK6udkwp1V8venvwVLu3L4xDv3dD4uEAgEAsFsxqw2MGTTweJdMNqM86UKTvvm/cikEvjFRW9FIuGxLp5gm7DEwxSUWolnQXsGW4aKjKDUKfEUy1abMSkoG/sDgpJMeJrqkU0l4HlBFxAQthgTFrRnAQzWvfYPHbkrPnTkrnWfIxAIBALBbMWcUFDyhoLyn4+ux3ObBvHka/3YUuuUsRUU2yRLP+9UK6tsNxWUtE1QdJNsoJzMqykor9cISs6Ip/c8TxvmR2oO4e9PfwuO2XMBbv3IEY1vgkAgEAgEcxCznKDUkmSZguL7Pm7+/Svq9039NYJSifKg8C6egOgsm9cCwPag5BqUeNpViYc8KAFByTrSYHdfEBpdWw0PyvIFbfjhR47A0XsuiL54gUAgEAjmMGY1QSEvB1dQHlu7HS9sHlK/E0kwFRRnF0/tZzKmbjO7eJwm2YqVg0IlHiI4poICBCRE7StjExiBQCAQCHZkzGqCQm3Go6w9mE8kBkKjarFi5qCEcfVAoLwoBaUnUFCoxJMv2SZZrsAQQWk3TLIEs60Y0NuHTQ+KQCAQCAQ7OmY1QXHN0xnI6504m2sEhUyyaSNJlrYtVqooVwPXKs2/2VrHJNueCQkKmWTpfEyCsrS7xTp3UVAEAoFAIIjGrCYoPIuEMDCqB5dtNEo82YhpxiOM5Ow0T1dQXG3GfHtqVTa7eAjLeuwsEx5hb3pQBAKBQCDY0THLCUqwsA/mQ1IyUPu5s0YWNtbC0kqVQB2xpxkHBIVKPbl0otbmGyTL5kuViC6egKz0j5ZUCaij1sXTmkmqUhIA7OIgKPxvQ0ZAnEAgEAgEOzpmNUHpZCoGhbJRuWWvxR0AwhKPOYuHyM1IsYJqNfSftGVS6MylkKoFpG0fKYbTjB05KJR1EvwtnNXDyzwugsJn72xigW4CgUAgEAhmOUFpq6kkVR8YrZEIKvHstSgooVCJpxCRgwIE6gkpKa3ZJDzPw7y2sBOnXomH9t+STiLFVBNe5nERFAB4y06dAICT91sS57IFAoFAIJjzmNXmh5Z0EgkvICiD+TJaMyllkt2zRlD6RkrIlyqhSbZGInLphNp2uFBRZlkyz85vy+CNwQLeGCwo86xmklXlJb2Dh8DLQS4PCgDcesGReGHTIA7eZd447oJAIBAIBHMPs1pB8TzPIgrkR1nW06rUks0DhdAkW/ub53ma0ZU8KDQXx4yrBwwPimFs7TBahYdYN1FXi97VQ+jMpXHIrj3wPPe8HYFAIBAIdlRMCkF57bXX8MEPfhDz589HS0sL9ttvPzzyyCPqcd/3ceWVV2LJkiVoaWnBypUr8cILL4zpWB25YPEnoymVeDpzafR25gAEZRhTQQF0o+xIjaAQaemplXheZwQl49hW/W4oKGa7s0AgEAgEguYx4QRl+/bteOtb34p0Oo3/+Z//wdNPP41vfOMbmDcvLGNcc801uO6663DjjTfiwQcfRFtbG0444QTk8/HNotTaO6QUlODfzpYU5rcHJGPrUFimyTjKNMOFsirxkIJCBIUUlFTC0zwmbcb8nA6zxONIjxUIBAKBQNAcJtyD8rWvfQ3Lli3DTTfdpP62fPly9bPv+7j22mtx+eWX47TTTgMA3HLLLVi8eDFuv/12nHXWWbGO167KNIFyErYZp1VpZctQQT0/4+jEGeIKSq10QybZDf1BMq1JONpMBcX4/R/PPACf+PfH8flT9411PQKBQCAQCCZBQfnlL3+JQw89FO9973uxaNEiHHTQQfjXf/1X9fjLL7+MjRs3YuXKlepvXV1dOOKII7B69WrnPguFAgYGBrT/CFRaGciXUSxXVSYJJyhvDBXV89PJ0O+hFJQiU1BqykhPrU2YFBRz4F82lcCCdj6RWPeZHLH7fDz8dytxyv5LI+6UQCAQCASCKEw4QXnppZfwrW99C3vttRfuvPNOfPzjH8df//Vf4+abbwYAbNy4EQCwePFibbvFixerx0xcffXV6OrqUv8tW7ZMPaYUlHxZC2xrz6XQmXMoKI4yDR/4pzwotbA2RVAMBcXzPBy4rFv9bpZ4BAKBQCAQjB0TTlCq1SoOPvhgfOUrX8FBBx2ECy+8EBdccAFuvPHGMe/zsssuQ39/v/pv3bp16jHlQSmUlTG1PZtCMuGFCspgQFDSSU/rmOED//pGAnLT3RKoIj21Lp7BGnFxeUo4QTFLPAKBQCAQCMaOCScoS5Yswb776r6LffbZB2vXrgUA9Pb2AgA2bdqkPWfTpk3qMRPZbBadnZ3af4R25iMJO3iCv5keFK6e8G2HC2X017albea16SUbHtJGOHBZaPw1u3gEAoFAIBCMHRNOUN761rfiueee0/72/PPPY9dddwUQGGZ7e3tx1113qccHBgbw4IMP4qijjop9PGozHsyXWQdPuvZvQBqIoKQjjK6c3BBBmd+W1Z5LAwQ59tu5S/1MOSsCgUAgEAjGjwn/2v+pT30KRx99NL7yla/gzDPPxEMPPYTvfOc7+M53vgMg8G5cfPHF+NKXvoS99toLy5cvxxVXXIGlS5fi9NNPj328djYwkDp4OkwFZTAwydZTUPpGg+fQDB0+SwcAdl/QZh2bB7DVRgEJBAKBQCCYAEw4QTnssMPw85//HJdddhmuuuoqLF++HNdeey3OPvts9ZxLL70Uw8PDuPDCC9HX14djjjkGd9xxB3K5XOzjtedcJZ609i/N6cmYCkot82S4ULFKPLl0Em2ZJIZrQwR3X9juPP517z8IP3tsPT501K6xz10gEAgEAoEbk2KcOOWUU3DKKadEPu55Hq666ipcddVV4z5Wh9bFE5R4SEHpNCLmTQWFl3hMggIAPe0ZDG8LclB2X2grKADw5wcsxZ8fIK3EAoFAIBBMJGZ93KmmoFBIW41kmDNwTAWFSjzbhosqP6WLlXb4vJ0ogiIQCAQCgWDiMesJCjfJbh8JfCRdyiSrE5R0hIKyoS9QSRIe0M5IySCbp7OwXTfNCgQCgUAgmDzMeoLCTbKbB4JunUUdAZnoyKbABwUv7dY9LkRQtg4HxKazJY1EItyACA8AmTgsEAgEAsEUYtYTFB7UtqkWyLawIyAiiYSnPCoAsNeiDm1bM1yt21Bc5rVmIBAIBAKBYOox6wkKlXOqPvCnzUMAgMWdYTmGe0r2Wqx34pgTiU3PynXvPwgrejvwo48cMaHnLBAIBAKBoD5mffxpLp3Ewo4s3hgsqHk6izrDUk7Qahx4TPZcpBMUU0ExPSuH7DoPd1z89kk4a4FAIBAIBPUw6xUUAFhmpLxyQ2upEia87mFkmXTk0silw1vQLSUdgUAgEAhmBOYGQelpVT/3tGW0duLNg+Ek41xaL+kkEx4O2Llb/d7VMusFJYFAIBAI5gTmBkGZFxIU6uAh0JTiKByyazjwz/SgCAQCgUAgmB7MDYLSE5Z4uP8EAN5/+DIAwKkRaa+coHS3SIlHIBAIBIKZgDlR09iZKSiLDQXl8pP3xTF7LsSxb1ro3PagXUKC4kMm/gkEAoFAMBMwJwiKVuLp1AlKWzaFk/dfErltT1uomrRm5sTtEAgEAoFg1mNOrMhLunNIeEEWyqKO+BORbz7vcPzm6U044+CdJ+HsBAKBQCAQxMWcICjpZAJLulrwWt+oFtLWLN6x90K8Y293CUggEAgEAsHUY06YZAHgzEOXYY+FbThst57pPhWBQCAQCATjhOf7/qxzhg4MDKCrqwv9/f3o7Oyc7tMRCAQCgUDQBOKs33NGQREIBAKBQDB3IARFIBAIBALBjIMQFIFAIBAIBDMOQlAEAoFAIBDMOAhBEQgEAoFAMOMgBEUgEAgEAsGMgxAUgUAgEAgEMw5CUAQCgUAgEMw4CEERCAQCgUAw4yAERSAQCAQCwYyDEBSBQCAQCAQzDkJQBAKBQCAQzDgIQREIBAKBQDDjIARFIBAIBALBjENquk9gLPB9H0AwtlkgEAgEAsHsAK3btI7Xw6wkKIODgwCAZcuWTfOZCAQCgUAgiIvBwUF0dXXVfY7nN0NjZhiq1Sr23ntvPProo/A8L/b2hx12GB5++OExHXs6th0YGMCyZcuwbt06dHZ2TtlxZ+O2471X4zn2bNxW3lvNQ+5V85B71Tx2tHvl+z4OOeQQPP/880gk6rtMZqWCkkgkkMlkGrKvKCSTyTEvXtO1LQB0dnaOafvZeL3Tda/Ge+zZuC0g7604kHvVPOReNY8d6V5lMpmG5ASYxSbZiy66aIfadjyYjdc7XfdqvMeejduOB7PxeuVeTc2248FsvF65VxO/7aws8exoGBgYQFdXF/r7+8fF0HcEyL2KB7lfzUPuVfOQe9U85F5FY9YqKDsSstksPv/5zyObzU73qcx4yL2KB7lfzUPuVfOQe9U85F5FQxQUgUAgEAgEMw6ioAgEAoFAIJhxEIIiEAgEAoFgxkEIikAgEAgEghkHISgCgUAgEAhmHISgTBHuvfdenHrqqVi6dCk8z8Ptt9+uPb5p0yace+65WLp0KVpbW3HiiSfihRde0J6zceNGfOhDH0Jvby/a2tpw8MEH46c//an2nMceewx/9md/hu7ubsyfPx8XXnghhoaGJvvyJhQTca/+9Kc/4S/+4i+wcOFCdHZ24swzz8SmTZucxysUCjjwwAPheR7WrFkzSVc1OZiqezUX3ldXX301DjvsMHR0dGDRokU4/fTT8dxzz2nPyefzuOiiizB//ny0t7fjjDPOsO7F2rVrcfLJJ6O1tRWLFi3CZz7zGZTLZecx77//fqRSKRx44IGTdVmTgqm8VzfccAP22WcftLS04E1vehNuueWWSb++icZE3a+//uu/xiGHHIJsNtvwPfPiiy+io6MD3d3dE3w1MwdCUKYIw8PDOOCAA3DDDTdYj/m+j9NPPx0vvfQSfvGLX+Dxxx/HrrvuipUrV2J4eFg975xzzsFzzz2HX/7yl3jyySfxnve8B2eeeSYef/xxAMCGDRuwcuVK7LnnnnjwwQdxxx134KmnnsK55547VZc5IRjvvRoeHsbxxx8Pz/Nw99134/7770exWMSpp56KarVq7fPSSy/F0qVLJ/26JgNTca/myvvqnnvuwUUXXYQHHngAq1atQqlUwvHHH6/9P/apT30Kv/rVr3DbbbfhnnvuwYYNG/Ce97xHPV6pVHDyySejWCzi97//PW6++WZ8//vfx5VXXmkdr6+vD+eccw6OO+64Kbm+icRU3atvfetbuOyyy/CFL3wBTz31FL74xS/ioosuwq9+9aspvd7xYiLuF+G8887D+973vrrHK5VKeP/734+3ve1tE34tMwq+YMoBwP/5z3+ufn/uued8AP4f//hH9bdKpeIvXLjQ/9d//Vf1t7a2Nv+WW27R9tXT06Oe8+1vf9tftGiRX6lU1ONPPPGED8B/4YUXJulqJhdjuVd33nmnn0gk/P7+fvWcvr4+3/M8f9WqVdr+//u//9tfsWKF/9RTT/kA/Mcff3xSr2cyMVn3ai6+r3zf9zdv3uwD8O+55x7f94PrTqfT/m233aae88wzz/gA/NWrV/u+H7xfEomEv3HjRvWcb33rW35nZ6dfKBS0/b/vfe/zL7/8cv/zn/+8f8ABB0z+BU0iJuteHXXUUf6nP/1p7ViXXHKJ/9a3vnWyL2lSMZb7xdHoPXPppZf6H/zgB/2bbrrJ7+rqmujTnzEQBWUGoFAoAAByuZz6WyKRQDabxe9+9zv1t6OPPho//vGPsW3bNlSrVfzHf/wH8vk8jj32WLUfc8ZBS0sLAGj7mc1o5l4VCgV4nqcFH+VyOSQSCe0+bNq0CRdccAF+8IMfoLW1dYquYOowUfdqrr6v+vv7AQA9PT0AgEcffRSlUgkrV65Uz1mxYgV22WUXrF69GgCwevVq7Lfffli8eLF6zgknnICBgQE89dRT6m833XQTXnrpJXz+85+fikuZdEzWvSoUCtr7EwjeWw899BBKpdKkXtNkYiz3q1ncfffduO2225yq6VyDEJQZAHqjXnbZZdi+fTuKxSK+9rWvYf369Xj99dfV837yk5+gVCph/vz5yGaz+OhHP4qf//zn2HPPPQEA73rXu7Bx40b8wz/8A4rFIrZv347Pfe5zAKDtZzajmXt15JFHoq2tDZ/97GcxMjKC4eFhfPrTn0alUlHP8X0f5557Lj72sY/h0EMPnc5LmjRM1L2ai++rarWKiy++GG9961vxlre8BUDg8cpkMlZNf/Hixdi4caN6Dl9w6XF6DABeeOEFfO5zn8MPf/hDpFKzch6rhsm8VyeccAK++93v4tFHH4Xv+3jkkUfw3e9+F6VSCVu2bJnkK5scjPV+NYOtW7fi3HPPxfe///0dIhZfCMoMQDqdxs9+9jM8//zz6OnpQWtrK37729/ipJNO0r61XnHFFejr68NvfvMbPPLII7jkkktw5pln4sknnwQAvPnNb8bNN9+Mb3zjG2htbUVvby+WL1+OxYsXNzU5cjagmXu1cOFC3HbbbfjVr36F9vZ2dHV1oa+vDwcffLB6zvXXX4/BwUFcdtll03k5k4qJuldz8X110UUX4Y9//CP+4z/+Y0L3W6lU8IEPfABf/OIXsffee0/ovqcLk3WvgOAz7aSTTsKRRx6JdDqN0047DR/+8IcBQN5bDlxwwQX4wAc+gLe//e0Tvu8ZiemuMe2IgOEV4Ojr6/M3b97s+77vH3744f5f/dVf+b7v+y+++KLlJ/B93z/uuOP8j370o9Z+Nm7c6A8ODvpDQ0N+IpHwf/KTn0zsRUwRxnKvON544w1/+/btvu/7/uLFi/1rrrnG933fP+200/xEIuEnk0n1HwA/mUz655xzzqRcy2Rjsu4Vx1x4X1100UX+zjvv7L/00kva3++66y4fgLoHhF122cX/x3/8R9/3ff+KK66wvAEvvfSSD8B/7LHH/O3bt6v3Ef3neZ7621133TWZlzbhmMx7xVEsFv1169b55XLZ/5d/+Re/o6ND8zzNFoznfnFEeVC6urq091YikVDvre9973sTeSkzAkJQpgH1FhLC888/7ycSCf/OO+/0fT80JT799NPa844//nj/ggsuiNzP9773Pb+1tdX6H2O2YCz3yoW77rrL9zzPf/bZZ33f9/1XX33Vf/LJJ9V/d955pw/A/8///E9/3bp1E3kJU4bJulcuzMb3VbVa9S+66CJ/6f/f3v2FNNn2cQD/zr0azrJamaVlFqUnptlBoWBRVBgZlRAW0TI6aJOgP0pRdNBBNYomqR3UiRoShEjoQbiD3HZQiNRYuFo6DZcny4qwGuZ02+85iGc8e9SXF9rmre/3Azu5r2v3ff9+3IMvN9fFMjLE7XZPGf97IWNbW1v4WF9f37QLP0dGRsJzHj58KKmpqTI+Pi7BYDDiuXI6nWIwGCQ3N1ecTqf4fL7YFxoF8ejVTLZv3y7Hjh2LYjWxF41+/dNMAcXlckU8Wzdu3JBFixaJ0+mUb9++RbUmJWBAiZOfP3+Kw+EQh8MhAKS2tlYcDod8/PhRRERaW1vFarXKhw8fpL29XdauXSvl5eXh709MTMiGDRukpKREenp6ZHBwUO7evSsqlUqePXsWntfQ0CB2u136+/vl/v37kpycLHV1dXGv90/8aa9ERBobG6W7u1sGBwelpaVFtFqtXLx4ccZrDg0NzcldPPHq1Xx4rgwGgyxevFhsNpt4vd7wZ2xsLDxHr9dLVlaWWCwWef36tRQVFUlRUVF4PBAISF5enuzdu1fevHkjZrNZ0tLS5MqVKzNedy7u4olXr/r7+6WlpUXcbrf09PRIRUWFaLVaGRoaime5fywa/RIRGRgYEIfDIWfOnJGcnJzwb/vfO8T+Nt938TCgxInVahUAUz4nT54UEZG6ujpZvXq1JCYmSlZWlly7dm3KQ+l2u6W8vFxWrFghGo1G8vPzp2w7PnHihGi1WklKSpp2fC6IRq8uX74s6enpkpiYKBs3bhSTySShUGjGa87VgBKvXs2H52q6PgGQpqam8Jxfv35JVVWVLF26VDQajRw+fFi8Xm/EeTwej+zbt0+Sk5Nl+fLlUl1dLZOTkzNedy4GlHj1yuVyyebNmyU5OVlSU1Pl4MGD//XNnVJFq187duyY9jwzBbb5HlBUIiJ/vpKFiIiIKHrm5jJpIiIimtcYUIiIiEhxGFCIiIhIcRhQiIiISHEYUIiIiEhxGFCIiIhIcRhQiIiISHEYUIiIiEhxGFCIiIhIcRhQiCgmKisroVKpoFKpkJiYiPT0dOzZsweNjY0IhUL/83mam5uxZMmS2N0oESkSAwoRxUxpaSm8Xi88Hg86Ozuxc+dOnDt3DmVlZQgEArN9e0SkYAwoRBQzCxYswMqVK5GZmYktW7bg6tWr6OjoQGdnJ5qbmwEAtbW12LRpE1JSUrBmzRpUVVXB5/MBAGw2G06dOoXv37+H38Zcv34dAOD3+1FTU4PMzEykpKRg27ZtsNlss1MoEUUdAwoRxdWuXbtQUFCAp0+fAgASEhJQX1+Pd+/e4dGjR7BYLLh06RIAoLi4GPfu3UNqaiq8Xi+8Xi9qamoAAGfPnkV3dzeePHmC3t5eHDlyBKWlpRgYGJi12ogoevhvxkQUE5WVlRgdHUV7e/uUsaNHj6K3txcul2vKWFtbG/R6Pb5+/Qrg9xqU8+fPY3R0NDxneHgY69evx/DwMDIyMsLHd+/eja1bt+LWrVtRr4eI4us/s30DRPT/R0SgUqkAAM+fP4fRaERfXx9+/PiBQCCA8fFxjI2NQaPRTPt9p9OJYDCInJyciON+vx/Lli2L+f0TUewxoBBR3L1//x7r1q2Dx+NBWVkZDAYDbt68Ca1WixcvXuD06dOYmJiYMaD4fD6o1WrY7Xao1eqIsYULF8ajBCKKMQYUIoori8UCp9OJCxcuwG63IxQKwWQyISHh95K41tbWiPlJSUkIBoMRxwoLCxEMBvH582eUlJTE7d6JKH4YUIgoZvx+Pz59+oRgMIiRkRGYzWYYjUaUlZVBp9Ph7du3mJycRENDAw4cOICXL1/iwYMHEefIzs6Gz+dDV1cXCgoKoNFokJOTg+PHj0On08FkMqGwsBBfvnxBV1cX8vPzsX///lmqmIiihbt4iChmzGYzVq1ahezsbJSWlsJqtaK+vh4dHR1Qq9UoKChAbW0tbt++jby8PDx+/BhGozHiHMXFxdDr9aioqEBaWhru3LkDAGhqaoJOp0N1dTVyc3Nx6NAhvHr1CllZWbNRKhFFGXfxEBERkeLwDQoREREpDgMKERERKQ4DChERESkOAwoREREpDgMKERERKQ4DChERESkOAwoREREpDgMKERERKQ4DChERESkOAwoREREpDgMKERERKc5f64zSvctvCBAAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df.plot();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXj1Z6_vgSIZ"
      },
      "source": [
        "---\n",
        "### 2.1 Load and prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Yd1P3PV4nZ2X"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(dataset, look_back):\n",
        "    \"\"\"Transform a time series data into a prediction dataset\n",
        "\n",
        "    Args:\n",
        "        dataset: A numpy array of time series, first dimension is the time steps\n",
        "        look_back: Size of window for prediction\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    dataset = np.array(dataset)\n",
        "    data_length = len(dataset)\n",
        "    for i in range(look_back, data_length):\n",
        "        input = dataset[i-look_back: i]\n",
        "        output = dataset[i]\n",
        "        X.append(input)\n",
        "        y.append(output)\n",
        "\n",
        "    return torch.tensor(np.array(X)), torch.tensor(np.array(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsSFy2eNF5e-",
        "outputId": "38f01038-f712-40d6-e6a5-e18adce26674"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH2FMeIhh169",
        "outputId": "a495d89b-6b49-4fb8-b24e-d82986ad98ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X_train data -> torch.Size([237, 60, 1]) \n",
            "Shape of y_train data -> torch.Size([237, 1]) \n",
            "Shape of X_val data -> torch.Size([40, 60, 1]) \n",
            "Shape of y_val data -> torch.Size([40, 1]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "look_back = 60\n",
        "\n",
        "data_length = len(df)\n",
        "\n",
        "train_data_size = int(data_length * 0.75)\n",
        "validaion_data_size = int(data_length * 0.25)\n",
        "\n",
        "\n",
        "train_data = df[: train_data_size]\n",
        "validation_data = df[train_data_size: ]\n",
        "\n",
        "\n",
        "X_train, y_train = prepare_dataset(train_data, look_back)\n",
        "X_val, y_val = prepare_dataset(validation_data, look_back)\n",
        "\n",
        "\n",
        "X_train = X_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "X_val = X_val.to(device)\n",
        "y_val = y_val.to(device)\n",
        "\n",
        "\n",
        "print(  f\"Shape of X_train data -> {X_train.shape} \\n\"\n",
        "        f\"Shape of y_train data -> {y_train.shape} \\n\"\n",
        "        f\"Shape of X_val data -> {X_val.shape} \\n\"\n",
        "        f\"Shape of y_val data -> {y_val.shape} \\n\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u78V0kKvjp8f"
      },
      "source": [
        "---\n",
        "---\n",
        "## 3 Trainer function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oEIg5tjsjnEi"
      },
      "outputs": [],
      "source": [
        "def trainer(model, X_train, y_train, X_val, y_val, optimizer, criterion, n_epochs):\n",
        "    early_stopping_patience = 150\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    valid_loss_min=np.inf\n",
        "    best_model = copy.deepcopy(model)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "\n",
        "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "        X_val, y_val = X_val.to(device), y_val.to(device)\n",
        "\n",
        "        # Forward and loss\n",
        "        out = model(X_train)\n",
        "        loss = criterion(out, y_train)\n",
        "\n",
        "        # Backward and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            output_val = model(X_val)\n",
        "            valid_loss = criterion(output_val, y_val)\n",
        "            val_losses.append(valid_loss.item())\n",
        "\n",
        "            if valid_loss <= valid_loss_min:\n",
        "                best_model = best_model = copy.deepcopy(model)\n",
        "                print(f'Epoch {epoch + 0:01}: Validation loss decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}).')\n",
        "                valid_loss_min = valid_loss\n",
        "                early_stopping_counter = 0    # Reset counter if validation loss decreases\n",
        "            else:\n",
        "                print(f'Epoch {epoch + 0:01}: Validation loss did not decrease')\n",
        "                early_stopping_counter += 1\n",
        "\n",
        "            if early_stopping_counter > early_stopping_patience:\n",
        "                print('Early stopped at epoch :', epoch)\n",
        "                break\n",
        "\n",
        "            print(f'\\t Train_Loss: {loss:.4f} Val_Loss: {valid_loss:.4f}  BEST VAL Loss: {valid_loss_min:.4f}\\n')\n",
        "\n",
        "    return best_model, train_losses, val_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIfT0qVkOTzG"
      },
      "source": [
        "---\n",
        "---\n",
        "## 4 RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwmm5DhEBl8d"
      },
      "source": [
        "---\n",
        "### 4.1 Define single RNN cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "5YeW2H2cKsH7"
      },
      "outputs": [],
      "source": [
        "class RNNCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, bias=True, nonlinearity=\"tanh\"):\n",
        "        super(RNNCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bias = bias\n",
        "        self.nonlinearity = nonlinearity\n",
        "\n",
        "        # Define Needed Layers\n",
        "        self.input_w = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
        "        self.hidden_w = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "\n",
        "        if bias:\n",
        "            self.bias_ih = nn.Parameter(torch.Tensor(hidden_size)) # input2hidden\n",
        "            self.bias_hh = nn.Parameter(torch.Tensor(hidden_size)) # hidden2hidden\n",
        "        else:\n",
        "            self.register_parameter('bias_ih', None)\n",
        "            self.register_parameter('bias_hh', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        std = 1.0 / np.sqrt(self.hidden_size)\n",
        "        for w in self.parameters():\n",
        "            w.data.uniform_(-std, std)\n",
        "\n",
        "\n",
        "    def forward(self, input, hx=None):\n",
        "        if hx is None:\n",
        "            hx = torch.zeros(input.size(0), self.hidden_size, dtype=input.dtype, device=input.device)\n",
        "        gates = input @ self.input_w.t() + hx @ self.hidden_w.t()\n",
        "\n",
        "        if self.bias:\n",
        "            gates = gates + self.bias_ih + self.bias_hh\n",
        "\n",
        "        if self.nonlinearity == \"tanh\":\n",
        "            hy = torch.tanh(gates)\n",
        "        elif self.nonlinearity == \"relu\":\n",
        "            hy = torch.relu(gates)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid activation function\")\n",
        "        return hy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ay_sL8ZBq1R"
      },
      "source": [
        "---\n",
        "### 4.2 RNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KAcO44hi5HUf"
      },
      "outputs": [],
      "source": [
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, bias, output_size):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bias = bias\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.rnn = []\n",
        "        for _ in range(num_layers):\n",
        "            self.rnn.append(RNNCell(input_size, hidden_size, bias))\n",
        "        self.rnn = nn.ModuleList(self.rnn)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hx=None):\n",
        "        batch_size, sequence, _ = input.size()\n",
        "        if hx is None:\n",
        "            hx = [torch.zeros(batch_size, self.hidden_size, dtype=input.dtype, device=input.device) for _ in range(self.num_layers)]\n",
        "        out = []\n",
        "        for s in range(sequence):\n",
        "            i = input[:,s]\n",
        "            for layer in range(self.num_layers):\n",
        "                hx[layer] = self.rnn[layer](i, hx[layer])\n",
        "                i = hx[layer]\n",
        "            out.append(hx[-1])\n",
        "        out = torch.stack(out)\n",
        "        out = self.fc(out[-1])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msF4j7_fB6S5"
      },
      "source": [
        "---\n",
        "### 4.3 Train RNN model and plot losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwHvK1bI5GBz",
        "outputId": "a75074f9-f59b-4ad6-ff1d-02cacd0c9c6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SimpleRNN(\n",
              "  (rnn): ModuleList(\n",
              "    (0): RNNCell()\n",
              "  )\n",
              "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Instantiate model\n",
        "SimpleRNN_model = SimpleRNN(input_size=1, hidden_size=50, num_layers=1, bias=True, output_size=1)\n",
        "SimpleRNN_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "431lZ_uOlpsQ"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.008\n",
        "n_epochs = 2000\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(SimpleRNN_model.parameters(), lr = learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWMAFQBKQ5mq",
        "outputId": "011bedb9-12cb-4925-d578-e002b0215faf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Validation loss decreased (inf --> 10467.428711).\n",
            "\t Train_Loss: 7893.8696 Val_Loss: 10467.4287  BEST VAL Loss: 10467.4287\n",
            "\n",
            "Epoch 1: Validation loss decreased (10467.428711 --> 10349.484375).\n",
            "\t Train_Loss: 7788.5195 Val_Loss: 10349.4844  BEST VAL Loss: 10349.4844\n",
            "\n",
            "Epoch 2: Validation loss decreased (10349.484375 --> 10254.936523).\n",
            "\t Train_Loss: 7690.7451 Val_Loss: 10254.9365  BEST VAL Loss: 10254.9365\n",
            "\n",
            "Epoch 3: Validation loss decreased (10254.936523 --> 10170.878906).\n",
            "\t Train_Loss: 7610.3784 Val_Loss: 10170.8789  BEST VAL Loss: 10170.8789\n",
            "\n",
            "Epoch 4: Validation loss decreased (10170.878906 --> 10091.643555).\n",
            "\t Train_Loss: 7538.4692 Val_Loss: 10091.6436  BEST VAL Loss: 10091.6436\n",
            "\n",
            "Epoch 5: Validation loss decreased (10091.643555 --> 10012.592773).\n",
            "\t Train_Loss: 7470.1978 Val_Loss: 10012.5928  BEST VAL Loss: 10012.5928\n",
            "\n",
            "Epoch 6: Validation loss decreased (10012.592773 --> 9932.448242).\n",
            "\t Train_Loss: 7401.7241 Val_Loss: 9932.4482  BEST VAL Loss: 9932.4482\n",
            "\n",
            "Epoch 7: Validation loss decreased (9932.448242 --> 9849.732422).\n",
            "\t Train_Loss: 7331.6704 Val_Loss: 9849.7324  BEST VAL Loss: 9849.7324\n",
            "\n",
            "Epoch 8: Validation loss decreased (9849.732422 --> 9765.754883).\n",
            "\t Train_Loss: 7259.9438 Val_Loss: 9765.7549  BEST VAL Loss: 9765.7549\n",
            "\n",
            "Epoch 9: Validation loss decreased (9765.754883 --> 9686.881836).\n",
            "\t Train_Loss: 7189.6284 Val_Loss: 9686.8818  BEST VAL Loss: 9686.8818\n",
            "\n",
            "Epoch 10: Validation loss decreased (9686.881836 --> 9609.936523).\n",
            "\t Train_Loss: 7123.3271 Val_Loss: 9609.9365  BEST VAL Loss: 9609.9365\n",
            "\n",
            "Epoch 11: Validation loss decreased (9609.936523 --> 9528.887695).\n",
            "\t Train_Loss: 7057.0757 Val_Loss: 9528.8877  BEST VAL Loss: 9528.8877\n",
            "\n",
            "Epoch 12: Validation loss decreased (9528.887695 --> 9446.999023).\n",
            "\t Train_Loss: 6987.4775 Val_Loss: 9446.9990  BEST VAL Loss: 9446.9990\n",
            "\n",
            "Epoch 13: Validation loss decreased (9446.999023 --> 9371.897461).\n",
            "\t Train_Loss: 6918.4819 Val_Loss: 9371.8975  BEST VAL Loss: 9371.8975\n",
            "\n",
            "Epoch 14: Validation loss decreased (9371.897461 --> 9299.666992).\n",
            "\t Train_Loss: 6854.5557 Val_Loss: 9299.6670  BEST VAL Loss: 9299.6670\n",
            "\n",
            "Epoch 15: Validation loss decreased (9299.666992 --> 9229.514648).\n",
            "\t Train_Loss: 6793.3174 Val_Loss: 9229.5146  BEST VAL Loss: 9229.5146\n",
            "\n",
            "Epoch 16: Validation loss decreased (9229.514648 --> 9161.462891).\n",
            "\t Train_Loss: 6734.3110 Val_Loss: 9161.4629  BEST VAL Loss: 9161.4629\n",
            "\n",
            "Epoch 17: Validation loss decreased (9161.462891 --> 9093.770508).\n",
            "\t Train_Loss: 6676.8271 Val_Loss: 9093.7705  BEST VAL Loss: 9093.7705\n",
            "\n",
            "Epoch 18: Validation loss decreased (9093.770508 --> 9025.366211).\n",
            "\t Train_Loss: 6619.3130 Val_Loss: 9025.3662  BEST VAL Loss: 9025.3662\n",
            "\n",
            "Epoch 19: Validation loss decreased (9025.366211 --> 8956.331055).\n",
            "\t Train_Loss: 6561.2266 Val_Loss: 8956.3311  BEST VAL Loss: 8956.3311\n",
            "\n",
            "Epoch 20: Validation loss decreased (8956.331055 --> 8886.832031).\n",
            "\t Train_Loss: 6502.6655 Val_Loss: 8886.8320  BEST VAL Loss: 8886.8320\n",
            "\n",
            "Epoch 21: Validation loss decreased (8886.832031 --> 8816.998047).\n",
            "\t Train_Loss: 6443.7583 Val_Loss: 8816.9980  BEST VAL Loss: 8816.9980\n",
            "\n",
            "Epoch 22: Validation loss decreased (8816.998047 --> 8746.934570).\n",
            "\t Train_Loss: 6384.6104 Val_Loss: 8746.9346  BEST VAL Loss: 8746.9346\n",
            "\n",
            "Epoch 23: Validation loss decreased (8746.934570 --> 8676.726562).\n",
            "\t Train_Loss: 6325.3115 Val_Loss: 8676.7266  BEST VAL Loss: 8676.7266\n",
            "\n",
            "Epoch 24: Validation loss decreased (8676.726562 --> 8606.450195).\n",
            "\t Train_Loss: 6265.9351 Val_Loss: 8606.4502  BEST VAL Loss: 8606.4502\n",
            "\n",
            "Epoch 25: Validation loss decreased (8606.450195 --> 8536.173828).\n",
            "\t Train_Loss: 6206.5449 Val_Loss: 8536.1738  BEST VAL Loss: 8536.1738\n",
            "\n",
            "Epoch 26: Validation loss decreased (8536.173828 --> 8465.952148).\n",
            "\t Train_Loss: 6147.2002 Val_Loss: 8465.9521  BEST VAL Loss: 8465.9521\n",
            "\n",
            "Epoch 27: Validation loss decreased (8465.952148 --> 8395.831055).\n",
            "\t Train_Loss: 6087.9463 Val_Loss: 8395.8311  BEST VAL Loss: 8395.8311\n",
            "\n",
            "Epoch 28: Validation loss decreased (8395.831055 --> 8325.862305).\n",
            "\t Train_Loss: 6028.8257 Val_Loss: 8325.8623  BEST VAL Loss: 8325.8623\n",
            "\n",
            "Epoch 29: Validation loss decreased (8325.862305 --> 8256.076172).\n",
            "\t Train_Loss: 5969.8770 Val_Loss: 8256.0762  BEST VAL Loss: 8256.0762\n",
            "\n",
            "Epoch 30: Validation loss decreased (8256.076172 --> 8186.509277).\n",
            "\t Train_Loss: 5911.1304 Val_Loss: 8186.5093  BEST VAL Loss: 8186.5093\n",
            "\n",
            "Epoch 31: Validation loss decreased (8186.509277 --> 8117.189941).\n",
            "\t Train_Loss: 5852.6152 Val_Loss: 8117.1899  BEST VAL Loss: 8117.1899\n",
            "\n",
            "Epoch 32: Validation loss decreased (8117.189941 --> 8048.145508).\n",
            "\t Train_Loss: 5794.3564 Val_Loss: 8048.1455  BEST VAL Loss: 8048.1455\n",
            "\n",
            "Epoch 33: Validation loss decreased (8048.145508 --> 7979.395996).\n",
            "\t Train_Loss: 5736.3760 Val_Loss: 7979.3960  BEST VAL Loss: 7979.3960\n",
            "\n",
            "Epoch 34: Validation loss decreased (7979.395996 --> 7910.966309).\n",
            "\t Train_Loss: 5678.6919 Val_Loss: 7910.9663  BEST VAL Loss: 7910.9663\n",
            "\n",
            "Epoch 35: Validation loss decreased (7910.966309 --> 7842.869629).\n",
            "\t Train_Loss: 5621.3218 Val_Loss: 7842.8696  BEST VAL Loss: 7842.8696\n",
            "\n",
            "Epoch 36: Validation loss decreased (7842.869629 --> 7775.122559).\n",
            "\t Train_Loss: 5564.2808 Val_Loss: 7775.1226  BEST VAL Loss: 7775.1226\n",
            "\n",
            "Epoch 37: Validation loss decreased (7775.122559 --> 7707.739746).\n",
            "\t Train_Loss: 5507.5811 Val_Loss: 7707.7397  BEST VAL Loss: 7707.7397\n",
            "\n",
            "Epoch 38: Validation loss decreased (7707.739746 --> 7640.733887).\n",
            "\t Train_Loss: 5451.2339 Val_Loss: 7640.7339  BEST VAL Loss: 7640.7339\n",
            "\n",
            "Epoch 39: Validation loss decreased (7640.733887 --> 7574.112793).\n",
            "\t Train_Loss: 5395.2505 Val_Loss: 7574.1128  BEST VAL Loss: 7574.1128\n",
            "\n",
            "Epoch 40: Validation loss decreased (7574.112793 --> 7507.887695).\n",
            "\t Train_Loss: 5339.6377 Val_Loss: 7507.8877  BEST VAL Loss: 7507.8877\n",
            "\n",
            "Epoch 41: Validation loss decreased (7507.887695 --> 7442.064941).\n",
            "\t Train_Loss: 5284.4043 Val_Loss: 7442.0649  BEST VAL Loss: 7442.0649\n",
            "\n",
            "Epoch 42: Validation loss decreased (7442.064941 --> 7376.654785).\n",
            "\t Train_Loss: 5229.5557 Val_Loss: 7376.6548  BEST VAL Loss: 7376.6548\n",
            "\n",
            "Epoch 43: Validation loss decreased (7376.654785 --> 7311.659668).\n",
            "\t Train_Loss: 5175.0991 Val_Loss: 7311.6597  BEST VAL Loss: 7311.6597\n",
            "\n",
            "Epoch 44: Validation loss decreased (7311.659668 --> 7247.085938).\n",
            "\t Train_Loss: 5121.0366 Val_Loss: 7247.0859  BEST VAL Loss: 7247.0859\n",
            "\n",
            "Epoch 45: Validation loss decreased (7247.085938 --> 7182.938965).\n",
            "\t Train_Loss: 5067.3745 Val_Loss: 7182.9390  BEST VAL Loss: 7182.9390\n",
            "\n",
            "Epoch 46: Validation loss decreased (7182.938965 --> 7119.222168).\n",
            "\t Train_Loss: 5014.1157 Val_Loss: 7119.2222  BEST VAL Loss: 7119.2222\n",
            "\n",
            "Epoch 47: Validation loss decreased (7119.222168 --> 7055.937500).\n",
            "\t Train_Loss: 4961.2632 Val_Loss: 7055.9375  BEST VAL Loss: 7055.9375\n",
            "\n",
            "Epoch 48: Validation loss decreased (7055.937500 --> 6993.089844).\n",
            "\t Train_Loss: 4908.8179 Val_Loss: 6993.0898  BEST VAL Loss: 6993.0898\n",
            "\n",
            "Epoch 49: Validation loss decreased (6993.089844 --> 6930.679199).\n",
            "\t Train_Loss: 4856.7832 Val_Loss: 6930.6792  BEST VAL Loss: 6930.6792\n",
            "\n",
            "Epoch 50: Validation loss decreased (6930.679199 --> 6868.708008).\n",
            "\t Train_Loss: 4805.1592 Val_Loss: 6868.7080  BEST VAL Loss: 6868.7080\n",
            "\n",
            "Epoch 51: Validation loss decreased (6868.708008 --> 6807.176758).\n",
            "\t Train_Loss: 4753.9478 Val_Loss: 6807.1768  BEST VAL Loss: 6807.1768\n",
            "\n",
            "Epoch 52: Validation loss decreased (6807.176758 --> 6746.087402).\n",
            "\t Train_Loss: 4703.1494 Val_Loss: 6746.0874  BEST VAL Loss: 6746.0874\n",
            "\n",
            "Epoch 53: Validation loss decreased (6746.087402 --> 6685.440918).\n",
            "\t Train_Loss: 4652.7642 Val_Loss: 6685.4409  BEST VAL Loss: 6685.4409\n",
            "\n",
            "Epoch 54: Validation loss decreased (6685.440918 --> 6625.235840).\n",
            "\t Train_Loss: 4602.7925 Val_Loss: 6625.2358  BEST VAL Loss: 6625.2358\n",
            "\n",
            "Epoch 55: Validation loss decreased (6625.235840 --> 6565.472656).\n",
            "\t Train_Loss: 4553.2339 Val_Loss: 6565.4727  BEST VAL Loss: 6565.4727\n",
            "\n",
            "Epoch 56: Validation loss decreased (6565.472656 --> 6506.151855).\n",
            "\t Train_Loss: 4504.0879 Val_Loss: 6506.1519  BEST VAL Loss: 6506.1519\n",
            "\n",
            "Epoch 57: Validation loss decreased (6506.151855 --> 6447.272949).\n",
            "\t Train_Loss: 4455.3550 Val_Loss: 6447.2729  BEST VAL Loss: 6447.2729\n",
            "\n",
            "Epoch 58: Validation loss decreased (6447.272949 --> 6388.835449).\n",
            "\t Train_Loss: 4407.0327 Val_Loss: 6388.8354  BEST VAL Loss: 6388.8354\n",
            "\n",
            "Epoch 59: Validation loss decreased (6388.835449 --> 6330.837402).\n",
            "\t Train_Loss: 4359.1211 Val_Loss: 6330.8374  BEST VAL Loss: 6330.8374\n",
            "\n",
            "Epoch 60: Validation loss decreased (6330.837402 --> 6273.280273).\n",
            "\t Train_Loss: 4311.6191 Val_Loss: 6273.2803  BEST VAL Loss: 6273.2803\n",
            "\n",
            "Epoch 61: Validation loss decreased (6273.280273 --> 6216.160156).\n",
            "\t Train_Loss: 4264.5259 Val_Loss: 6216.1602  BEST VAL Loss: 6216.1602\n",
            "\n",
            "Epoch 62: Validation loss decreased (6216.160156 --> 6159.476562).\n",
            "\t Train_Loss: 4217.8389 Val_Loss: 6159.4766  BEST VAL Loss: 6159.4766\n",
            "\n",
            "Epoch 63: Validation loss decreased (6159.476562 --> 6103.230469).\n",
            "\t Train_Loss: 4171.5571 Val_Loss: 6103.2305  BEST VAL Loss: 6103.2305\n",
            "\n",
            "Epoch 64: Validation loss decreased (6103.230469 --> 6047.417969).\n",
            "\t Train_Loss: 4125.6802 Val_Loss: 6047.4180  BEST VAL Loss: 6047.4180\n",
            "\n",
            "Epoch 65: Validation loss decreased (6047.417969 --> 5992.037598).\n",
            "\t Train_Loss: 4080.2056 Val_Loss: 5992.0376  BEST VAL Loss: 5992.0376\n",
            "\n",
            "Epoch 66: Validation loss decreased (5992.037598 --> 5937.088867).\n",
            "\t Train_Loss: 4035.1309 Val_Loss: 5937.0889  BEST VAL Loss: 5937.0889\n",
            "\n",
            "Epoch 67: Validation loss decreased (5937.088867 --> 5882.569824).\n",
            "\t Train_Loss: 3990.4558 Val_Loss: 5882.5698  BEST VAL Loss: 5882.5698\n",
            "\n",
            "Epoch 68: Validation loss decreased (5882.569824 --> 5828.478027).\n",
            "\t Train_Loss: 3946.1777 Val_Loss: 5828.4780  BEST VAL Loss: 5828.4780\n",
            "\n",
            "Epoch 69: Validation loss decreased (5828.478027 --> 5774.812500).\n",
            "\t Train_Loss: 3902.2954 Val_Loss: 5774.8125  BEST VAL Loss: 5774.8125\n",
            "\n",
            "Epoch 70: Validation loss decreased (5774.812500 --> 5721.571289).\n",
            "\t Train_Loss: 3858.8059 Val_Loss: 5721.5713  BEST VAL Loss: 5721.5713\n",
            "\n",
            "Epoch 71: Validation loss decreased (5721.571289 --> 5668.752441).\n",
            "\t Train_Loss: 3815.7075 Val_Loss: 5668.7524  BEST VAL Loss: 5668.7524\n",
            "\n",
            "Epoch 72: Validation loss decreased (5668.752441 --> 5616.353027).\n",
            "\t Train_Loss: 3772.9995 Val_Loss: 5616.3530  BEST VAL Loss: 5616.3530\n",
            "\n",
            "Epoch 73: Validation loss decreased (5616.353027 --> 5564.372559).\n",
            "\t Train_Loss: 3730.6777 Val_Loss: 5564.3726  BEST VAL Loss: 5564.3726\n",
            "\n",
            "Epoch 74: Validation loss decreased (5564.372559 --> 5512.809570).\n",
            "\t Train_Loss: 3688.7419 Val_Loss: 5512.8096  BEST VAL Loss: 5512.8096\n",
            "\n",
            "Epoch 75: Validation loss decreased (5512.809570 --> 5461.658691).\n",
            "\t Train_Loss: 3647.1897 Val_Loss: 5461.6587  BEST VAL Loss: 5461.6587\n",
            "\n",
            "Epoch 76: Validation loss decreased (5461.658691 --> 5410.921875).\n",
            "\t Train_Loss: 3606.0178 Val_Loss: 5410.9219  BEST VAL Loss: 5410.9219\n",
            "\n",
            "Epoch 77: Validation loss decreased (5410.921875 --> 5360.593750).\n",
            "\t Train_Loss: 3565.2256 Val_Loss: 5360.5938  BEST VAL Loss: 5360.5938\n",
            "\n",
            "Epoch 78: Validation loss decreased (5360.593750 --> 5310.675293).\n",
            "\t Train_Loss: 3524.8096 Val_Loss: 5310.6753  BEST VAL Loss: 5310.6753\n",
            "\n",
            "Epoch 79: Validation loss decreased (5310.675293 --> 5261.162598).\n",
            "\t Train_Loss: 3484.7690 Val_Loss: 5261.1626  BEST VAL Loss: 5261.1626\n",
            "\n",
            "Epoch 80: Validation loss decreased (5261.162598 --> 5212.053223).\n",
            "\t Train_Loss: 3445.1011 Val_Loss: 5212.0532  BEST VAL Loss: 5212.0532\n",
            "\n",
            "Epoch 81: Validation loss decreased (5212.053223 --> 5163.345703).\n",
            "\t Train_Loss: 3405.8027 Val_Loss: 5163.3457  BEST VAL Loss: 5163.3457\n",
            "\n",
            "Epoch 82: Validation loss decreased (5163.345703 --> 5115.036621).\n",
            "\t Train_Loss: 3366.8728 Val_Loss: 5115.0366  BEST VAL Loss: 5115.0366\n",
            "\n",
            "Epoch 83: Validation loss decreased (5115.036621 --> 5067.126465).\n",
            "\t Train_Loss: 3328.3088 Val_Loss: 5067.1265  BEST VAL Loss: 5067.1265\n",
            "\n",
            "Epoch 84: Validation loss decreased (5067.126465 --> 5019.611816).\n",
            "\t Train_Loss: 3290.1086 Val_Loss: 5019.6118  BEST VAL Loss: 5019.6118\n",
            "\n",
            "Epoch 85: Validation loss decreased (5019.611816 --> 4972.488770).\n",
            "\t Train_Loss: 3252.2705 Val_Loss: 4972.4888  BEST VAL Loss: 4972.4888\n",
            "\n",
            "Epoch 86: Validation loss decreased (4972.488770 --> 4925.757812).\n",
            "\t Train_Loss: 3214.7908 Val_Loss: 4925.7578  BEST VAL Loss: 4925.7578\n",
            "\n",
            "Epoch 87: Validation loss decreased (4925.757812 --> 4879.414062).\n",
            "\t Train_Loss: 3177.6687 Val_Loss: 4879.4141  BEST VAL Loss: 4879.4141\n",
            "\n",
            "Epoch 88: Validation loss decreased (4879.414062 --> 4833.458008).\n",
            "\t Train_Loss: 3140.9006 Val_Loss: 4833.4580  BEST VAL Loss: 4833.4580\n",
            "\n",
            "Epoch 89: Validation loss decreased (4833.458008 --> 4787.886230).\n",
            "\t Train_Loss: 3104.4861 Val_Loss: 4787.8862  BEST VAL Loss: 4787.8862\n",
            "\n",
            "Epoch 90: Validation loss decreased (4787.886230 --> 4742.696289).\n",
            "\t Train_Loss: 3068.4214 Val_Loss: 4742.6963  BEST VAL Loss: 4742.6963\n",
            "\n",
            "Epoch 91: Validation loss decreased (4742.696289 --> 4697.886230).\n",
            "\t Train_Loss: 3032.7056 Val_Loss: 4697.8862  BEST VAL Loss: 4697.8862\n",
            "\n",
            "Epoch 92: Validation loss decreased (4697.886230 --> 4653.454102).\n",
            "\t Train_Loss: 2997.3347 Val_Loss: 4653.4541  BEST VAL Loss: 4653.4541\n",
            "\n",
            "Epoch 93: Validation loss decreased (4653.454102 --> 4609.396484).\n",
            "\t Train_Loss: 2962.3079 Val_Loss: 4609.3965  BEST VAL Loss: 4609.3965\n",
            "\n",
            "Epoch 94: Validation loss decreased (4609.396484 --> 4565.712402).\n",
            "\t Train_Loss: 2927.6228 Val_Loss: 4565.7124  BEST VAL Loss: 4565.7124\n",
            "\n",
            "Epoch 95: Validation loss decreased (4565.712402 --> 4522.399902).\n",
            "\t Train_Loss: 2893.2761 Val_Loss: 4522.3999  BEST VAL Loss: 4522.3999\n",
            "\n",
            "Epoch 96: Validation loss decreased (4522.399902 --> 4479.456543).\n",
            "\t Train_Loss: 2859.2671 Val_Loss: 4479.4565  BEST VAL Loss: 4479.4565\n",
            "\n",
            "Epoch 97: Validation loss decreased (4479.456543 --> 4436.878906).\n",
            "\t Train_Loss: 2825.5933 Val_Loss: 4436.8789  BEST VAL Loss: 4436.8789\n",
            "\n",
            "Epoch 98: Validation loss decreased (4436.878906 --> 4394.666504).\n",
            "\t Train_Loss: 2792.2512 Val_Loss: 4394.6665  BEST VAL Loss: 4394.6665\n",
            "\n",
            "Epoch 99: Validation loss decreased (4394.666504 --> 4352.815918).\n",
            "\t Train_Loss: 2759.2395 Val_Loss: 4352.8159  BEST VAL Loss: 4352.8159\n",
            "\n",
            "Epoch 100: Validation loss decreased (4352.815918 --> 4311.325684).\n",
            "\t Train_Loss: 2726.5562 Val_Loss: 4311.3257  BEST VAL Loss: 4311.3257\n",
            "\n",
            "Epoch 101: Validation loss decreased (4311.325684 --> 4270.193359).\n",
            "\t Train_Loss: 2694.1985 Val_Loss: 4270.1934  BEST VAL Loss: 4270.1934\n",
            "\n",
            "Epoch 102: Validation loss decreased (4270.193359 --> 4229.416016).\n",
            "\t Train_Loss: 2662.1648 Val_Loss: 4229.4160  BEST VAL Loss: 4229.4160\n",
            "\n",
            "Epoch 103: Validation loss decreased (4229.416016 --> 4188.992188).\n",
            "\t Train_Loss: 2630.4514 Val_Loss: 4188.9922  BEST VAL Loss: 4188.9922\n",
            "\n",
            "Epoch 104: Validation loss decreased (4188.992188 --> 4148.921387).\n",
            "\t Train_Loss: 2599.0581 Val_Loss: 4148.9214  BEST VAL Loss: 4148.9214\n",
            "\n",
            "Epoch 105: Validation loss decreased (4148.921387 --> 4109.198242).\n",
            "\t Train_Loss: 2567.9824 Val_Loss: 4109.1982  BEST VAL Loss: 4109.1982\n",
            "\n",
            "Epoch 106: Validation loss decreased (4109.198242 --> 4069.822266).\n",
            "\t Train_Loss: 2537.2209 Val_Loss: 4069.8223  BEST VAL Loss: 4069.8223\n",
            "\n",
            "Epoch 107: Validation loss decreased (4069.822266 --> 4030.792236).\n",
            "\t Train_Loss: 2506.7722 Val_Loss: 4030.7922  BEST VAL Loss: 4030.7922\n",
            "\n",
            "Epoch 108: Validation loss decreased (4030.792236 --> 3992.104004).\n",
            "\t Train_Loss: 2476.6343 Val_Loss: 3992.1040  BEST VAL Loss: 3992.1040\n",
            "\n",
            "Epoch 109: Validation loss decreased (3992.104004 --> 3953.756348).\n",
            "\t Train_Loss: 2446.8047 Val_Loss: 3953.7563  BEST VAL Loss: 3953.7563\n",
            "\n",
            "Epoch 110: Validation loss decreased (3953.756348 --> 3915.746826).\n",
            "\t Train_Loss: 2417.2810 Val_Loss: 3915.7468  BEST VAL Loss: 3915.7468\n",
            "\n",
            "Epoch 111: Validation loss decreased (3915.746826 --> 3878.072998).\n",
            "\t Train_Loss: 2388.0610 Val_Loss: 3878.0730  BEST VAL Loss: 3878.0730\n",
            "\n",
            "Epoch 112: Validation loss decreased (3878.072998 --> 3840.733643).\n",
            "\t Train_Loss: 2359.1428 Val_Loss: 3840.7336  BEST VAL Loss: 3840.7336\n",
            "\n",
            "Epoch 113: Validation loss decreased (3840.733643 --> 3803.727295).\n",
            "\t Train_Loss: 2330.5249 Val_Loss: 3803.7273  BEST VAL Loss: 3803.7273\n",
            "\n",
            "Epoch 114: Validation loss decreased (3803.727295 --> 3767.048828).\n",
            "\t Train_Loss: 2302.2041 Val_Loss: 3767.0488  BEST VAL Loss: 3767.0488\n",
            "\n",
            "Epoch 115: Validation loss decreased (3767.048828 --> 3730.699219).\n",
            "\t Train_Loss: 2274.1782 Val_Loss: 3730.6992  BEST VAL Loss: 3730.6992\n",
            "\n",
            "Epoch 116: Validation loss decreased (3730.699219 --> 3694.675049).\n",
            "\t Train_Loss: 2246.4460 Val_Loss: 3694.6750  BEST VAL Loss: 3694.6750\n",
            "\n",
            "Epoch 117: Validation loss decreased (3694.675049 --> 3658.973877).\n",
            "\t Train_Loss: 2219.0046 Val_Loss: 3658.9739  BEST VAL Loss: 3658.9739\n",
            "\n",
            "Epoch 118: Validation loss decreased (3658.973877 --> 3623.594482).\n",
            "\t Train_Loss: 2191.8523 Val_Loss: 3623.5945  BEST VAL Loss: 3623.5945\n",
            "\n",
            "Epoch 119: Validation loss decreased (3623.594482 --> 3588.534912).\n",
            "\t Train_Loss: 2164.9871 Val_Loss: 3588.5349  BEST VAL Loss: 3588.5349\n",
            "\n",
            "Epoch 120: Validation loss decreased (3588.534912 --> 3553.791016).\n",
            "\t Train_Loss: 2138.4065 Val_Loss: 3553.7910  BEST VAL Loss: 3553.7910\n",
            "\n",
            "Epoch 121: Validation loss decreased (3553.791016 --> 3519.363281).\n",
            "\t Train_Loss: 2112.1079 Val_Loss: 3519.3633  BEST VAL Loss: 3519.3633\n",
            "\n",
            "Epoch 122: Validation loss decreased (3519.363281 --> 3485.246826).\n",
            "\t Train_Loss: 2086.0903 Val_Loss: 3485.2468  BEST VAL Loss: 3485.2468\n",
            "\n",
            "Epoch 123: Validation loss decreased (3485.246826 --> 3451.443115).\n",
            "\t Train_Loss: 2060.3506 Val_Loss: 3451.4431  BEST VAL Loss: 3451.4431\n",
            "\n",
            "Epoch 124: Validation loss decreased (3451.443115 --> 3417.946533).\n",
            "\t Train_Loss: 2034.8876 Val_Loss: 3417.9465  BEST VAL Loss: 3417.9465\n",
            "\n",
            "Epoch 125: Validation loss decreased (3417.946533 --> 3384.757080).\n",
            "\t Train_Loss: 2009.6985 Val_Loss: 3384.7571  BEST VAL Loss: 3384.7571\n",
            "\n",
            "Epoch 126: Validation loss decreased (3384.757080 --> 3351.871826).\n",
            "\t Train_Loss: 1984.7812 Val_Loss: 3351.8718  BEST VAL Loss: 3351.8718\n",
            "\n",
            "Epoch 127: Validation loss decreased (3351.871826 --> 3319.289795).\n",
            "\t Train_Loss: 1960.1343 Val_Loss: 3319.2898  BEST VAL Loss: 3319.2898\n",
            "\n",
            "Epoch 128: Validation loss decreased (3319.289795 --> 3287.007812).\n",
            "\t Train_Loss: 1935.7555 Val_Loss: 3287.0078  BEST VAL Loss: 3287.0078\n",
            "\n",
            "Epoch 129: Validation loss decreased (3287.007812 --> 3255.023193).\n",
            "\t Train_Loss: 1911.6421 Val_Loss: 3255.0232  BEST VAL Loss: 3255.0232\n",
            "\n",
            "Epoch 130: Validation loss decreased (3255.023193 --> 3223.336426).\n",
            "\t Train_Loss: 1887.7922 Val_Loss: 3223.3364  BEST VAL Loss: 3223.3364\n",
            "\n",
            "Epoch 131: Validation loss decreased (3223.336426 --> 3191.943115).\n",
            "\t Train_Loss: 1864.2047 Val_Loss: 3191.9431  BEST VAL Loss: 3191.9431\n",
            "\n",
            "Epoch 132: Validation loss decreased (3191.943115 --> 3160.841553).\n",
            "\t Train_Loss: 1840.8766 Val_Loss: 3160.8416  BEST VAL Loss: 3160.8416\n",
            "\n",
            "Epoch 133: Validation loss decreased (3160.841553 --> 3130.031494).\n",
            "\t Train_Loss: 1817.8063 Val_Loss: 3130.0315  BEST VAL Loss: 3130.0315\n",
            "\n",
            "Epoch 134: Validation loss decreased (3130.031494 --> 3099.508301).\n",
            "\t Train_Loss: 1794.9917 Val_Loss: 3099.5083  BEST VAL Loss: 3099.5083\n",
            "\n",
            "Epoch 135: Validation loss decreased (3099.508301 --> 3069.271973).\n",
            "\t Train_Loss: 1772.4303 Val_Loss: 3069.2720  BEST VAL Loss: 3069.2720\n",
            "\n",
            "Epoch 136: Validation loss decreased (3069.271973 --> 3039.319092).\n",
            "\t Train_Loss: 1750.1210 Val_Loss: 3039.3191  BEST VAL Loss: 3039.3191\n",
            "\n",
            "Epoch 137: Validation loss decreased (3039.319092 --> 3009.648438).\n",
            "\t Train_Loss: 1728.0608 Val_Loss: 3009.6484  BEST VAL Loss: 3009.6484\n",
            "\n",
            "Epoch 138: Validation loss decreased (3009.648438 --> 2980.259033).\n",
            "\t Train_Loss: 1706.2480 Val_Loss: 2980.2590  BEST VAL Loss: 2980.2590\n",
            "\n",
            "Epoch 139: Validation loss decreased (2980.259033 --> 2951.146729).\n",
            "\t Train_Loss: 1684.6816 Val_Loss: 2951.1467  BEST VAL Loss: 2951.1467\n",
            "\n",
            "Epoch 140: Validation loss decreased (2951.146729 --> 2922.311035).\n",
            "\t Train_Loss: 1663.3582 Val_Loss: 2922.3110  BEST VAL Loss: 2922.3110\n",
            "\n",
            "Epoch 141: Validation loss decreased (2922.311035 --> 2893.748535).\n",
            "\t Train_Loss: 1642.2766 Val_Loss: 2893.7485  BEST VAL Loss: 2893.7485\n",
            "\n",
            "Epoch 142: Validation loss decreased (2893.748535 --> 2865.458984).\n",
            "\t Train_Loss: 1621.4342 Val_Loss: 2865.4590  BEST VAL Loss: 2865.4590\n",
            "\n",
            "Epoch 143: Validation loss decreased (2865.458984 --> 2837.439453).\n",
            "\t Train_Loss: 1600.8300 Val_Loss: 2837.4395  BEST VAL Loss: 2837.4395\n",
            "\n",
            "Epoch 144: Validation loss decreased (2837.439453 --> 2809.687988).\n",
            "\t Train_Loss: 1580.4612 Val_Loss: 2809.6880  BEST VAL Loss: 2809.6880\n",
            "\n",
            "Epoch 145: Validation loss decreased (2809.687988 --> 2782.203125).\n",
            "\t Train_Loss: 1560.3259 Val_Loss: 2782.2031  BEST VAL Loss: 2782.2031\n",
            "\n",
            "Epoch 146: Validation loss decreased (2782.203125 --> 2754.982178).\n",
            "\t Train_Loss: 1540.4226 Val_Loss: 2754.9822  BEST VAL Loss: 2754.9822\n",
            "\n",
            "Epoch 147: Validation loss decreased (2754.982178 --> 2728.023682).\n",
            "\t Train_Loss: 1520.7490 Val_Loss: 2728.0237  BEST VAL Loss: 2728.0237\n",
            "\n",
            "Epoch 148: Validation loss decreased (2728.023682 --> 2701.325439).\n",
            "\t Train_Loss: 1501.3035 Val_Loss: 2701.3254  BEST VAL Loss: 2701.3254\n",
            "\n",
            "Epoch 149: Validation loss decreased (2701.325439 --> 2674.885498).\n",
            "\t Train_Loss: 1482.0837 Val_Loss: 2674.8855  BEST VAL Loss: 2674.8855\n",
            "\n",
            "Epoch 150: Validation loss decreased (2674.885498 --> 2648.702148).\n",
            "\t Train_Loss: 1463.0880 Val_Loss: 2648.7021  BEST VAL Loss: 2648.7021\n",
            "\n",
            "Epoch 151: Validation loss decreased (2648.702148 --> 2622.774170).\n",
            "\t Train_Loss: 1444.3143 Val_Loss: 2622.7742  BEST VAL Loss: 2622.7742\n",
            "\n",
            "Epoch 152: Validation loss decreased (2622.774170 --> 2597.097656).\n",
            "\t Train_Loss: 1425.7611 Val_Loss: 2597.0977  BEST VAL Loss: 2597.0977\n",
            "\n",
            "Epoch 153: Validation loss decreased (2597.097656 --> 2571.673096).\n",
            "\t Train_Loss: 1407.4259 Val_Loss: 2571.6731  BEST VAL Loss: 2571.6731\n",
            "\n",
            "Epoch 154: Validation loss decreased (2571.673096 --> 2546.497070).\n",
            "\t Train_Loss: 1389.3075 Val_Loss: 2546.4971  BEST VAL Loss: 2546.4971\n",
            "\n",
            "Epoch 155: Validation loss decreased (2546.497070 --> 2521.567871).\n",
            "\t Train_Loss: 1371.4037 Val_Loss: 2521.5679  BEST VAL Loss: 2521.5679\n",
            "\n",
            "Epoch 156: Validation loss decreased (2521.567871 --> 2496.884033).\n",
            "\t Train_Loss: 1353.7120 Val_Loss: 2496.8840  BEST VAL Loss: 2496.8840\n",
            "\n",
            "Epoch 157: Validation loss decreased (2496.884033 --> 2472.443359).\n",
            "\t Train_Loss: 1336.2317 Val_Loss: 2472.4434  BEST VAL Loss: 2472.4434\n",
            "\n",
            "Epoch 158: Validation loss decreased (2472.443359 --> 2448.243164).\n",
            "\t Train_Loss: 1318.9601 Val_Loss: 2448.2432  BEST VAL Loss: 2448.2432\n",
            "\n",
            "Epoch 159: Validation loss decreased (2448.243164 --> 2424.283936).\n",
            "\t Train_Loss: 1301.8953 Val_Loss: 2424.2839  BEST VAL Loss: 2424.2839\n",
            "\n",
            "Epoch 160: Validation loss decreased (2424.283936 --> 2400.562256).\n",
            "\t Train_Loss: 1285.0363 Val_Loss: 2400.5623  BEST VAL Loss: 2400.5623\n",
            "\n",
            "Epoch 161: Validation loss decreased (2400.562256 --> 2377.075928).\n",
            "\t Train_Loss: 1268.3806 Val_Loss: 2377.0759  BEST VAL Loss: 2377.0759\n",
            "\n",
            "Epoch 162: Validation loss decreased (2377.075928 --> 2353.822998).\n",
            "\t Train_Loss: 1251.9264 Val_Loss: 2353.8230  BEST VAL Loss: 2353.8230\n",
            "\n",
            "Epoch 163: Validation loss decreased (2353.822998 --> 2330.802734).\n",
            "\t Train_Loss: 1235.6719 Val_Loss: 2330.8027  BEST VAL Loss: 2330.8027\n",
            "\n",
            "Epoch 164: Validation loss decreased (2330.802734 --> 2308.013428).\n",
            "\t Train_Loss: 1219.6154 Val_Loss: 2308.0134  BEST VAL Loss: 2308.0134\n",
            "\n",
            "Epoch 165: Validation loss decreased (2308.013428 --> 2285.451660).\n",
            "\t Train_Loss: 1203.7555 Val_Loss: 2285.4517  BEST VAL Loss: 2285.4517\n",
            "\n",
            "Epoch 166: Validation loss decreased (2285.451660 --> 2263.116943).\n",
            "\t Train_Loss: 1188.0894 Val_Loss: 2263.1169  BEST VAL Loss: 2263.1169\n",
            "\n",
            "Epoch 167: Validation loss decreased (2263.116943 --> 2241.007080).\n",
            "\t Train_Loss: 1172.6161 Val_Loss: 2241.0071  BEST VAL Loss: 2241.0071\n",
            "\n",
            "Epoch 168: Validation loss decreased (2241.007080 --> 2219.120361).\n",
            "\t Train_Loss: 1157.3339 Val_Loss: 2219.1204  BEST VAL Loss: 2219.1204\n",
            "\n",
            "Epoch 169: Validation loss decreased (2219.120361 --> 2197.454590).\n",
            "\t Train_Loss: 1142.2405 Val_Loss: 2197.4546  BEST VAL Loss: 2197.4546\n",
            "\n",
            "Epoch 170: Validation loss decreased (2197.454590 --> 2176.008545).\n",
            "\t Train_Loss: 1127.3341 Val_Loss: 2176.0085  BEST VAL Loss: 2176.0085\n",
            "\n",
            "Epoch 171: Validation loss decreased (2176.008545 --> 2154.780029).\n",
            "\t Train_Loss: 1112.6136 Val_Loss: 2154.7800  BEST VAL Loss: 2154.7800\n",
            "\n",
            "Epoch 172: Validation loss decreased (2154.780029 --> 2133.767578).\n",
            "\t Train_Loss: 1098.0768 Val_Loss: 2133.7676  BEST VAL Loss: 2133.7676\n",
            "\n",
            "Epoch 173: Validation loss decreased (2133.767578 --> 2112.969482).\n",
            "\t Train_Loss: 1083.7220 Val_Loss: 2112.9695  BEST VAL Loss: 2112.9695\n",
            "\n",
            "Epoch 174: Validation loss decreased (2112.969482 --> 2092.383789).\n",
            "\t Train_Loss: 1069.5477 Val_Loss: 2092.3838  BEST VAL Loss: 2092.3838\n",
            "\n",
            "Epoch 175: Validation loss decreased (2092.383789 --> 2072.009033).\n",
            "\t Train_Loss: 1055.5520 Val_Loss: 2072.0090  BEST VAL Loss: 2072.0090\n",
            "\n",
            "Epoch 176: Validation loss decreased (2072.009033 --> 2051.843506).\n",
            "\t Train_Loss: 1041.7333 Val_Loss: 2051.8435  BEST VAL Loss: 2051.8435\n",
            "\n",
            "Epoch 177: Validation loss decreased (2051.843506 --> 2031.884399).\n",
            "\t Train_Loss: 1028.0900 Val_Loss: 2031.8844  BEST VAL Loss: 2031.8844\n",
            "\n",
            "Epoch 178: Validation loss decreased (2031.884399 --> 2012.131226).\n",
            "\t Train_Loss: 1014.6198 Val_Loss: 2012.1312  BEST VAL Loss: 2012.1312\n",
            "\n",
            "Epoch 179: Validation loss decreased (2012.131226 --> 1992.581909).\n",
            "\t Train_Loss: 1001.3217 Val_Loss: 1992.5819  BEST VAL Loss: 1992.5819\n",
            "\n",
            "Epoch 180: Validation loss decreased (1992.581909 --> 1973.234741).\n",
            "\t Train_Loss: 988.1938 Val_Loss: 1973.2347  BEST VAL Loss: 1973.2347\n",
            "\n",
            "Epoch 181: Validation loss decreased (1973.234741 --> 1954.087891).\n",
            "\t Train_Loss: 975.2345 Val_Loss: 1954.0879  BEST VAL Loss: 1954.0879\n",
            "\n",
            "Epoch 182: Validation loss decreased (1954.087891 --> 1935.139526).\n",
            "\t Train_Loss: 962.4419 Val_Loss: 1935.1395  BEST VAL Loss: 1935.1395\n",
            "\n",
            "Epoch 183: Validation loss decreased (1935.139526 --> 1916.388306).\n",
            "\t Train_Loss: 949.8141 Val_Loss: 1916.3883  BEST VAL Loss: 1916.3883\n",
            "\n",
            "Epoch 184: Validation loss decreased (1916.388306 --> 1897.832886).\n",
            "\t Train_Loss: 937.3504 Val_Loss: 1897.8329  BEST VAL Loss: 1897.8329\n",
            "\n",
            "Epoch 185: Validation loss decreased (1897.832886 --> 1879.470703).\n",
            "\t Train_Loss: 925.0489 Val_Loss: 1879.4707  BEST VAL Loss: 1879.4707\n",
            "\n",
            "Epoch 186: Validation loss decreased (1879.470703 --> 1861.300781).\n",
            "\t Train_Loss: 912.9072 Val_Loss: 1861.3008  BEST VAL Loss: 1861.3008\n",
            "\n",
            "Epoch 187: Validation loss decreased (1861.300781 --> 1843.321533).\n",
            "\t Train_Loss: 900.9244 Val_Loss: 1843.3215  BEST VAL Loss: 1843.3215\n",
            "\n",
            "Epoch 188: Validation loss decreased (1843.321533 --> 1825.530151).\n",
            "\t Train_Loss: 889.0989 Val_Loss: 1825.5302  BEST VAL Loss: 1825.5302\n",
            "\n",
            "Epoch 189: Validation loss decreased (1825.530151 --> 1807.926392).\n",
            "\t Train_Loss: 877.4283 Val_Loss: 1807.9264  BEST VAL Loss: 1807.9264\n",
            "\n",
            "Epoch 190: Validation loss decreased (1807.926392 --> 1790.508179).\n",
            "\t Train_Loss: 865.9123 Val_Loss: 1790.5082  BEST VAL Loss: 1790.5082\n",
            "\n",
            "Epoch 191: Validation loss decreased (1790.508179 --> 1773.274414).\n",
            "\t Train_Loss: 854.5484 Val_Loss: 1773.2744  BEST VAL Loss: 1773.2744\n",
            "\n",
            "Epoch 192: Validation loss decreased (1773.274414 --> 1756.221924).\n",
            "\t Train_Loss: 843.3357 Val_Loss: 1756.2219  BEST VAL Loss: 1756.2219\n",
            "\n",
            "Epoch 193: Validation loss decreased (1756.221924 --> 1739.350830).\n",
            "\t Train_Loss: 832.2717 Val_Loss: 1739.3508  BEST VAL Loss: 1739.3508\n",
            "\n",
            "Epoch 194: Validation loss decreased (1739.350830 --> 1722.659058).\n",
            "\t Train_Loss: 821.3557 Val_Loss: 1722.6591  BEST VAL Loss: 1722.6591\n",
            "\n",
            "Epoch 195: Validation loss decreased (1722.659058 --> 1706.144409).\n",
            "\t Train_Loss: 810.5861 Val_Loss: 1706.1444  BEST VAL Loss: 1706.1444\n",
            "\n",
            "Epoch 196: Validation loss decreased (1706.144409 --> 1689.805908).\n",
            "\t Train_Loss: 799.9610 Val_Loss: 1689.8059  BEST VAL Loss: 1689.8059\n",
            "\n",
            "Epoch 197: Validation loss decreased (1689.805908 --> 1673.641968).\n",
            "\t Train_Loss: 789.4790 Val_Loss: 1673.6420  BEST VAL Loss: 1673.6420\n",
            "\n",
            "Epoch 198: Validation loss decreased (1673.641968 --> 1657.650024).\n",
            "\t Train_Loss: 779.1390 Val_Loss: 1657.6500  BEST VAL Loss: 1657.6500\n",
            "\n",
            "Epoch 199: Validation loss decreased (1657.650024 --> 1641.830078).\n",
            "\t Train_Loss: 768.9385 Val_Loss: 1641.8301  BEST VAL Loss: 1641.8301\n",
            "\n",
            "Epoch 200: Validation loss decreased (1641.830078 --> 1626.180420).\n",
            "\t Train_Loss: 758.8770 Val_Loss: 1626.1804  BEST VAL Loss: 1626.1804\n",
            "\n",
            "Epoch 201: Validation loss decreased (1626.180420 --> 1610.698486).\n",
            "\t Train_Loss: 748.9529 Val_Loss: 1610.6985  BEST VAL Loss: 1610.6985\n",
            "\n",
            "Epoch 202: Validation loss decreased (1610.698486 --> 1595.383667).\n",
            "\t Train_Loss: 739.1643 Val_Loss: 1595.3837  BEST VAL Loss: 1595.3837\n",
            "\n",
            "Epoch 203: Validation loss decreased (1595.383667 --> 1580.233643).\n",
            "\t Train_Loss: 729.5102 Val_Loss: 1580.2336  BEST VAL Loss: 1580.2336\n",
            "\n",
            "Epoch 204: Validation loss decreased (1580.233643 --> 1565.247314).\n",
            "\t Train_Loss: 719.9888 Val_Loss: 1565.2473  BEST VAL Loss: 1565.2473\n",
            "\n",
            "Epoch 205: Validation loss decreased (1565.247314 --> 1550.423218).\n",
            "\t Train_Loss: 710.5988 Val_Loss: 1550.4232  BEST VAL Loss: 1550.4232\n",
            "\n",
            "Epoch 206: Validation loss decreased (1550.423218 --> 1535.760376).\n",
            "\t Train_Loss: 701.3384 Val_Loss: 1535.7604  BEST VAL Loss: 1535.7604\n",
            "\n",
            "Epoch 207: Validation loss decreased (1535.760376 --> 1521.255981).\n",
            "\t Train_Loss: 692.2069 Val_Loss: 1521.2560  BEST VAL Loss: 1521.2560\n",
            "\n",
            "Epoch 208: Validation loss decreased (1521.255981 --> 1506.910400).\n",
            "\t Train_Loss: 683.2021 Val_Loss: 1506.9104  BEST VAL Loss: 1506.9104\n",
            "\n",
            "Epoch 209: Validation loss decreased (1506.910400 --> 1492.719849).\n",
            "\t Train_Loss: 674.3237 Val_Loss: 1492.7198  BEST VAL Loss: 1492.7198\n",
            "\n",
            "Epoch 210: Validation loss decreased (1492.719849 --> 1478.686157).\n",
            "\t Train_Loss: 665.5688 Val_Loss: 1478.6862  BEST VAL Loss: 1478.6862\n",
            "\n",
            "Epoch 211: Validation loss decreased (1478.686157 --> 1464.804565).\n",
            "\t Train_Loss: 656.9380 Val_Loss: 1464.8046  BEST VAL Loss: 1464.8046\n",
            "\n",
            "Epoch 212: Validation loss decreased (1464.804565 --> 1451.074829).\n",
            "\t Train_Loss: 648.4277 Val_Loss: 1451.0748  BEST VAL Loss: 1451.0748\n",
            "\n",
            "Epoch 213: Validation loss decreased (1451.074829 --> 1437.496094).\n",
            "\t Train_Loss: 640.0378 Val_Loss: 1437.4961  BEST VAL Loss: 1437.4961\n",
            "\n",
            "Epoch 214: Validation loss decreased (1437.496094 --> 1424.066650).\n",
            "\t Train_Loss: 631.7669 Val_Loss: 1424.0667  BEST VAL Loss: 1424.0667\n",
            "\n",
            "Epoch 215: Validation loss decreased (1424.066650 --> 1410.785034).\n",
            "\t Train_Loss: 623.6137 Val_Loss: 1410.7850  BEST VAL Loss: 1410.7850\n",
            "\n",
            "Epoch 216: Validation loss decreased (1410.785034 --> 1397.649902).\n",
            "\t Train_Loss: 615.5766 Val_Loss: 1397.6499  BEST VAL Loss: 1397.6499\n",
            "\n",
            "Epoch 217: Validation loss decreased (1397.649902 --> 1384.659546).\n",
            "\t Train_Loss: 607.6543 Val_Loss: 1384.6595  BEST VAL Loss: 1384.6595\n",
            "\n",
            "Epoch 218: Validation loss decreased (1384.659546 --> 1371.813354).\n",
            "\t Train_Loss: 599.8456 Val_Loss: 1371.8134  BEST VAL Loss: 1371.8134\n",
            "\n",
            "Epoch 219: Validation loss decreased (1371.813354 --> 1359.108154).\n",
            "\t Train_Loss: 592.1492 Val_Loss: 1359.1082  BEST VAL Loss: 1359.1082\n",
            "\n",
            "Epoch 220: Validation loss decreased (1359.108154 --> 1346.544922).\n",
            "\t Train_Loss: 584.5632 Val_Loss: 1346.5449  BEST VAL Loss: 1346.5449\n",
            "\n",
            "Epoch 221: Validation loss decreased (1346.544922 --> 1334.121338).\n",
            "\t Train_Loss: 577.0875 Val_Loss: 1334.1213  BEST VAL Loss: 1334.1213\n",
            "\n",
            "Epoch 222: Validation loss decreased (1334.121338 --> 1321.835815).\n",
            "\t Train_Loss: 569.7201 Val_Loss: 1321.8358  BEST VAL Loss: 1321.8358\n",
            "\n",
            "Epoch 223: Validation loss decreased (1321.835815 --> 1309.686768).\n",
            "\t Train_Loss: 562.4598 Val_Loss: 1309.6868  BEST VAL Loss: 1309.6868\n",
            "\n",
            "Epoch 224: Validation loss decreased (1309.686768 --> 1297.673828).\n",
            "\t Train_Loss: 555.3051 Val_Loss: 1297.6738  BEST VAL Loss: 1297.6738\n",
            "\n",
            "Epoch 225: Validation loss decreased (1297.673828 --> 1285.794556).\n",
            "\t Train_Loss: 548.2554 Val_Loss: 1285.7946  BEST VAL Loss: 1285.7946\n",
            "\n",
            "Epoch 226: Validation loss decreased (1285.794556 --> 1274.048828).\n",
            "\t Train_Loss: 541.3087 Val_Loss: 1274.0488  BEST VAL Loss: 1274.0488\n",
            "\n",
            "Epoch 227: Validation loss decreased (1274.048828 --> 1262.434326).\n",
            "\t Train_Loss: 534.4645 Val_Loss: 1262.4343  BEST VAL Loss: 1262.4343\n",
            "\n",
            "Epoch 228: Validation loss decreased (1262.434326 --> 1250.950073).\n",
            "\t Train_Loss: 527.7211 Val_Loss: 1250.9501  BEST VAL Loss: 1250.9501\n",
            "\n",
            "Epoch 229: Validation loss decreased (1250.950073 --> 1239.595093).\n",
            "\t Train_Loss: 521.0773 Val_Loss: 1239.5951  BEST VAL Loss: 1239.5951\n",
            "\n",
            "Epoch 230: Validation loss decreased (1239.595093 --> 1228.368408).\n",
            "\t Train_Loss: 514.5322 Val_Loss: 1228.3684  BEST VAL Loss: 1228.3684\n",
            "\n",
            "Epoch 231: Validation loss decreased (1228.368408 --> 1217.267944).\n",
            "\t Train_Loss: 508.0846 Val_Loss: 1217.2679  BEST VAL Loss: 1217.2679\n",
            "\n",
            "Epoch 232: Validation loss decreased (1217.267944 --> 1206.292969).\n",
            "\t Train_Loss: 501.7331 Val_Loss: 1206.2930  BEST VAL Loss: 1206.2930\n",
            "\n",
            "Epoch 233: Validation loss decreased (1206.292969 --> 1195.441284).\n",
            "\t Train_Loss: 495.4768 Val_Loss: 1195.4413  BEST VAL Loss: 1195.4413\n",
            "\n",
            "Epoch 234: Validation loss decreased (1195.441284 --> 1184.712769).\n",
            "\t Train_Loss: 489.3140 Val_Loss: 1184.7128  BEST VAL Loss: 1184.7128\n",
            "\n",
            "Epoch 235: Validation loss decreased (1184.712769 --> 1174.106323).\n",
            "\t Train_Loss: 483.2439 Val_Loss: 1174.1063  BEST VAL Loss: 1174.1063\n",
            "\n",
            "Epoch 236: Validation loss decreased (1174.106323 --> 1163.620483).\n",
            "\t Train_Loss: 477.2657 Val_Loss: 1163.6205  BEST VAL Loss: 1163.6205\n",
            "\n",
            "Epoch 237: Validation loss decreased (1163.620483 --> 1153.253296).\n",
            "\t Train_Loss: 471.3779 Val_Loss: 1153.2533  BEST VAL Loss: 1153.2533\n",
            "\n",
            "Epoch 238: Validation loss decreased (1153.253296 --> 1143.004272).\n",
            "\t Train_Loss: 465.5794 Val_Loss: 1143.0043  BEST VAL Loss: 1143.0043\n",
            "\n",
            "Epoch 239: Validation loss decreased (1143.004272 --> 1132.871948).\n",
            "\t Train_Loss: 459.8690 Val_Loss: 1132.8719  BEST VAL Loss: 1132.8719\n",
            "\n",
            "Epoch 240: Validation loss decreased (1132.871948 --> 1122.854858).\n",
            "\t Train_Loss: 454.2458 Val_Loss: 1122.8549  BEST VAL Loss: 1122.8549\n",
            "\n",
            "Epoch 241: Validation loss decreased (1122.854858 --> 1112.952637).\n",
            "\t Train_Loss: 448.7085 Val_Loss: 1112.9526  BEST VAL Loss: 1112.9526\n",
            "\n",
            "Epoch 242: Validation loss decreased (1112.952637 --> 1103.163940).\n",
            "\t Train_Loss: 443.2563 Val_Loss: 1103.1639  BEST VAL Loss: 1103.1639\n",
            "\n",
            "Epoch 243: Validation loss decreased (1103.163940 --> 1093.486694).\n",
            "\t Train_Loss: 437.8880 Val_Loss: 1093.4867  BEST VAL Loss: 1093.4867\n",
            "\n",
            "Epoch 244: Validation loss decreased (1093.486694 --> 1083.921265).\n",
            "\t Train_Loss: 432.6023 Val_Loss: 1083.9213  BEST VAL Loss: 1083.9213\n",
            "\n",
            "Epoch 245: Validation loss decreased (1083.921265 --> 1074.465088).\n",
            "\t Train_Loss: 427.3987 Val_Loss: 1074.4651  BEST VAL Loss: 1074.4651\n",
            "\n",
            "Epoch 246: Validation loss decreased (1074.465088 --> 1065.118042).\n",
            "\t Train_Loss: 422.2755 Val_Loss: 1065.1180  BEST VAL Loss: 1065.1180\n",
            "\n",
            "Epoch 247: Validation loss decreased (1065.118042 --> 1055.878296).\n",
            "\t Train_Loss: 417.2322 Val_Loss: 1055.8783  BEST VAL Loss: 1055.8783\n",
            "\n",
            "Epoch 248: Validation loss decreased (1055.878296 --> 1046.744751).\n",
            "\t Train_Loss: 412.2675 Val_Loss: 1046.7448  BEST VAL Loss: 1046.7448\n",
            "\n",
            "Epoch 249: Validation loss decreased (1046.744751 --> 1037.717041).\n",
            "\t Train_Loss: 407.3803 Val_Loss: 1037.7170  BEST VAL Loss: 1037.7170\n",
            "\n",
            "Epoch 250: Validation loss decreased (1037.717041 --> 1028.793457).\n",
            "\t Train_Loss: 402.5699 Val_Loss: 1028.7935  BEST VAL Loss: 1028.7935\n",
            "\n",
            "Epoch 251: Validation loss decreased (1028.793457 --> 1019.972839).\n",
            "\t Train_Loss: 397.8351 Val_Loss: 1019.9728  BEST VAL Loss: 1019.9728\n",
            "\n",
            "Epoch 252: Validation loss decreased (1019.972839 --> 1011.255310).\n",
            "\t Train_Loss: 393.1749 Val_Loss: 1011.2553  BEST VAL Loss: 1011.2553\n",
            "\n",
            "Epoch 253: Validation loss decreased (1011.255310 --> 1002.637512).\n",
            "\t Train_Loss: 388.5887 Val_Loss: 1002.6375  BEST VAL Loss: 1002.6375\n",
            "\n",
            "Epoch 254: Validation loss decreased (1002.637512 --> 994.120728).\n",
            "\t Train_Loss: 384.0745 Val_Loss: 994.1207  BEST VAL Loss: 994.1207\n",
            "\n",
            "Epoch 255: Validation loss decreased (994.120728 --> 985.702148).\n",
            "\t Train_Loss: 379.6328 Val_Loss: 985.7021  BEST VAL Loss: 985.7021\n",
            "\n",
            "Epoch 256: Validation loss decreased (985.702148 --> 977.382141).\n",
            "\t Train_Loss: 375.2613 Val_Loss: 977.3821  BEST VAL Loss: 977.3821\n",
            "\n",
            "Epoch 257: Validation loss decreased (977.382141 --> 969.158630).\n",
            "\t Train_Loss: 370.9601 Val_Loss: 969.1586  BEST VAL Loss: 969.1586\n",
            "\n",
            "Epoch 258: Validation loss decreased (969.158630 --> 961.031067).\n",
            "\t Train_Loss: 366.7276 Val_Loss: 961.0311  BEST VAL Loss: 961.0311\n",
            "\n",
            "Epoch 259: Validation loss decreased (961.031067 --> 952.998840).\n",
            "\t Train_Loss: 362.5632 Val_Loss: 952.9988  BEST VAL Loss: 952.9988\n",
            "\n",
            "Epoch 260: Validation loss decreased (952.998840 --> 945.059753).\n",
            "\t Train_Loss: 358.4661 Val_Loss: 945.0598  BEST VAL Loss: 945.0598\n",
            "\n",
            "Epoch 261: Validation loss decreased (945.059753 --> 937.213989).\n",
            "\t Train_Loss: 354.4348 Val_Loss: 937.2140  BEST VAL Loss: 937.2140\n",
            "\n",
            "Epoch 262: Validation loss decreased (937.213989 --> 929.459290).\n",
            "\t Train_Loss: 350.4690 Val_Loss: 929.4593  BEST VAL Loss: 929.4593\n",
            "\n",
            "Epoch 263: Validation loss decreased (929.459290 --> 921.795715).\n",
            "\t Train_Loss: 346.5673 Val_Loss: 921.7957  BEST VAL Loss: 921.7957\n",
            "\n",
            "Epoch 264: Validation loss decreased (921.795715 --> 914.222656).\n",
            "\t Train_Loss: 342.7292 Val_Loss: 914.2227  BEST VAL Loss: 914.2227\n",
            "\n",
            "Epoch 265: Validation loss decreased (914.222656 --> 906.738464).\n",
            "\t Train_Loss: 338.9542 Val_Loss: 906.7385  BEST VAL Loss: 906.7385\n",
            "\n",
            "Epoch 266: Validation loss decreased (906.738464 --> 899.341797).\n",
            "\t Train_Loss: 335.2408 Val_Loss: 899.3418  BEST VAL Loss: 899.3418\n",
            "\n",
            "Epoch 267: Validation loss decreased (899.341797 --> 892.032410).\n",
            "\t Train_Loss: 331.5882 Val_Loss: 892.0324  BEST VAL Loss: 892.0324\n",
            "\n",
            "Epoch 268: Validation loss decreased (892.032410 --> 884.809570).\n",
            "\t Train_Loss: 327.9959 Val_Loss: 884.8096  BEST VAL Loss: 884.8096\n",
            "\n",
            "Epoch 269: Validation loss decreased (884.809570 --> 877.671082).\n",
            "\t Train_Loss: 324.4629 Val_Loss: 877.6711  BEST VAL Loss: 877.6711\n",
            "\n",
            "Epoch 270: Validation loss decreased (877.671082 --> 870.616882).\n",
            "\t Train_Loss: 320.9881 Val_Loss: 870.6169  BEST VAL Loss: 870.6169\n",
            "\n",
            "Epoch 271: Validation loss decreased (870.616882 --> 863.646118).\n",
            "\t Train_Loss: 317.5710 Val_Loss: 863.6461  BEST VAL Loss: 863.6461\n",
            "\n",
            "Epoch 272: Validation loss decreased (863.646118 --> 856.757996).\n",
            "\t Train_Loss: 314.2107 Val_Loss: 856.7580  BEST VAL Loss: 856.7580\n",
            "\n",
            "Epoch 273: Validation loss decreased (856.757996 --> 849.951660).\n",
            "\t Train_Loss: 310.9064 Val_Loss: 849.9517  BEST VAL Loss: 849.9517\n",
            "\n",
            "Epoch 274: Validation loss decreased (849.951660 --> 843.225708).\n",
            "\t Train_Loss: 307.6577 Val_Loss: 843.2257  BEST VAL Loss: 843.2257\n",
            "\n",
            "Epoch 275: Validation loss decreased (843.225708 --> 836.579407).\n",
            "\t Train_Loss: 304.4632 Val_Loss: 836.5794  BEST VAL Loss: 836.5794\n",
            "\n",
            "Epoch 276: Validation loss decreased (836.579407 --> 830.012329).\n",
            "\t Train_Loss: 301.3223 Val_Loss: 830.0123  BEST VAL Loss: 830.0123\n",
            "\n",
            "Epoch 277: Validation loss decreased (830.012329 --> 823.522644).\n",
            "\t Train_Loss: 298.2346 Val_Loss: 823.5226  BEST VAL Loss: 823.5226\n",
            "\n",
            "Epoch 278: Validation loss decreased (823.522644 --> 817.110413).\n",
            "\t Train_Loss: 295.1988 Val_Loss: 817.1104  BEST VAL Loss: 817.1104\n",
            "\n",
            "Epoch 279: Validation loss decreased (817.110413 --> 810.774048).\n",
            "\t Train_Loss: 292.2144 Val_Loss: 810.7740  BEST VAL Loss: 810.7740\n",
            "\n",
            "Epoch 280: Validation loss decreased (810.774048 --> 804.513550).\n",
            "\t Train_Loss: 289.2805 Val_Loss: 804.5135  BEST VAL Loss: 804.5135\n",
            "\n",
            "Epoch 281: Validation loss decreased (804.513550 --> 798.327759).\n",
            "\t Train_Loss: 286.3967 Val_Loss: 798.3278  BEST VAL Loss: 798.3278\n",
            "\n",
            "Epoch 282: Validation loss decreased (798.327759 --> 792.215759).\n",
            "\t Train_Loss: 283.5623 Val_Loss: 792.2158  BEST VAL Loss: 792.2158\n",
            "\n",
            "Epoch 283: Validation loss decreased (792.215759 --> 786.176270).\n",
            "\t Train_Loss: 280.7763 Val_Loss: 786.1763  BEST VAL Loss: 786.1763\n",
            "\n",
            "Epoch 284: Validation loss decreased (786.176270 --> 780.209290).\n",
            "\t Train_Loss: 278.0379 Val_Loss: 780.2093  BEST VAL Loss: 780.2093\n",
            "\n",
            "Epoch 285: Validation loss decreased (780.209290 --> 774.313477).\n",
            "\t Train_Loss: 275.3467 Val_Loss: 774.3135  BEST VAL Loss: 774.3135\n",
            "\n",
            "Epoch 286: Validation loss decreased (774.313477 --> 768.488159).\n",
            "\t Train_Loss: 272.7019 Val_Loss: 768.4882  BEST VAL Loss: 768.4882\n",
            "\n",
            "Epoch 287: Validation loss decreased (768.488159 --> 762.733215).\n",
            "\t Train_Loss: 270.1027 Val_Loss: 762.7332  BEST VAL Loss: 762.7332\n",
            "\n",
            "Epoch 288: Validation loss decreased (762.733215 --> 757.046692).\n",
            "\t Train_Loss: 267.5488 Val_Loss: 757.0467  BEST VAL Loss: 757.0467\n",
            "\n",
            "Epoch 289: Validation loss decreased (757.046692 --> 751.428650).\n",
            "\t Train_Loss: 265.0390 Val_Loss: 751.4286  BEST VAL Loss: 751.4286\n",
            "\n",
            "Epoch 290: Validation loss decreased (751.428650 --> 745.877319).\n",
            "\t Train_Loss: 262.5731 Val_Loss: 745.8773  BEST VAL Loss: 745.8773\n",
            "\n",
            "Epoch 291: Validation loss decreased (745.877319 --> 740.393494).\n",
            "\t Train_Loss: 260.1499 Val_Loss: 740.3935  BEST VAL Loss: 740.3935\n",
            "\n",
            "Epoch 292: Validation loss decreased (740.393494 --> 734.975281).\n",
            "\t Train_Loss: 257.7694 Val_Loss: 734.9753  BEST VAL Loss: 734.9753\n",
            "\n",
            "Epoch 293: Validation loss decreased (734.975281 --> 729.622253).\n",
            "\t Train_Loss: 255.4307 Val_Loss: 729.6223  BEST VAL Loss: 729.6223\n",
            "\n",
            "Epoch 294: Validation loss decreased (729.622253 --> 724.333801).\n",
            "\t Train_Loss: 253.1330 Val_Loss: 724.3338  BEST VAL Loss: 724.3338\n",
            "\n",
            "Epoch 295: Validation loss decreased (724.333801 --> 719.108704).\n",
            "\t Train_Loss: 250.8759 Val_Loss: 719.1087  BEST VAL Loss: 719.1087\n",
            "\n",
            "Epoch 296: Validation loss decreased (719.108704 --> 713.946411).\n",
            "\t Train_Loss: 248.6585 Val_Loss: 713.9464  BEST VAL Loss: 713.9464\n",
            "\n",
            "Epoch 297: Validation loss decreased (713.946411 --> 708.847107).\n",
            "\t Train_Loss: 246.4803 Val_Loss: 708.8471  BEST VAL Loss: 708.8471\n",
            "\n",
            "Epoch 298: Validation loss decreased (708.847107 --> 703.808838).\n",
            "\t Train_Loss: 244.3412 Val_Loss: 703.8088  BEST VAL Loss: 703.8088\n",
            "\n",
            "Epoch 299: Validation loss decreased (703.808838 --> 698.831238).\n",
            "\t Train_Loss: 242.2399 Val_Loss: 698.8312  BEST VAL Loss: 698.8312\n",
            "\n",
            "Epoch 300: Validation loss decreased (698.831238 --> 693.914062).\n",
            "\t Train_Loss: 240.1761 Val_Loss: 693.9141  BEST VAL Loss: 693.9141\n",
            "\n",
            "Epoch 301: Validation loss decreased (693.914062 --> 689.057007).\n",
            "\t Train_Loss: 238.1492 Val_Loss: 689.0570  BEST VAL Loss: 689.0570\n",
            "\n",
            "Epoch 302: Validation loss decreased (689.057007 --> 684.257812).\n",
            "\t Train_Loss: 236.1590 Val_Loss: 684.2578  BEST VAL Loss: 684.2578\n",
            "\n",
            "Epoch 303: Validation loss decreased (684.257812 --> 679.517273).\n",
            "\t Train_Loss: 234.2042 Val_Loss: 679.5173  BEST VAL Loss: 679.5173\n",
            "\n",
            "Epoch 304: Validation loss decreased (679.517273 --> 674.833679).\n",
            "\t Train_Loss: 232.2849 Val_Loss: 674.8337  BEST VAL Loss: 674.8337\n",
            "\n",
            "Epoch 305: Validation loss decreased (674.833679 --> 670.207703).\n",
            "\t Train_Loss: 230.4000 Val_Loss: 670.2077  BEST VAL Loss: 670.2077\n",
            "\n",
            "Epoch 306: Validation loss decreased (670.207703 --> 665.637024).\n",
            "\t Train_Loss: 228.5496 Val_Loss: 665.6370  BEST VAL Loss: 665.6370\n",
            "\n",
            "Epoch 307: Validation loss decreased (665.637024 --> 661.122192).\n",
            "\t Train_Loss: 226.7325 Val_Loss: 661.1222  BEST VAL Loss: 661.1222\n",
            "\n",
            "Epoch 308: Validation loss decreased (661.122192 --> 656.662170).\n",
            "\t Train_Loss: 224.9485 Val_Loss: 656.6622  BEST VAL Loss: 656.6622\n",
            "\n",
            "Epoch 309: Validation loss decreased (656.662170 --> 652.256409).\n",
            "\t Train_Loss: 223.1971 Val_Loss: 652.2564  BEST VAL Loss: 652.2564\n",
            "\n",
            "Epoch 310: Validation loss decreased (652.256409 --> 647.903992).\n",
            "\t Train_Loss: 221.4778 Val_Loss: 647.9040  BEST VAL Loss: 647.9040\n",
            "\n",
            "Epoch 311: Validation loss decreased (647.903992 --> 643.604736).\n",
            "\t Train_Loss: 219.7899 Val_Loss: 643.6047  BEST VAL Loss: 643.6047\n",
            "\n",
            "Epoch 312: Validation loss decreased (643.604736 --> 639.357849).\n",
            "\t Train_Loss: 218.1331 Val_Loss: 639.3578  BEST VAL Loss: 639.3578\n",
            "\n",
            "Epoch 313: Validation loss decreased (639.357849 --> 635.162354).\n",
            "\t Train_Loss: 216.5068 Val_Loss: 635.1624  BEST VAL Loss: 635.1624\n",
            "\n",
            "Epoch 314: Validation loss decreased (635.162354 --> 631.018188).\n",
            "\t Train_Loss: 214.9103 Val_Loss: 631.0182  BEST VAL Loss: 631.0182\n",
            "\n",
            "Epoch 315: Validation loss decreased (631.018188 --> 626.924927).\n",
            "\t Train_Loss: 213.3435 Val_Loss: 626.9249  BEST VAL Loss: 626.9249\n",
            "\n",
            "Epoch 316: Validation loss decreased (626.924927 --> 622.880981).\n",
            "\t Train_Loss: 211.8059 Val_Loss: 622.8810  BEST VAL Loss: 622.8810\n",
            "\n",
            "Epoch 317: Validation loss decreased (622.880981 --> 618.887146).\n",
            "\t Train_Loss: 210.2967 Val_Loss: 618.8871  BEST VAL Loss: 618.8871\n",
            "\n",
            "Epoch 318: Validation loss decreased (618.887146 --> 614.941101).\n",
            "\t Train_Loss: 208.8159 Val_Loss: 614.9411  BEST VAL Loss: 614.9411\n",
            "\n",
            "Epoch 319: Validation loss decreased (614.941101 --> 611.044006).\n",
            "\t Train_Loss: 207.3623 Val_Loss: 611.0440  BEST VAL Loss: 611.0440\n",
            "\n",
            "Epoch 320: Validation loss decreased (611.044006 --> 607.194336).\n",
            "\t Train_Loss: 205.9363 Val_Loss: 607.1943  BEST VAL Loss: 607.1943\n",
            "\n",
            "Epoch 321: Validation loss decreased (607.194336 --> 603.391724).\n",
            "\t Train_Loss: 204.5369 Val_Loss: 603.3917  BEST VAL Loss: 603.3917\n",
            "\n",
            "Epoch 322: Validation loss decreased (603.391724 --> 599.636108).\n",
            "\t Train_Loss: 203.1638 Val_Loss: 599.6361  BEST VAL Loss: 599.6361\n",
            "\n",
            "Epoch 323: Validation loss decreased (599.636108 --> 595.926086).\n",
            "\t Train_Loss: 201.8169 Val_Loss: 595.9261  BEST VAL Loss: 595.9261\n",
            "\n",
            "Epoch 324: Validation loss decreased (595.926086 --> 592.261353).\n",
            "\t Train_Loss: 200.4952 Val_Loss: 592.2614  BEST VAL Loss: 592.2614\n",
            "\n",
            "Epoch 325: Validation loss decreased (592.261353 --> 588.641602).\n",
            "\t Train_Loss: 199.1985 Val_Loss: 588.6416  BEST VAL Loss: 588.6416\n",
            "\n",
            "Epoch 326: Validation loss decreased (588.641602 --> 585.066772).\n",
            "\t Train_Loss: 197.9264 Val_Loss: 585.0668  BEST VAL Loss: 585.0668\n",
            "\n",
            "Epoch 327: Validation loss decreased (585.066772 --> 581.535034).\n",
            "\t Train_Loss: 196.6788 Val_Loss: 581.5350  BEST VAL Loss: 581.5350\n",
            "\n",
            "Epoch 328: Validation loss decreased (581.535034 --> 578.047180).\n",
            "\t Train_Loss: 195.4547 Val_Loss: 578.0472  BEST VAL Loss: 578.0472\n",
            "\n",
            "Epoch 329: Validation loss decreased (578.047180 --> 574.601746).\n",
            "\t Train_Loss: 194.2541 Val_Loss: 574.6017  BEST VAL Loss: 574.6017\n",
            "\n",
            "Epoch 330: Validation loss decreased (574.601746 --> 571.198914).\n",
            "\t Train_Loss: 193.0765 Val_Loss: 571.1989  BEST VAL Loss: 571.1989\n",
            "\n",
            "Epoch 331: Validation loss decreased (571.198914 --> 567.838013).\n",
            "\t Train_Loss: 191.9215 Val_Loss: 567.8380  BEST VAL Loss: 567.8380\n",
            "\n",
            "Epoch 332: Validation loss decreased (567.838013 --> 564.518005).\n",
            "\t Train_Loss: 190.7888 Val_Loss: 564.5180  BEST VAL Loss: 564.5180\n",
            "\n",
            "Epoch 333: Validation loss decreased (564.518005 --> 561.238953).\n",
            "\t Train_Loss: 189.6779 Val_Loss: 561.2390  BEST VAL Loss: 561.2390\n",
            "\n",
            "Epoch 334: Validation loss decreased (561.238953 --> 558.000610).\n",
            "\t Train_Loss: 188.5884 Val_Loss: 558.0006  BEST VAL Loss: 558.0006\n",
            "\n",
            "Epoch 335: Validation loss decreased (558.000610 --> 554.801697).\n",
            "\t Train_Loss: 187.5202 Val_Loss: 554.8017  BEST VAL Loss: 554.8017\n",
            "\n",
            "Epoch 336: Validation loss decreased (554.801697 --> 551.642761).\n",
            "\t Train_Loss: 186.4726 Val_Loss: 551.6428  BEST VAL Loss: 551.6428\n",
            "\n",
            "Epoch 337: Validation loss decreased (551.642761 --> 548.521973).\n",
            "\t Train_Loss: 185.4456 Val_Loss: 548.5220  BEST VAL Loss: 548.5220\n",
            "\n",
            "Epoch 338: Validation loss decreased (548.521973 --> 545.440308).\n",
            "\t Train_Loss: 184.4384 Val_Loss: 545.4403  BEST VAL Loss: 545.4403\n",
            "\n",
            "Epoch 339: Validation loss decreased (545.440308 --> 542.396484).\n",
            "\t Train_Loss: 183.4510 Val_Loss: 542.3965  BEST VAL Loss: 542.3965\n",
            "\n",
            "Epoch 340: Validation loss decreased (542.396484 --> 539.390015).\n",
            "\t Train_Loss: 182.4830 Val_Loss: 539.3900  BEST VAL Loss: 539.3900\n",
            "\n",
            "Epoch 341: Validation loss decreased (539.390015 --> 536.420471).\n",
            "\t Train_Loss: 181.5340 Val_Loss: 536.4205  BEST VAL Loss: 536.4205\n",
            "\n",
            "Epoch 342: Validation loss decreased (536.420471 --> 533.487976).\n",
            "\t Train_Loss: 180.6035 Val_Loss: 533.4880  BEST VAL Loss: 533.4880\n",
            "\n",
            "Epoch 343: Validation loss decreased (533.487976 --> 530.591736).\n",
            "\t Train_Loss: 179.6916 Val_Loss: 530.5917  BEST VAL Loss: 530.5917\n",
            "\n",
            "Epoch 344: Validation loss decreased (530.591736 --> 527.730957).\n",
            "\t Train_Loss: 178.7977 Val_Loss: 527.7310  BEST VAL Loss: 527.7310\n",
            "\n",
            "Epoch 345: Validation loss decreased (527.730957 --> 524.905762).\n",
            "\t Train_Loss: 177.9214 Val_Loss: 524.9058  BEST VAL Loss: 524.9058\n",
            "\n",
            "Epoch 346: Validation loss decreased (524.905762 --> 522.115051).\n",
            "\t Train_Loss: 177.0626 Val_Loss: 522.1151  BEST VAL Loss: 522.1151\n",
            "\n",
            "Epoch 347: Validation loss decreased (522.115051 --> 519.359436).\n",
            "\t Train_Loss: 176.2207 Val_Loss: 519.3594  BEST VAL Loss: 519.3594\n",
            "\n",
            "Epoch 348: Validation loss decreased (519.359436 --> 516.637695).\n",
            "\t Train_Loss: 175.3959 Val_Loss: 516.6377  BEST VAL Loss: 516.6377\n",
            "\n",
            "Epoch 349: Validation loss decreased (516.637695 --> 513.949951).\n",
            "\t Train_Loss: 174.5874 Val_Loss: 513.9500  BEST VAL Loss: 513.9500\n",
            "\n",
            "Epoch 350: Validation loss decreased (513.949951 --> 511.295013).\n",
            "\t Train_Loss: 173.7953 Val_Loss: 511.2950  BEST VAL Loss: 511.2950\n",
            "\n",
            "Epoch 351: Validation loss decreased (511.295013 --> 508.673248).\n",
            "\t Train_Loss: 173.0190 Val_Loss: 508.6732  BEST VAL Loss: 508.6732\n",
            "\n",
            "Epoch 352: Validation loss decreased (508.673248 --> 506.083893).\n",
            "\t Train_Loss: 172.2583 Val_Loss: 506.0839  BEST VAL Loss: 506.0839\n",
            "\n",
            "Epoch 353: Validation loss decreased (506.083893 --> 503.526581).\n",
            "\t Train_Loss: 171.5129 Val_Loss: 503.5266  BEST VAL Loss: 503.5266\n",
            "\n",
            "Epoch 354: Validation loss decreased (503.526581 --> 501.000793).\n",
            "\t Train_Loss: 170.7826 Val_Loss: 501.0008  BEST VAL Loss: 501.0008\n",
            "\n",
            "Epoch 355: Validation loss decreased (501.000793 --> 498.506683).\n",
            "\t Train_Loss: 170.0670 Val_Loss: 498.5067  BEST VAL Loss: 498.5067\n",
            "\n",
            "Epoch 356: Validation loss decreased (498.506683 --> 496.043762).\n",
            "\t Train_Loss: 169.3661 Val_Loss: 496.0438  BEST VAL Loss: 496.0438\n",
            "\n",
            "Epoch 357: Validation loss decreased (496.043762 --> 493.610504).\n",
            "\t Train_Loss: 168.6795 Val_Loss: 493.6105  BEST VAL Loss: 493.6105\n",
            "\n",
            "Epoch 358: Validation loss decreased (493.610504 --> 491.208313).\n",
            "\t Train_Loss: 168.0066 Val_Loss: 491.2083  BEST VAL Loss: 491.2083\n",
            "\n",
            "Epoch 359: Validation loss decreased (491.208313 --> 488.835907).\n",
            "\t Train_Loss: 167.3478 Val_Loss: 488.8359  BEST VAL Loss: 488.8359\n",
            "\n",
            "Epoch 360: Validation loss decreased (488.835907 --> 486.492340).\n",
            "\t Train_Loss: 166.7024 Val_Loss: 486.4923  BEST VAL Loss: 486.4923\n",
            "\n",
            "Epoch 361: Validation loss decreased (486.492340 --> 484.178528).\n",
            "\t Train_Loss: 166.0700 Val_Loss: 484.1785  BEST VAL Loss: 484.1785\n",
            "\n",
            "Epoch 362: Validation loss decreased (484.178528 --> 481.893066).\n",
            "\t Train_Loss: 165.4509 Val_Loss: 481.8931  BEST VAL Loss: 481.8931\n",
            "\n",
            "Epoch 363: Validation loss decreased (481.893066 --> 479.636444).\n",
            "\t Train_Loss: 164.8444 Val_Loss: 479.6364  BEST VAL Loss: 479.6364\n",
            "\n",
            "Epoch 364: Validation loss decreased (479.636444 --> 477.407532).\n",
            "\t Train_Loss: 164.2505 Val_Loss: 477.4075  BEST VAL Loss: 477.4075\n",
            "\n",
            "Epoch 365: Validation loss decreased (477.407532 --> 475.206635).\n",
            "\t Train_Loss: 163.6689 Val_Loss: 475.2066  BEST VAL Loss: 475.2066\n",
            "\n",
            "Epoch 366: Validation loss decreased (475.206635 --> 473.032806).\n",
            "\t Train_Loss: 163.0993 Val_Loss: 473.0328  BEST VAL Loss: 473.0328\n",
            "\n",
            "Epoch 367: Validation loss decreased (473.032806 --> 470.886475).\n",
            "\t Train_Loss: 162.5415 Val_Loss: 470.8865  BEST VAL Loss: 470.8865\n",
            "\n",
            "Epoch 368: Validation loss decreased (470.886475 --> 468.766449).\n",
            "\t Train_Loss: 161.9955 Val_Loss: 468.7664  BEST VAL Loss: 468.7664\n",
            "\n",
            "Epoch 369: Validation loss decreased (468.766449 --> 466.673492).\n",
            "\t Train_Loss: 161.4606 Val_Loss: 466.6735  BEST VAL Loss: 466.6735\n",
            "\n",
            "Epoch 370: Validation loss decreased (466.673492 --> 464.606018).\n",
            "\t Train_Loss: 160.9373 Val_Loss: 464.6060  BEST VAL Loss: 464.6060\n",
            "\n",
            "Epoch 371: Validation loss decreased (464.606018 --> 462.564667).\n",
            "\t Train_Loss: 160.4246 Val_Loss: 462.5647  BEST VAL Loss: 462.5647\n",
            "\n",
            "Epoch 372: Validation loss decreased (462.564667 --> 460.548431).\n",
            "\t Train_Loss: 159.9229 Val_Loss: 460.5484  BEST VAL Loss: 460.5484\n",
            "\n",
            "Epoch 373: Validation loss decreased (460.548431 --> 458.557709).\n",
            "\t Train_Loss: 159.4316 Val_Loss: 458.5577  BEST VAL Loss: 458.5577\n",
            "\n",
            "Epoch 374: Validation loss decreased (458.557709 --> 456.591461).\n",
            "\t Train_Loss: 158.9507 Val_Loss: 456.5915  BEST VAL Loss: 456.5915\n",
            "\n",
            "Epoch 375: Validation loss decreased (456.591461 --> 454.650116).\n",
            "\t Train_Loss: 158.4799 Val_Loss: 454.6501  BEST VAL Loss: 454.6501\n",
            "\n",
            "Epoch 376: Validation loss decreased (454.650116 --> 452.732971).\n",
            "\t Train_Loss: 158.0192 Val_Loss: 452.7330  BEST VAL Loss: 452.7330\n",
            "\n",
            "Epoch 377: Validation loss decreased (452.732971 --> 450.839844).\n",
            "\t Train_Loss: 157.5682 Val_Loss: 450.8398  BEST VAL Loss: 450.8398\n",
            "\n",
            "Epoch 378: Validation loss decreased (450.839844 --> 448.970276).\n",
            "\t Train_Loss: 157.1269 Val_Loss: 448.9703  BEST VAL Loss: 448.9703\n",
            "\n",
            "Epoch 379: Validation loss decreased (448.970276 --> 447.124420).\n",
            "\t Train_Loss: 156.6949 Val_Loss: 447.1244  BEST VAL Loss: 447.1244\n",
            "\n",
            "Epoch 380: Validation loss decreased (447.124420 --> 445.301025).\n",
            "\t Train_Loss: 156.2722 Val_Loss: 445.3010  BEST VAL Loss: 445.3010\n",
            "\n",
            "Epoch 381: Validation loss decreased (445.301025 --> 443.501038).\n",
            "\t Train_Loss: 155.8584 Val_Loss: 443.5010  BEST VAL Loss: 443.5010\n",
            "\n",
            "Epoch 382: Validation loss decreased (443.501038 --> 441.723724).\n",
            "\t Train_Loss: 155.4536 Val_Loss: 441.7237  BEST VAL Loss: 441.7237\n",
            "\n",
            "Epoch 383: Validation loss decreased (441.723724 --> 439.967865).\n",
            "\t Train_Loss: 155.0577 Val_Loss: 439.9679  BEST VAL Loss: 439.9679\n",
            "\n",
            "Epoch 384: Validation loss decreased (439.967865 --> 438.234528).\n",
            "\t Train_Loss: 154.6699 Val_Loss: 438.2345  BEST VAL Loss: 438.2345\n",
            "\n",
            "Epoch 385: Validation loss decreased (438.234528 --> 436.522919).\n",
            "\t Train_Loss: 154.2907 Val_Loss: 436.5229  BEST VAL Loss: 436.5229\n",
            "\n",
            "Epoch 386: Validation loss decreased (436.522919 --> 434.832520).\n",
            "\t Train_Loss: 153.9197 Val_Loss: 434.8325  BEST VAL Loss: 434.8325\n",
            "\n",
            "Epoch 387: Validation loss decreased (434.832520 --> 433.163788).\n",
            "\t Train_Loss: 153.5566 Val_Loss: 433.1638  BEST VAL Loss: 433.1638\n",
            "\n",
            "Epoch 388: Validation loss decreased (433.163788 --> 431.515717).\n",
            "\t Train_Loss: 153.2015 Val_Loss: 431.5157  BEST VAL Loss: 431.5157\n",
            "\n",
            "Epoch 389: Validation loss decreased (431.515717 --> 429.888336).\n",
            "\t Train_Loss: 152.8542 Val_Loss: 429.8883  BEST VAL Loss: 429.8883\n",
            "\n",
            "Epoch 390: Validation loss decreased (429.888336 --> 428.281250).\n",
            "\t Train_Loss: 152.5143 Val_Loss: 428.2812  BEST VAL Loss: 428.2812\n",
            "\n",
            "Epoch 391: Validation loss decreased (428.281250 --> 426.694550).\n",
            "\t Train_Loss: 152.1819 Val_Loss: 426.6945  BEST VAL Loss: 426.6945\n",
            "\n",
            "Epoch 392: Validation loss decreased (426.694550 --> 425.127350).\n",
            "\t Train_Loss: 151.8567 Val_Loss: 425.1273  BEST VAL Loss: 425.1273\n",
            "\n",
            "Epoch 393: Validation loss decreased (425.127350 --> 423.580383).\n",
            "\t Train_Loss: 151.5386 Val_Loss: 423.5804  BEST VAL Loss: 423.5804\n",
            "\n",
            "Epoch 394: Validation loss decreased (423.580383 --> 422.052643).\n",
            "\t Train_Loss: 151.2276 Val_Loss: 422.0526  BEST VAL Loss: 422.0526\n",
            "\n",
            "Epoch 395: Validation loss decreased (422.052643 --> 420.544586).\n",
            "\t Train_Loss: 150.9234 Val_Loss: 420.5446  BEST VAL Loss: 420.5446\n",
            "\n",
            "Epoch 396: Validation loss decreased (420.544586 --> 419.054749).\n",
            "\t Train_Loss: 150.6261 Val_Loss: 419.0547  BEST VAL Loss: 419.0547\n",
            "\n",
            "Epoch 397: Validation loss decreased (419.054749 --> 417.584076).\n",
            "\t Train_Loss: 150.3350 Val_Loss: 417.5841  BEST VAL Loss: 417.5841\n",
            "\n",
            "Epoch 398: Validation loss decreased (417.584076 --> 416.131989).\n",
            "\t Train_Loss: 150.0506 Val_Loss: 416.1320  BEST VAL Loss: 416.1320\n",
            "\n",
            "Epoch 399: Validation loss decreased (416.131989 --> 414.698059).\n",
            "\t Train_Loss: 149.7725 Val_Loss: 414.6981  BEST VAL Loss: 414.6981\n",
            "\n",
            "Epoch 400: Validation loss decreased (414.698059 --> 413.282227).\n",
            "\t Train_Loss: 149.5005 Val_Loss: 413.2822  BEST VAL Loss: 413.2822\n",
            "\n",
            "Epoch 401: Validation loss decreased (413.282227 --> 411.884583).\n",
            "\t Train_Loss: 149.2346 Val_Loss: 411.8846  BEST VAL Loss: 411.8846\n",
            "\n",
            "Epoch 402: Validation loss decreased (411.884583 --> 410.504059).\n",
            "\t Train_Loss: 148.9746 Val_Loss: 410.5041  BEST VAL Loss: 410.5041\n",
            "\n",
            "Epoch 403: Validation loss decreased (410.504059 --> 409.141235).\n",
            "\t Train_Loss: 148.7204 Val_Loss: 409.1412  BEST VAL Loss: 409.1412\n",
            "\n",
            "Epoch 404: Validation loss decreased (409.141235 --> 407.795349).\n",
            "\t Train_Loss: 148.4720 Val_Loss: 407.7953  BEST VAL Loss: 407.7953\n",
            "\n",
            "Epoch 405: Validation loss decreased (407.795349 --> 406.467255).\n",
            "\t Train_Loss: 148.2290 Val_Loss: 406.4673  BEST VAL Loss: 406.4673\n",
            "\n",
            "Epoch 406: Validation loss decreased (406.467255 --> 405.155090).\n",
            "\t Train_Loss: 147.9917 Val_Loss: 405.1551  BEST VAL Loss: 405.1551\n",
            "\n",
            "Epoch 407: Validation loss decreased (405.155090 --> 403.859711).\n",
            "\t Train_Loss: 147.7596 Val_Loss: 403.8597  BEST VAL Loss: 403.8597\n",
            "\n",
            "Epoch 408: Validation loss decreased (403.859711 --> 402.580933).\n",
            "\t Train_Loss: 147.5328 Val_Loss: 402.5809  BEST VAL Loss: 402.5809\n",
            "\n",
            "Epoch 409: Validation loss decreased (402.580933 --> 401.318024).\n",
            "\t Train_Loss: 147.3110 Val_Loss: 401.3180  BEST VAL Loss: 401.3180\n",
            "\n",
            "Epoch 410: Validation loss decreased (401.318024 --> 400.071198).\n",
            "\t Train_Loss: 147.0943 Val_Loss: 400.0712  BEST VAL Loss: 400.0712\n",
            "\n",
            "Epoch 411: Validation loss decreased (400.071198 --> 398.840363).\n",
            "\t Train_Loss: 146.8825 Val_Loss: 398.8404  BEST VAL Loss: 398.8404\n",
            "\n",
            "Epoch 412: Validation loss decreased (398.840363 --> 397.625153).\n",
            "\t Train_Loss: 146.6756 Val_Loss: 397.6252  BEST VAL Loss: 397.6252\n",
            "\n",
            "Epoch 413: Validation loss decreased (397.625153 --> 396.425018).\n",
            "\t Train_Loss: 146.4734 Val_Loss: 396.4250  BEST VAL Loss: 396.4250\n",
            "\n",
            "Epoch 414: Validation loss decreased (396.425018 --> 395.240326).\n",
            "\t Train_Loss: 146.2758 Val_Loss: 395.2403  BEST VAL Loss: 395.2403\n",
            "\n",
            "Epoch 415: Validation loss decreased (395.240326 --> 394.070526).\n",
            "\t Train_Loss: 146.0827 Val_Loss: 394.0705  BEST VAL Loss: 394.0705\n",
            "\n",
            "Epoch 416: Validation loss decreased (394.070526 --> 392.915649).\n",
            "\t Train_Loss: 145.8940 Val_Loss: 392.9156  BEST VAL Loss: 392.9156\n",
            "\n",
            "Epoch 417: Validation loss decreased (392.915649 --> 391.775330).\n",
            "\t Train_Loss: 145.7097 Val_Loss: 391.7753  BEST VAL Loss: 391.7753\n",
            "\n",
            "Epoch 418: Validation loss decreased (391.775330 --> 390.649750).\n",
            "\t Train_Loss: 145.5295 Val_Loss: 390.6497  BEST VAL Loss: 390.6497\n",
            "\n",
            "Epoch 419: Validation loss decreased (390.649750 --> 389.537994).\n",
            "\t Train_Loss: 145.3537 Val_Loss: 389.5380  BEST VAL Loss: 389.5380\n",
            "\n",
            "Epoch 420: Validation loss decreased (389.537994 --> 388.441040).\n",
            "\t Train_Loss: 145.1817 Val_Loss: 388.4410  BEST VAL Loss: 388.4410\n",
            "\n",
            "Epoch 421: Validation loss decreased (388.441040 --> 387.357727).\n",
            "\t Train_Loss: 145.0138 Val_Loss: 387.3577  BEST VAL Loss: 387.3577\n",
            "\n",
            "Epoch 422: Validation loss decreased (387.357727 --> 386.288300).\n",
            "\t Train_Loss: 144.8499 Val_Loss: 386.2883  BEST VAL Loss: 386.2883\n",
            "\n",
            "Epoch 423: Validation loss decreased (386.288300 --> 385.232422).\n",
            "\t Train_Loss: 144.6897 Val_Loss: 385.2324  BEST VAL Loss: 385.2324\n",
            "\n",
            "Epoch 424: Validation loss decreased (385.232422 --> 384.189758).\n",
            "\t Train_Loss: 144.5332 Val_Loss: 384.1898  BEST VAL Loss: 384.1898\n",
            "\n",
            "Epoch 425: Validation loss decreased (384.189758 --> 383.160736).\n",
            "\t Train_Loss: 144.3803 Val_Loss: 383.1607  BEST VAL Loss: 383.1607\n",
            "\n",
            "Epoch 426: Validation loss decreased (383.160736 --> 382.144623).\n",
            "\t Train_Loss: 144.2311 Val_Loss: 382.1446  BEST VAL Loss: 382.1446\n",
            "\n",
            "Epoch 427: Validation loss decreased (382.144623 --> 381.141510).\n",
            "\t Train_Loss: 144.0853 Val_Loss: 381.1415  BEST VAL Loss: 381.1415\n",
            "\n",
            "Epoch 428: Validation loss decreased (381.141510 --> 380.151367).\n",
            "\t Train_Loss: 143.9430 Val_Loss: 380.1514  BEST VAL Loss: 380.1514\n",
            "\n",
            "Epoch 429: Validation loss decreased (380.151367 --> 379.173920).\n",
            "\t Train_Loss: 143.8040 Val_Loss: 379.1739  BEST VAL Loss: 379.1739\n",
            "\n",
            "Epoch 430: Validation loss decreased (379.173920 --> 378.208557).\n",
            "\t Train_Loss: 143.6683 Val_Loss: 378.2086  BEST VAL Loss: 378.2086\n",
            "\n",
            "Epoch 431: Validation loss decreased (378.208557 --> 377.255707).\n",
            "\t Train_Loss: 143.5357 Val_Loss: 377.2557  BEST VAL Loss: 377.2557\n",
            "\n",
            "Epoch 432: Validation loss decreased (377.255707 --> 376.315430).\n",
            "\t Train_Loss: 143.4063 Val_Loss: 376.3154  BEST VAL Loss: 376.3154\n",
            "\n",
            "Epoch 433: Validation loss decreased (376.315430 --> 375.387024).\n",
            "\t Train_Loss: 143.2800 Val_Loss: 375.3870  BEST VAL Loss: 375.3870\n",
            "\n",
            "Epoch 434: Validation loss decreased (375.387024 --> 374.470276).\n",
            "\t Train_Loss: 143.1567 Val_Loss: 374.4703  BEST VAL Loss: 374.4703\n",
            "\n",
            "Epoch 435: Validation loss decreased (374.470276 --> 373.565491).\n",
            "\t Train_Loss: 143.0363 Val_Loss: 373.5655  BEST VAL Loss: 373.5655\n",
            "\n",
            "Epoch 436: Validation loss decreased (373.565491 --> 372.672180).\n",
            "\t Train_Loss: 142.9187 Val_Loss: 372.6722  BEST VAL Loss: 372.6722\n",
            "\n",
            "Epoch 437: Validation loss decreased (372.672180 --> 371.790253).\n",
            "\t Train_Loss: 142.8040 Val_Loss: 371.7903  BEST VAL Loss: 371.7903\n",
            "\n",
            "Epoch 438: Validation loss decreased (371.790253 --> 370.919891).\n",
            "\t Train_Loss: 142.6920 Val_Loss: 370.9199  BEST VAL Loss: 370.9199\n",
            "\n",
            "Epoch 439: Validation loss decreased (370.919891 --> 370.060394).\n",
            "\t Train_Loss: 142.5827 Val_Loss: 370.0604  BEST VAL Loss: 370.0604\n",
            "\n",
            "Epoch 440: Validation loss decreased (370.060394 --> 369.212250).\n",
            "\t Train_Loss: 142.4759 Val_Loss: 369.2122  BEST VAL Loss: 369.2122\n",
            "\n",
            "Epoch 441: Validation loss decreased (369.212250 --> 368.374908).\n",
            "\t Train_Loss: 142.3718 Val_Loss: 368.3749  BEST VAL Loss: 368.3749\n",
            "\n",
            "Epoch 442: Validation loss decreased (368.374908 --> 367.548248).\n",
            "\t Train_Loss: 142.2701 Val_Loss: 367.5482  BEST VAL Loss: 367.5482\n",
            "\n",
            "Epoch 443: Validation loss decreased (367.548248 --> 366.732391).\n",
            "\t Train_Loss: 142.1709 Val_Loss: 366.7324  BEST VAL Loss: 366.7324\n",
            "\n",
            "Epoch 444: Validation loss decreased (366.732391 --> 365.927002).\n",
            "\t Train_Loss: 142.0741 Val_Loss: 365.9270  BEST VAL Loss: 365.9270\n",
            "\n",
            "Epoch 445: Validation loss decreased (365.927002 --> 365.131989).\n",
            "\t Train_Loss: 141.9797 Val_Loss: 365.1320  BEST VAL Loss: 365.1320\n",
            "\n",
            "Epoch 446: Validation loss decreased (365.131989 --> 364.347565).\n",
            "\t Train_Loss: 141.8876 Val_Loss: 364.3476  BEST VAL Loss: 364.3476\n",
            "\n",
            "Epoch 447: Validation loss decreased (364.347565 --> 363.572906).\n",
            "\t Train_Loss: 141.7977 Val_Loss: 363.5729  BEST VAL Loss: 363.5729\n",
            "\n",
            "Epoch 448: Validation loss decreased (363.572906 --> 362.808258).\n",
            "\t Train_Loss: 141.7099 Val_Loss: 362.8083  BEST VAL Loss: 362.8083\n",
            "\n",
            "Epoch 449: Validation loss decreased (362.808258 --> 362.053314).\n",
            "\t Train_Loss: 141.6243 Val_Loss: 362.0533  BEST VAL Loss: 362.0533\n",
            "\n",
            "Epoch 450: Validation loss decreased (362.053314 --> 361.308319).\n",
            "\t Train_Loss: 141.5407 Val_Loss: 361.3083  BEST VAL Loss: 361.3083\n",
            "\n",
            "Epoch 451: Validation loss decreased (361.308319 --> 360.573212).\n",
            "\t Train_Loss: 141.4593 Val_Loss: 360.5732  BEST VAL Loss: 360.5732\n",
            "\n",
            "Epoch 452: Validation loss decreased (360.573212 --> 359.847382).\n",
            "\t Train_Loss: 141.3798 Val_Loss: 359.8474  BEST VAL Loss: 359.8474\n",
            "\n",
            "Epoch 453: Validation loss decreased (359.847382 --> 359.130707).\n",
            "\t Train_Loss: 141.3023 Val_Loss: 359.1307  BEST VAL Loss: 359.1307\n",
            "\n",
            "Epoch 454: Validation loss decreased (359.130707 --> 358.423798).\n",
            "\t Train_Loss: 141.2267 Val_Loss: 358.4238  BEST VAL Loss: 358.4238\n",
            "\n",
            "Epoch 455: Validation loss decreased (358.423798 --> 357.725647).\n",
            "\t Train_Loss: 141.1529 Val_Loss: 357.7256  BEST VAL Loss: 357.7256\n",
            "\n",
            "Epoch 456: Validation loss decreased (357.725647 --> 357.036957).\n",
            "\t Train_Loss: 141.0809 Val_Loss: 357.0370  BEST VAL Loss: 357.0370\n",
            "\n",
            "Epoch 457: Validation loss decreased (357.036957 --> 356.356934).\n",
            "\t Train_Loss: 141.0108 Val_Loss: 356.3569  BEST VAL Loss: 356.3569\n",
            "\n",
            "Epoch 458: Validation loss decreased (356.356934 --> 355.685852).\n",
            "\t Train_Loss: 140.9423 Val_Loss: 355.6859  BEST VAL Loss: 355.6859\n",
            "\n",
            "Epoch 459: Validation loss decreased (355.685852 --> 355.023651).\n",
            "\t Train_Loss: 140.8756 Val_Loss: 355.0237  BEST VAL Loss: 355.0237\n",
            "\n",
            "Epoch 460: Validation loss decreased (355.023651 --> 354.369843).\n",
            "\t Train_Loss: 140.8105 Val_Loss: 354.3698  BEST VAL Loss: 354.3698\n",
            "\n",
            "Epoch 461: Validation loss decreased (354.369843 --> 353.724823).\n",
            "\t Train_Loss: 140.7470 Val_Loss: 353.7248  BEST VAL Loss: 353.7248\n",
            "\n",
            "Epoch 462: Validation loss decreased (353.724823 --> 353.088104).\n",
            "\t Train_Loss: 140.6852 Val_Loss: 353.0881  BEST VAL Loss: 353.0881\n",
            "\n",
            "Epoch 463: Validation loss decreased (353.088104 --> 352.459656).\n",
            "\t Train_Loss: 140.6249 Val_Loss: 352.4597  BEST VAL Loss: 352.4597\n",
            "\n",
            "Epoch 464: Validation loss decreased (352.459656 --> 351.839691).\n",
            "\t Train_Loss: 140.5661 Val_Loss: 351.8397  BEST VAL Loss: 351.8397\n",
            "\n",
            "Epoch 465: Validation loss decreased (351.839691 --> 351.227264).\n",
            "\t Train_Loss: 140.5088 Val_Loss: 351.2273  BEST VAL Loss: 351.2273\n",
            "\n",
            "Epoch 466: Validation loss decreased (351.227264 --> 350.623505).\n",
            "\t Train_Loss: 140.4528 Val_Loss: 350.6235  BEST VAL Loss: 350.6235\n",
            "\n",
            "Epoch 467: Validation loss decreased (350.623505 --> 350.027161).\n",
            "\t Train_Loss: 140.3983 Val_Loss: 350.0272  BEST VAL Loss: 350.0272\n",
            "\n",
            "Epoch 468: Validation loss decreased (350.027161 --> 349.438690).\n",
            "\t Train_Loss: 140.3452 Val_Loss: 349.4387  BEST VAL Loss: 349.4387\n",
            "\n",
            "Epoch 469: Validation loss decreased (349.438690 --> 348.858398).\n",
            "\t Train_Loss: 140.2934 Val_Loss: 348.8584  BEST VAL Loss: 348.8584\n",
            "\n",
            "Epoch 470: Validation loss decreased (348.858398 --> 348.285217).\n",
            "\t Train_Loss: 140.2430 Val_Loss: 348.2852  BEST VAL Loss: 348.2852\n",
            "\n",
            "Epoch 471: Validation loss decreased (348.285217 --> 347.719696).\n",
            "\t Train_Loss: 140.1937 Val_Loss: 347.7197  BEST VAL Loss: 347.7197\n",
            "\n",
            "Epoch 472: Validation loss decreased (347.719696 --> 347.161591).\n",
            "\t Train_Loss: 140.1458 Val_Loss: 347.1616  BEST VAL Loss: 347.1616\n",
            "\n",
            "Epoch 473: Validation loss decreased (347.161591 --> 346.611084).\n",
            "\t Train_Loss: 140.0990 Val_Loss: 346.6111  BEST VAL Loss: 346.6111\n",
            "\n",
            "Epoch 474: Validation loss decreased (346.611084 --> 346.067627).\n",
            "\t Train_Loss: 140.0535 Val_Loss: 346.0676  BEST VAL Loss: 346.0676\n",
            "\n",
            "Epoch 475: Validation loss decreased (346.067627 --> 345.531189).\n",
            "\t Train_Loss: 140.0091 Val_Loss: 345.5312  BEST VAL Loss: 345.5312\n",
            "\n",
            "Epoch 476: Validation loss decreased (345.531189 --> 345.001953).\n",
            "\t Train_Loss: 139.9659 Val_Loss: 345.0020  BEST VAL Loss: 345.0020\n",
            "\n",
            "Epoch 477: Validation loss decreased (345.001953 --> 344.479736).\n",
            "\t Train_Loss: 139.9237 Val_Loss: 344.4797  BEST VAL Loss: 344.4797\n",
            "\n",
            "Epoch 478: Validation loss decreased (344.479736 --> 343.964447).\n",
            "\t Train_Loss: 139.8826 Val_Loss: 343.9644  BEST VAL Loss: 343.9644\n",
            "\n",
            "Epoch 479: Validation loss decreased (343.964447 --> 343.455902).\n",
            "\t Train_Loss: 139.8426 Val_Loss: 343.4559  BEST VAL Loss: 343.4559\n",
            "\n",
            "Epoch 480: Validation loss decreased (343.455902 --> 342.953979).\n",
            "\t Train_Loss: 139.8036 Val_Loss: 342.9540  BEST VAL Loss: 342.9540\n",
            "\n",
            "Epoch 481: Validation loss decreased (342.953979 --> 342.458740).\n",
            "\t Train_Loss: 139.7656 Val_Loss: 342.4587  BEST VAL Loss: 342.4587\n",
            "\n",
            "Epoch 482: Validation loss decreased (342.458740 --> 341.970398).\n",
            "\t Train_Loss: 139.7286 Val_Loss: 341.9704  BEST VAL Loss: 341.9704\n",
            "\n",
            "Epoch 483: Validation loss decreased (341.970398 --> 341.488190).\n",
            "\t Train_Loss: 139.6926 Val_Loss: 341.4882  BEST VAL Loss: 341.4882\n",
            "\n",
            "Epoch 484: Validation loss decreased (341.488190 --> 341.012238).\n",
            "\t Train_Loss: 139.6575 Val_Loss: 341.0122  BEST VAL Loss: 341.0122\n",
            "\n",
            "Epoch 485: Validation loss decreased (341.012238 --> 340.543060).\n",
            "\t Train_Loss: 139.6232 Val_Loss: 340.5431  BEST VAL Loss: 340.5431\n",
            "\n",
            "Epoch 486: Validation loss decreased (340.543060 --> 340.079681).\n",
            "\t Train_Loss: 139.5900 Val_Loss: 340.0797  BEST VAL Loss: 340.0797\n",
            "\n",
            "Epoch 487: Validation loss decreased (340.079681 --> 339.622650).\n",
            "\t Train_Loss: 139.5574 Val_Loss: 339.6227  BEST VAL Loss: 339.6227\n",
            "\n",
            "Epoch 488: Validation loss decreased (339.622650 --> 339.171844).\n",
            "\t Train_Loss: 139.5258 Val_Loss: 339.1718  BEST VAL Loss: 339.1718\n",
            "\n",
            "Epoch 489: Validation loss decreased (339.171844 --> 338.727142).\n",
            "\t Train_Loss: 139.4951 Val_Loss: 338.7271  BEST VAL Loss: 338.7271\n",
            "\n",
            "Epoch 490: Validation loss decreased (338.727142 --> 338.288147).\n",
            "\t Train_Loss: 139.4651 Val_Loss: 338.2881  BEST VAL Loss: 338.2881\n",
            "\n",
            "Epoch 491: Validation loss decreased (338.288147 --> 337.855042).\n",
            "\t Train_Loss: 139.4359 Val_Loss: 337.8550  BEST VAL Loss: 337.8550\n",
            "\n",
            "Epoch 492: Validation loss decreased (337.855042 --> 337.427887).\n",
            "\t Train_Loss: 139.4075 Val_Loss: 337.4279  BEST VAL Loss: 337.4279\n",
            "\n",
            "Epoch 493: Validation loss decreased (337.427887 --> 337.006470).\n",
            "\t Train_Loss: 139.3798 Val_Loss: 337.0065  BEST VAL Loss: 337.0065\n",
            "\n",
            "Epoch 494: Validation loss decreased (337.006470 --> 336.590332).\n",
            "\t Train_Loss: 139.3529 Val_Loss: 336.5903  BEST VAL Loss: 336.5903\n",
            "\n",
            "Epoch 495: Validation loss decreased (336.590332 --> 336.180237).\n",
            "\t Train_Loss: 139.3266 Val_Loss: 336.1802  BEST VAL Loss: 336.1802\n",
            "\n",
            "Epoch 496: Validation loss decreased (336.180237 --> 335.775269).\n",
            "\t Train_Loss: 139.3011 Val_Loss: 335.7753  BEST VAL Loss: 335.7753\n",
            "\n",
            "Epoch 497: Validation loss decreased (335.775269 --> 335.376068).\n",
            "\t Train_Loss: 139.2762 Val_Loss: 335.3761  BEST VAL Loss: 335.3761\n",
            "\n",
            "Epoch 498: Validation loss decreased (335.376068 --> 334.982330).\n",
            "\t Train_Loss: 139.2520 Val_Loss: 334.9823  BEST VAL Loss: 334.9823\n",
            "\n",
            "Epoch 499: Validation loss decreased (334.982330 --> 334.593506).\n",
            "\t Train_Loss: 139.2285 Val_Loss: 334.5935  BEST VAL Loss: 334.5935\n",
            "\n",
            "Epoch 500: Validation loss decreased (334.593506 --> 334.209930).\n",
            "\t Train_Loss: 139.2055 Val_Loss: 334.2099  BEST VAL Loss: 334.2099\n",
            "\n",
            "Epoch 501: Validation loss decreased (334.209930 --> 333.831757).\n",
            "\t Train_Loss: 139.1831 Val_Loss: 333.8318  BEST VAL Loss: 333.8318\n",
            "\n",
            "Epoch 502: Validation loss decreased (333.831757 --> 333.458496).\n",
            "\t Train_Loss: 139.1614 Val_Loss: 333.4585  BEST VAL Loss: 333.4585\n",
            "\n",
            "Epoch 503: Validation loss decreased (333.458496 --> 333.090393).\n",
            "\t Train_Loss: 139.1402 Val_Loss: 333.0904  BEST VAL Loss: 333.0904\n",
            "\n",
            "Epoch 504: Validation loss decreased (333.090393 --> 332.727264).\n",
            "\t Train_Loss: 139.1197 Val_Loss: 332.7273  BEST VAL Loss: 332.7273\n",
            "\n",
            "Epoch 505: Validation loss decreased (332.727264 --> 332.369049).\n",
            "\t Train_Loss: 139.0996 Val_Loss: 332.3690  BEST VAL Loss: 332.3690\n",
            "\n",
            "Epoch 506: Validation loss decreased (332.369049 --> 332.015594).\n",
            "\t Train_Loss: 139.0801 Val_Loss: 332.0156  BEST VAL Loss: 332.0156\n",
            "\n",
            "Epoch 507: Validation loss decreased (332.015594 --> 331.666962).\n",
            "\t Train_Loss: 139.0611 Val_Loss: 331.6670  BEST VAL Loss: 331.6670\n",
            "\n",
            "Epoch 508: Validation loss decreased (331.666962 --> 331.323273).\n",
            "\t Train_Loss: 139.0426 Val_Loss: 331.3233  BEST VAL Loss: 331.3233\n",
            "\n",
            "Epoch 509: Validation loss decreased (331.323273 --> 330.984283).\n",
            "\t Train_Loss: 139.0247 Val_Loss: 330.9843  BEST VAL Loss: 330.9843\n",
            "\n",
            "Epoch 510: Validation loss decreased (330.984283 --> 330.649506).\n",
            "\t Train_Loss: 139.0072 Val_Loss: 330.6495  BEST VAL Loss: 330.6495\n",
            "\n",
            "Epoch 511: Validation loss decreased (330.649506 --> 330.319550).\n",
            "\t Train_Loss: 138.9902 Val_Loss: 330.3195  BEST VAL Loss: 330.3195\n",
            "\n",
            "Epoch 512: Validation loss decreased (330.319550 --> 329.994141).\n",
            "\t Train_Loss: 138.9736 Val_Loss: 329.9941  BEST VAL Loss: 329.9941\n",
            "\n",
            "Epoch 513: Validation loss decreased (329.994141 --> 329.673157).\n",
            "\t Train_Loss: 138.9576 Val_Loss: 329.6732  BEST VAL Loss: 329.6732\n",
            "\n",
            "Epoch 514: Validation loss decreased (329.673157 --> 329.356262).\n",
            "\t Train_Loss: 138.9419 Val_Loss: 329.3563  BEST VAL Loss: 329.3563\n",
            "\n",
            "Epoch 515: Validation loss decreased (329.356262 --> 329.044098).\n",
            "\t Train_Loss: 138.9266 Val_Loss: 329.0441  BEST VAL Loss: 329.0441\n",
            "\n",
            "Epoch 516: Validation loss decreased (329.044098 --> 328.736023).\n",
            "\t Train_Loss: 138.9118 Val_Loss: 328.7360  BEST VAL Loss: 328.7360\n",
            "\n",
            "Epoch 517: Validation loss decreased (328.736023 --> 328.432373).\n",
            "\t Train_Loss: 138.8973 Val_Loss: 328.4324  BEST VAL Loss: 328.4324\n",
            "\n",
            "Epoch 518: Validation loss decreased (328.432373 --> 328.132874).\n",
            "\t Train_Loss: 138.8834 Val_Loss: 328.1329  BEST VAL Loss: 328.1329\n",
            "\n",
            "Epoch 519: Validation loss decreased (328.132874 --> 327.837158).\n",
            "\t Train_Loss: 138.8697 Val_Loss: 327.8372  BEST VAL Loss: 327.8372\n",
            "\n",
            "Epoch 520: Validation loss decreased (327.837158 --> 327.545807).\n",
            "\t Train_Loss: 138.8565 Val_Loss: 327.5458  BEST VAL Loss: 327.5458\n",
            "\n",
            "Epoch 521: Validation loss decreased (327.545807 --> 327.258301).\n",
            "\t Train_Loss: 138.8436 Val_Loss: 327.2583  BEST VAL Loss: 327.2583\n",
            "\n",
            "Epoch 522: Validation loss decreased (327.258301 --> 326.975006).\n",
            "\t Train_Loss: 138.8310 Val_Loss: 326.9750  BEST VAL Loss: 326.9750\n",
            "\n",
            "Epoch 523: Validation loss decreased (326.975006 --> 326.695160).\n",
            "\t Train_Loss: 138.8188 Val_Loss: 326.6952  BEST VAL Loss: 326.6952\n",
            "\n",
            "Epoch 524: Validation loss decreased (326.695160 --> 326.419586).\n",
            "\t Train_Loss: 138.8070 Val_Loss: 326.4196  BEST VAL Loss: 326.4196\n",
            "\n",
            "Epoch 525: Validation loss decreased (326.419586 --> 326.147766).\n",
            "\t Train_Loss: 138.7955 Val_Loss: 326.1478  BEST VAL Loss: 326.1478\n",
            "\n",
            "Epoch 526: Validation loss decreased (326.147766 --> 325.879822).\n",
            "\t Train_Loss: 138.7842 Val_Loss: 325.8798  BEST VAL Loss: 325.8798\n",
            "\n",
            "Epoch 527: Validation loss decreased (325.879822 --> 325.615204).\n",
            "\t Train_Loss: 138.7733 Val_Loss: 325.6152  BEST VAL Loss: 325.6152\n",
            "\n",
            "Epoch 528: Validation loss decreased (325.615204 --> 325.354156).\n",
            "\t Train_Loss: 138.7627 Val_Loss: 325.3542  BEST VAL Loss: 325.3542\n",
            "\n",
            "Epoch 529: Validation loss decreased (325.354156 --> 325.097107).\n",
            "\t Train_Loss: 138.7524 Val_Loss: 325.0971  BEST VAL Loss: 325.0971\n",
            "\n",
            "Epoch 530: Validation loss decreased (325.097107 --> 324.843842).\n",
            "\t Train_Loss: 138.7423 Val_Loss: 324.8438  BEST VAL Loss: 324.8438\n",
            "\n",
            "Epoch 531: Validation loss decreased (324.843842 --> 324.593658).\n",
            "\t Train_Loss: 138.7326 Val_Loss: 324.5937  BEST VAL Loss: 324.5937\n",
            "\n",
            "Epoch 532: Validation loss decreased (324.593658 --> 324.347504).\n",
            "\t Train_Loss: 138.7232 Val_Loss: 324.3475  BEST VAL Loss: 324.3475\n",
            "\n",
            "Epoch 533: Validation loss decreased (324.347504 --> 324.104218).\n",
            "\t Train_Loss: 138.7140 Val_Loss: 324.1042  BEST VAL Loss: 324.1042\n",
            "\n",
            "Epoch 534: Validation loss decreased (324.104218 --> 323.864655).\n",
            "\t Train_Loss: 138.7050 Val_Loss: 323.8647  BEST VAL Loss: 323.8647\n",
            "\n",
            "Epoch 535: Validation loss decreased (323.864655 --> 323.628235).\n",
            "\t Train_Loss: 138.6964 Val_Loss: 323.6282  BEST VAL Loss: 323.6282\n",
            "\n",
            "Epoch 536: Validation loss decreased (323.628235 --> 323.395325).\n",
            "\t Train_Loss: 138.6879 Val_Loss: 323.3953  BEST VAL Loss: 323.3953\n",
            "\n",
            "Epoch 537: Validation loss decreased (323.395325 --> 323.165588).\n",
            "\t Train_Loss: 138.6797 Val_Loss: 323.1656  BEST VAL Loss: 323.1656\n",
            "\n",
            "Epoch 538: Validation loss decreased (323.165588 --> 322.938965).\n",
            "\t Train_Loss: 138.6717 Val_Loss: 322.9390  BEST VAL Loss: 322.9390\n",
            "\n",
            "Epoch 539: Validation loss decreased (322.938965 --> 322.715637).\n",
            "\t Train_Loss: 138.6639 Val_Loss: 322.7156  BEST VAL Loss: 322.7156\n",
            "\n",
            "Epoch 540: Validation loss decreased (322.715637 --> 322.495422).\n",
            "\t Train_Loss: 138.6564 Val_Loss: 322.4954  BEST VAL Loss: 322.4954\n",
            "\n",
            "Epoch 541: Validation loss decreased (322.495422 --> 322.278168).\n",
            "\t Train_Loss: 138.6491 Val_Loss: 322.2782  BEST VAL Loss: 322.2782\n",
            "\n",
            "Epoch 542: Validation loss decreased (322.278168 --> 322.063934).\n",
            "\t Train_Loss: 138.6420 Val_Loss: 322.0639  BEST VAL Loss: 322.0639\n",
            "\n",
            "Epoch 543: Validation loss decreased (322.063934 --> 321.853241).\n",
            "\t Train_Loss: 138.6350 Val_Loss: 321.8532  BEST VAL Loss: 321.8532\n",
            "\n",
            "Epoch 544: Validation loss decreased (321.853241 --> 321.645111).\n",
            "\t Train_Loss: 138.6284 Val_Loss: 321.6451  BEST VAL Loss: 321.6451\n",
            "\n",
            "Epoch 545: Validation loss decreased (321.645111 --> 321.439819).\n",
            "\t Train_Loss: 138.6218 Val_Loss: 321.4398  BEST VAL Loss: 321.4398\n",
            "\n",
            "Epoch 546: Validation loss decreased (321.439819 --> 321.237762).\n",
            "\t Train_Loss: 138.6155 Val_Loss: 321.2378  BEST VAL Loss: 321.2378\n",
            "\n",
            "Epoch 547: Validation loss decreased (321.237762 --> 321.038269).\n",
            "\t Train_Loss: 138.6093 Val_Loss: 321.0383  BEST VAL Loss: 321.0383\n",
            "\n",
            "Epoch 548: Validation loss decreased (321.038269 --> 320.841766).\n",
            "\t Train_Loss: 138.6033 Val_Loss: 320.8418  BEST VAL Loss: 320.8418\n",
            "\n",
            "Epoch 549: Validation loss decreased (320.841766 --> 320.648285).\n",
            "\t Train_Loss: 138.5975 Val_Loss: 320.6483  BEST VAL Loss: 320.6483\n",
            "\n",
            "Epoch 550: Validation loss decreased (320.648285 --> 320.457428).\n",
            "\t Train_Loss: 138.5919 Val_Loss: 320.4574  BEST VAL Loss: 320.4574\n",
            "\n",
            "Epoch 551: Validation loss decreased (320.457428 --> 320.269287).\n",
            "\t Train_Loss: 138.5864 Val_Loss: 320.2693  BEST VAL Loss: 320.2693\n",
            "\n",
            "Epoch 552: Validation loss decreased (320.269287 --> 320.083710).\n",
            "\t Train_Loss: 138.5811 Val_Loss: 320.0837  BEST VAL Loss: 320.0837\n",
            "\n",
            "Epoch 553: Validation loss decreased (320.083710 --> 319.900940).\n",
            "\t Train_Loss: 138.5760 Val_Loss: 319.9009  BEST VAL Loss: 319.9009\n",
            "\n",
            "Epoch 554: Validation loss decreased (319.900940 --> 319.720673).\n",
            "\t Train_Loss: 138.5709 Val_Loss: 319.7207  BEST VAL Loss: 319.7207\n",
            "\n",
            "Epoch 555: Validation loss decreased (319.720673 --> 319.542725).\n",
            "\t Train_Loss: 138.5661 Val_Loss: 319.5427  BEST VAL Loss: 319.5427\n",
            "\n",
            "Epoch 556: Validation loss decreased (319.542725 --> 319.367767).\n",
            "\t Train_Loss: 138.5613 Val_Loss: 319.3678  BEST VAL Loss: 319.3678\n",
            "\n",
            "Epoch 557: Validation loss decreased (319.367767 --> 319.195129).\n",
            "\t Train_Loss: 138.5567 Val_Loss: 319.1951  BEST VAL Loss: 319.1951\n",
            "\n",
            "Epoch 558: Validation loss decreased (319.195129 --> 319.024811).\n",
            "\t Train_Loss: 138.5523 Val_Loss: 319.0248  BEST VAL Loss: 319.0248\n",
            "\n",
            "Epoch 559: Validation loss decreased (319.024811 --> 318.857239).\n",
            "\t Train_Loss: 138.5479 Val_Loss: 318.8572  BEST VAL Loss: 318.8572\n",
            "\n",
            "Epoch 560: Validation loss decreased (318.857239 --> 318.691742).\n",
            "\t Train_Loss: 138.5437 Val_Loss: 318.6917  BEST VAL Loss: 318.6917\n",
            "\n",
            "Epoch 561: Validation loss decreased (318.691742 --> 318.529053).\n",
            "\t Train_Loss: 138.5396 Val_Loss: 318.5291  BEST VAL Loss: 318.5291\n",
            "\n",
            "Epoch 562: Validation loss decreased (318.529053 --> 318.368561).\n",
            "\t Train_Loss: 138.5357 Val_Loss: 318.3686  BEST VAL Loss: 318.3686\n",
            "\n",
            "Epoch 563: Validation loss decreased (318.368561 --> 318.210205).\n",
            "\t Train_Loss: 138.5319 Val_Loss: 318.2102  BEST VAL Loss: 318.2102\n",
            "\n",
            "Epoch 564: Validation loss decreased (318.210205 --> 318.054535).\n",
            "\t Train_Loss: 138.5281 Val_Loss: 318.0545  BEST VAL Loss: 318.0545\n",
            "\n",
            "Epoch 565: Validation loss decreased (318.054535 --> 317.900787).\n",
            "\t Train_Loss: 138.5246 Val_Loss: 317.9008  BEST VAL Loss: 317.9008\n",
            "\n",
            "Epoch 566: Validation loss decreased (317.900787 --> 317.749268).\n",
            "\t Train_Loss: 138.5210 Val_Loss: 317.7493  BEST VAL Loss: 317.7493\n",
            "\n",
            "Epoch 567: Validation loss decreased (317.749268 --> 317.600067).\n",
            "\t Train_Loss: 138.5175 Val_Loss: 317.6001  BEST VAL Loss: 317.6001\n",
            "\n",
            "Epoch 568: Validation loss decreased (317.600067 --> 317.453033).\n",
            "\t Train_Loss: 138.5143 Val_Loss: 317.4530  BEST VAL Loss: 317.4530\n",
            "\n",
            "Epoch 569: Validation loss decreased (317.453033 --> 317.308044).\n",
            "\t Train_Loss: 138.5111 Val_Loss: 317.3080  BEST VAL Loss: 317.3080\n",
            "\n",
            "Epoch 570: Validation loss decreased (317.308044 --> 317.165466).\n",
            "\t Train_Loss: 138.5079 Val_Loss: 317.1655  BEST VAL Loss: 317.1655\n",
            "\n",
            "Epoch 571: Validation loss decreased (317.165466 --> 317.024719).\n",
            "\t Train_Loss: 138.5049 Val_Loss: 317.0247  BEST VAL Loss: 317.0247\n",
            "\n",
            "Epoch 572: Validation loss decreased (317.024719 --> 316.886047).\n",
            "\t Train_Loss: 138.5020 Val_Loss: 316.8860  BEST VAL Loss: 316.8860\n",
            "\n",
            "Epoch 573: Validation loss decreased (316.886047 --> 316.749146).\n",
            "\t Train_Loss: 138.4991 Val_Loss: 316.7491  BEST VAL Loss: 316.7491\n",
            "\n",
            "Epoch 574: Validation loss decreased (316.749146 --> 316.614563).\n",
            "\t Train_Loss: 138.4963 Val_Loss: 316.6146  BEST VAL Loss: 316.6146\n",
            "\n",
            "Epoch 575: Validation loss decreased (316.614563 --> 316.481995).\n",
            "\t Train_Loss: 138.4936 Val_Loss: 316.4820  BEST VAL Loss: 316.4820\n",
            "\n",
            "Epoch 576: Validation loss decreased (316.481995 --> 316.351379).\n",
            "\t Train_Loss: 138.4910 Val_Loss: 316.3514  BEST VAL Loss: 316.3514\n",
            "\n",
            "Epoch 577: Validation loss decreased (316.351379 --> 316.222565).\n",
            "\t Train_Loss: 138.4886 Val_Loss: 316.2226  BEST VAL Loss: 316.2226\n",
            "\n",
            "Epoch 578: Validation loss decreased (316.222565 --> 316.095703).\n",
            "\t Train_Loss: 138.4861 Val_Loss: 316.0957  BEST VAL Loss: 316.0957\n",
            "\n",
            "Epoch 579: Validation loss decreased (316.095703 --> 315.970673).\n",
            "\t Train_Loss: 138.4837 Val_Loss: 315.9707  BEST VAL Loss: 315.9707\n",
            "\n",
            "Epoch 580: Validation loss decreased (315.970673 --> 315.847626).\n",
            "\t Train_Loss: 138.4814 Val_Loss: 315.8476  BEST VAL Loss: 315.8476\n",
            "\n",
            "Epoch 581: Validation loss decreased (315.847626 --> 315.726013).\n",
            "\t Train_Loss: 138.4793 Val_Loss: 315.7260  BEST VAL Loss: 315.7260\n",
            "\n",
            "Epoch 582: Validation loss decreased (315.726013 --> 315.606598).\n",
            "\t Train_Loss: 138.4770 Val_Loss: 315.6066  BEST VAL Loss: 315.6066\n",
            "\n",
            "Epoch 583: Validation loss decreased (315.606598 --> 315.488617).\n",
            "\t Train_Loss: 138.4750 Val_Loss: 315.4886  BEST VAL Loss: 315.4886\n",
            "\n",
            "Epoch 584: Validation loss decreased (315.488617 --> 315.372803).\n",
            "\t Train_Loss: 138.4729 Val_Loss: 315.3728  BEST VAL Loss: 315.3728\n",
            "\n",
            "Epoch 585: Validation loss decreased (315.372803 --> 315.258453).\n",
            "\t Train_Loss: 138.4709 Val_Loss: 315.2585  BEST VAL Loss: 315.2585\n",
            "\n",
            "Epoch 586: Validation loss decreased (315.258453 --> 315.145844).\n",
            "\t Train_Loss: 138.4690 Val_Loss: 315.1458  BEST VAL Loss: 315.1458\n",
            "\n",
            "Epoch 587: Validation loss decreased (315.145844 --> 315.034821).\n",
            "\t Train_Loss: 138.4672 Val_Loss: 315.0348  BEST VAL Loss: 315.0348\n",
            "\n",
            "Epoch 588: Validation loss decreased (315.034821 --> 314.925873).\n",
            "\t Train_Loss: 138.4653 Val_Loss: 314.9259  BEST VAL Loss: 314.9259\n",
            "\n",
            "Epoch 589: Validation loss decreased (314.925873 --> 314.818024).\n",
            "\t Train_Loss: 138.4636 Val_Loss: 314.8180  BEST VAL Loss: 314.8180\n",
            "\n",
            "Epoch 590: Validation loss decreased (314.818024 --> 314.712219).\n",
            "\t Train_Loss: 138.4619 Val_Loss: 314.7122  BEST VAL Loss: 314.7122\n",
            "\n",
            "Epoch 591: Validation loss decreased (314.712219 --> 314.607758).\n",
            "\t Train_Loss: 138.4603 Val_Loss: 314.6078  BEST VAL Loss: 314.6078\n",
            "\n",
            "Epoch 592: Validation loss decreased (314.607758 --> 314.505096).\n",
            "\t Train_Loss: 138.4587 Val_Loss: 314.5051  BEST VAL Loss: 314.5051\n",
            "\n",
            "Epoch 593: Validation loss decreased (314.505096 --> 314.403412).\n",
            "\t Train_Loss: 138.4572 Val_Loss: 314.4034  BEST VAL Loss: 314.4034\n",
            "\n",
            "Epoch 594: Validation loss decreased (314.403412 --> 314.303802).\n",
            "\t Train_Loss: 138.4556 Val_Loss: 314.3038  BEST VAL Loss: 314.3038\n",
            "\n",
            "Epoch 595: Validation loss decreased (314.303802 --> 314.205200).\n",
            "\t Train_Loss: 138.4542 Val_Loss: 314.2052  BEST VAL Loss: 314.2052\n",
            "\n",
            "Epoch 596: Validation loss decreased (314.205200 --> 314.108612).\n",
            "\t Train_Loss: 138.4528 Val_Loss: 314.1086  BEST VAL Loss: 314.1086\n",
            "\n",
            "Epoch 597: Validation loss decreased (314.108612 --> 314.013153).\n",
            "\t Train_Loss: 138.4514 Val_Loss: 314.0132  BEST VAL Loss: 314.0132\n",
            "\n",
            "Epoch 598: Validation loss decreased (314.013153 --> 313.919281).\n",
            "\t Train_Loss: 138.4501 Val_Loss: 313.9193  BEST VAL Loss: 313.9193\n",
            "\n",
            "Epoch 599: Validation loss decreased (313.919281 --> 313.826721).\n",
            "\t Train_Loss: 138.4488 Val_Loss: 313.8267  BEST VAL Loss: 313.8267\n",
            "\n",
            "Epoch 600: Validation loss decreased (313.826721 --> 313.735657).\n",
            "\t Train_Loss: 138.4476 Val_Loss: 313.7357  BEST VAL Loss: 313.7357\n",
            "\n",
            "Epoch 601: Validation loss decreased (313.735657 --> 313.646149).\n",
            "\t Train_Loss: 138.4464 Val_Loss: 313.6461  BEST VAL Loss: 313.6461\n",
            "\n",
            "Epoch 602: Validation loss decreased (313.646149 --> 313.557465).\n",
            "\t Train_Loss: 138.4453 Val_Loss: 313.5575  BEST VAL Loss: 313.5575\n",
            "\n",
            "Epoch 603: Validation loss decreased (313.557465 --> 313.470520).\n",
            "\t Train_Loss: 138.4441 Val_Loss: 313.4705  BEST VAL Loss: 313.4705\n",
            "\n",
            "Epoch 604: Validation loss decreased (313.470520 --> 313.384979).\n",
            "\t Train_Loss: 138.4430 Val_Loss: 313.3850  BEST VAL Loss: 313.3850\n",
            "\n",
            "Epoch 605: Validation loss decreased (313.384979 --> 313.300507).\n",
            "\t Train_Loss: 138.4419 Val_Loss: 313.3005  BEST VAL Loss: 313.3005\n",
            "\n",
            "Epoch 606: Validation loss decreased (313.300507 --> 313.217590).\n",
            "\t Train_Loss: 138.4409 Val_Loss: 313.2176  BEST VAL Loss: 313.2176\n",
            "\n",
            "Epoch 607: Validation loss decreased (313.217590 --> 313.135803).\n",
            "\t Train_Loss: 138.4400 Val_Loss: 313.1358  BEST VAL Loss: 313.1358\n",
            "\n",
            "Epoch 608: Validation loss decreased (313.135803 --> 313.055389).\n",
            "\t Train_Loss: 138.4389 Val_Loss: 313.0554  BEST VAL Loss: 313.0554\n",
            "\n",
            "Epoch 609: Validation loss decreased (313.055389 --> 312.976074).\n",
            "\t Train_Loss: 138.4381 Val_Loss: 312.9761  BEST VAL Loss: 312.9761\n",
            "\n",
            "Epoch 610: Validation loss decreased (312.976074 --> 312.897919).\n",
            "\t Train_Loss: 138.4372 Val_Loss: 312.8979  BEST VAL Loss: 312.8979\n",
            "\n",
            "Epoch 611: Validation loss decreased (312.897919 --> 312.821014).\n",
            "\t Train_Loss: 138.4363 Val_Loss: 312.8210  BEST VAL Loss: 312.8210\n",
            "\n",
            "Epoch 612: Validation loss decreased (312.821014 --> 312.745300).\n",
            "\t Train_Loss: 138.4355 Val_Loss: 312.7453  BEST VAL Loss: 312.7453\n",
            "\n",
            "Epoch 613: Validation loss decreased (312.745300 --> 312.670898).\n",
            "\t Train_Loss: 138.4346 Val_Loss: 312.6709  BEST VAL Loss: 312.6709\n",
            "\n",
            "Epoch 614: Validation loss decreased (312.670898 --> 312.597473).\n",
            "\t Train_Loss: 138.4338 Val_Loss: 312.5975  BEST VAL Loss: 312.5975\n",
            "\n",
            "Epoch 615: Validation loss decreased (312.597473 --> 312.525238).\n",
            "\t Train_Loss: 138.4331 Val_Loss: 312.5252  BEST VAL Loss: 312.5252\n",
            "\n",
            "Epoch 616: Validation loss decreased (312.525238 --> 312.453949).\n",
            "\t Train_Loss: 138.4323 Val_Loss: 312.4539  BEST VAL Loss: 312.4539\n",
            "\n",
            "Epoch 617: Validation loss decreased (312.453949 --> 312.384003).\n",
            "\t Train_Loss: 138.4316 Val_Loss: 312.3840  BEST VAL Loss: 312.3840\n",
            "\n",
            "Epoch 618: Validation loss decreased (312.384003 --> 312.314972).\n",
            "\t Train_Loss: 138.4309 Val_Loss: 312.3150  BEST VAL Loss: 312.3150\n",
            "\n",
            "Epoch 619: Validation loss decreased (312.314972 --> 312.247040).\n",
            "\t Train_Loss: 138.4302 Val_Loss: 312.2470  BEST VAL Loss: 312.2470\n",
            "\n",
            "Epoch 620: Validation loss decreased (312.247040 --> 312.180481).\n",
            "\t Train_Loss: 138.4295 Val_Loss: 312.1805  BEST VAL Loss: 312.1805\n",
            "\n",
            "Epoch 621: Validation loss decreased (312.180481 --> 312.114746).\n",
            "\t Train_Loss: 138.4290 Val_Loss: 312.1147  BEST VAL Loss: 312.1147\n",
            "\n",
            "Epoch 622: Validation loss decreased (312.114746 --> 312.050049).\n",
            "\t Train_Loss: 138.4284 Val_Loss: 312.0500  BEST VAL Loss: 312.0500\n",
            "\n",
            "Epoch 623: Validation loss decreased (312.050049 --> 311.986237).\n",
            "\t Train_Loss: 138.4277 Val_Loss: 311.9862  BEST VAL Loss: 311.9862\n",
            "\n",
            "Epoch 624: Validation loss decreased (311.986237 --> 311.923431).\n",
            "\t Train_Loss: 138.4271 Val_Loss: 311.9234  BEST VAL Loss: 311.9234\n",
            "\n",
            "Epoch 625: Validation loss decreased (311.923431 --> 311.861786).\n",
            "\t Train_Loss: 138.4266 Val_Loss: 311.8618  BEST VAL Loss: 311.8618\n",
            "\n",
            "Epoch 626: Validation loss decreased (311.861786 --> 311.801178).\n",
            "\t Train_Loss: 138.4260 Val_Loss: 311.8012  BEST VAL Loss: 311.8012\n",
            "\n",
            "Epoch 627: Validation loss decreased (311.801178 --> 311.741089).\n",
            "\t Train_Loss: 138.4256 Val_Loss: 311.7411  BEST VAL Loss: 311.7411\n",
            "\n",
            "Epoch 628: Validation loss decreased (311.741089 --> 311.682037).\n",
            "\t Train_Loss: 138.4250 Val_Loss: 311.6820  BEST VAL Loss: 311.6820\n",
            "\n",
            "Epoch 629: Validation loss decreased (311.682037 --> 311.624329).\n",
            "\t Train_Loss: 138.4245 Val_Loss: 311.6243  BEST VAL Loss: 311.6243\n",
            "\n",
            "Epoch 630: Validation loss decreased (311.624329 --> 311.567139).\n",
            "\t Train_Loss: 138.4240 Val_Loss: 311.5671  BEST VAL Loss: 311.5671\n",
            "\n",
            "Epoch 631: Validation loss decreased (311.567139 --> 311.511169).\n",
            "\t Train_Loss: 138.4236 Val_Loss: 311.5112  BEST VAL Loss: 311.5112\n",
            "\n",
            "Epoch 632: Validation loss decreased (311.511169 --> 311.455719).\n",
            "\t Train_Loss: 138.4232 Val_Loss: 311.4557  BEST VAL Loss: 311.4557\n",
            "\n",
            "Epoch 633: Validation loss decreased (311.455719 --> 311.401581).\n",
            "\t Train_Loss: 138.4227 Val_Loss: 311.4016  BEST VAL Loss: 311.4016\n",
            "\n",
            "Epoch 634: Validation loss decreased (311.401581 --> 311.347900).\n",
            "\t Train_Loss: 138.4223 Val_Loss: 311.3479  BEST VAL Loss: 311.3479\n",
            "\n",
            "Epoch 635: Validation loss decreased (311.347900 --> 311.295502).\n",
            "\t Train_Loss: 138.4219 Val_Loss: 311.2955  BEST VAL Loss: 311.2955\n",
            "\n",
            "Epoch 636: Validation loss decreased (311.295502 --> 311.243622).\n",
            "\t Train_Loss: 138.4215 Val_Loss: 311.2436  BEST VAL Loss: 311.2436\n",
            "\n",
            "Epoch 637: Validation loss decreased (311.243622 --> 311.192566).\n",
            "\t Train_Loss: 138.4212 Val_Loss: 311.1926  BEST VAL Loss: 311.1926\n",
            "\n",
            "Epoch 638: Validation loss decreased (311.192566 --> 311.142395).\n",
            "\t Train_Loss: 138.4207 Val_Loss: 311.1424  BEST VAL Loss: 311.1424\n",
            "\n",
            "Epoch 639: Validation loss decreased (311.142395 --> 311.093140).\n",
            "\t Train_Loss: 138.4204 Val_Loss: 311.0931  BEST VAL Loss: 311.0931\n",
            "\n",
            "Epoch 640: Validation loss decreased (311.093140 --> 311.044189).\n",
            "\t Train_Loss: 138.4201 Val_Loss: 311.0442  BEST VAL Loss: 311.0442\n",
            "\n",
            "Epoch 641: Validation loss decreased (311.044189 --> 310.996582).\n",
            "\t Train_Loss: 138.4197 Val_Loss: 310.9966  BEST VAL Loss: 310.9966\n",
            "\n",
            "Epoch 642: Validation loss decreased (310.996582 --> 310.949554).\n",
            "\t Train_Loss: 138.4195 Val_Loss: 310.9496  BEST VAL Loss: 310.9496\n",
            "\n",
            "Epoch 643: Validation loss decreased (310.949554 --> 310.903381).\n",
            "\t Train_Loss: 138.4191 Val_Loss: 310.9034  BEST VAL Loss: 310.9034\n",
            "\n",
            "Epoch 644: Validation loss decreased (310.903381 --> 310.857666).\n",
            "\t Train_Loss: 138.4189 Val_Loss: 310.8577  BEST VAL Loss: 310.8577\n",
            "\n",
            "Epoch 645: Validation loss decreased (310.857666 --> 310.812836).\n",
            "\t Train_Loss: 138.4186 Val_Loss: 310.8128  BEST VAL Loss: 310.8128\n",
            "\n",
            "Epoch 646: Validation loss decreased (310.812836 --> 310.768890).\n",
            "\t Train_Loss: 138.4183 Val_Loss: 310.7689  BEST VAL Loss: 310.7689\n",
            "\n",
            "Epoch 647: Validation loss decreased (310.768890 --> 310.725372).\n",
            "\t Train_Loss: 138.4180 Val_Loss: 310.7254  BEST VAL Loss: 310.7254\n",
            "\n",
            "Epoch 648: Validation loss decreased (310.725372 --> 310.682739).\n",
            "\t Train_Loss: 138.4178 Val_Loss: 310.6827  BEST VAL Loss: 310.6827\n",
            "\n",
            "Epoch 649: Validation loss decreased (310.682739 --> 310.640594).\n",
            "\t Train_Loss: 138.4176 Val_Loss: 310.6406  BEST VAL Loss: 310.6406\n",
            "\n",
            "Epoch 650: Validation loss decreased (310.640594 --> 310.599487).\n",
            "\t Train_Loss: 138.4172 Val_Loss: 310.5995  BEST VAL Loss: 310.5995\n",
            "\n",
            "Epoch 651: Validation loss decreased (310.599487 --> 310.558655).\n",
            "\t Train_Loss: 138.4171 Val_Loss: 310.5587  BEST VAL Loss: 310.5587\n",
            "\n",
            "Epoch 652: Validation loss decreased (310.558655 --> 310.518921).\n",
            "\t Train_Loss: 138.4167 Val_Loss: 310.5189  BEST VAL Loss: 310.5189\n",
            "\n",
            "Epoch 653: Validation loss decreased (310.518921 --> 310.479370).\n",
            "\t Train_Loss: 138.4166 Val_Loss: 310.4794  BEST VAL Loss: 310.4794\n",
            "\n",
            "Epoch 654: Validation loss decreased (310.479370 --> 310.440765).\n",
            "\t Train_Loss: 138.4164 Val_Loss: 310.4408  BEST VAL Loss: 310.4408\n",
            "\n",
            "Epoch 655: Validation loss decreased (310.440765 --> 310.402740).\n",
            "\t Train_Loss: 138.4161 Val_Loss: 310.4027  BEST VAL Loss: 310.4027\n",
            "\n",
            "Epoch 656: Validation loss decreased (310.402740 --> 310.365448).\n",
            "\t Train_Loss: 138.4160 Val_Loss: 310.3654  BEST VAL Loss: 310.3654\n",
            "\n",
            "Epoch 657: Validation loss decreased (310.365448 --> 310.328766).\n",
            "\t Train_Loss: 138.4158 Val_Loss: 310.3288  BEST VAL Loss: 310.3288\n",
            "\n",
            "Epoch 658: Validation loss decreased (310.328766 --> 310.292084).\n",
            "\t Train_Loss: 138.4156 Val_Loss: 310.2921  BEST VAL Loss: 310.2921\n",
            "\n",
            "Epoch 659: Validation loss decreased (310.292084 --> 310.256805).\n",
            "\t Train_Loss: 138.4154 Val_Loss: 310.2568  BEST VAL Loss: 310.2568\n",
            "\n",
            "Epoch 660: Validation loss decreased (310.256805 --> 310.221436).\n",
            "\t Train_Loss: 138.4153 Val_Loss: 310.2214  BEST VAL Loss: 310.2214\n",
            "\n",
            "Epoch 661: Validation loss decreased (310.221436 --> 310.187195).\n",
            "\t Train_Loss: 138.4150 Val_Loss: 310.1872  BEST VAL Loss: 310.1872\n",
            "\n",
            "Epoch 662: Validation loss decreased (310.187195 --> 310.153229).\n",
            "\t Train_Loss: 138.4149 Val_Loss: 310.1532  BEST VAL Loss: 310.1532\n",
            "\n",
            "Epoch 663: Validation loss decreased (310.153229 --> 310.120087).\n",
            "\t Train_Loss: 138.4147 Val_Loss: 310.1201  BEST VAL Loss: 310.1201\n",
            "\n",
            "Epoch 664: Validation loss decreased (310.120087 --> 310.087189).\n",
            "\t Train_Loss: 138.4146 Val_Loss: 310.0872  BEST VAL Loss: 310.0872\n",
            "\n",
            "Epoch 665: Validation loss decreased (310.087189 --> 310.054901).\n",
            "\t Train_Loss: 138.4144 Val_Loss: 310.0549  BEST VAL Loss: 310.0549\n",
            "\n",
            "Epoch 666: Validation loss decreased (310.054901 --> 310.023132).\n",
            "\t Train_Loss: 138.4143 Val_Loss: 310.0231  BEST VAL Loss: 310.0231\n",
            "\n",
            "Epoch 667: Validation loss decreased (310.023132 --> 309.991943).\n",
            "\t Train_Loss: 138.4141 Val_Loss: 309.9919  BEST VAL Loss: 309.9919\n",
            "\n",
            "Epoch 668: Validation loss decreased (309.991943 --> 309.961517).\n",
            "\t Train_Loss: 138.4138 Val_Loss: 309.9615  BEST VAL Loss: 309.9615\n",
            "\n",
            "Epoch 669: Validation loss decreased (309.961517 --> 309.931061).\n",
            "\t Train_Loss: 138.4136 Val_Loss: 309.9311  BEST VAL Loss: 309.9311\n",
            "\n",
            "Epoch 670: Validation loss decreased (309.931061 --> 309.902100).\n",
            "\t Train_Loss: 138.4128 Val_Loss: 309.9021  BEST VAL Loss: 309.9021\n",
            "\n",
            "Epoch 671: Validation loss did not decrease\n",
            "\t Train_Loss: 138.4071 Val_Loss: 309.9896  BEST VAL Loss: 309.9021\n",
            "\n",
            "Epoch 672: Validation loss did not decrease\n",
            "\t Train_Loss: 137.7762 Val_Loss: 330.3114  BEST VAL Loss: 309.9021\n",
            "\n",
            "Epoch 673: Validation loss did not decrease\n",
            "\t Train_Loss: 128.5654 Val_Loss: 310.3348  BEST VAL Loss: 309.9021\n",
            "\n",
            "Epoch 674: Validation loss did not decrease\n",
            "\t Train_Loss: 135.9809 Val_Loss: 310.9414  BEST VAL Loss: 309.9021\n",
            "\n",
            "Epoch 675: Validation loss did not decrease\n",
            "\t Train_Loss: 133.4625 Val_Loss: 356.4060  BEST VAL Loss: 309.9021\n",
            "\n",
            "Epoch 676: Validation loss did not decrease\n",
            "\t Train_Loss: 133.7345 Val_Loss: 314.8973  BEST VAL Loss: 309.9021\n",
            "\n",
            "Epoch 677: Validation loss did not decrease\n",
            "\t Train_Loss: 127.1083 Val_Loss: 312.1369  BEST VAL Loss: 309.9021\n",
            "\n",
            "Epoch 678: Validation loss did not decrease\n",
            "\t Train_Loss: 128.6347 Val_Loss: 319.8792  BEST VAL Loss: 309.9021\n",
            "\n",
            "Epoch 679: Validation loss did not decrease\n",
            "\t Train_Loss: 124.7341 Val_Loss: 337.5287  BEST VAL Loss: 309.9021\n",
            "\n",
            "Epoch 680: Validation loss did not decrease\n",
            "\t Train_Loss: 127.8621 Val_Loss: 317.6168  BEST VAL Loss: 309.9021\n",
            "\n",
            "Epoch 681: Validation loss did not decrease\n",
            "\t Train_Loss: 123.7116 Val_Loss: 311.1880  BEST VAL Loss: 309.9021\n",
            "\n",
            "Epoch 682: Validation loss did not decrease\n",
            "\t Train_Loss: 125.4632 Val_Loss: 312.6087  BEST VAL Loss: 309.9021\n",
            "\n",
            "Epoch 683: Validation loss did not decrease\n",
            "\t Train_Loss: 123.4551 Val_Loss: 322.7215  BEST VAL Loss: 309.9021\n",
            "\n",
            "Epoch 684: Validation loss did not decrease\n",
            "\t Train_Loss: 123.1988 Val_Loss: 322.4975  BEST VAL Loss: 309.9021\n",
            "\n",
            "Epoch 685: Validation loss did not decrease\n",
            "\t Train_Loss: 122.9709 Val_Loss: 312.1484  BEST VAL Loss: 309.9021\n",
            "\n",
            "Epoch 686: Validation loss decreased (309.902100 --> 308.998505).\n",
            "\t Train_Loss: 121.5085 Val_Loss: 308.9985  BEST VAL Loss: 308.9985\n",
            "\n",
            "Epoch 687: Validation loss did not decrease\n",
            "\t Train_Loss: 122.2069 Val_Loss: 310.1790  BEST VAL Loss: 308.9985\n",
            "\n",
            "Epoch 688: Validation loss did not decrease\n",
            "\t Train_Loss: 120.8174 Val_Loss: 315.2621  BEST VAL Loss: 308.9985\n",
            "\n",
            "Epoch 689: Validation loss did not decrease\n",
            "\t Train_Loss: 120.7369 Val_Loss: 315.2772  BEST VAL Loss: 308.9985\n",
            "\n",
            "Epoch 690: Validation loss did not decrease\n",
            "\t Train_Loss: 120.6571 Val_Loss: 309.2897  BEST VAL Loss: 308.9985\n",
            "\n",
            "Epoch 691: Validation loss decreased (308.998505 --> 306.241272).\n",
            "\t Train_Loss: 119.6167 Val_Loss: 306.2413  BEST VAL Loss: 306.2413\n",
            "\n",
            "Epoch 692: Validation loss decreased (306.241272 --> 306.039368).\n",
            "\t Train_Loss: 119.9564 Val_Loss: 306.0394  BEST VAL Loss: 306.0394\n",
            "\n",
            "Epoch 693: Validation loss did not decrease\n",
            "\t Train_Loss: 119.4138 Val_Loss: 308.0629  BEST VAL Loss: 306.0394\n",
            "\n",
            "Epoch 694: Validation loss did not decrease\n",
            "\t Train_Loss: 118.8989 Val_Loss: 309.5153  BEST VAL Loss: 306.0394\n",
            "\n",
            "Epoch 695: Validation loss did not decrease\n",
            "\t Train_Loss: 119.1208 Val_Loss: 306.7759  BEST VAL Loss: 306.0394\n",
            "\n",
            "Epoch 696: Validation loss decreased (306.039368 --> 303.687195).\n",
            "\t Train_Loss: 118.5199 Val_Loss: 303.6872  BEST VAL Loss: 303.6872\n",
            "\n",
            "Epoch 697: Validation loss decreased (303.687195 --> 302.464813).\n",
            "\t Train_Loss: 118.3525 Val_Loss: 302.4648  BEST VAL Loss: 302.4648\n",
            "\n",
            "Epoch 698: Validation loss did not decrease\n",
            "\t Train_Loss: 118.3886 Val_Loss: 302.6670  BEST VAL Loss: 302.4648\n",
            "\n",
            "Epoch 699: Validation loss did not decrease\n",
            "\t Train_Loss: 117.9157 Val_Loss: 303.7587  BEST VAL Loss: 302.4648\n",
            "\n",
            "Epoch 700: Validation loss did not decrease\n",
            "\t Train_Loss: 117.8357 Val_Loss: 303.5619  BEST VAL Loss: 302.4648\n",
            "\n",
            "Epoch 701: Validation loss decreased (302.464813 --> 301.472473).\n",
            "\t Train_Loss: 117.8004 Val_Loss: 301.4725  BEST VAL Loss: 301.4725\n",
            "\n",
            "Epoch 702: Validation loss decreased (301.472473 --> 299.690857).\n",
            "\t Train_Loss: 117.4308 Val_Loss: 299.6909  BEST VAL Loss: 299.6909\n",
            "\n",
            "Epoch 703: Validation loss decreased (299.690857 --> 298.926392).\n",
            "\t Train_Loss: 117.3994 Val_Loss: 298.9264  BEST VAL Loss: 298.9264\n",
            "\n",
            "Epoch 704: Validation loss did not decrease\n",
            "\t Train_Loss: 117.3211 Val_Loss: 298.9841  BEST VAL Loss: 298.9264\n",
            "\n",
            "Epoch 705: Validation loss did not decrease\n",
            "\t Train_Loss: 117.0436 Val_Loss: 299.3673  BEST VAL Loss: 298.9264\n",
            "\n",
            "Epoch 706: Validation loss decreased (298.926392 --> 298.874573).\n",
            "\t Train_Loss: 117.0053 Val_Loss: 298.8746  BEST VAL Loss: 298.8746\n",
            "\n",
            "Epoch 707: Validation loss decreased (298.874573 --> 297.440491).\n",
            "\t Train_Loss: 116.9251 Val_Loss: 297.4405  BEST VAL Loss: 297.4405\n",
            "\n",
            "Epoch 708: Validation loss decreased (297.440491 --> 296.198456).\n",
            "\t Train_Loss: 116.7074 Val_Loss: 296.1985  BEST VAL Loss: 296.1985\n",
            "\n",
            "Epoch 709: Validation loss decreased (296.198456 --> 295.557617).\n",
            "\t Train_Loss: 116.6669 Val_Loss: 295.5576  BEST VAL Loss: 295.5576\n",
            "\n",
            "Epoch 710: Validation loss decreased (295.557617 --> 295.428680).\n",
            "\t Train_Loss: 116.5892 Val_Loss: 295.4287  BEST VAL Loss: 295.4287\n",
            "\n",
            "Epoch 711: Validation loss did not decrease\n",
            "\t Train_Loss: 116.4160 Val_Loss: 295.4655  BEST VAL Loss: 295.4287\n",
            "\n",
            "Epoch 712: Validation loss decreased (295.428680 --> 295.006195).\n",
            "\t Train_Loss: 116.3645 Val_Loss: 295.0062  BEST VAL Loss: 295.0062\n",
            "\n",
            "Epoch 713: Validation loss decreased (295.006195 --> 293.988342).\n",
            "\t Train_Loss: 116.2968 Val_Loss: 293.9883  BEST VAL Loss: 293.9883\n",
            "\n",
            "Epoch 714: Validation loss decreased (293.988342 --> 293.021271).\n",
            "\t Train_Loss: 116.1543 Val_Loss: 293.0213  BEST VAL Loss: 293.0213\n",
            "\n",
            "Epoch 715: Validation loss decreased (293.021271 --> 292.420135).\n",
            "\t Train_Loss: 116.0985 Val_Loss: 292.4201  BEST VAL Loss: 292.4201\n",
            "\n",
            "Epoch 716: Validation loss decreased (292.420135 --> 292.163055).\n",
            "\t Train_Loss: 116.0373 Val_Loss: 292.1631  BEST VAL Loss: 292.1631\n",
            "\n",
            "Epoch 717: Validation loss decreased (292.163055 --> 292.035706).\n",
            "\t Train_Loss: 115.9193 Val_Loss: 292.0357  BEST VAL Loss: 292.0357\n",
            "\n",
            "Epoch 718: Validation loss decreased (292.035706 --> 291.650787).\n",
            "\t Train_Loss: 115.8577 Val_Loss: 291.6508  BEST VAL Loss: 291.6508\n",
            "\n",
            "Epoch 719: Validation loss decreased (291.650787 --> 290.902039).\n",
            "\t Train_Loss: 115.8035 Val_Loss: 290.9020  BEST VAL Loss: 290.9020\n",
            "\n",
            "Epoch 720: Validation loss decreased (290.902039 --> 290.113037).\n",
            "\t Train_Loss: 115.7035 Val_Loss: 290.1130  BEST VAL Loss: 290.1130\n",
            "\n",
            "Epoch 721: Validation loss decreased (290.113037 --> 289.538300).\n",
            "\t Train_Loss: 115.6406 Val_Loss: 289.5383  BEST VAL Loss: 289.5383\n",
            "\n",
            "Epoch 722: Validation loss decreased (289.538300 --> 289.206085).\n",
            "\t Train_Loss: 115.5899 Val_Loss: 289.2061  BEST VAL Loss: 289.2061\n",
            "\n",
            "Epoch 723: Validation loss decreased (289.206085 --> 288.994141).\n",
            "\t Train_Loss: 115.5052 Val_Loss: 288.9941  BEST VAL Loss: 288.9941\n",
            "\n",
            "Epoch 724: Validation loss decreased (288.994141 --> 288.666595).\n",
            "\t Train_Loss: 115.4410 Val_Loss: 288.6666  BEST VAL Loss: 288.6666\n",
            "\n",
            "Epoch 725: Validation loss decreased (288.666595 --> 288.101410).\n",
            "\t Train_Loss: 115.3932 Val_Loss: 288.1014  BEST VAL Loss: 288.1014\n",
            "\n",
            "Epoch 726: Validation loss decreased (288.101410 --> 287.449921).\n",
            "\t Train_Loss: 115.3202 Val_Loss: 287.4499  BEST VAL Loss: 287.4499\n",
            "\n",
            "Epoch 727: Validation loss decreased (287.449921 --> 286.908264).\n",
            "\t Train_Loss: 115.2573 Val_Loss: 286.9083  BEST VAL Loss: 286.9083\n",
            "\n",
            "Epoch 728: Validation loss decreased (286.908264 --> 286.537018).\n",
            "\t Train_Loss: 115.2104 Val_Loss: 286.5370  BEST VAL Loss: 286.5370\n",
            "\n",
            "Epoch 729: Validation loss decreased (286.537018 --> 286.277313).\n",
            "\t Train_Loss: 115.1470 Val_Loss: 286.2773  BEST VAL Loss: 286.2773\n",
            "\n",
            "Epoch 730: Validation loss decreased (286.277313 --> 285.985779).\n",
            "\t Train_Loss: 115.0860 Val_Loss: 285.9858  BEST VAL Loss: 285.9858\n",
            "\n",
            "Epoch 731: Validation loss decreased (285.985779 --> 285.547943).\n",
            "\t Train_Loss: 115.0396 Val_Loss: 285.5479  BEST VAL Loss: 285.5479\n",
            "\n",
            "Epoch 732: Validation loss decreased (285.547943 --> 285.011963).\n",
            "\t Train_Loss: 114.9834 Val_Loss: 285.0120  BEST VAL Loss: 285.0120\n",
            "\n",
            "Epoch 733: Validation loss decreased (285.011963 --> 284.513153).\n",
            "\t Train_Loss: 114.9253 Val_Loss: 284.5132  BEST VAL Loss: 284.5132\n",
            "\n",
            "Epoch 734: Validation loss decreased (284.513153 --> 284.127533).\n",
            "\t Train_Loss: 114.8790 Val_Loss: 284.1275  BEST VAL Loss: 284.1275\n",
            "\n",
            "Epoch 735: Validation loss decreased (284.127533 --> 283.840088).\n",
            "\t Train_Loss: 114.8282 Val_Loss: 283.8401  BEST VAL Loss: 283.8401\n",
            "\n",
            "Epoch 736: Validation loss decreased (283.840088 --> 283.567719).\n",
            "\t Train_Loss: 114.7733 Val_Loss: 283.5677  BEST VAL Loss: 283.5677\n",
            "\n",
            "Epoch 737: Validation loss decreased (283.567719 --> 283.216858).\n",
            "\t Train_Loss: 114.7271 Val_Loss: 283.2169  BEST VAL Loss: 283.2169\n",
            "\n",
            "Epoch 738: Validation loss decreased (283.216858 --> 282.779114).\n",
            "\t Train_Loss: 114.6802 Val_Loss: 282.7791  BEST VAL Loss: 282.7791\n",
            "\n",
            "Epoch 739: Validation loss decreased (282.779114 --> 282.332489).\n",
            "\t Train_Loss: 114.6288 Val_Loss: 282.3325  BEST VAL Loss: 282.3325\n",
            "\n",
            "Epoch 740: Validation loss decreased (282.332489 --> 281.950562).\n",
            "\t Train_Loss: 114.5829 Val_Loss: 281.9506  BEST VAL Loss: 281.9506\n",
            "\n",
            "Epoch 741: Validation loss decreased (281.950562 --> 281.648651).\n",
            "\t Train_Loss: 114.5385 Val_Loss: 281.6487  BEST VAL Loss: 281.6487\n",
            "\n",
            "Epoch 742: Validation loss decreased (281.648651 --> 281.384521).\n",
            "\t Train_Loss: 114.4904 Val_Loss: 281.3845  BEST VAL Loss: 281.3845\n",
            "\n",
            "Epoch 743: Validation loss decreased (281.384521 --> 281.090942).\n",
            "\t Train_Loss: 114.4450 Val_Loss: 281.0909  BEST VAL Loss: 281.0909\n",
            "\n",
            "Epoch 744: Validation loss decreased (281.090942 --> 280.733490).\n",
            "\t Train_Loss: 114.4024 Val_Loss: 280.7335  BEST VAL Loss: 280.7335\n",
            "\n",
            "Epoch 745: Validation loss decreased (280.733490 --> 280.344208).\n",
            "\t Train_Loss: 114.3571 Val_Loss: 280.3442  BEST VAL Loss: 280.3442\n",
            "\n",
            "Epoch 746: Validation loss decreased (280.344208 --> 279.981201).\n",
            "\t Train_Loss: 114.3127 Val_Loss: 279.9812  BEST VAL Loss: 279.9812\n",
            "\n",
            "Epoch 747: Validation loss decreased (279.981201 --> 279.675323).\n",
            "\t Train_Loss: 114.2711 Val_Loss: 279.6753  BEST VAL Loss: 279.6753\n",
            "\n",
            "Epoch 748: Validation loss decreased (279.675323 --> 279.414215).\n",
            "\t Train_Loss: 114.2281 Val_Loss: 279.4142  BEST VAL Loss: 279.4142\n",
            "\n",
            "Epoch 749: Validation loss decreased (279.414215 --> 279.155487).\n",
            "\t Train_Loss: 114.1849 Val_Loss: 279.1555  BEST VAL Loss: 279.1555\n",
            "\n",
            "Epoch 750: Validation loss decreased (279.155487 --> 278.860260).\n",
            "\t Train_Loss: 114.1440 Val_Loss: 278.8603  BEST VAL Loss: 278.8603\n",
            "\n",
            "Epoch 751: Validation loss decreased (278.860260 --> 278.528961).\n",
            "\t Train_Loss: 114.1028 Val_Loss: 278.5290  BEST VAL Loss: 278.5290\n",
            "\n",
            "Epoch 752: Validation loss decreased (278.528961 --> 278.196198).\n",
            "\t Train_Loss: 114.0609 Val_Loss: 278.1962  BEST VAL Loss: 278.1962\n",
            "\n",
            "Epoch 753: Validation loss decreased (278.196198 --> 277.896637).\n",
            "\t Train_Loss: 114.0205 Val_Loss: 277.8966  BEST VAL Loss: 277.8966\n",
            "\n",
            "Epoch 754: Validation loss decreased (277.896637 --> 277.637451).\n",
            "\t Train_Loss: 113.9806 Val_Loss: 277.6375  BEST VAL Loss: 277.6375\n",
            "\n",
            "Epoch 755: Validation loss decreased (277.637451 --> 277.398254).\n",
            "\t Train_Loss: 113.9399 Val_Loss: 277.3983  BEST VAL Loss: 277.3983\n",
            "\n",
            "Epoch 756: Validation loss decreased (277.398254 --> 277.147125).\n",
            "\t Train_Loss: 113.9000 Val_Loss: 277.1471  BEST VAL Loss: 277.1471\n",
            "\n",
            "Epoch 757: Validation loss decreased (277.147125 --> 276.868164).\n",
            "\t Train_Loss: 113.8609 Val_Loss: 276.8682  BEST VAL Loss: 276.8682\n",
            "\n",
            "Epoch 758: Validation loss decreased (276.868164 --> 276.573822).\n",
            "\t Train_Loss: 113.8214 Val_Loss: 276.5738  BEST VAL Loss: 276.5738\n",
            "\n",
            "Epoch 759: Validation loss decreased (276.573822 --> 276.290863).\n",
            "\t Train_Loss: 113.7820 Val_Loss: 276.2909  BEST VAL Loss: 276.2909\n",
            "\n",
            "Epoch 760: Validation loss decreased (276.290863 --> 276.036499).\n",
            "\t Train_Loss: 113.7435 Val_Loss: 276.0365  BEST VAL Loss: 276.0365\n",
            "\n",
            "Epoch 761: Validation loss decreased (276.036499 --> 275.807831).\n",
            "\t Train_Loss: 113.7048 Val_Loss: 275.8078  BEST VAL Loss: 275.8078\n",
            "\n",
            "Epoch 762: Validation loss decreased (275.807831 --> 275.585358).\n",
            "\t Train_Loss: 113.6659 Val_Loss: 275.5854  BEST VAL Loss: 275.5854\n",
            "\n",
            "Epoch 763: Validation loss decreased (275.585358 --> 275.349457).\n",
            "\t Train_Loss: 113.6277 Val_Loss: 275.3495  BEST VAL Loss: 275.3495\n",
            "\n",
            "Epoch 764: Validation loss decreased (275.349457 --> 275.095795).\n",
            "\t Train_Loss: 113.5896 Val_Loss: 275.0958  BEST VAL Loss: 275.0958\n",
            "\n",
            "Epoch 765: Validation loss decreased (275.095795 --> 274.838196).\n",
            "\t Train_Loss: 113.5513 Val_Loss: 274.8382  BEST VAL Loss: 274.8382\n",
            "\n",
            "Epoch 766: Validation loss decreased (274.838196 --> 274.594696).\n",
            "\t Train_Loss: 113.5132 Val_Loss: 274.5947  BEST VAL Loss: 274.5947\n",
            "\n",
            "Epoch 767: Validation loss decreased (274.594696 --> 274.373505).\n",
            "\t Train_Loss: 113.4754 Val_Loss: 274.3735  BEST VAL Loss: 274.3735\n",
            "\n",
            "Epoch 768: Validation loss decreased (274.373505 --> 274.167572).\n",
            "\t Train_Loss: 113.4375 Val_Loss: 274.1676  BEST VAL Loss: 274.1676\n",
            "\n",
            "Epoch 769: Validation loss decreased (274.167572 --> 273.962708).\n",
            "\t Train_Loss: 113.3996 Val_Loss: 273.9627  BEST VAL Loss: 273.9627\n",
            "\n",
            "Epoch 770: Validation loss decreased (273.962708 --> 273.746552).\n",
            "\t Train_Loss: 113.3620 Val_Loss: 273.7466  BEST VAL Loss: 273.7466\n",
            "\n",
            "Epoch 771: Validation loss decreased (273.746552 --> 273.520599).\n",
            "\t Train_Loss: 113.3242 Val_Loss: 273.5206  BEST VAL Loss: 273.5206\n",
            "\n",
            "Epoch 772: Validation loss decreased (273.520599 --> 273.295593).\n",
            "\t Train_Loss: 113.2864 Val_Loss: 273.2956  BEST VAL Loss: 273.2956\n",
            "\n",
            "Epoch 773: Validation loss decreased (273.295593 --> 273.083710).\n",
            "\t Train_Loss: 113.2487 Val_Loss: 273.0837  BEST VAL Loss: 273.0837\n",
            "\n",
            "Epoch 774: Validation loss decreased (273.083710 --> 272.888062).\n",
            "\t Train_Loss: 113.2111 Val_Loss: 272.8881  BEST VAL Loss: 272.8881\n",
            "\n",
            "Epoch 775: Validation loss decreased (272.888062 --> 272.702515).\n",
            "\t Train_Loss: 113.1733 Val_Loss: 272.7025  BEST VAL Loss: 272.7025\n",
            "\n",
            "Epoch 776: Validation loss decreased (272.702515 --> 272.516174).\n",
            "\t Train_Loss: 113.1355 Val_Loss: 272.5162  BEST VAL Loss: 272.5162\n",
            "\n",
            "Epoch 777: Validation loss decreased (272.516174 --> 272.322327).\n",
            "\t Train_Loss: 113.0977 Val_Loss: 272.3223  BEST VAL Loss: 272.3223\n",
            "\n",
            "Epoch 778: Validation loss decreased (272.322327 --> 272.123047).\n",
            "\t Train_Loss: 113.0597 Val_Loss: 272.1230  BEST VAL Loss: 272.1230\n",
            "\n",
            "Epoch 779: Validation loss decreased (272.123047 --> 271.926239).\n",
            "\t Train_Loss: 113.0217 Val_Loss: 271.9262  BEST VAL Loss: 271.9262\n",
            "\n",
            "Epoch 780: Validation loss decreased (271.926239 --> 271.740082).\n",
            "\t Train_Loss: 112.9836 Val_Loss: 271.7401  BEST VAL Loss: 271.7401\n",
            "\n",
            "Epoch 781: Validation loss decreased (271.740082 --> 271.566376).\n",
            "\t Train_Loss: 112.9454 Val_Loss: 271.5664  BEST VAL Loss: 271.5664\n",
            "\n",
            "Epoch 782: Validation loss decreased (271.566376 --> 271.399750).\n",
            "\t Train_Loss: 112.9071 Val_Loss: 271.3997  BEST VAL Loss: 271.3997\n",
            "\n",
            "Epoch 783: Validation loss decreased (271.399750 --> 271.233246).\n",
            "\t Train_Loss: 112.8686 Val_Loss: 271.2332  BEST VAL Loss: 271.2332\n",
            "\n",
            "Epoch 784: Validation loss decreased (271.233246 --> 271.062073).\n",
            "\t Train_Loss: 112.8300 Val_Loss: 271.0621  BEST VAL Loss: 271.0621\n",
            "\n",
            "Epoch 785: Validation loss decreased (271.062073 --> 270.887634).\n",
            "\t Train_Loss: 112.7912 Val_Loss: 270.8876  BEST VAL Loss: 270.8876\n",
            "\n",
            "Epoch 786: Validation loss decreased (270.887634 --> 270.715759).\n",
            "\t Train_Loss: 112.7522 Val_Loss: 270.7158  BEST VAL Loss: 270.7158\n",
            "\n",
            "Epoch 787: Validation loss decreased (270.715759 --> 270.552094).\n",
            "\t Train_Loss: 112.7130 Val_Loss: 270.5521  BEST VAL Loss: 270.5521\n",
            "\n",
            "Epoch 788: Validation loss decreased (270.552094 --> 270.397614).\n",
            "\t Train_Loss: 112.6737 Val_Loss: 270.3976  BEST VAL Loss: 270.3976\n",
            "\n",
            "Epoch 789: Validation loss decreased (270.397614 --> 270.249756).\n",
            "\t Train_Loss: 112.6340 Val_Loss: 270.2498  BEST VAL Loss: 270.2498\n",
            "\n",
            "Epoch 790: Validation loss decreased (270.249756 --> 270.103424).\n",
            "\t Train_Loss: 112.5941 Val_Loss: 270.1034  BEST VAL Loss: 270.1034\n",
            "\n",
            "Epoch 791: Validation loss decreased (270.103424 --> 269.954559).\n",
            "\t Train_Loss: 112.5540 Val_Loss: 269.9546  BEST VAL Loss: 269.9546\n",
            "\n",
            "Epoch 792: Validation loss decreased (269.954559 --> 269.803558).\n",
            "\t Train_Loss: 112.5136 Val_Loss: 269.8036  BEST VAL Loss: 269.8036\n",
            "\n",
            "Epoch 793: Validation loss decreased (269.803558 --> 269.654388).\n",
            "\t Train_Loss: 112.4729 Val_Loss: 269.6544  BEST VAL Loss: 269.6544\n",
            "\n",
            "Epoch 794: Validation loss decreased (269.654388 --> 269.510986).\n",
            "\t Train_Loss: 112.4319 Val_Loss: 269.5110  BEST VAL Loss: 269.5110\n",
            "\n",
            "Epoch 795: Validation loss decreased (269.510986 --> 269.375305).\n",
            "\t Train_Loss: 112.3906 Val_Loss: 269.3753  BEST VAL Loss: 269.3753\n",
            "\n",
            "Epoch 796: Validation loss decreased (269.375305 --> 269.245544).\n",
            "\t Train_Loss: 112.3489 Val_Loss: 269.2455  BEST VAL Loss: 269.2455\n",
            "\n",
            "Epoch 797: Validation loss decreased (269.245544 --> 269.118652).\n",
            "\t Train_Loss: 112.3068 Val_Loss: 269.1187  BEST VAL Loss: 269.1187\n",
            "\n",
            "Epoch 798: Validation loss decreased (269.118652 --> 268.991425).\n",
            "\t Train_Loss: 112.2644 Val_Loss: 268.9914  BEST VAL Loss: 268.9914\n",
            "\n",
            "Epoch 799: Validation loss decreased (268.991425 --> 268.863251).\n",
            "\t Train_Loss: 112.2216 Val_Loss: 268.8633  BEST VAL Loss: 268.8633\n",
            "\n",
            "Epoch 800: Validation loss decreased (268.863251 --> 268.735474).\n",
            "\t Train_Loss: 112.1784 Val_Loss: 268.7355  BEST VAL Loss: 268.7355\n",
            "\n",
            "Epoch 801: Validation loss decreased (268.735474 --> 268.611481).\n",
            "\t Train_Loss: 112.1347 Val_Loss: 268.6115  BEST VAL Loss: 268.6115\n",
            "\n",
            "Epoch 802: Validation loss decreased (268.611481 --> 268.493317).\n",
            "\t Train_Loss: 112.0905 Val_Loss: 268.4933  BEST VAL Loss: 268.4933\n",
            "\n",
            "Epoch 803: Validation loss decreased (268.493317 --> 268.381256).\n",
            "\t Train_Loss: 112.0459 Val_Loss: 268.3813  BEST VAL Loss: 268.3813\n",
            "\n",
            "Epoch 804: Validation loss decreased (268.381256 --> 268.272797).\n",
            "\t Train_Loss: 112.0008 Val_Loss: 268.2728  BEST VAL Loss: 268.2728\n",
            "\n",
            "Epoch 805: Validation loss decreased (268.272797 --> 268.165985).\n",
            "\t Train_Loss: 111.9552 Val_Loss: 268.1660  BEST VAL Loss: 268.1660\n",
            "\n",
            "Epoch 806: Validation loss decreased (268.165985 --> 268.058929).\n",
            "\t Train_Loss: 111.9091 Val_Loss: 268.0589  BEST VAL Loss: 268.0589\n",
            "\n",
            "Epoch 807: Validation loss decreased (268.058929 --> 267.952393).\n",
            "\t Train_Loss: 111.8624 Val_Loss: 267.9524  BEST VAL Loss: 267.9524\n",
            "\n",
            "Epoch 808: Validation loss decreased (267.952393 --> 267.847565).\n",
            "\t Train_Loss: 111.8151 Val_Loss: 267.8476  BEST VAL Loss: 267.8476\n",
            "\n",
            "Epoch 809: Validation loss decreased (267.847565 --> 267.746246).\n",
            "\t Train_Loss: 111.7672 Val_Loss: 267.7462  BEST VAL Loss: 267.7462\n",
            "\n",
            "Epoch 810: Validation loss decreased (267.746246 --> 267.649902).\n",
            "\t Train_Loss: 111.7188 Val_Loss: 267.6499  BEST VAL Loss: 267.6499\n",
            "\n",
            "Epoch 811: Validation loss decreased (267.649902 --> 267.557861).\n",
            "\t Train_Loss: 111.6697 Val_Loss: 267.5579  BEST VAL Loss: 267.5579\n",
            "\n",
            "Epoch 812: Validation loss decreased (267.557861 --> 267.468506).\n",
            "\t Train_Loss: 111.6199 Val_Loss: 267.4685  BEST VAL Loss: 267.4685\n",
            "\n",
            "Epoch 813: Validation loss decreased (267.468506 --> 267.380219).\n",
            "\t Train_Loss: 111.5695 Val_Loss: 267.3802  BEST VAL Loss: 267.3802\n",
            "\n",
            "Epoch 814: Validation loss decreased (267.380219 --> 267.291992).\n",
            "\t Train_Loss: 111.5185 Val_Loss: 267.2920  BEST VAL Loss: 267.2920\n",
            "\n",
            "Epoch 815: Validation loss decreased (267.291992 --> 267.204529).\n",
            "\t Train_Loss: 111.4668 Val_Loss: 267.2045  BEST VAL Loss: 267.2045\n",
            "\n",
            "Epoch 816: Validation loss decreased (267.204529 --> 267.118469).\n",
            "\t Train_Loss: 111.4144 Val_Loss: 267.1185  BEST VAL Loss: 267.1185\n",
            "\n",
            "Epoch 817: Validation loss decreased (267.118469 --> 267.035004).\n",
            "\t Train_Loss: 111.3613 Val_Loss: 267.0350  BEST VAL Loss: 267.0350\n",
            "\n",
            "Epoch 818: Validation loss decreased (267.035004 --> 266.954803).\n",
            "\t Train_Loss: 111.3075 Val_Loss: 266.9548  BEST VAL Loss: 266.9548\n",
            "\n",
            "Epoch 819: Validation loss decreased (266.954803 --> 266.877167).\n",
            "\t Train_Loss: 111.2530 Val_Loss: 266.8772  BEST VAL Loss: 266.8772\n",
            "\n",
            "Epoch 820: Validation loss decreased (266.877167 --> 266.801331).\n",
            "\t Train_Loss: 111.1979 Val_Loss: 266.8013  BEST VAL Loss: 266.8013\n",
            "\n",
            "Epoch 821: Validation loss decreased (266.801331 --> 266.725922).\n",
            "\t Train_Loss: 111.1420 Val_Loss: 266.7259  BEST VAL Loss: 266.7259\n",
            "\n",
            "Epoch 822: Validation loss decreased (266.725922 --> 266.650360).\n",
            "\t Train_Loss: 111.0856 Val_Loss: 266.6504  BEST VAL Loss: 266.6504\n",
            "\n",
            "Epoch 823: Validation loss decreased (266.650360 --> 266.574371).\n",
            "\t Train_Loss: 111.0284 Val_Loss: 266.5744  BEST VAL Loss: 266.5744\n",
            "\n",
            "Epoch 824: Validation loss decreased (266.574371 --> 266.498413).\n",
            "\t Train_Loss: 110.9706 Val_Loss: 266.4984  BEST VAL Loss: 266.4984\n",
            "\n",
            "Epoch 825: Validation loss decreased (266.498413 --> 266.423248).\n",
            "\t Train_Loss: 110.9123 Val_Loss: 266.4232  BEST VAL Loss: 266.4232\n",
            "\n",
            "Epoch 826: Validation loss decreased (266.423248 --> 266.349213).\n",
            "\t Train_Loss: 110.8534 Val_Loss: 266.3492  BEST VAL Loss: 266.3492\n",
            "\n",
            "Epoch 827: Validation loss decreased (266.349213 --> 266.275940).\n",
            "\t Train_Loss: 110.7939 Val_Loss: 266.2759  BEST VAL Loss: 266.2759\n",
            "\n",
            "Epoch 828: Validation loss decreased (266.275940 --> 266.202850).\n",
            "\t Train_Loss: 110.7340 Val_Loss: 266.2029  BEST VAL Loss: 266.2029\n",
            "\n",
            "Epoch 829: Validation loss decreased (266.202850 --> 266.129303).\n",
            "\t Train_Loss: 110.6737 Val_Loss: 266.1293  BEST VAL Loss: 266.1293\n",
            "\n",
            "Epoch 830: Validation loss decreased (266.129303 --> 266.054260).\n",
            "\t Train_Loss: 110.6129 Val_Loss: 266.0543  BEST VAL Loss: 266.0543\n",
            "\n",
            "Epoch 831: Validation loss decreased (266.054260 --> 265.977631).\n",
            "\t Train_Loss: 110.5519 Val_Loss: 265.9776  BEST VAL Loss: 265.9776\n",
            "\n",
            "Epoch 832: Validation loss decreased (265.977631 --> 265.899750).\n",
            "\t Train_Loss: 110.4906 Val_Loss: 265.8997  BEST VAL Loss: 265.8997\n",
            "\n",
            "Epoch 833: Validation loss decreased (265.899750 --> 265.820221).\n",
            "\t Train_Loss: 110.4291 Val_Loss: 265.8202  BEST VAL Loss: 265.8202\n",
            "\n",
            "Epoch 834: Validation loss decreased (265.820221 --> 265.739899).\n",
            "\t Train_Loss: 110.3676 Val_Loss: 265.7399  BEST VAL Loss: 265.7399\n",
            "\n",
            "Epoch 835: Validation loss decreased (265.739899 --> 265.658661).\n",
            "\t Train_Loss: 110.3060 Val_Loss: 265.6587  BEST VAL Loss: 265.6587\n",
            "\n",
            "Epoch 836: Validation loss decreased (265.658661 --> 265.576263).\n",
            "\t Train_Loss: 110.2444 Val_Loss: 265.5763  BEST VAL Loss: 265.5763\n",
            "\n",
            "Epoch 837: Validation loss decreased (265.576263 --> 265.492126).\n",
            "\t Train_Loss: 110.1830 Val_Loss: 265.4921  BEST VAL Loss: 265.4921\n",
            "\n",
            "Epoch 838: Validation loss decreased (265.492126 --> 265.406219).\n",
            "\t Train_Loss: 110.1217 Val_Loss: 265.4062  BEST VAL Loss: 265.4062\n",
            "\n",
            "Epoch 839: Validation loss decreased (265.406219 --> 265.317841).\n",
            "\t Train_Loss: 110.0608 Val_Loss: 265.3178  BEST VAL Loss: 265.3178\n",
            "\n",
            "Epoch 840: Validation loss decreased (265.317841 --> 265.227051).\n",
            "\t Train_Loss: 110.0003 Val_Loss: 265.2271  BEST VAL Loss: 265.2271\n",
            "\n",
            "Epoch 841: Validation loss decreased (265.227051 --> 265.134094).\n",
            "\t Train_Loss: 109.9401 Val_Loss: 265.1341  BEST VAL Loss: 265.1341\n",
            "\n",
            "Epoch 842: Validation loss decreased (265.134094 --> 265.039001).\n",
            "\t Train_Loss: 109.8806 Val_Loss: 265.0390  BEST VAL Loss: 265.0390\n",
            "\n",
            "Epoch 843: Validation loss decreased (265.039001 --> 264.942169).\n",
            "\t Train_Loss: 109.8216 Val_Loss: 264.9422  BEST VAL Loss: 264.9422\n",
            "\n",
            "Epoch 844: Validation loss decreased (264.942169 --> 264.843536).\n",
            "\t Train_Loss: 109.7633 Val_Loss: 264.8435  BEST VAL Loss: 264.8435\n",
            "\n",
            "Epoch 845: Validation loss decreased (264.843536 --> 264.743469).\n",
            "\t Train_Loss: 109.7056 Val_Loss: 264.7435  BEST VAL Loss: 264.7435\n",
            "\n",
            "Epoch 846: Validation loss decreased (264.743469 --> 264.641388).\n",
            "\t Train_Loss: 109.6487 Val_Loss: 264.6414  BEST VAL Loss: 264.6414\n",
            "\n",
            "Epoch 847: Validation loss decreased (264.641388 --> 264.537720).\n",
            "\t Train_Loss: 109.5927 Val_Loss: 264.5377  BEST VAL Loss: 264.5377\n",
            "\n",
            "Epoch 848: Validation loss decreased (264.537720 --> 264.432098).\n",
            "\t Train_Loss: 109.5375 Val_Loss: 264.4321  BEST VAL Loss: 264.4321\n",
            "\n",
            "Epoch 849: Validation loss decreased (264.432098 --> 264.324310).\n",
            "\t Train_Loss: 109.4831 Val_Loss: 264.3243  BEST VAL Loss: 264.3243\n",
            "\n",
            "Epoch 850: Validation loss decreased (264.324310 --> 264.214996).\n",
            "\t Train_Loss: 109.4296 Val_Loss: 264.2150  BEST VAL Loss: 264.2150\n",
            "\n",
            "Epoch 851: Validation loss decreased (264.214996 --> 264.104156).\n",
            "\t Train_Loss: 109.3769 Val_Loss: 264.1042  BEST VAL Loss: 264.1042\n",
            "\n",
            "Epoch 852: Validation loss decreased (264.104156 --> 263.992126).\n",
            "\t Train_Loss: 109.3251 Val_Loss: 263.9921  BEST VAL Loss: 263.9921\n",
            "\n",
            "Epoch 853: Validation loss decreased (263.992126 --> 263.878876).\n",
            "\t Train_Loss: 109.2742 Val_Loss: 263.8789  BEST VAL Loss: 263.8789\n",
            "\n",
            "Epoch 854: Validation loss decreased (263.878876 --> 263.764496).\n",
            "\t Train_Loss: 109.2241 Val_Loss: 263.7645  BEST VAL Loss: 263.7645\n",
            "\n",
            "Epoch 855: Validation loss decreased (263.764496 --> 263.649384).\n",
            "\t Train_Loss: 109.1748 Val_Loss: 263.6494  BEST VAL Loss: 263.6494\n",
            "\n",
            "Epoch 856: Validation loss decreased (263.649384 --> 263.533356).\n",
            "\t Train_Loss: 109.1263 Val_Loss: 263.5334  BEST VAL Loss: 263.5334\n",
            "\n",
            "Epoch 857: Validation loss decreased (263.533356 --> 263.416260).\n",
            "\t Train_Loss: 109.0785 Val_Loss: 263.4163  BEST VAL Loss: 263.4163\n",
            "\n",
            "Epoch 858: Validation loss decreased (263.416260 --> 263.298553).\n",
            "\t Train_Loss: 109.0314 Val_Loss: 263.2986  BEST VAL Loss: 263.2986\n",
            "\n",
            "Epoch 859: Validation loss decreased (263.298553 --> 263.179749).\n",
            "\t Train_Loss: 108.9849 Val_Loss: 263.1797  BEST VAL Loss: 263.1797\n",
            "\n",
            "Epoch 860: Validation loss decreased (263.179749 --> 263.060120).\n",
            "\t Train_Loss: 108.9390 Val_Loss: 263.0601  BEST VAL Loss: 263.0601\n",
            "\n",
            "Epoch 861: Validation loss decreased (263.060120 --> 262.940002).\n",
            "\t Train_Loss: 108.8936 Val_Loss: 262.9400  BEST VAL Loss: 262.9400\n",
            "\n",
            "Epoch 862: Validation loss decreased (262.940002 --> 262.819550).\n",
            "\t Train_Loss: 108.8488 Val_Loss: 262.8195  BEST VAL Loss: 262.8195\n",
            "\n",
            "Epoch 863: Validation loss decreased (262.819550 --> 262.698700).\n",
            "\t Train_Loss: 108.8043 Val_Loss: 262.6987  BEST VAL Loss: 262.6987\n",
            "\n",
            "Epoch 864: Validation loss decreased (262.698700 --> 262.577637).\n",
            "\t Train_Loss: 108.7603 Val_Loss: 262.5776  BEST VAL Loss: 262.5776\n",
            "\n",
            "Epoch 865: Validation loss decreased (262.577637 --> 262.456207).\n",
            "\t Train_Loss: 108.7165 Val_Loss: 262.4562  BEST VAL Loss: 262.4562\n",
            "\n",
            "Epoch 866: Validation loss decreased (262.456207 --> 262.334625).\n",
            "\t Train_Loss: 108.6731 Val_Loss: 262.3346  BEST VAL Loss: 262.3346\n",
            "\n",
            "Epoch 867: Validation loss decreased (262.334625 --> 262.212738).\n",
            "\t Train_Loss: 108.6299 Val_Loss: 262.2127  BEST VAL Loss: 262.2127\n",
            "\n",
            "Epoch 868: Validation loss decreased (262.212738 --> 262.090454).\n",
            "\t Train_Loss: 108.5870 Val_Loss: 262.0905  BEST VAL Loss: 262.0905\n",
            "\n",
            "Epoch 869: Validation loss decreased (262.090454 --> 261.968231).\n",
            "\t Train_Loss: 108.5442 Val_Loss: 261.9682  BEST VAL Loss: 261.9682\n",
            "\n",
            "Epoch 870: Validation loss decreased (261.968231 --> 261.845764).\n",
            "\t Train_Loss: 108.5016 Val_Loss: 261.8458  BEST VAL Loss: 261.8458\n",
            "\n",
            "Epoch 871: Validation loss decreased (261.845764 --> 261.723450).\n",
            "\t Train_Loss: 108.4591 Val_Loss: 261.7234  BEST VAL Loss: 261.7234\n",
            "\n",
            "Epoch 872: Validation loss decreased (261.723450 --> 261.601440).\n",
            "\t Train_Loss: 108.4168 Val_Loss: 261.6014  BEST VAL Loss: 261.6014\n",
            "\n",
            "Epoch 873: Validation loss decreased (261.601440 --> 261.479340).\n",
            "\t Train_Loss: 108.3745 Val_Loss: 261.4793  BEST VAL Loss: 261.4793\n",
            "\n",
            "Epoch 874: Validation loss decreased (261.479340 --> 261.357391).\n",
            "\t Train_Loss: 108.3324 Val_Loss: 261.3574  BEST VAL Loss: 261.3574\n",
            "\n",
            "Epoch 875: Validation loss decreased (261.357391 --> 261.235748).\n",
            "\t Train_Loss: 108.2903 Val_Loss: 261.2357  BEST VAL Loss: 261.2357\n",
            "\n",
            "Epoch 876: Validation loss decreased (261.235748 --> 261.114502).\n",
            "\t Train_Loss: 108.2483 Val_Loss: 261.1145  BEST VAL Loss: 261.1145\n",
            "\n",
            "Epoch 877: Validation loss decreased (261.114502 --> 260.993500).\n",
            "\t Train_Loss: 108.2064 Val_Loss: 260.9935  BEST VAL Loss: 260.9935\n",
            "\n",
            "Epoch 878: Validation loss decreased (260.993500 --> 260.872864).\n",
            "\t Train_Loss: 108.1645 Val_Loss: 260.8729  BEST VAL Loss: 260.8729\n",
            "\n",
            "Epoch 879: Validation loss decreased (260.872864 --> 260.752411).\n",
            "\t Train_Loss: 108.1228 Val_Loss: 260.7524  BEST VAL Loss: 260.7524\n",
            "\n",
            "Epoch 880: Validation loss decreased (260.752411 --> 260.632660).\n",
            "\t Train_Loss: 108.0810 Val_Loss: 260.6327  BEST VAL Loss: 260.6327\n",
            "\n",
            "Epoch 881: Validation loss decreased (260.632660 --> 260.513123).\n",
            "\t Train_Loss: 108.0394 Val_Loss: 260.5131  BEST VAL Loss: 260.5131\n",
            "\n",
            "Epoch 882: Validation loss decreased (260.513123 --> 260.394379).\n",
            "\t Train_Loss: 107.9977 Val_Loss: 260.3944  BEST VAL Loss: 260.3944\n",
            "\n",
            "Epoch 883: Validation loss decreased (260.394379 --> 260.276154).\n",
            "\t Train_Loss: 107.9562 Val_Loss: 260.2762  BEST VAL Loss: 260.2762\n",
            "\n",
            "Epoch 884: Validation loss decreased (260.276154 --> 260.158508).\n",
            "\t Train_Loss: 107.9147 Val_Loss: 260.1585  BEST VAL Loss: 260.1585\n",
            "\n",
            "Epoch 885: Validation loss decreased (260.158508 --> 260.041595).\n",
            "\t Train_Loss: 107.8733 Val_Loss: 260.0416  BEST VAL Loss: 260.0416\n",
            "\n",
            "Epoch 886: Validation loss decreased (260.041595 --> 259.925262).\n",
            "\t Train_Loss: 107.8319 Val_Loss: 259.9253  BEST VAL Loss: 259.9253\n",
            "\n",
            "Epoch 887: Validation loss decreased (259.925262 --> 259.809814).\n",
            "\t Train_Loss: 107.7905 Val_Loss: 259.8098  BEST VAL Loss: 259.8098\n",
            "\n",
            "Epoch 888: Validation loss decreased (259.809814 --> 259.695099).\n",
            "\t Train_Loss: 107.7491 Val_Loss: 259.6951  BEST VAL Loss: 259.6951\n",
            "\n",
            "Epoch 889: Validation loss decreased (259.695099 --> 259.581085).\n",
            "\t Train_Loss: 107.7077 Val_Loss: 259.5811  BEST VAL Loss: 259.5811\n",
            "\n",
            "Epoch 890: Validation loss decreased (259.581085 --> 259.467712).\n",
            "\t Train_Loss: 107.6661 Val_Loss: 259.4677  BEST VAL Loss: 259.4677\n",
            "\n",
            "Epoch 891: Validation loss decreased (259.467712 --> 259.355621).\n",
            "\t Train_Loss: 107.6243 Val_Loss: 259.3556  BEST VAL Loss: 259.3556\n",
            "\n",
            "Epoch 892: Validation loss decreased (259.355621 --> 259.244690).\n",
            "\t Train_Loss: 107.5818 Val_Loss: 259.2447  BEST VAL Loss: 259.2447\n",
            "\n",
            "Epoch 893: Validation loss decreased (259.244690 --> 259.135223).\n",
            "\t Train_Loss: 107.5382 Val_Loss: 259.1352  BEST VAL Loss: 259.1352\n",
            "\n",
            "Epoch 894: Validation loss decreased (259.135223 --> 259.029205).\n",
            "\t Train_Loss: 107.4914 Val_Loss: 259.0292  BEST VAL Loss: 259.0292\n",
            "\n",
            "Epoch 895: Validation loss decreased (259.029205 --> 258.938507).\n",
            "\t Train_Loss: 107.4353 Val_Loss: 258.9385  BEST VAL Loss: 258.9385\n",
            "\n",
            "Epoch 896: Validation loss did not decrease\n",
            "\t Train_Loss: 107.3347 Val_Loss: 259.1402  BEST VAL Loss: 258.9385\n",
            "\n",
            "Epoch 897: Validation loss did not decrease\n",
            "\t Train_Loss: 106.6385 Val_Loss: 315.7511  BEST VAL Loss: 258.9385\n",
            "\n",
            "Epoch 898: Validation loss did not decrease\n",
            "\t Train_Loss: 108.9873 Val_Loss: 259.1334  BEST VAL Loss: 258.9385\n",
            "\n",
            "Epoch 899: Validation loss decreased (258.938507 --> 257.728943).\n",
            "\t Train_Loss: 105.3347 Val_Loss: 257.7289  BEST VAL Loss: 257.7289\n",
            "\n",
            "Epoch 900: Validation loss decreased (257.728943 --> 257.512451).\n",
            "\t Train_Loss: 107.0466 Val_Loss: 257.5125  BEST VAL Loss: 257.5125\n",
            "\n",
            "Epoch 901: Validation loss did not decrease\n",
            "\t Train_Loss: 107.2157 Val_Loss: 257.5453  BEST VAL Loss: 257.5125\n",
            "\n",
            "Epoch 902: Validation loss did not decrease\n",
            "\t Train_Loss: 107.1900 Val_Loss: 257.7062  BEST VAL Loss: 257.5125\n",
            "\n",
            "Epoch 903: Validation loss did not decrease\n",
            "\t Train_Loss: 107.1445 Val_Loss: 257.8725  BEST VAL Loss: 257.5125\n",
            "\n",
            "Epoch 904: Validation loss did not decrease\n",
            "\t Train_Loss: 107.1119 Val_Loss: 257.9205  BEST VAL Loss: 257.5125\n",
            "\n",
            "Epoch 905: Validation loss did not decrease\n",
            "\t Train_Loss: 107.0842 Val_Loss: 257.7796  BEST VAL Loss: 257.5125\n",
            "\n",
            "Epoch 906: Validation loss decreased (257.512451 --> 257.466431).\n",
            "\t Train_Loss: 107.0453 Val_Loss: 257.4664  BEST VAL Loss: 257.4664\n",
            "\n",
            "Epoch 907: Validation loss decreased (257.466431 --> 257.068146).\n",
            "\t Train_Loss: 106.9955 Val_Loss: 257.0681  BEST VAL Loss: 257.0681\n",
            "\n",
            "Epoch 908: Validation loss decreased (257.068146 --> 256.691650).\n",
            "\t Train_Loss: 106.9486 Val_Loss: 256.6917  BEST VAL Loss: 256.6917\n",
            "\n",
            "Epoch 909: Validation loss decreased (256.691650 --> 256.415680).\n",
            "\t Train_Loss: 106.9122 Val_Loss: 256.4157  BEST VAL Loss: 256.4157\n",
            "\n",
            "Epoch 910: Validation loss decreased (256.415680 --> 256.275726).\n",
            "\t Train_Loss: 106.8798 Val_Loss: 256.2757  BEST VAL Loss: 256.2757\n",
            "\n",
            "Epoch 911: Validation loss decreased (256.275726 --> 256.263397).\n",
            "\t Train_Loss: 106.8420 Val_Loss: 256.2634  BEST VAL Loss: 256.2634\n",
            "\n",
            "Epoch 912: Validation loss did not decrease\n",
            "\t Train_Loss: 106.7977 Val_Loss: 256.3351  BEST VAL Loss: 256.2634\n",
            "\n",
            "Epoch 913: Validation loss did not decrease\n",
            "\t Train_Loss: 106.7537 Val_Loss: 256.4234  BEST VAL Loss: 256.2634\n",
            "\n",
            "Epoch 914: Validation loss did not decrease\n",
            "\t Train_Loss: 106.7155 Val_Loss: 256.4575  BEST VAL Loss: 256.2634\n",
            "\n",
            "Epoch 915: Validation loss did not decrease\n",
            "\t Train_Loss: 106.6808 Val_Loss: 256.3912  BEST VAL Loss: 256.2634\n",
            "\n",
            "Epoch 916: Validation loss decreased (256.263397 --> 256.220703).\n",
            "\t Train_Loss: 106.6436 Val_Loss: 256.2207  BEST VAL Loss: 256.2207\n",
            "\n",
            "Epoch 917: Validation loss decreased (256.220703 --> 255.984482).\n",
            "\t Train_Loss: 106.6022 Val_Loss: 255.9845  BEST VAL Loss: 255.9845\n",
            "\n",
            "Epoch 918: Validation loss decreased (255.984482 --> 255.740143).\n",
            "\t Train_Loss: 106.5604 Val_Loss: 255.7401  BEST VAL Loss: 255.7401\n",
            "\n",
            "Epoch 919: Validation loss decreased (255.740143 --> 255.539993).\n",
            "\t Train_Loss: 106.5219 Val_Loss: 255.5400  BEST VAL Loss: 255.5400\n",
            "\n",
            "Epoch 920: Validation loss decreased (255.539993 --> 255.413803).\n",
            "\t Train_Loss: 106.4857 Val_Loss: 255.4138  BEST VAL Loss: 255.4138\n",
            "\n",
            "Epoch 921: Validation loss decreased (255.413803 --> 255.365967).\n",
            "\t Train_Loss: 106.4486 Val_Loss: 255.3660  BEST VAL Loss: 255.3660\n",
            "\n",
            "Epoch 922: Validation loss did not decrease\n",
            "\t Train_Loss: 106.4090 Val_Loss: 255.3759  BEST VAL Loss: 255.3660\n",
            "\n",
            "Epoch 923: Validation loss did not decrease\n",
            "\t Train_Loss: 106.3688 Val_Loss: 255.4069  BEST VAL Loss: 255.3660\n",
            "\n",
            "Epoch 924: Validation loss did not decrease\n",
            "\t Train_Loss: 106.3301 Val_Loss: 255.4177  BEST VAL Loss: 255.3660\n",
            "\n",
            "Epoch 925: Validation loss did not decrease\n",
            "\t Train_Loss: 106.2932 Val_Loss: 255.3773  BEST VAL Loss: 255.3660\n",
            "\n",
            "Epoch 926: Validation loss decreased (255.365967 --> 255.275879).\n",
            "\t Train_Loss: 106.2560 Val_Loss: 255.2759  BEST VAL Loss: 255.2759\n",
            "\n",
            "Epoch 927: Validation loss decreased (255.275879 --> 255.127396).\n",
            "\t Train_Loss: 106.2175 Val_Loss: 255.1274  BEST VAL Loss: 255.1274\n",
            "\n",
            "Epoch 928: Validation loss decreased (255.127396 --> 254.961426).\n",
            "\t Train_Loss: 106.1783 Val_Loss: 254.9614  BEST VAL Loss: 254.9614\n",
            "\n",
            "Epoch 929: Validation loss decreased (254.961426 --> 254.809921).\n",
            "\t Train_Loss: 106.1399 Val_Loss: 254.8099  BEST VAL Loss: 254.8099\n",
            "\n",
            "Epoch 930: Validation loss decreased (254.809921 --> 254.695801).\n",
            "\t Train_Loss: 106.1027 Val_Loss: 254.6958  BEST VAL Loss: 254.6958\n",
            "\n",
            "Epoch 931: Validation loss decreased (254.695801 --> 254.627594).\n",
            "\t Train_Loss: 106.0655 Val_Loss: 254.6276  BEST VAL Loss: 254.6276\n",
            "\n",
            "Epoch 932: Validation loss decreased (254.627594 --> 254.598251).\n",
            "\t Train_Loss: 106.0276 Val_Loss: 254.5983  BEST VAL Loss: 254.5983\n",
            "\n",
            "Epoch 933: Validation loss decreased (254.598251 --> 254.589401).\n",
            "\t Train_Loss: 105.9891 Val_Loss: 254.5894  BEST VAL Loss: 254.5894\n",
            "\n",
            "Epoch 934: Validation loss decreased (254.589401 --> 254.577011).\n",
            "\t Train_Loss: 105.9510 Val_Loss: 254.5770  BEST VAL Loss: 254.5770\n",
            "\n",
            "Epoch 935: Validation loss decreased (254.577011 --> 254.540817).\n",
            "\t Train_Loss: 105.9136 Val_Loss: 254.5408  BEST VAL Loss: 254.5408\n",
            "\n",
            "Epoch 936: Validation loss decreased (254.540817 --> 254.470322).\n",
            "\t Train_Loss: 105.8764 Val_Loss: 254.4703  BEST VAL Loss: 254.4703\n",
            "\n",
            "Epoch 937: Validation loss decreased (254.470322 --> 254.368652).\n",
            "\t Train_Loss: 105.8389 Val_Loss: 254.3687  BEST VAL Loss: 254.3687\n",
            "\n",
            "Epoch 938: Validation loss decreased (254.368652 --> 254.249252).\n",
            "\t Train_Loss: 105.8010 Val_Loss: 254.2493  BEST VAL Loss: 254.2493\n",
            "\n",
            "Epoch 939: Validation loss decreased (254.249252 --> 254.130417).\n",
            "\t Train_Loss: 105.7633 Val_Loss: 254.1304  BEST VAL Loss: 254.1304\n",
            "\n",
            "Epoch 940: Validation loss decreased (254.130417 --> 254.028122).\n",
            "\t Train_Loss: 105.7259 Val_Loss: 254.0281  BEST VAL Loss: 254.0281\n",
            "\n",
            "Epoch 941: Validation loss decreased (254.028122 --> 253.951172).\n",
            "\t Train_Loss: 105.6888 Val_Loss: 253.9512  BEST VAL Loss: 253.9512\n",
            "\n",
            "Epoch 942: Validation loss decreased (253.951172 --> 253.899078).\n",
            "\t Train_Loss: 105.6515 Val_Loss: 253.8991  BEST VAL Loss: 253.8991\n",
            "\n",
            "Epoch 943: Validation loss decreased (253.899078 --> 253.863892).\n",
            "\t Train_Loss: 105.6140 Val_Loss: 253.8639  BEST VAL Loss: 253.8639\n",
            "\n",
            "Epoch 944: Validation loss decreased (253.863892 --> 253.832321).\n",
            "\t Train_Loss: 105.5765 Val_Loss: 253.8323  BEST VAL Loss: 253.8323\n",
            "\n",
            "Epoch 945: Validation loss decreased (253.832321 --> 253.791779).\n",
            "\t Train_Loss: 105.5392 Val_Loss: 253.7918  BEST VAL Loss: 253.7918\n",
            "\n",
            "Epoch 946: Validation loss decreased (253.791779 --> 253.733017).\n",
            "\t Train_Loss: 105.5022 Val_Loss: 253.7330  BEST VAL Loss: 253.7330\n",
            "\n",
            "Epoch 947: Validation loss decreased (253.733017 --> 253.655106).\n",
            "\t Train_Loss: 105.4651 Val_Loss: 253.6551  BEST VAL Loss: 253.6551\n",
            "\n",
            "Epoch 948: Validation loss decreased (253.655106 --> 253.562897).\n",
            "\t Train_Loss: 105.4279 Val_Loss: 253.5629  BEST VAL Loss: 253.5629\n",
            "\n",
            "Epoch 949: Validation loss decreased (253.562897 --> 253.466110).\n",
            "\t Train_Loss: 105.3906 Val_Loss: 253.4661  BEST VAL Loss: 253.4661\n",
            "\n",
            "Epoch 950: Validation loss decreased (253.466110 --> 253.374863).\n",
            "\t Train_Loss: 105.3535 Val_Loss: 253.3749  BEST VAL Loss: 253.3749\n",
            "\n",
            "Epoch 951: Validation loss decreased (253.374863 --> 253.296310).\n",
            "\t Train_Loss: 105.3166 Val_Loss: 253.2963  BEST VAL Loss: 253.2963\n",
            "\n",
            "Epoch 952: Validation loss decreased (253.296310 --> 253.232697).\n",
            "\t Train_Loss: 105.2796 Val_Loss: 253.2327  BEST VAL Loss: 253.2327\n",
            "\n",
            "Epoch 953: Validation loss decreased (253.232697 --> 253.181442).\n",
            "\t Train_Loss: 105.2426 Val_Loss: 253.1814  BEST VAL Loss: 253.1814\n",
            "\n",
            "Epoch 954: Validation loss decreased (253.181442 --> 253.135941).\n",
            "\t Train_Loss: 105.2056 Val_Loss: 253.1359  BEST VAL Loss: 253.1359\n",
            "\n",
            "Epoch 955: Validation loss decreased (253.135941 --> 253.088577).\n",
            "\t Train_Loss: 105.1687 Val_Loss: 253.0886  BEST VAL Loss: 253.0886\n",
            "\n",
            "Epoch 956: Validation loss decreased (253.088577 --> 253.033157).\n",
            "\t Train_Loss: 105.1319 Val_Loss: 253.0332  BEST VAL Loss: 253.0332\n",
            "\n",
            "Epoch 957: Validation loss decreased (253.033157 --> 252.966415).\n",
            "\t Train_Loss: 105.0950 Val_Loss: 252.9664  BEST VAL Loss: 252.9664\n",
            "\n",
            "Epoch 958: Validation loss decreased (252.966415 --> 252.889771).\n",
            "\t Train_Loss: 105.0582 Val_Loss: 252.8898  BEST VAL Loss: 252.8898\n",
            "\n",
            "Epoch 959: Validation loss decreased (252.889771 --> 252.807327).\n",
            "\t Train_Loss: 105.0214 Val_Loss: 252.8073  BEST VAL Loss: 252.8073\n",
            "\n",
            "Epoch 960: Validation loss decreased (252.807327 --> 252.725449).\n",
            "\t Train_Loss: 104.9846 Val_Loss: 252.7254  BEST VAL Loss: 252.7254\n",
            "\n",
            "Epoch 961: Validation loss decreased (252.725449 --> 252.648926).\n",
            "\t Train_Loss: 104.9479 Val_Loss: 252.6489  BEST VAL Loss: 252.6489\n",
            "\n",
            "Epoch 962: Validation loss decreased (252.648926 --> 252.580688).\n",
            "\t Train_Loss: 104.9113 Val_Loss: 252.5807  BEST VAL Loss: 252.5807\n",
            "\n",
            "Epoch 963: Validation loss decreased (252.580688 --> 252.520950).\n",
            "\t Train_Loss: 104.8746 Val_Loss: 252.5210  BEST VAL Loss: 252.5210\n",
            "\n",
            "Epoch 964: Validation loss decreased (252.520950 --> 252.466309).\n",
            "\t Train_Loss: 104.8379 Val_Loss: 252.4663  BEST VAL Loss: 252.4663\n",
            "\n",
            "Epoch 965: Validation loss decreased (252.466309 --> 252.412888).\n",
            "\t Train_Loss: 104.8013 Val_Loss: 252.4129  BEST VAL Loss: 252.4129\n",
            "\n",
            "Epoch 966: Validation loss decreased (252.412888 --> 252.356552).\n",
            "\t Train_Loss: 104.7647 Val_Loss: 252.3566  BEST VAL Loss: 252.3566\n",
            "\n",
            "Epoch 967: Validation loss decreased (252.356552 --> 252.294434).\n",
            "\t Train_Loss: 104.7282 Val_Loss: 252.2944  BEST VAL Loss: 252.2944\n",
            "\n",
            "Epoch 968: Validation loss decreased (252.294434 --> 252.226105).\n",
            "\t Train_Loss: 104.6917 Val_Loss: 252.2261  BEST VAL Loss: 252.2261\n",
            "\n",
            "Epoch 969: Validation loss decreased (252.226105 --> 252.153061).\n",
            "\t Train_Loss: 104.6552 Val_Loss: 252.1531  BEST VAL Loss: 252.1531\n",
            "\n",
            "Epoch 970: Validation loss decreased (252.153061 --> 252.078293).\n",
            "\t Train_Loss: 104.6187 Val_Loss: 252.0783  BEST VAL Loss: 252.0783\n",
            "\n",
            "Epoch 971: Validation loss decreased (252.078293 --> 252.005234).\n",
            "\t Train_Loss: 104.5822 Val_Loss: 252.0052  BEST VAL Loss: 252.0052\n",
            "\n",
            "Epoch 972: Validation loss decreased (252.005234 --> 251.936279).\n",
            "\t Train_Loss: 104.5458 Val_Loss: 251.9363  BEST VAL Loss: 251.9363\n",
            "\n",
            "Epoch 973: Validation loss decreased (251.936279 --> 251.872345).\n",
            "\t Train_Loss: 104.5094 Val_Loss: 251.8723  BEST VAL Loss: 251.8723\n",
            "\n",
            "Epoch 974: Validation loss decreased (251.872345 --> 251.812454).\n",
            "\t Train_Loss: 104.4730 Val_Loss: 251.8125  BEST VAL Loss: 251.8125\n",
            "\n",
            "Epoch 975: Validation loss decreased (251.812454 --> 251.754517).\n",
            "\t Train_Loss: 104.4367 Val_Loss: 251.7545  BEST VAL Loss: 251.7545\n",
            "\n",
            "Epoch 976: Validation loss decreased (251.754517 --> 251.696640).\n",
            "\t Train_Loss: 104.4003 Val_Loss: 251.6966  BEST VAL Loss: 251.6966\n",
            "\n",
            "Epoch 977: Validation loss decreased (251.696640 --> 251.635849).\n",
            "\t Train_Loss: 104.3641 Val_Loss: 251.6358  BEST VAL Loss: 251.6358\n",
            "\n",
            "Epoch 978: Validation loss decreased (251.635849 --> 251.571442).\n",
            "\t Train_Loss: 104.3278 Val_Loss: 251.5714  BEST VAL Loss: 251.5714\n",
            "\n",
            "Epoch 979: Validation loss decreased (251.571442 --> 251.503891).\n",
            "\t Train_Loss: 104.2915 Val_Loss: 251.5039  BEST VAL Loss: 251.5039\n",
            "\n",
            "Epoch 980: Validation loss decreased (251.503891 --> 251.434204).\n",
            "\t Train_Loss: 104.2553 Val_Loss: 251.4342  BEST VAL Loss: 251.4342\n",
            "\n",
            "Epoch 981: Validation loss decreased (251.434204 --> 251.364609).\n",
            "\t Train_Loss: 104.2191 Val_Loss: 251.3646  BEST VAL Loss: 251.3646\n",
            "\n",
            "Epoch 982: Validation loss decreased (251.364609 --> 251.296616).\n",
            "\t Train_Loss: 104.1829 Val_Loss: 251.2966  BEST VAL Loss: 251.2966\n",
            "\n",
            "Epoch 983: Validation loss decreased (251.296616 --> 251.231400).\n",
            "\t Train_Loss: 104.1468 Val_Loss: 251.2314  BEST VAL Loss: 251.2314\n",
            "\n",
            "Epoch 984: Validation loss decreased (251.231400 --> 251.168686).\n",
            "\t Train_Loss: 104.1106 Val_Loss: 251.1687  BEST VAL Loss: 251.1687\n",
            "\n",
            "Epoch 985: Validation loss decreased (251.168686 --> 251.108078).\n",
            "\t Train_Loss: 104.0745 Val_Loss: 251.1081  BEST VAL Loss: 251.1081\n",
            "\n",
            "Epoch 986: Validation loss decreased (251.108078 --> 251.048203).\n",
            "\t Train_Loss: 104.0384 Val_Loss: 251.0482  BEST VAL Loss: 251.0482\n",
            "\n",
            "Epoch 987: Validation loss decreased (251.048203 --> 250.987350).\n",
            "\t Train_Loss: 104.0023 Val_Loss: 250.9874  BEST VAL Loss: 250.9874\n",
            "\n",
            "Epoch 988: Validation loss decreased (250.987350 --> 250.924789).\n",
            "\t Train_Loss: 103.9663 Val_Loss: 250.9248  BEST VAL Loss: 250.9248\n",
            "\n",
            "Epoch 989: Validation loss decreased (250.924789 --> 250.860306).\n",
            "\t Train_Loss: 103.9303 Val_Loss: 250.8603  BEST VAL Loss: 250.8603\n",
            "\n",
            "Epoch 990: Validation loss decreased (250.860306 --> 250.793945).\n",
            "\t Train_Loss: 103.8943 Val_Loss: 250.7939  BEST VAL Loss: 250.7939\n",
            "\n",
            "Epoch 991: Validation loss decreased (250.793945 --> 250.727127).\n",
            "\t Train_Loss: 103.8583 Val_Loss: 250.7271  BEST VAL Loss: 250.7271\n",
            "\n",
            "Epoch 992: Validation loss decreased (250.727127 --> 250.660507).\n",
            "\t Train_Loss: 103.8223 Val_Loss: 250.6605  BEST VAL Loss: 250.6605\n",
            "\n",
            "Epoch 993: Validation loss decreased (250.660507 --> 250.595123).\n",
            "\t Train_Loss: 103.7864 Val_Loss: 250.5951  BEST VAL Loss: 250.5951\n",
            "\n",
            "Epoch 994: Validation loss decreased (250.595123 --> 250.531647).\n",
            "\t Train_Loss: 103.7504 Val_Loss: 250.5316  BEST VAL Loss: 250.5316\n",
            "\n",
            "Epoch 995: Validation loss decreased (250.531647 --> 250.469391).\n",
            "\t Train_Loss: 103.7145 Val_Loss: 250.4694  BEST VAL Loss: 250.4694\n",
            "\n",
            "Epoch 996: Validation loss decreased (250.469391 --> 250.407913).\n",
            "\t Train_Loss: 103.6786 Val_Loss: 250.4079  BEST VAL Loss: 250.4079\n",
            "\n",
            "Epoch 997: Validation loss decreased (250.407913 --> 250.346802).\n",
            "\t Train_Loss: 103.6427 Val_Loss: 250.3468  BEST VAL Loss: 250.3468\n",
            "\n",
            "Epoch 998: Validation loss decreased (250.346802 --> 250.284668).\n",
            "\t Train_Loss: 103.6069 Val_Loss: 250.2847  BEST VAL Loss: 250.2847\n",
            "\n",
            "Epoch 999: Validation loss decreased (250.284668 --> 250.221466).\n",
            "\t Train_Loss: 103.5711 Val_Loss: 250.2215  BEST VAL Loss: 250.2215\n",
            "\n",
            "Epoch 1000: Validation loss decreased (250.221466 --> 250.157181).\n",
            "\t Train_Loss: 103.5353 Val_Loss: 250.1572  BEST VAL Loss: 250.1572\n",
            "\n",
            "Epoch 1001: Validation loss decreased (250.157181 --> 250.092194).\n",
            "\t Train_Loss: 103.4995 Val_Loss: 250.0922  BEST VAL Loss: 250.0922\n",
            "\n",
            "Epoch 1002: Validation loss decreased (250.092194 --> 250.027145).\n",
            "\t Train_Loss: 103.4637 Val_Loss: 250.0271  BEST VAL Loss: 250.0271\n",
            "\n",
            "Epoch 1003: Validation loss decreased (250.027145 --> 249.962402).\n",
            "\t Train_Loss: 103.4280 Val_Loss: 249.9624  BEST VAL Loss: 249.9624\n",
            "\n",
            "Epoch 1004: Validation loss decreased (249.962402 --> 249.898483).\n",
            "\t Train_Loss: 103.3923 Val_Loss: 249.8985  BEST VAL Loss: 249.8985\n",
            "\n",
            "Epoch 1005: Validation loss decreased (249.898483 --> 249.835678).\n",
            "\t Train_Loss: 103.3565 Val_Loss: 249.8357  BEST VAL Loss: 249.8357\n",
            "\n",
            "Epoch 1006: Validation loss decreased (249.835678 --> 249.773392).\n",
            "\t Train_Loss: 103.3208 Val_Loss: 249.7734  BEST VAL Loss: 249.7734\n",
            "\n",
            "Epoch 1007: Validation loss decreased (249.773392 --> 249.711624).\n",
            "\t Train_Loss: 103.2851 Val_Loss: 249.7116  BEST VAL Loss: 249.7116\n",
            "\n",
            "Epoch 1008: Validation loss decreased (249.711624 --> 249.649567).\n",
            "\t Train_Loss: 103.2495 Val_Loss: 249.6496  BEST VAL Loss: 249.6496\n",
            "\n",
            "Epoch 1009: Validation loss decreased (249.649567 --> 249.586914).\n",
            "\t Train_Loss: 103.2138 Val_Loss: 249.5869  BEST VAL Loss: 249.5869\n",
            "\n",
            "Epoch 1010: Validation loss decreased (249.586914 --> 249.523926).\n",
            "\t Train_Loss: 103.1782 Val_Loss: 249.5239  BEST VAL Loss: 249.5239\n",
            "\n",
            "Epoch 1011: Validation loss decreased (249.523926 --> 249.460037).\n",
            "\t Train_Loss: 103.1426 Val_Loss: 249.4600  BEST VAL Loss: 249.4600\n",
            "\n",
            "Epoch 1012: Validation loss decreased (249.460037 --> 249.396149).\n",
            "\t Train_Loss: 103.1070 Val_Loss: 249.3961  BEST VAL Loss: 249.3961\n",
            "\n",
            "Epoch 1013: Validation loss decreased (249.396149 --> 249.332016).\n",
            "\t Train_Loss: 103.0714 Val_Loss: 249.3320  BEST VAL Loss: 249.3320\n",
            "\n",
            "Epoch 1014: Validation loss decreased (249.332016 --> 249.268265).\n",
            "\t Train_Loss: 103.0358 Val_Loss: 249.2683  BEST VAL Loss: 249.2683\n",
            "\n",
            "Epoch 1015: Validation loss decreased (249.268265 --> 249.205353).\n",
            "\t Train_Loss: 103.0003 Val_Loss: 249.2054  BEST VAL Loss: 249.2054\n",
            "\n",
            "Epoch 1016: Validation loss decreased (249.205353 --> 249.142685).\n",
            "\t Train_Loss: 102.9648 Val_Loss: 249.1427  BEST VAL Loss: 249.1427\n",
            "\n",
            "Epoch 1017: Validation loss decreased (249.142685 --> 249.080612).\n",
            "\t Train_Loss: 102.9293 Val_Loss: 249.0806  BEST VAL Loss: 249.0806\n",
            "\n",
            "Epoch 1018: Validation loss decreased (249.080612 --> 249.018311).\n",
            "\t Train_Loss: 102.8938 Val_Loss: 249.0183  BEST VAL Loss: 249.0183\n",
            "\n",
            "Epoch 1019: Validation loss decreased (249.018311 --> 248.956009).\n",
            "\t Train_Loss: 102.8583 Val_Loss: 248.9560  BEST VAL Loss: 248.9560\n",
            "\n",
            "Epoch 1020: Validation loss decreased (248.956009 --> 248.893417).\n",
            "\t Train_Loss: 102.8228 Val_Loss: 248.8934  BEST VAL Loss: 248.8934\n",
            "\n",
            "Epoch 1021: Validation loss decreased (248.893417 --> 248.830276).\n",
            "\t Train_Loss: 102.7874 Val_Loss: 248.8303  BEST VAL Loss: 248.8303\n",
            "\n",
            "Epoch 1022: Validation loss decreased (248.830276 --> 248.766998).\n",
            "\t Train_Loss: 102.7519 Val_Loss: 248.7670  BEST VAL Loss: 248.7670\n",
            "\n",
            "Epoch 1023: Validation loss decreased (248.766998 --> 248.703568).\n",
            "\t Train_Loss: 102.7165 Val_Loss: 248.7036  BEST VAL Loss: 248.7036\n",
            "\n",
            "Epoch 1024: Validation loss decreased (248.703568 --> 248.640381).\n",
            "\t Train_Loss: 102.6811 Val_Loss: 248.6404  BEST VAL Loss: 248.6404\n",
            "\n",
            "Epoch 1025: Validation loss decreased (248.640381 --> 248.577255).\n",
            "\t Train_Loss: 102.6457 Val_Loss: 248.5773  BEST VAL Loss: 248.5773\n",
            "\n",
            "Epoch 1026: Validation loss decreased (248.577255 --> 248.514450).\n",
            "\t Train_Loss: 102.6103 Val_Loss: 248.5145  BEST VAL Loss: 248.5145\n",
            "\n",
            "Epoch 1027: Validation loss decreased (248.514450 --> 248.452194).\n",
            "\t Train_Loss: 102.5750 Val_Loss: 248.4522  BEST VAL Loss: 248.4522\n",
            "\n",
            "Epoch 1028: Validation loss decreased (248.452194 --> 248.389771).\n",
            "\t Train_Loss: 102.5396 Val_Loss: 248.3898  BEST VAL Loss: 248.3898\n",
            "\n",
            "Epoch 1029: Validation loss decreased (248.389771 --> 248.327515).\n",
            "\t Train_Loss: 102.5043 Val_Loss: 248.3275  BEST VAL Loss: 248.3275\n",
            "\n",
            "Epoch 1030: Validation loss decreased (248.327515 --> 248.265045).\n",
            "\t Train_Loss: 102.4690 Val_Loss: 248.2650  BEST VAL Loss: 248.2650\n",
            "\n",
            "Epoch 1031: Validation loss decreased (248.265045 --> 248.202637).\n",
            "\t Train_Loss: 102.4337 Val_Loss: 248.2026  BEST VAL Loss: 248.2026\n",
            "\n",
            "Epoch 1032: Validation loss decreased (248.202637 --> 248.139694).\n",
            "\t Train_Loss: 102.3984 Val_Loss: 248.1397  BEST VAL Loss: 248.1397\n",
            "\n",
            "Epoch 1033: Validation loss decreased (248.139694 --> 248.076706).\n",
            "\t Train_Loss: 102.3631 Val_Loss: 248.0767  BEST VAL Loss: 248.0767\n",
            "\n",
            "Epoch 1034: Validation loss decreased (248.076706 --> 248.013870).\n",
            "\t Train_Loss: 102.3279 Val_Loss: 248.0139  BEST VAL Loss: 248.0139\n",
            "\n",
            "Epoch 1035: Validation loss decreased (248.013870 --> 247.951004).\n",
            "\t Train_Loss: 102.2926 Val_Loss: 247.9510  BEST VAL Loss: 247.9510\n",
            "\n",
            "Epoch 1036: Validation loss decreased (247.951004 --> 247.888382).\n",
            "\t Train_Loss: 102.2574 Val_Loss: 247.8884  BEST VAL Loss: 247.8884\n",
            "\n",
            "Epoch 1037: Validation loss decreased (247.888382 --> 247.825851).\n",
            "\t Train_Loss: 102.2221 Val_Loss: 247.8259  BEST VAL Loss: 247.8259\n",
            "\n",
            "Epoch 1038: Validation loss decreased (247.825851 --> 247.763779).\n",
            "\t Train_Loss: 102.1870 Val_Loss: 247.7638  BEST VAL Loss: 247.7638\n",
            "\n",
            "Epoch 1039: Validation loss decreased (247.763779 --> 247.701340).\n",
            "\t Train_Loss: 102.1518 Val_Loss: 247.7013  BEST VAL Loss: 247.7013\n",
            "\n",
            "Epoch 1040: Validation loss decreased (247.701340 --> 247.638870).\n",
            "\t Train_Loss: 102.1166 Val_Loss: 247.6389  BEST VAL Loss: 247.6389\n",
            "\n",
            "Epoch 1041: Validation loss decreased (247.638870 --> 247.576584).\n",
            "\t Train_Loss: 102.0814 Val_Loss: 247.5766  BEST VAL Loss: 247.5766\n",
            "\n",
            "Epoch 1042: Validation loss decreased (247.576584 --> 247.513962).\n",
            "\t Train_Loss: 102.0463 Val_Loss: 247.5140  BEST VAL Loss: 247.5140\n",
            "\n",
            "Epoch 1043: Validation loss decreased (247.513962 --> 247.451492).\n",
            "\t Train_Loss: 102.0111 Val_Loss: 247.4515  BEST VAL Loss: 247.4515\n",
            "\n",
            "Epoch 1044: Validation loss decreased (247.451492 --> 247.388672).\n",
            "\t Train_Loss: 101.9760 Val_Loss: 247.3887  BEST VAL Loss: 247.3887\n",
            "\n",
            "Epoch 1045: Validation loss decreased (247.388672 --> 247.326126).\n",
            "\t Train_Loss: 101.9409 Val_Loss: 247.3261  BEST VAL Loss: 247.3261\n",
            "\n",
            "Epoch 1046: Validation loss decreased (247.326126 --> 247.263550).\n",
            "\t Train_Loss: 101.9058 Val_Loss: 247.2635  BEST VAL Loss: 247.2635\n",
            "\n",
            "Epoch 1047: Validation loss decreased (247.263550 --> 247.201172).\n",
            "\t Train_Loss: 101.8707 Val_Loss: 247.2012  BEST VAL Loss: 247.2012\n",
            "\n",
            "Epoch 1048: Validation loss decreased (247.201172 --> 247.138840).\n",
            "\t Train_Loss: 101.8356 Val_Loss: 247.1388  BEST VAL Loss: 247.1388\n",
            "\n",
            "Epoch 1049: Validation loss decreased (247.138840 --> 247.076538).\n",
            "\t Train_Loss: 101.8006 Val_Loss: 247.0765  BEST VAL Loss: 247.0765\n",
            "\n",
            "Epoch 1050: Validation loss decreased (247.076538 --> 247.014404).\n",
            "\t Train_Loss: 101.7655 Val_Loss: 247.0144  BEST VAL Loss: 247.0144\n",
            "\n",
            "Epoch 1051: Validation loss decreased (247.014404 --> 246.952148).\n",
            "\t Train_Loss: 101.7304 Val_Loss: 246.9521  BEST VAL Loss: 246.9521\n",
            "\n",
            "Epoch 1052: Validation loss decreased (246.952148 --> 246.889969).\n",
            "\t Train_Loss: 101.6954 Val_Loss: 246.8900  BEST VAL Loss: 246.8900\n",
            "\n",
            "Epoch 1053: Validation loss decreased (246.889969 --> 246.827560).\n",
            "\t Train_Loss: 101.6604 Val_Loss: 246.8276  BEST VAL Loss: 246.8276\n",
            "\n",
            "Epoch 1054: Validation loss decreased (246.827560 --> 246.765289).\n",
            "\t Train_Loss: 101.6254 Val_Loss: 246.7653  BEST VAL Loss: 246.7653\n",
            "\n",
            "Epoch 1055: Validation loss decreased (246.765289 --> 246.702744).\n",
            "\t Train_Loss: 101.5904 Val_Loss: 246.7027  BEST VAL Loss: 246.7027\n",
            "\n",
            "Epoch 1056: Validation loss decreased (246.702744 --> 246.640533).\n",
            "\t Train_Loss: 101.5554 Val_Loss: 246.6405  BEST VAL Loss: 246.6405\n",
            "\n",
            "Epoch 1057: Validation loss decreased (246.640533 --> 246.578079).\n",
            "\t Train_Loss: 101.5204 Val_Loss: 246.5781  BEST VAL Loss: 246.5781\n",
            "\n",
            "Epoch 1058: Validation loss decreased (246.578079 --> 246.515976).\n",
            "\t Train_Loss: 101.4854 Val_Loss: 246.5160  BEST VAL Loss: 246.5160\n",
            "\n",
            "Epoch 1059: Validation loss decreased (246.515976 --> 246.453690).\n",
            "\t Train_Loss: 101.4504 Val_Loss: 246.4537  BEST VAL Loss: 246.4537\n",
            "\n",
            "Epoch 1060: Validation loss decreased (246.453690 --> 246.391479).\n",
            "\t Train_Loss: 101.4153 Val_Loss: 246.3915  BEST VAL Loss: 246.3915\n",
            "\n",
            "Epoch 1061: Validation loss decreased (246.391479 --> 246.329544).\n",
            "\t Train_Loss: 101.3802 Val_Loss: 246.3295  BEST VAL Loss: 246.3295\n",
            "\n",
            "Epoch 1062: Validation loss decreased (246.329544 --> 246.267380).\n",
            "\t Train_Loss: 101.3451 Val_Loss: 246.2674  BEST VAL Loss: 246.2674\n",
            "\n",
            "Epoch 1063: Validation loss decreased (246.267380 --> 246.205475).\n",
            "\t Train_Loss: 101.3096 Val_Loss: 246.2055  BEST VAL Loss: 246.2055\n",
            "\n",
            "Epoch 1064: Validation loss decreased (246.205475 --> 246.143951).\n",
            "\t Train_Loss: 101.2735 Val_Loss: 246.1440  BEST VAL Loss: 246.1440\n",
            "\n",
            "Epoch 1065: Validation loss decreased (246.143951 --> 246.090576).\n",
            "\t Train_Loss: 101.2343 Val_Loss: 246.0906  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1066: Validation loss did not decrease\n",
            "\t Train_Loss: 101.1639 Val_Loss: 249.3440  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1067: Validation loss did not decrease\n",
            "\t Train_Loss: 97.4524 Val_Loss: 338.7175  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1068: Validation loss did not decrease\n",
            "\t Train_Loss: 111.0567 Val_Loss: 341.2055  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1069: Validation loss did not decrease\n",
            "\t Train_Loss: 111.6103 Val_Loss: 340.2180  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1070: Validation loss did not decrease\n",
            "\t Train_Loss: 111.3898 Val_Loss: 338.9834  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1071: Validation loss did not decrease\n",
            "\t Train_Loss: 111.2826 Val_Loss: 337.9034  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1072: Validation loss did not decrease\n",
            "\t Train_Loss: 111.2794 Val_Loss: 336.9767  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1073: Validation loss did not decrease\n",
            "\t Train_Loss: 111.1722 Val_Loss: 336.1766  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1074: Validation loss did not decrease\n",
            "\t Train_Loss: 110.8888 Val_Loss: 335.5161  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1075: Validation loss did not decrease\n",
            "\t Train_Loss: 110.5435 Val_Loss: 334.9656  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1076: Validation loss did not decrease\n",
            "\t Train_Loss: 110.2899 Val_Loss: 334.3506  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1077: Validation loss did not decrease\n",
            "\t Train_Loss: 110.1303 Val_Loss: 333.4296  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1078: Validation loss did not decrease\n",
            "\t Train_Loss: 109.9487 Val_Loss: 332.0871  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1079: Validation loss did not decrease\n",
            "\t Train_Loss: 109.6789 Val_Loss: 330.4060  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1080: Validation loss did not decrease\n",
            "\t Train_Loss: 109.3507 Val_Loss: 328.5887  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1081: Validation loss did not decrease\n",
            "\t Train_Loss: 109.0435 Val_Loss: 326.8260  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1082: Validation loss did not decrease\n",
            "\t Train_Loss: 108.8026 Val_Loss: 325.2132  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1083: Validation loss did not decrease\n",
            "\t Train_Loss: 108.5881 Val_Loss: 323.7695  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1084: Validation loss did not decrease\n",
            "\t Train_Loss: 108.3353 Val_Loss: 322.4864  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1085: Validation loss did not decrease\n",
            "\t Train_Loss: 108.0367 Val_Loss: 321.3351  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1086: Validation loss did not decrease\n",
            "\t Train_Loss: 107.7400 Val_Loss: 320.2428  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1087: Validation loss did not decrease\n",
            "\t Train_Loss: 107.4822 Val_Loss: 319.0965  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1088: Validation loss did not decrease\n",
            "\t Train_Loss: 107.2508 Val_Loss: 317.7987  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1089: Validation loss did not decrease\n",
            "\t Train_Loss: 107.0138 Val_Loss: 316.3180  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1090: Validation loss did not decrease\n",
            "\t Train_Loss: 106.7574 Val_Loss: 314.6965  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1091: Validation loss did not decrease\n",
            "\t Train_Loss: 106.4924 Val_Loss: 313.0212  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1092: Validation loss did not decrease\n",
            "\t Train_Loss: 106.2392 Val_Loss: 311.3810  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1093: Validation loss did not decrease\n",
            "\t Train_Loss: 106.0084 Val_Loss: 309.8381  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1094: Validation loss did not decrease\n",
            "\t Train_Loss: 105.7913 Val_Loss: 308.4205  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1095: Validation loss did not decrease\n",
            "\t Train_Loss: 105.5713 Val_Loss: 307.1286  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1096: Validation loss did not decrease\n",
            "\t Train_Loss: 105.3433 Val_Loss: 305.9408  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1097: Validation loss did not decrease\n",
            "\t Train_Loss: 105.1167 Val_Loss: 304.8165  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1098: Validation loss did not decrease\n",
            "\t Train_Loss: 104.9017 Val_Loss: 303.7038  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1099: Validation loss did not decrease\n",
            "\t Train_Loss: 104.6997 Val_Loss: 302.5539  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1100: Validation loss did not decrease\n",
            "\t Train_Loss: 104.5047 Val_Loss: 301.3381  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1101: Validation loss did not decrease\n",
            "\t Train_Loss: 104.3111 Val_Loss: 300.0540  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1102: Validation loss did not decrease\n",
            "\t Train_Loss: 104.1176 Val_Loss: 298.7226  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1103: Validation loss did not decrease\n",
            "\t Train_Loss: 103.9270 Val_Loss: 297.3791  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1104: Validation loss did not decrease\n",
            "\t Train_Loss: 103.7429 Val_Loss: 296.0604  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1105: Validation loss did not decrease\n",
            "\t Train_Loss: 103.5670 Val_Loss: 294.7962  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1106: Validation loss did not decrease\n",
            "\t Train_Loss: 103.3979 Val_Loss: 293.6047  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1107: Validation loss did not decrease\n",
            "\t Train_Loss: 103.2325 Val_Loss: 292.4917  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1108: Validation loss did not decrease\n",
            "\t Train_Loss: 103.0693 Val_Loss: 291.4509  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1109: Validation loss did not decrease\n",
            "\t Train_Loss: 102.9084 Val_Loss: 290.4672  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1110: Validation loss did not decrease\n",
            "\t Train_Loss: 102.7515 Val_Loss: 289.5194  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1111: Validation loss did not decrease\n",
            "\t Train_Loss: 102.6001 Val_Loss: 288.5836  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1112: Validation loss did not decrease\n",
            "\t Train_Loss: 102.4541 Val_Loss: 287.6383  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1113: Validation loss did not decrease\n",
            "\t Train_Loss: 102.3125 Val_Loss: 286.6697  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1114: Validation loss did not decrease\n",
            "\t Train_Loss: 102.1742 Val_Loss: 285.6735  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1115: Validation loss did not decrease\n",
            "\t Train_Loss: 102.0384 Val_Loss: 284.6562  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1116: Validation loss did not decrease\n",
            "\t Train_Loss: 101.9054 Val_Loss: 283.6325  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1117: Validation loss did not decrease\n",
            "\t Train_Loss: 101.7760 Val_Loss: 282.6201  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1118: Validation loss did not decrease\n",
            "\t Train_Loss: 101.6507 Val_Loss: 281.6356  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1119: Validation loss did not decrease\n",
            "\t Train_Loss: 101.5292 Val_Loss: 280.6917  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1120: Validation loss did not decrease\n",
            "\t Train_Loss: 101.4111 Val_Loss: 279.7951  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1121: Validation loss did not decrease\n",
            "\t Train_Loss: 101.2957 Val_Loss: 278.9457  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1122: Validation loss did not decrease\n",
            "\t Train_Loss: 101.1827 Val_Loss: 278.1377  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1123: Validation loss did not decrease\n",
            "\t Train_Loss: 101.0723 Val_Loss: 277.3602  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1124: Validation loss did not decrease\n",
            "\t Train_Loss: 100.9649 Val_Loss: 276.6003  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1125: Validation loss did not decrease\n",
            "\t Train_Loss: 100.8606 Val_Loss: 275.8444  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1126: Validation loss did not decrease\n",
            "\t Train_Loss: 100.7591 Val_Loss: 275.0833  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1127: Validation loss did not decrease\n",
            "\t Train_Loss: 100.6601 Val_Loss: 274.3113  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1128: Validation loss did not decrease\n",
            "\t Train_Loss: 100.5633 Val_Loss: 273.5302  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1129: Validation loss did not decrease\n",
            "\t Train_Loss: 100.4686 Val_Loss: 272.7466  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1130: Validation loss did not decrease\n",
            "\t Train_Loss: 100.3761 Val_Loss: 271.9707  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1131: Validation loss did not decrease\n",
            "\t Train_Loss: 100.2859 Val_Loss: 271.2125  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1132: Validation loss did not decrease\n",
            "\t Train_Loss: 100.1981 Val_Loss: 270.4805  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1133: Validation loss did not decrease\n",
            "\t Train_Loss: 100.1123 Val_Loss: 269.7795  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1134: Validation loss did not decrease\n",
            "\t Train_Loss: 100.0285 Val_Loss: 269.1104  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1135: Validation loss did not decrease\n",
            "\t Train_Loss: 99.9464 Val_Loss: 268.4694  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1136: Validation loss did not decrease\n",
            "\t Train_Loss: 99.8661 Val_Loss: 267.8499  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1137: Validation loss did not decrease\n",
            "\t Train_Loss: 99.7875 Val_Loss: 267.2436  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1138: Validation loss did not decrease\n",
            "\t Train_Loss: 99.7108 Val_Loss: 266.6427  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1139: Validation loss did not decrease\n",
            "\t Train_Loss: 99.6358 Val_Loss: 266.0415  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1140: Validation loss did not decrease\n",
            "\t Train_Loss: 99.5624 Val_Loss: 265.4376  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1141: Validation loss did not decrease\n",
            "\t Train_Loss: 99.4904 Val_Loss: 264.8326  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1142: Validation loss did not decrease\n",
            "\t Train_Loss: 99.4198 Val_Loss: 264.2306  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1143: Validation loss did not decrease\n",
            "\t Train_Loss: 99.3507 Val_Loss: 263.6375  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1144: Validation loss did not decrease\n",
            "\t Train_Loss: 99.2829 Val_Loss: 263.0593  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1145: Validation loss did not decrease\n",
            "\t Train_Loss: 99.2166 Val_Loss: 262.5003  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1146: Validation loss did not decrease\n",
            "\t Train_Loss: 99.1515 Val_Loss: 261.9623  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1147: Validation loss did not decrease\n",
            "\t Train_Loss: 99.0876 Val_Loss: 261.4448  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1148: Validation loss did not decrease\n",
            "\t Train_Loss: 99.0248 Val_Loss: 260.9445  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1149: Validation loss did not decrease\n",
            "\t Train_Loss: 98.9632 Val_Loss: 260.4565  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1150: Validation loss did not decrease\n",
            "\t Train_Loss: 98.9027 Val_Loss: 259.9762  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1151: Validation loss did not decrease\n",
            "\t Train_Loss: 98.8434 Val_Loss: 259.4992  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1152: Validation loss did not decrease\n",
            "\t Train_Loss: 98.7850 Val_Loss: 259.0234  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1153: Validation loss did not decrease\n",
            "\t Train_Loss: 98.7275 Val_Loss: 258.5486  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1154: Validation loss did not decrease\n",
            "\t Train_Loss: 98.6710 Val_Loss: 258.0769  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1155: Validation loss did not decrease\n",
            "\t Train_Loss: 98.6154 Val_Loss: 257.6115  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1156: Validation loss did not decrease\n",
            "\t Train_Loss: 98.5607 Val_Loss: 257.1559  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1157: Validation loss did not decrease\n",
            "\t Train_Loss: 98.5069 Val_Loss: 256.7128  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1158: Validation loss did not decrease\n",
            "\t Train_Loss: 98.4538 Val_Loss: 256.2838  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1159: Validation loss did not decrease\n",
            "\t Train_Loss: 98.4016 Val_Loss: 255.8685  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1160: Validation loss did not decrease\n",
            "\t Train_Loss: 98.3500 Val_Loss: 255.4656  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1161: Validation loss did not decrease\n",
            "\t Train_Loss: 98.2992 Val_Loss: 255.0719  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1162: Validation loss did not decrease\n",
            "\t Train_Loss: 98.2491 Val_Loss: 254.6842  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1163: Validation loss did not decrease\n",
            "\t Train_Loss: 98.1996 Val_Loss: 254.3008  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1164: Validation loss did not decrease\n",
            "\t Train_Loss: 98.1508 Val_Loss: 253.9197  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1165: Validation loss did not decrease\n",
            "\t Train_Loss: 98.1026 Val_Loss: 253.5409  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1166: Validation loss did not decrease\n",
            "\t Train_Loss: 98.0550 Val_Loss: 253.1657  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1167: Validation loss did not decrease\n",
            "\t Train_Loss: 98.0080 Val_Loss: 252.7958  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1168: Validation loss did not decrease\n",
            "\t Train_Loss: 97.9614 Val_Loss: 252.4333  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1169: Validation loss did not decrease\n",
            "\t Train_Loss: 97.9155 Val_Loss: 252.0798  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1170: Validation loss did not decrease\n",
            "\t Train_Loss: 97.8700 Val_Loss: 251.7362  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1171: Validation loss did not decrease\n",
            "\t Train_Loss: 97.8250 Val_Loss: 251.4018  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1172: Validation loss did not decrease\n",
            "\t Train_Loss: 97.7805 Val_Loss: 251.0757  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1173: Validation loss did not decrease\n",
            "\t Train_Loss: 97.7365 Val_Loss: 250.7560  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1174: Validation loss did not decrease\n",
            "\t Train_Loss: 97.6928 Val_Loss: 250.4408  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1175: Validation loss did not decrease\n",
            "\t Train_Loss: 97.6496 Val_Loss: 250.1292  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1176: Validation loss did not decrease\n",
            "\t Train_Loss: 97.6068 Val_Loss: 249.8202  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1177: Validation loss did not decrease\n",
            "\t Train_Loss: 97.5643 Val_Loss: 249.5139  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1178: Validation loss did not decrease\n",
            "\t Train_Loss: 97.5222 Val_Loss: 249.2115  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1179: Validation loss did not decrease\n",
            "\t Train_Loss: 97.4805 Val_Loss: 248.9140  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1180: Validation loss did not decrease\n",
            "\t Train_Loss: 97.4391 Val_Loss: 248.6228  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1181: Validation loss did not decrease\n",
            "\t Train_Loss: 97.3981 Val_Loss: 248.3382  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1182: Validation loss did not decrease\n",
            "\t Train_Loss: 97.3573 Val_Loss: 248.0605  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1183: Validation loss did not decrease\n",
            "\t Train_Loss: 97.3168 Val_Loss: 247.7892  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1184: Validation loss did not decrease\n",
            "\t Train_Loss: 97.2767 Val_Loss: 247.5234  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1185: Validation loss did not decrease\n",
            "\t Train_Loss: 97.2368 Val_Loss: 247.2618  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1186: Validation loss did not decrease\n",
            "\t Train_Loss: 97.1972 Val_Loss: 247.0036  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1187: Validation loss did not decrease\n",
            "\t Train_Loss: 97.1578 Val_Loss: 246.7482  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1188: Validation loss did not decrease\n",
            "\t Train_Loss: 97.1187 Val_Loss: 246.4953  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1189: Validation loss did not decrease\n",
            "\t Train_Loss: 97.0798 Val_Loss: 246.2457  BEST VAL Loss: 246.0906\n",
            "\n",
            "Epoch 1190: Validation loss decreased (246.090576 --> 245.999863).\n",
            "\t Train_Loss: 97.0411 Val_Loss: 245.9999  BEST VAL Loss: 245.9999\n",
            "\n",
            "Epoch 1191: Validation loss decreased (245.999863 --> 245.758530).\n",
            "\t Train_Loss: 97.0027 Val_Loss: 245.7585  BEST VAL Loss: 245.7585\n",
            "\n",
            "Epoch 1192: Validation loss decreased (245.758530 --> 245.521973).\n",
            "\t Train_Loss: 96.9645 Val_Loss: 245.5220  BEST VAL Loss: 245.5220\n",
            "\n",
            "Epoch 1193: Validation loss decreased (245.521973 --> 245.290359).\n",
            "\t Train_Loss: 96.9264 Val_Loss: 245.2904  BEST VAL Loss: 245.2904\n",
            "\n",
            "Epoch 1194: Validation loss decreased (245.290359 --> 245.063583).\n",
            "\t Train_Loss: 96.8885 Val_Loss: 245.0636  BEST VAL Loss: 245.0636\n",
            "\n",
            "Epoch 1195: Validation loss decreased (245.063583 --> 244.841110).\n",
            "\t Train_Loss: 96.8509 Val_Loss: 244.8411  BEST VAL Loss: 244.8411\n",
            "\n",
            "Epoch 1196: Validation loss decreased (244.841110 --> 244.622223).\n",
            "\t Train_Loss: 96.8134 Val_Loss: 244.6222  BEST VAL Loss: 244.6222\n",
            "\n",
            "Epoch 1197: Validation loss decreased (244.622223 --> 244.406357).\n",
            "\t Train_Loss: 96.7761 Val_Loss: 244.4064  BEST VAL Loss: 244.4064\n",
            "\n",
            "Epoch 1198: Validation loss decreased (244.406357 --> 244.193069).\n",
            "\t Train_Loss: 96.7389 Val_Loss: 244.1931  BEST VAL Loss: 244.1931\n",
            "\n",
            "Epoch 1199: Validation loss decreased (244.193069 --> 243.982086).\n",
            "\t Train_Loss: 96.7020 Val_Loss: 243.9821  BEST VAL Loss: 243.9821\n",
            "\n",
            "Epoch 1200: Validation loss decreased (243.982086 --> 243.773788).\n",
            "\t Train_Loss: 96.6651 Val_Loss: 243.7738  BEST VAL Loss: 243.7738\n",
            "\n",
            "Epoch 1201: Validation loss decreased (243.773788 --> 243.568481).\n",
            "\t Train_Loss: 96.6284 Val_Loss: 243.5685  BEST VAL Loss: 243.5685\n",
            "\n",
            "Epoch 1202: Validation loss decreased (243.568481 --> 243.366608).\n",
            "\t Train_Loss: 96.5919 Val_Loss: 243.3666  BEST VAL Loss: 243.3666\n",
            "\n",
            "Epoch 1203: Validation loss decreased (243.366608 --> 243.168579).\n",
            "\t Train_Loss: 96.5554 Val_Loss: 243.1686  BEST VAL Loss: 243.1686\n",
            "\n",
            "Epoch 1204: Validation loss decreased (243.168579 --> 242.974106).\n",
            "\t Train_Loss: 96.5191 Val_Loss: 242.9741  BEST VAL Loss: 242.9741\n",
            "\n",
            "Epoch 1205: Validation loss decreased (242.974106 --> 242.783371).\n",
            "\t Train_Loss: 96.4829 Val_Loss: 242.7834  BEST VAL Loss: 242.7834\n",
            "\n",
            "Epoch 1206: Validation loss decreased (242.783371 --> 242.595810).\n",
            "\t Train_Loss: 96.4469 Val_Loss: 242.5958  BEST VAL Loss: 242.5958\n",
            "\n",
            "Epoch 1207: Validation loss decreased (242.595810 --> 242.411041).\n",
            "\t Train_Loss: 96.4109 Val_Loss: 242.4110  BEST VAL Loss: 242.4110\n",
            "\n",
            "Epoch 1208: Validation loss decreased (242.411041 --> 242.228622).\n",
            "\t Train_Loss: 96.3751 Val_Loss: 242.2286  BEST VAL Loss: 242.2286\n",
            "\n",
            "Epoch 1209: Validation loss decreased (242.228622 --> 242.048584).\n",
            "\t Train_Loss: 96.3394 Val_Loss: 242.0486  BEST VAL Loss: 242.0486\n",
            "\n",
            "Epoch 1210: Validation loss decreased (242.048584 --> 241.870407).\n",
            "\t Train_Loss: 96.3037 Val_Loss: 241.8704  BEST VAL Loss: 241.8704\n",
            "\n",
            "Epoch 1211: Validation loss decreased (241.870407 --> 241.694931).\n",
            "\t Train_Loss: 96.2682 Val_Loss: 241.6949  BEST VAL Loss: 241.6949\n",
            "\n",
            "Epoch 1212: Validation loss decreased (241.694931 --> 241.521530).\n",
            "\t Train_Loss: 96.2327 Val_Loss: 241.5215  BEST VAL Loss: 241.5215\n",
            "\n",
            "Epoch 1213: Validation loss decreased (241.521530 --> 241.351074).\n",
            "\t Train_Loss: 96.1974 Val_Loss: 241.3511  BEST VAL Loss: 241.3511\n",
            "\n",
            "Epoch 1214: Validation loss decreased (241.351074 --> 241.183350).\n",
            "\t Train_Loss: 96.1621 Val_Loss: 241.1833  BEST VAL Loss: 241.1833\n",
            "\n",
            "Epoch 1215: Validation loss decreased (241.183350 --> 241.018478).\n",
            "\t Train_Loss: 96.1269 Val_Loss: 241.0185  BEST VAL Loss: 241.0185\n",
            "\n",
            "Epoch 1216: Validation loss decreased (241.018478 --> 240.856277).\n",
            "\t Train_Loss: 96.0918 Val_Loss: 240.8563  BEST VAL Loss: 240.8563\n",
            "\n",
            "Epoch 1217: Validation loss decreased (240.856277 --> 240.696289).\n",
            "\t Train_Loss: 96.0568 Val_Loss: 240.6963  BEST VAL Loss: 240.6963\n",
            "\n",
            "Epoch 1218: Validation loss decreased (240.696289 --> 240.538986).\n",
            "\t Train_Loss: 96.0218 Val_Loss: 240.5390  BEST VAL Loss: 240.5390\n",
            "\n",
            "Epoch 1219: Validation loss decreased (240.538986 --> 240.383209).\n",
            "\t Train_Loss: 95.9869 Val_Loss: 240.3832  BEST VAL Loss: 240.3832\n",
            "\n",
            "Epoch 1220: Validation loss decreased (240.383209 --> 240.229401).\n",
            "\t Train_Loss: 95.9521 Val_Loss: 240.2294  BEST VAL Loss: 240.2294\n",
            "\n",
            "Epoch 1221: Validation loss decreased (240.229401 --> 240.077423).\n",
            "\t Train_Loss: 95.9174 Val_Loss: 240.0774  BEST VAL Loss: 240.0774\n",
            "\n",
            "Epoch 1222: Validation loss decreased (240.077423 --> 239.927322).\n",
            "\t Train_Loss: 95.8827 Val_Loss: 239.9273  BEST VAL Loss: 239.9273\n",
            "\n",
            "Epoch 1223: Validation loss decreased (239.927322 --> 239.779541).\n",
            "\t Train_Loss: 95.8480 Val_Loss: 239.7795  BEST VAL Loss: 239.7795\n",
            "\n",
            "Epoch 1224: Validation loss decreased (239.779541 --> 239.633530).\n",
            "\t Train_Loss: 95.8135 Val_Loss: 239.6335  BEST VAL Loss: 239.6335\n",
            "\n",
            "Epoch 1225: Validation loss decreased (239.633530 --> 239.489899).\n",
            "\t Train_Loss: 95.7790 Val_Loss: 239.4899  BEST VAL Loss: 239.4899\n",
            "\n",
            "Epoch 1226: Validation loss decreased (239.489899 --> 239.348312).\n",
            "\t Train_Loss: 95.7445 Val_Loss: 239.3483  BEST VAL Loss: 239.3483\n",
            "\n",
            "Epoch 1227: Validation loss decreased (239.348312 --> 239.208618).\n",
            "\t Train_Loss: 95.7101 Val_Loss: 239.2086  BEST VAL Loss: 239.2086\n",
            "\n",
            "Epoch 1228: Validation loss decreased (239.208618 --> 239.070953).\n",
            "\t Train_Loss: 95.6757 Val_Loss: 239.0710  BEST VAL Loss: 239.0710\n",
            "\n",
            "Epoch 1229: Validation loss decreased (239.070953 --> 238.934967).\n",
            "\t Train_Loss: 95.6414 Val_Loss: 238.9350  BEST VAL Loss: 238.9350\n",
            "\n",
            "Epoch 1230: Validation loss decreased (238.934967 --> 238.800400).\n",
            "\t Train_Loss: 95.6072 Val_Loss: 238.8004  BEST VAL Loss: 238.8004\n",
            "\n",
            "Epoch 1231: Validation loss decreased (238.800400 --> 238.667435).\n",
            "\t Train_Loss: 95.5729 Val_Loss: 238.6674  BEST VAL Loss: 238.6674\n",
            "\n",
            "Epoch 1232: Validation loss decreased (238.667435 --> 238.535889).\n",
            "\t Train_Loss: 95.5387 Val_Loss: 238.5359  BEST VAL Loss: 238.5359\n",
            "\n",
            "Epoch 1233: Validation loss decreased (238.535889 --> 238.405914).\n",
            "\t Train_Loss: 95.5046 Val_Loss: 238.4059  BEST VAL Loss: 238.4059\n",
            "\n",
            "Epoch 1234: Validation loss decreased (238.405914 --> 238.277878).\n",
            "\t Train_Loss: 95.4705 Val_Loss: 238.2779  BEST VAL Loss: 238.2779\n",
            "\n",
            "Epoch 1235: Validation loss decreased (238.277878 --> 238.151245).\n",
            "\t Train_Loss: 95.4365 Val_Loss: 238.1512  BEST VAL Loss: 238.1512\n",
            "\n",
            "Epoch 1236: Validation loss decreased (238.151245 --> 238.026443).\n",
            "\t Train_Loss: 95.4024 Val_Loss: 238.0264  BEST VAL Loss: 238.0264\n",
            "\n",
            "Epoch 1237: Validation loss decreased (238.026443 --> 237.903183).\n",
            "\t Train_Loss: 95.3685 Val_Loss: 237.9032  BEST VAL Loss: 237.9032\n",
            "\n",
            "Epoch 1238: Validation loss decreased (237.903183 --> 237.781326).\n",
            "\t Train_Loss: 95.3345 Val_Loss: 237.7813  BEST VAL Loss: 237.7813\n",
            "\n",
            "Epoch 1239: Validation loss decreased (237.781326 --> 237.660812).\n",
            "\t Train_Loss: 95.3006 Val_Loss: 237.6608  BEST VAL Loss: 237.6608\n",
            "\n",
            "Epoch 1240: Validation loss decreased (237.660812 --> 237.541794).\n",
            "\t Train_Loss: 95.2667 Val_Loss: 237.5418  BEST VAL Loss: 237.5418\n",
            "\n",
            "Epoch 1241: Validation loss decreased (237.541794 --> 237.423981).\n",
            "\t Train_Loss: 95.2329 Val_Loss: 237.4240  BEST VAL Loss: 237.4240\n",
            "\n",
            "Epoch 1242: Validation loss decreased (237.423981 --> 237.307205).\n",
            "\t Train_Loss: 95.1990 Val_Loss: 237.3072  BEST VAL Loss: 237.3072\n",
            "\n",
            "Epoch 1243: Validation loss decreased (237.307205 --> 237.191940).\n",
            "\t Train_Loss: 95.1652 Val_Loss: 237.1919  BEST VAL Loss: 237.1919\n",
            "\n",
            "Epoch 1244: Validation loss decreased (237.191940 --> 237.077988).\n",
            "\t Train_Loss: 95.1315 Val_Loss: 237.0780  BEST VAL Loss: 237.0780\n",
            "\n",
            "Epoch 1245: Validation loss decreased (237.077988 --> 236.965118).\n",
            "\t Train_Loss: 95.0977 Val_Loss: 236.9651  BEST VAL Loss: 236.9651\n",
            "\n",
            "Epoch 1246: Validation loss decreased (236.965118 --> 236.853516).\n",
            "\t Train_Loss: 95.0640 Val_Loss: 236.8535  BEST VAL Loss: 236.8535\n",
            "\n",
            "Epoch 1247: Validation loss decreased (236.853516 --> 236.743454).\n",
            "\t Train_Loss: 95.0303 Val_Loss: 236.7435  BEST VAL Loss: 236.7435\n",
            "\n",
            "Epoch 1248: Validation loss decreased (236.743454 --> 236.634628).\n",
            "\t Train_Loss: 94.9967 Val_Loss: 236.6346  BEST VAL Loss: 236.6346\n",
            "\n",
            "Epoch 1249: Validation loss decreased (236.634628 --> 236.526718).\n",
            "\t Train_Loss: 94.9631 Val_Loss: 236.5267  BEST VAL Loss: 236.5267\n",
            "\n",
            "Epoch 1250: Validation loss decreased (236.526718 --> 236.419907).\n",
            "\t Train_Loss: 94.9295 Val_Loss: 236.4199  BEST VAL Loss: 236.4199\n",
            "\n",
            "Epoch 1251: Validation loss decreased (236.419907 --> 236.314072).\n",
            "\t Train_Loss: 94.8959 Val_Loss: 236.3141  BEST VAL Loss: 236.3141\n",
            "\n",
            "Epoch 1252: Validation loss decreased (236.314072 --> 236.209229).\n",
            "\t Train_Loss: 94.8623 Val_Loss: 236.2092  BEST VAL Loss: 236.2092\n",
            "\n",
            "Epoch 1253: Validation loss decreased (236.209229 --> 236.105392).\n",
            "\t Train_Loss: 94.8288 Val_Loss: 236.1054  BEST VAL Loss: 236.1054\n",
            "\n",
            "Epoch 1254: Validation loss decreased (236.105392 --> 236.002686).\n",
            "\t Train_Loss: 94.7952 Val_Loss: 236.0027  BEST VAL Loss: 236.0027\n",
            "\n",
            "Epoch 1255: Validation loss decreased (236.002686 --> 235.900986).\n",
            "\t Train_Loss: 94.7617 Val_Loss: 235.9010  BEST VAL Loss: 235.9010\n",
            "\n",
            "Epoch 1256: Validation loss decreased (235.900986 --> 235.800247).\n",
            "\t Train_Loss: 94.7282 Val_Loss: 235.8002  BEST VAL Loss: 235.8002\n",
            "\n",
            "Epoch 1257: Validation loss decreased (235.800247 --> 235.700729).\n",
            "\t Train_Loss: 94.6948 Val_Loss: 235.7007  BEST VAL Loss: 235.7007\n",
            "\n",
            "Epoch 1258: Validation loss decreased (235.700729 --> 235.601959).\n",
            "\t Train_Loss: 94.6614 Val_Loss: 235.6020  BEST VAL Loss: 235.6020\n",
            "\n",
            "Epoch 1259: Validation loss decreased (235.601959 --> 235.504105).\n",
            "\t Train_Loss: 94.6279 Val_Loss: 235.5041  BEST VAL Loss: 235.5041\n",
            "\n",
            "Epoch 1260: Validation loss decreased (235.504105 --> 235.407028).\n",
            "\t Train_Loss: 94.5945 Val_Loss: 235.4070  BEST VAL Loss: 235.4070\n",
            "\n",
            "Epoch 1261: Validation loss decreased (235.407028 --> 235.310944).\n",
            "\t Train_Loss: 94.5611 Val_Loss: 235.3109  BEST VAL Loss: 235.3109\n",
            "\n",
            "Epoch 1262: Validation loss decreased (235.310944 --> 235.215561).\n",
            "\t Train_Loss: 94.5277 Val_Loss: 235.2156  BEST VAL Loss: 235.2156\n",
            "\n",
            "Epoch 1263: Validation loss decreased (235.215561 --> 235.121140).\n",
            "\t Train_Loss: 94.4943 Val_Loss: 235.1211  BEST VAL Loss: 235.1211\n",
            "\n",
            "Epoch 1264: Validation loss decreased (235.121140 --> 235.027344).\n",
            "\t Train_Loss: 94.4610 Val_Loss: 235.0273  BEST VAL Loss: 235.0273\n",
            "\n",
            "Epoch 1265: Validation loss decreased (235.027344 --> 234.934570).\n",
            "\t Train_Loss: 94.4276 Val_Loss: 234.9346  BEST VAL Loss: 234.9346\n",
            "\n",
            "Epoch 1266: Validation loss decreased (234.934570 --> 234.842529).\n",
            "\t Train_Loss: 94.3943 Val_Loss: 234.8425  BEST VAL Loss: 234.8425\n",
            "\n",
            "Epoch 1267: Validation loss decreased (234.842529 --> 234.750977).\n",
            "\t Train_Loss: 94.3610 Val_Loss: 234.7510  BEST VAL Loss: 234.7510\n",
            "\n",
            "Epoch 1268: Validation loss decreased (234.750977 --> 234.660477).\n",
            "\t Train_Loss: 94.3276 Val_Loss: 234.6605  BEST VAL Loss: 234.6605\n",
            "\n",
            "Epoch 1269: Validation loss decreased (234.660477 --> 234.570511).\n",
            "\t Train_Loss: 94.2943 Val_Loss: 234.5705  BEST VAL Loss: 234.5705\n",
            "\n",
            "Epoch 1270: Validation loss decreased (234.570511 --> 234.481369).\n",
            "\t Train_Loss: 94.2610 Val_Loss: 234.4814  BEST VAL Loss: 234.4814\n",
            "\n",
            "Epoch 1271: Validation loss decreased (234.481369 --> 234.392899).\n",
            "\t Train_Loss: 94.2276 Val_Loss: 234.3929  BEST VAL Loss: 234.3929\n",
            "\n",
            "Epoch 1272: Validation loss decreased (234.392899 --> 234.305176).\n",
            "\t Train_Loss: 94.1943 Val_Loss: 234.3052  BEST VAL Loss: 234.3052\n",
            "\n",
            "Epoch 1273: Validation loss decreased (234.305176 --> 234.218018).\n",
            "\t Train_Loss: 94.1608 Val_Loss: 234.2180  BEST VAL Loss: 234.2180\n",
            "\n",
            "Epoch 1274: Validation loss decreased (234.218018 --> 234.131393).\n",
            "\t Train_Loss: 94.1273 Val_Loss: 234.1314  BEST VAL Loss: 234.1314\n",
            "\n",
            "Epoch 1275: Validation loss decreased (234.131393 --> 234.045654).\n",
            "\t Train_Loss: 94.0936 Val_Loss: 234.0457  BEST VAL Loss: 234.0457\n",
            "\n",
            "Epoch 1276: Validation loss decreased (234.045654 --> 233.960571).\n",
            "\t Train_Loss: 94.0594 Val_Loss: 233.9606  BEST VAL Loss: 233.9606\n",
            "\n",
            "Epoch 1277: Validation loss decreased (233.960571 --> 233.876633).\n",
            "\t Train_Loss: 94.0243 Val_Loss: 233.8766  BEST VAL Loss: 233.8766\n",
            "\n",
            "Epoch 1278: Validation loss decreased (233.876633 --> 233.795456).\n",
            "\t Train_Loss: 93.9860 Val_Loss: 233.7955  BEST VAL Loss: 233.7955\n",
            "\n",
            "Epoch 1279: Validation loss decreased (233.795456 --> 233.775665).\n",
            "\t Train_Loss: 93.9318 Val_Loss: 233.7757  BEST VAL Loss: 233.7757\n",
            "\n",
            "Epoch 1280: Validation loss did not decrease\n",
            "\t Train_Loss: 93.5978 Val_Loss: 273.7566  BEST VAL Loss: 233.7757\n",
            "\n",
            "Epoch 1281: Validation loss decreased (233.775665 --> 232.535202).\n",
            "\t Train_Loss: 91.1873 Val_Loss: 232.5352  BEST VAL Loss: 232.5352\n",
            "\n",
            "Epoch 1282: Validation loss decreased (232.535202 --> 231.643021).\n",
            "\t Train_Loss: 93.1836 Val_Loss: 231.6430  BEST VAL Loss: 231.6430\n",
            "\n",
            "Epoch 1283: Validation loss decreased (231.643021 --> 231.411331).\n",
            "\t Train_Loss: 93.9481 Val_Loss: 231.4113  BEST VAL Loss: 231.4113\n",
            "\n",
            "Epoch 1284: Validation loss did not decrease\n",
            "\t Train_Loss: 93.9405 Val_Loss: 231.6456  BEST VAL Loss: 231.4113\n",
            "\n",
            "Epoch 1285: Validation loss did not decrease\n",
            "\t Train_Loss: 93.8460 Val_Loss: 232.2474  BEST VAL Loss: 231.4113\n",
            "\n",
            "Epoch 1286: Validation loss did not decrease\n",
            "\t Train_Loss: 93.7446 Val_Loss: 232.9909  BEST VAL Loss: 231.4113\n",
            "\n",
            "Epoch 1287: Validation loss did not decrease\n",
            "\t Train_Loss: 93.7089 Val_Loss: 233.5205  BEST VAL Loss: 231.4113\n",
            "\n",
            "Epoch 1288: Validation loss did not decrease\n",
            "\t Train_Loss: 93.7324 Val_Loss: 233.5484  BEST VAL Loss: 231.4113\n",
            "\n",
            "Epoch 1289: Validation loss did not decrease\n",
            "\t Train_Loss: 93.7208 Val_Loss: 233.0475  BEST VAL Loss: 231.4113\n",
            "\n",
            "Epoch 1290: Validation loss did not decrease\n",
            "\t Train_Loss: 93.6421 Val_Loss: 232.2520  BEST VAL Loss: 231.4113\n",
            "\n",
            "Epoch 1291: Validation loss did not decrease\n",
            "\t Train_Loss: 93.5704 Val_Loss: 231.4857  BEST VAL Loss: 231.4113\n",
            "\n",
            "Epoch 1292: Validation loss decreased (231.411331 --> 230.972900).\n",
            "\t Train_Loss: 93.5485 Val_Loss: 230.9729  BEST VAL Loss: 230.9729\n",
            "\n",
            "Epoch 1293: Validation loss decreased (230.972900 --> 230.792282).\n",
            "\t Train_Loss: 93.5457 Val_Loss: 230.7923  BEST VAL Loss: 230.7923\n",
            "\n",
            "Epoch 1294: Validation loss did not decrease\n",
            "\t Train_Loss: 93.5213 Val_Loss: 230.9290  BEST VAL Loss: 230.7923\n",
            "\n",
            "Epoch 1295: Validation loss did not decrease\n",
            "\t Train_Loss: 93.4678 Val_Loss: 231.3011  BEST VAL Loss: 230.7923\n",
            "\n",
            "Epoch 1296: Validation loss did not decrease\n",
            "\t Train_Loss: 93.4096 Val_Loss: 231.7518  BEST VAL Loss: 230.7923\n",
            "\n",
            "Epoch 1297: Validation loss did not decrease\n",
            "\t Train_Loss: 93.3748 Val_Loss: 232.0765  BEST VAL Loss: 230.7923\n",
            "\n",
            "Epoch 1298: Validation loss did not decrease\n",
            "\t Train_Loss: 93.3609 Val_Loss: 232.1159  BEST VAL Loss: 230.7923\n",
            "\n",
            "Epoch 1299: Validation loss did not decrease\n",
            "\t Train_Loss: 93.3367 Val_Loss: 231.8473  BEST VAL Loss: 230.7923\n",
            "\n",
            "Epoch 1300: Validation loss did not decrease\n",
            "\t Train_Loss: 93.2895 Val_Loss: 231.3887  BEST VAL Loss: 230.7923\n",
            "\n",
            "Epoch 1301: Validation loss did not decrease\n",
            "\t Train_Loss: 93.2416 Val_Loss: 230.9199  BEST VAL Loss: 230.7923\n",
            "\n",
            "Epoch 1302: Validation loss decreased (230.792282 --> 230.588043).\n",
            "\t Train_Loss: 93.2108 Val_Loss: 230.5880  BEST VAL Loss: 230.5880\n",
            "\n",
            "Epoch 1303: Validation loss decreased (230.588043 --> 230.462891).\n",
            "\t Train_Loss: 93.1888 Val_Loss: 230.4629  BEST VAL Loss: 230.4629\n",
            "\n",
            "Epoch 1304: Validation loss did not decrease\n",
            "\t Train_Loss: 93.1597 Val_Loss: 230.5429  BEST VAL Loss: 230.4629\n",
            "\n",
            "Epoch 1305: Validation loss did not decrease\n",
            "\t Train_Loss: 93.1194 Val_Loss: 230.7677  BEST VAL Loss: 230.4629\n",
            "\n",
            "Epoch 1306: Validation loss did not decrease\n",
            "\t Train_Loss: 93.0774 Val_Loss: 231.0303  BEST VAL Loss: 230.4629\n",
            "\n",
            "Epoch 1307: Validation loss did not decrease\n",
            "\t Train_Loss: 93.0444 Val_Loss: 231.2070  BEST VAL Loss: 230.4629\n",
            "\n",
            "Epoch 1308: Validation loss did not decrease\n",
            "\t Train_Loss: 93.0184 Val_Loss: 231.2109  BEST VAL Loss: 230.4629\n",
            "\n",
            "Epoch 1309: Validation loss did not decrease\n",
            "\t Train_Loss: 92.9882 Val_Loss: 231.0337  BEST VAL Loss: 230.4629\n",
            "\n",
            "Epoch 1310: Validation loss did not decrease\n",
            "\t Train_Loss: 92.9502 Val_Loss: 230.7451  BEST VAL Loss: 230.4629\n",
            "\n",
            "Epoch 1311: Validation loss decreased (230.462891 --> 230.450241).\n",
            "\t Train_Loss: 92.9121 Val_Loss: 230.4502  BEST VAL Loss: 230.4502\n",
            "\n",
            "Epoch 1312: Validation loss decreased (230.450241 --> 230.239502).\n",
            "\t Train_Loss: 92.8801 Val_Loss: 230.2395  BEST VAL Loss: 230.2395\n",
            "\n",
            "Epoch 1313: Validation loss decreased (230.239502 --> 230.171799).\n",
            "\t Train_Loss: 92.8501 Val_Loss: 230.1718  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1314: Validation loss did not decrease\n",
            "\t Train_Loss: 92.7732 Val_Loss: 239.8995  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1315: Validation loss did not decrease\n",
            "\t Train_Loss: 88.6038 Val_Loss: 322.6764  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1316: Validation loss did not decrease\n",
            "\t Train_Loss: 103.0752 Val_Loss: 322.3621  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1317: Validation loss did not decrease\n",
            "\t Train_Loss: 102.8460 Val_Loss: 320.8420  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1318: Validation loss did not decrease\n",
            "\t Train_Loss: 102.4443 Val_Loss: 319.5046  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1319: Validation loss did not decrease\n",
            "\t Train_Loss: 102.4216 Val_Loss: 318.4485  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1320: Validation loss did not decrease\n",
            "\t Train_Loss: 102.4988 Val_Loss: 317.5428  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1321: Validation loss did not decrease\n",
            "\t Train_Loss: 102.3153 Val_Loss: 316.7759  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1322: Validation loss did not decrease\n",
            "\t Train_Loss: 101.8899 Val_Loss: 316.2127  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1323: Validation loss did not decrease\n",
            "\t Train_Loss: 101.5237 Val_Loss: 315.7457  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1324: Validation loss did not decrease\n",
            "\t Train_Loss: 101.3985 Val_Loss: 314.9569  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1325: Validation loss did not decrease\n",
            "\t Train_Loss: 101.2897 Val_Loss: 313.5552  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1326: Validation loss did not decrease\n",
            "\t Train_Loss: 101.0053 Val_Loss: 311.6705  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1327: Validation loss did not decrease\n",
            "\t Train_Loss: 100.6069 Val_Loss: 309.6828  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1328: Validation loss did not decrease\n",
            "\t Train_Loss: 100.2888 Val_Loss: 307.8697  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1329: Validation loss did not decrease\n",
            "\t Train_Loss: 100.1094 Val_Loss: 306.2828  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1330: Validation loss did not decrease\n",
            "\t Train_Loss: 99.9111 Val_Loss: 304.9030  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1331: Validation loss did not decrease\n",
            "\t Train_Loss: 99.5960 Val_Loss: 303.7203  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1332: Validation loss did not decrease\n",
            "\t Train_Loss: 99.2483 Val_Loss: 302.6642  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1333: Validation loss did not decrease\n",
            "\t Train_Loss: 98.9872 Val_Loss: 301.5385  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1334: Validation loss did not decrease\n",
            "\t Train_Loss: 98.7849 Val_Loss: 300.1512  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1335: Validation loss did not decrease\n",
            "\t Train_Loss: 98.5502 Val_Loss: 298.4701  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1336: Validation loss did not decrease\n",
            "\t Train_Loss: 98.2638 Val_Loss: 296.6233  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1337: Validation loss did not decrease\n",
            "\t Train_Loss: 97.9739 Val_Loss: 294.7931  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1338: Validation loss did not decrease\n",
            "\t Train_Loss: 97.7334 Val_Loss: 293.1057  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1339: Validation loss did not decrease\n",
            "\t Train_Loss: 97.5310 Val_Loss: 291.6040  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1340: Validation loss did not decrease\n",
            "\t Train_Loss: 97.3109 Val_Loss: 290.2835  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1341: Validation loss did not decrease\n",
            "\t Train_Loss: 97.0602 Val_Loss: 289.1054  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1342: Validation loss did not decrease\n",
            "\t Train_Loss: 96.8183 Val_Loss: 287.9863  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1343: Validation loss did not decrease\n",
            "\t Train_Loss: 96.6089 Val_Loss: 286.8220  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1344: Validation loss did not decrease\n",
            "\t Train_Loss: 96.4157 Val_Loss: 285.5404  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1345: Validation loss did not decrease\n",
            "\t Train_Loss: 96.2179 Val_Loss: 284.1330  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1346: Validation loss did not decrease\n",
            "\t Train_Loss: 96.0119 Val_Loss: 282.6436  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1347: Validation loss did not decrease\n",
            "\t Train_Loss: 95.8072 Val_Loss: 281.1425  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1348: Validation loss did not decrease\n",
            "\t Train_Loss: 95.6147 Val_Loss: 279.6970  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1349: Validation loss did not decrease\n",
            "\t Train_Loss: 95.4374 Val_Loss: 278.3521  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1350: Validation loss did not decrease\n",
            "\t Train_Loss: 95.2673 Val_Loss: 277.1268  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1351: Validation loss did not decrease\n",
            "\t Train_Loss: 95.0956 Val_Loss: 276.0165  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1352: Validation loss did not decrease\n",
            "\t Train_Loss: 94.9217 Val_Loss: 275.0001  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1353: Validation loss did not decrease\n",
            "\t Train_Loss: 94.7515 Val_Loss: 274.0425  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1354: Validation loss did not decrease\n",
            "\t Train_Loss: 94.5892 Val_Loss: 273.1032  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1355: Validation loss did not decrease\n",
            "\t Train_Loss: 94.4349 Val_Loss: 272.1443  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1356: Validation loss did not decrease\n",
            "\t Train_Loss: 94.2859 Val_Loss: 271.1395  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1357: Validation loss did not decrease\n",
            "\t Train_Loss: 94.1398 Val_Loss: 270.0792  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1358: Validation loss did not decrease\n",
            "\t Train_Loss: 93.9954 Val_Loss: 268.9726  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1359: Validation loss did not decrease\n",
            "\t Train_Loss: 93.8533 Val_Loss: 267.8434  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1360: Validation loss did not decrease\n",
            "\t Train_Loss: 93.7153 Val_Loss: 266.7222  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1361: Validation loss did not decrease\n",
            "\t Train_Loss: 93.5828 Val_Loss: 265.6371  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1362: Validation loss did not decrease\n",
            "\t Train_Loss: 93.4557 Val_Loss: 264.6093  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1363: Validation loss did not decrease\n",
            "\t Train_Loss: 93.3328 Val_Loss: 263.6496  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1364: Validation loss did not decrease\n",
            "\t Train_Loss: 93.2127 Val_Loss: 262.7584  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1365: Validation loss did not decrease\n",
            "\t Train_Loss: 93.0949 Val_Loss: 261.9258  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1366: Validation loss did not decrease\n",
            "\t Train_Loss: 92.9800 Val_Loss: 261.1339  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1367: Validation loss did not decrease\n",
            "\t Train_Loss: 92.8689 Val_Loss: 260.3606  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1368: Validation loss did not decrease\n",
            "\t Train_Loss: 92.7618 Val_Loss: 259.5833  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1369: Validation loss did not decrease\n",
            "\t Train_Loss: 92.6582 Val_Loss: 258.7854  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1370: Validation loss did not decrease\n",
            "\t Train_Loss: 92.5573 Val_Loss: 257.9609  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1371: Validation loss did not decrease\n",
            "\t Train_Loss: 92.4585 Val_Loss: 257.1149  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1372: Validation loss did not decrease\n",
            "\t Train_Loss: 92.3618 Val_Loss: 256.2613  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1373: Validation loss did not decrease\n",
            "\t Train_Loss: 92.2677 Val_Loss: 255.4197  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1374: Validation loss did not decrease\n",
            "\t Train_Loss: 92.1765 Val_Loss: 254.6071  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1375: Validation loss did not decrease\n",
            "\t Train_Loss: 92.0881 Val_Loss: 253.8371  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1376: Validation loss did not decrease\n",
            "\t Train_Loss: 92.0020 Val_Loss: 253.1152  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1377: Validation loss did not decrease\n",
            "\t Train_Loss: 91.9177 Val_Loss: 252.4393  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1378: Validation loss did not decrease\n",
            "\t Train_Loss: 91.8352 Val_Loss: 251.8006  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1379: Validation loss did not decrease\n",
            "\t Train_Loss: 91.7547 Val_Loss: 251.1857  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1380: Validation loss did not decrease\n",
            "\t Train_Loss: 91.6763 Val_Loss: 250.5793  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1381: Validation loss did not decrease\n",
            "\t Train_Loss: 91.6002 Val_Loss: 249.9687  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1382: Validation loss did not decrease\n",
            "\t Train_Loss: 91.5258 Val_Loss: 249.3469  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1383: Validation loss did not decrease\n",
            "\t Train_Loss: 91.4531 Val_Loss: 248.7139  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1384: Validation loss did not decrease\n",
            "\t Train_Loss: 91.3819 Val_Loss: 248.0763  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1385: Validation loss did not decrease\n",
            "\t Train_Loss: 91.3123 Val_Loss: 247.4448  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1386: Validation loss did not decrease\n",
            "\t Train_Loss: 91.2443 Val_Loss: 246.8312  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1387: Validation loss did not decrease\n",
            "\t Train_Loss: 91.1781 Val_Loss: 246.2444  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1388: Validation loss did not decrease\n",
            "\t Train_Loss: 91.1133 Val_Loss: 245.6892  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1389: Validation loss did not decrease\n",
            "\t Train_Loss: 91.0499 Val_Loss: 245.1652  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1390: Validation loss did not decrease\n",
            "\t Train_Loss: 90.9876 Val_Loss: 244.6675  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1391: Validation loss did not decrease\n",
            "\t Train_Loss: 90.9267 Val_Loss: 244.1873  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1392: Validation loss did not decrease\n",
            "\t Train_Loss: 90.8671 Val_Loss: 243.7156  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1393: Validation loss did not decrease\n",
            "\t Train_Loss: 90.8088 Val_Loss: 243.2439  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1394: Validation loss did not decrease\n",
            "\t Train_Loss: 90.7516 Val_Loss: 242.7679  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1395: Validation loss did not decrease\n",
            "\t Train_Loss: 90.6954 Val_Loss: 242.2875  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1396: Validation loss did not decrease\n",
            "\t Train_Loss: 90.6403 Val_Loss: 241.8063  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1397: Validation loss did not decrease\n",
            "\t Train_Loss: 90.5862 Val_Loss: 241.3306  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1398: Validation loss did not decrease\n",
            "\t Train_Loss: 90.5330 Val_Loss: 240.8678  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1399: Validation loss did not decrease\n",
            "\t Train_Loss: 90.4809 Val_Loss: 240.4226  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1400: Validation loss did not decrease\n",
            "\t Train_Loss: 90.4296 Val_Loss: 239.9982  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1401: Validation loss did not decrease\n",
            "\t Train_Loss: 90.3792 Val_Loss: 239.5939  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1402: Validation loss did not decrease\n",
            "\t Train_Loss: 90.3294 Val_Loss: 239.2063  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1403: Validation loss did not decrease\n",
            "\t Train_Loss: 90.2805 Val_Loss: 238.8297  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1404: Validation loss did not decrease\n",
            "\t Train_Loss: 90.2322 Val_Loss: 238.4585  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1405: Validation loss did not decrease\n",
            "\t Train_Loss: 90.1844 Val_Loss: 238.0981  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1406: Validation loss did not decrease\n",
            "\t Train_Loss: 90.0561 Val_Loss: 243.6708  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1407: Validation loss did not decrease\n",
            "\t Train_Loss: 83.4781 Val_Loss: 333.6543  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1408: Validation loss did not decrease\n",
            "\t Train_Loss: 103.4782 Val_Loss: 333.3827  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1409: Validation loss did not decrease\n",
            "\t Train_Loss: 103.2715 Val_Loss: 331.7703  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1410: Validation loss did not decrease\n",
            "\t Train_Loss: 102.9677 Val_Loss: 330.3227  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1411: Validation loss did not decrease\n",
            "\t Train_Loss: 102.8727 Val_Loss: 330.6981  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1412: Validation loss did not decrease\n",
            "\t Train_Loss: 101.3973 Val_Loss: 450.5456  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1413: Validation loss did not decrease\n",
            "\t Train_Loss: 136.3219 Val_Loss: 334.1579  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1414: Validation loss did not decrease\n",
            "\t Train_Loss: 100.0786 Val_Loss: 325.0233  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1415: Validation loss did not decrease\n",
            "\t Train_Loss: 101.3210 Val_Loss: 323.7063  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1416: Validation loss did not decrease\n",
            "\t Train_Loss: 101.1581 Val_Loss: 321.8377  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1417: Validation loss did not decrease\n",
            "\t Train_Loss: 100.7687 Val_Loss: 319.4059  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1418: Validation loss did not decrease\n",
            "\t Train_Loss: 100.1940 Val_Loss: 316.8531  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1419: Validation loss did not decrease\n",
            "\t Train_Loss: 99.7096 Val_Loss: 314.4985  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1420: Validation loss did not decrease\n",
            "\t Train_Loss: 99.4035 Val_Loss: 312.3794  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1421: Validation loss did not decrease\n",
            "\t Train_Loss: 99.0854 Val_Loss: 310.4738  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1422: Validation loss did not decrease\n",
            "\t Train_Loss: 98.6366 Val_Loss: 308.7906  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1423: Validation loss did not decrease\n",
            "\t Train_Loss: 98.1524 Val_Loss: 307.2738  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1424: Validation loss did not decrease\n",
            "\t Train_Loss: 97.7883 Val_Loss: 305.6775  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1425: Validation loss did not decrease\n",
            "\t Train_Loss: 97.5007 Val_Loss: 303.7480  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1426: Validation loss did not decrease\n",
            "\t Train_Loss: 97.1470 Val_Loss: 301.5059  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1427: Validation loss did not decrease\n",
            "\t Train_Loss: 96.7320 Val_Loss: 299.1983  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1428: Validation loss did not decrease\n",
            "\t Train_Loss: 96.3691 Val_Loss: 297.0435  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1429: Validation loss did not decrease\n",
            "\t Train_Loss: 96.0898 Val_Loss: 295.1184  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1430: Validation loss did not decrease\n",
            "\t Train_Loss: 95.8016 Val_Loss: 293.4226  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1431: Validation loss did not decrease\n",
            "\t Train_Loss: 95.4613 Val_Loss: 291.9167  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1432: Validation loss did not decrease\n",
            "\t Train_Loss: 95.1366 Val_Loss: 290.4801  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1433: Validation loss did not decrease\n",
            "\t Train_Loss: 94.8730 Val_Loss: 288.9411  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1434: Validation loss did not decrease\n",
            "\t Train_Loss: 94.6258 Val_Loss: 287.2086  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1435: Validation loss did not decrease\n",
            "\t Train_Loss: 94.3548 Val_Loss: 285.3350  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1436: Validation loss did not decrease\n",
            "\t Train_Loss: 94.0766 Val_Loss: 283.4563  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1437: Validation loss did not decrease\n",
            "\t Train_Loss: 93.8280 Val_Loss: 281.6948  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1438: Validation loss did not decrease\n",
            "\t Train_Loss: 93.6110 Val_Loss: 280.1082  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1439: Validation loss did not decrease\n",
            "\t Train_Loss: 93.3926 Val_Loss: 278.6980  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1440: Validation loss did not decrease\n",
            "\t Train_Loss: 93.1613 Val_Loss: 277.4235  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1441: Validation loss did not decrease\n",
            "\t Train_Loss: 92.9392 Val_Loss: 276.2079  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1442: Validation loss did not decrease\n",
            "\t Train_Loss: 92.7388 Val_Loss: 274.9692  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1443: Validation loss did not decrease\n",
            "\t Train_Loss: 92.5493 Val_Loss: 273.6602  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1444: Validation loss did not decrease\n",
            "\t Train_Loss: 92.3597 Val_Loss: 272.2822  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1445: Validation loss did not decrease\n",
            "\t Train_Loss: 92.1696 Val_Loss: 270.8712  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1446: Validation loss did not decrease\n",
            "\t Train_Loss: 91.9845 Val_Loss: 269.4773  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1447: Validation loss did not decrease\n",
            "\t Train_Loss: 91.8090 Val_Loss: 268.1465  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1448: Validation loss did not decrease\n",
            "\t Train_Loss: 91.6423 Val_Loss: 266.9082  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1449: Validation loss did not decrease\n",
            "\t Train_Loss: 91.4800 Val_Loss: 265.7736  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1450: Validation loss did not decrease\n",
            "\t Train_Loss: 91.3186 Val_Loss: 264.7377  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1451: Validation loss did not decrease\n",
            "\t Train_Loss: 91.1587 Val_Loss: 263.7816  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1452: Validation loss did not decrease\n",
            "\t Train_Loss: 91.0032 Val_Loss: 262.8761  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1453: Validation loss did not decrease\n",
            "\t Train_Loss: 90.8541 Val_Loss: 261.9844  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1454: Validation loss did not decrease\n",
            "\t Train_Loss: 90.7102 Val_Loss: 261.2131  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1455: Validation loss did not decrease\n",
            "\t Train_Loss: 89.9241 Val_Loss: 310.1221  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1456: Validation loss did not decrease\n",
            "\t Train_Loss: 95.2830 Val_Loss: 258.3905  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1457: Validation loss did not decrease\n",
            "\t Train_Loss: 89.2365 Val_Loss: 256.5563  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1458: Validation loss did not decrease\n",
            "\t Train_Loss: 90.2162 Val_Loss: 255.2872  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1459: Validation loss did not decrease\n",
            "\t Train_Loss: 90.1567 Val_Loss: 254.3489  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1460: Validation loss did not decrease\n",
            "\t Train_Loss: 90.0127 Val_Loss: 253.7180  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1461: Validation loss did not decrease\n",
            "\t Train_Loss: 89.8358 Val_Loss: 253.3431  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1462: Validation loss did not decrease\n",
            "\t Train_Loss: 89.6815 Val_Loss: 253.0983  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1463: Validation loss did not decrease\n",
            "\t Train_Loss: 89.5751 Val_Loss: 252.7867  BEST VAL Loss: 230.1718\n",
            "\n",
            "Epoch 1464: Validation loss did not decrease\n",
            "Early stopped at epoch : 1464\n"
          ]
        }
      ],
      "source": [
        "SimpleRNN_best_model, train_losses, val_losses = trainer(SimpleRNN_model, X_train, y_train, X_val, y_val, optimizer, criterion, n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "8x-goeR4Ef0o",
        "outputId": "8822ce58-d009-4dbd-f732-6d5929b0e9d3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(train_losses, label = 'Train_loss')\n",
        "plt.plot(val_losses, label = 'validation_loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-qi4WfFZQF-"
      },
      "source": [
        "---\n",
        "### 4.4 Evaluate model on validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "VCb4QQCPRrzm"
      },
      "outputs": [],
      "source": [
        "val_predict_RNN = SimpleRNN_best_model(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "kjQclrpURuOf",
        "outputId": "1dcc9b09-3f1d-4843-82c0-772953d7f319"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1400x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(df[-len(y_val):].index, y_val.cpu(), label=\"actual\", marker=\"o\")\n",
        "plt.plot(df[-len(y_val):].index, val_predict_RNN.detach().cpu(), label=\"predicted\", marker=\"o\")\n",
        "plt.title(\"Electric production IP prediction by Simple RNN model\", fontsize=25)\n",
        "plt.ylabel(\"ylabel\")\n",
        "plt.legend(title_fontsize=14, fontsize=13, fancybox=True, shadow=True, frameon=True)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utvDPP4kXEI-"
      },
      "source": [
        "---\n",
        "---\n",
        "## 5 GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkIg9lbrXiKZ"
      },
      "source": [
        "---\n",
        "### 5.1 Define single GRU cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "EDjVZTlBXIFj"
      },
      "outputs": [],
      "source": [
        "class GRUCell(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, input_length=10, hidden_size=20, bias=True):\n",
        "        super(GRUCell, self).__init__()\n",
        "        self.input_length = input_length\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bias = bias\n",
        "\n",
        "        self.w_z = nn.Parameter(torch.Tensor(hidden_size, input_length))\n",
        "        self.u_z = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.w_r = nn.Parameter(torch.Tensor(hidden_size, input_length))\n",
        "        self.u_r = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.w_c = nn.Parameter(torch.Tensor(hidden_size, input_length))\n",
        "        self.u_c = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "\n",
        "        if self.bias:\n",
        "            self.b_r = nn.Parameter(torch.Tensor(hidden_size))\n",
        "            self.b_z = nn.Parameter(torch.Tensor(hidden_size))\n",
        "            self.b_c = nn.Parameter(torch.Tensor(hidden_size))\n",
        "        else:\n",
        "            self.b_r = 0\n",
        "            self.b_z = 0\n",
        "            self.b_c = 0\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        std = 1.0 / np.sqrt(self.hidden_size)\n",
        "        for w in self.parameters():\n",
        "            w.data.uniform_(-std, std)\n",
        "\n",
        "\n",
        "    def reset_gate(self, x, h):\n",
        "        r = torch.sigmoid(x @ self.w_r.T + h @ self.u_r.T + self.b_r)\n",
        "        return r\n",
        "\n",
        "    def update_gate(self, x, h):\n",
        "        z = torch.sigmoid(x @ self.w_z.T + h @ self.u_z.T + self.b_z)\n",
        "        return z\n",
        "\n",
        "    def output_gate(self, x, h, r):\n",
        "        n_t = torch.tanh(x @ self.w_c.T + (r * h) @ self.u_c.T + self.b_c)\n",
        "        return n_t\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        z = self.update_gate(x, h)\n",
        "        r = self.reset_gate(x, h)\n",
        "        n_t = self.output_gate(x, h, r)\n",
        "\n",
        "        h_new = (1 - z) * n_t + z * h\n",
        "\n",
        "        return h_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SlbDcQ-XuYx"
      },
      "source": [
        "---\n",
        "### 5.2 GRU model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "g6frzQdtXyWW"
      },
      "outputs": [],
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, bias, output_size):\n",
        "        super(GRU, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bias = bias\n",
        "        self.output_size = output_size\n",
        "\n",
        "        cell_list = []\n",
        "        for i in range(num_layers):\n",
        "            if i == 0:\n",
        "                cell_list.append(GRUCell(input_size, hidden_size, bias))\n",
        "            else:\n",
        "                cell_list.append(GRUCell(hidden_size, hidden_size, bias))\n",
        "        self.rnn_cell_list = nn.ModuleList(cell_list)\n",
        "        self.fc = nn.Linear(hidden_size, output_size, bias)\n",
        "    \n",
        "    def forward(self, input, hx=None):\n",
        "        batch_size, sequence, _ = input.size()\n",
        "        if hx is None:\n",
        "            hx = [torch.zeros(batch_size, self.hidden_size, dtype=input.dtype, device=input.device) for _ in range(self.num_layers)]\n",
        "        out = []\n",
        "        for s in range(sequence):\n",
        "            i = input[:,s]\n",
        "            for layer in range(self.num_layers):\n",
        "                hx[layer] = self.rnn_cell_list[layer](i, hx[layer])\n",
        "                i = hx[layer]\n",
        "            out.append(hx[-1])\n",
        "        out = torch.stack(out)\n",
        "        out = self.fc(out[-1])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIIm3f8yX9XF"
      },
      "source": [
        "---\n",
        "### 5.3 Train GRU model and plot losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TTE5topYD_j",
        "outputId": "7f5e7900-60df-40ee-98ff-f95f4e8f55c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GRU(\n",
              "  (rnn_cell_list): ModuleList(\n",
              "    (0): GRUCell()\n",
              "  )\n",
              "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "GRU_model = GRU(input_size=1, hidden_size=50, num_layers=1, bias=True, output_size=1)\n",
        "GRU_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "SdCfe-xRYKtx"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.008\n",
        "n_epochs = 2000\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(GRU_model.parameters(), lr = learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1xTqKgoYNNM",
        "outputId": "20a3e48b-3793-417d-fd60-5c6752aaa3bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Validation loss decreased (inf --> 10652.772461).\n",
            "\t Train_Loss: 8034.0952 Val_Loss: 10652.7725  BEST VAL Loss: 10652.7725\n",
            "\n",
            "Epoch 1: Validation loss decreased (10652.772461 --> 10572.939453).\n",
            "\t Train_Loss: 7953.6113 Val_Loss: 10572.9395  BEST VAL Loss: 10572.9395\n",
            "\n",
            "Epoch 2: Validation loss decreased (10572.939453 --> 10502.244141).\n",
            "\t Train_Loss: 7881.8613 Val_Loss: 10502.2441  BEST VAL Loss: 10502.2441\n",
            "\n",
            "Epoch 3: Validation loss decreased (10502.244141 --> 10433.889648).\n",
            "\t Train_Loss: 7813.9697 Val_Loss: 10433.8896  BEST VAL Loss: 10433.8896\n",
            "\n",
            "Epoch 4: Validation loss decreased (10433.889648 --> 10356.876953).\n",
            "\t Train_Loss: 7744.8477 Val_Loss: 10356.8770  BEST VAL Loss: 10356.8770\n",
            "\n",
            "Epoch 5: Validation loss decreased (10356.876953 --> 10264.852539).\n",
            "\t Train_Loss: 7668.1523 Val_Loss: 10264.8525  BEST VAL Loss: 10264.8525\n",
            "\n",
            "Epoch 6: Validation loss decreased (10264.852539 --> 10161.007812).\n",
            "\t Train_Loss: 7581.7202 Val_Loss: 10161.0078  BEST VAL Loss: 10161.0078\n",
            "\n",
            "Epoch 7: Validation loss decreased (10161.007812 --> 10053.453125).\n",
            "\t Train_Loss: 7487.2163 Val_Loss: 10053.4531  BEST VAL Loss: 10053.4531\n",
            "\n",
            "Epoch 8: Validation loss decreased (10053.453125 --> 9942.010742).\n",
            "\t Train_Loss: 7393.0913 Val_Loss: 9942.0107  BEST VAL Loss: 9942.0107\n",
            "\n",
            "Epoch 9: Validation loss decreased (9942.010742 --> 9838.303711).\n",
            "\t Train_Loss: 7305.6304 Val_Loss: 9838.3037  BEST VAL Loss: 9838.3037\n",
            "\n",
            "Epoch 10: Validation loss decreased (9838.303711 --> 9741.861328).\n",
            "\t Train_Loss: 7227.6367 Val_Loss: 9741.8613  BEST VAL Loss: 9741.8613\n",
            "\n",
            "Epoch 11: Validation loss decreased (9741.861328 --> 9660.959961).\n",
            "\t Train_Loss: 7158.4707 Val_Loss: 9660.9600  BEST VAL Loss: 9660.9600\n",
            "\n",
            "Epoch 12: Validation loss decreased (9660.959961 --> 9596.619141).\n",
            "\t Train_Loss: 7099.6670 Val_Loss: 9596.6191  BEST VAL Loss: 9596.6191\n",
            "\n",
            "Epoch 13: Validation loss decreased (9596.619141 --> 9535.125000).\n",
            "\t Train_Loss: 7046.2129 Val_Loss: 9535.1250  BEST VAL Loss: 9535.1250\n",
            "\n",
            "Epoch 14: Validation loss decreased (9535.125000 --> 9472.970703).\n",
            "\t Train_Loss: 6993.6396 Val_Loss: 9472.9707  BEST VAL Loss: 9472.9707\n",
            "\n",
            "Epoch 15: Validation loss decreased (9472.970703 --> 9409.754883).\n",
            "\t Train_Loss: 6940.2803 Val_Loss: 9409.7549  BEST VAL Loss: 9409.7549\n",
            "\n",
            "Epoch 16: Validation loss decreased (9409.754883 --> 9345.518555).\n",
            "\t Train_Loss: 6885.8550 Val_Loss: 9345.5186  BEST VAL Loss: 9345.5186\n",
            "\n",
            "Epoch 17: Validation loss decreased (9345.518555 --> 9280.879883).\n",
            "\t Train_Loss: 6830.5547 Val_Loss: 9280.8799  BEST VAL Loss: 9280.8799\n",
            "\n",
            "Epoch 18: Validation loss decreased (9280.879883 --> 9215.424805).\n",
            "\t Train_Loss: 6775.1763 Val_Loss: 9215.4248  BEST VAL Loss: 9215.4248\n",
            "\n",
            "Epoch 19: Validation loss decreased (9215.424805 --> 9148.166016).\n",
            "\t Train_Loss: 6719.8677 Val_Loss: 9148.1660  BEST VAL Loss: 9148.1660\n",
            "\n",
            "Epoch 20: Validation loss decreased (9148.166016 --> 9081.471680).\n",
            "\t Train_Loss: 6664.5293 Val_Loss: 9081.4717  BEST VAL Loss: 9081.4717\n",
            "\n",
            "Epoch 21: Validation loss decreased (9081.471680 --> 9014.726562).\n",
            "\t Train_Loss: 6609.3438 Val_Loss: 9014.7266  BEST VAL Loss: 9014.7266\n",
            "\n",
            "Epoch 22: Validation loss decreased (9014.726562 --> 8947.664062).\n",
            "\t Train_Loss: 6553.1582 Val_Loss: 8947.6641  BEST VAL Loss: 8947.6641\n",
            "\n",
            "Epoch 23: Validation loss decreased (8947.664062 --> 8881.420898).\n",
            "\t Train_Loss: 6496.1821 Val_Loss: 8881.4209  BEST VAL Loss: 8881.4209\n",
            "\n",
            "Epoch 24: Validation loss decreased (8881.420898 --> 8815.799805).\n",
            "\t Train_Loss: 6439.5601 Val_Loss: 8815.7998  BEST VAL Loss: 8815.7998\n",
            "\n",
            "Epoch 25: Validation loss decreased (8815.799805 --> 8750.333008).\n",
            "\t Train_Loss: 6383.6440 Val_Loss: 8750.3330  BEST VAL Loss: 8750.3330\n",
            "\n",
            "Epoch 26: Validation loss decreased (8750.333008 --> 8684.909180).\n",
            "\t Train_Loss: 6328.0503 Val_Loss: 8684.9092  BEST VAL Loss: 8684.9092\n",
            "\n",
            "Epoch 27: Validation loss decreased (8684.909180 --> 8619.528320).\n",
            "\t Train_Loss: 6272.5020 Val_Loss: 8619.5283  BEST VAL Loss: 8619.5283\n",
            "\n",
            "Epoch 28: Validation loss decreased (8619.528320 --> 8554.164062).\n",
            "\t Train_Loss: 6216.8301 Val_Loss: 8554.1641  BEST VAL Loss: 8554.1641\n",
            "\n",
            "Epoch 29: Validation loss decreased (8554.164062 --> 8488.673828).\n",
            "\t Train_Loss: 6160.7856 Val_Loss: 8488.6738  BEST VAL Loss: 8488.6738\n",
            "\n",
            "Epoch 30: Validation loss decreased (8488.673828 --> 8422.545898).\n",
            "\t Train_Loss: 6103.9443 Val_Loss: 8422.5459  BEST VAL Loss: 8422.5459\n",
            "\n",
            "Epoch 31: Validation loss decreased (8422.545898 --> 8354.246094).\n",
            "\t Train_Loss: 6045.8633 Val_Loss: 8354.2461  BEST VAL Loss: 8354.2461\n",
            "\n",
            "Epoch 32: Validation loss decreased (8354.246094 --> 8281.137695).\n",
            "\t Train_Loss: 5986.5864 Val_Loss: 8281.1377  BEST VAL Loss: 8281.1377\n",
            "\n",
            "Epoch 33: Validation loss decreased (8281.137695 --> 8208.835938).\n",
            "\t Train_Loss: 5927.4102 Val_Loss: 8208.8359  BEST VAL Loss: 8208.8359\n",
            "\n",
            "Epoch 34: Validation loss decreased (8208.835938 --> 8143.037598).\n",
            "\t Train_Loss: 5870.9263 Val_Loss: 8143.0376  BEST VAL Loss: 8143.0376\n",
            "\n",
            "Epoch 35: Validation loss decreased (8143.037598 --> 8077.886230).\n",
            "\t Train_Loss: 5815.8955 Val_Loss: 8077.8862  BEST VAL Loss: 8077.8862\n",
            "\n",
            "Epoch 36: Validation loss decreased (8077.886230 --> 8013.018066).\n",
            "\t Train_Loss: 5761.0684 Val_Loss: 8013.0181  BEST VAL Loss: 8013.0181\n",
            "\n",
            "Epoch 37: Validation loss decreased (8013.018066 --> 7948.240723).\n",
            "\t Train_Loss: 5706.3682 Val_Loss: 7948.2407  BEST VAL Loss: 7948.2407\n",
            "\n",
            "Epoch 38: Validation loss decreased (7948.240723 --> 7883.519043).\n",
            "\t Train_Loss: 5651.5415 Val_Loss: 7883.5190  BEST VAL Loss: 7883.5190\n",
            "\n",
            "Epoch 39: Validation loss decreased (7883.519043 --> 7818.164062).\n",
            "\t Train_Loss: 5596.1294 Val_Loss: 7818.1641  BEST VAL Loss: 7818.1641\n",
            "\n",
            "Epoch 40: Validation loss decreased (7818.164062 --> 7750.394531).\n",
            "\t Train_Loss: 5539.5474 Val_Loss: 7750.3945  BEST VAL Loss: 7750.3945\n",
            "\n",
            "Epoch 41: Validation loss decreased (7750.394531 --> 7678.773438).\n",
            "\t Train_Loss: 5481.8394 Val_Loss: 7678.7734  BEST VAL Loss: 7678.7734\n",
            "\n",
            "Epoch 42: Validation loss decreased (7678.773438 --> 7611.519043).\n",
            "\t Train_Loss: 5424.8838 Val_Loss: 7611.5190  BEST VAL Loss: 7611.5190\n",
            "\n",
            "Epoch 43: Validation loss decreased (7611.519043 --> 7547.376465).\n",
            "\t Train_Loss: 5370.5894 Val_Loss: 7547.3765  BEST VAL Loss: 7547.3765\n",
            "\n",
            "Epoch 44: Validation loss decreased (7547.376465 --> 7483.546875).\n",
            "\t Train_Loss: 5316.9858 Val_Loss: 7483.5469  BEST VAL Loss: 7483.5469\n",
            "\n",
            "Epoch 45: Validation loss decreased (7483.546875 --> 7420.003418).\n",
            "\t Train_Loss: 5263.6016 Val_Loss: 7420.0034  BEST VAL Loss: 7420.0034\n",
            "\n",
            "Epoch 46: Validation loss decreased (7420.003418 --> 7356.679688).\n",
            "\t Train_Loss: 5210.3701 Val_Loss: 7356.6797  BEST VAL Loss: 7356.6797\n",
            "\n",
            "Epoch 47: Validation loss decreased (7356.679688 --> 7293.274902).\n",
            "\t Train_Loss: 5157.1084 Val_Loss: 7293.2749  BEST VAL Loss: 7293.2749\n",
            "\n",
            "Epoch 48: Validation loss decreased (7293.274902 --> 7228.799316).\n",
            "\t Train_Loss: 5103.4458 Val_Loss: 7228.7993  BEST VAL Loss: 7228.7993\n",
            "\n",
            "Epoch 49: Validation loss decreased (7228.799316 --> 7161.938965).\n",
            "\t Train_Loss: 5049.1138 Val_Loss: 7161.9390  BEST VAL Loss: 7161.9390\n",
            "\n",
            "Epoch 50: Validation loss decreased (7161.938965 --> 7096.724121).\n",
            "\t Train_Loss: 4994.9663 Val_Loss: 7096.7241  BEST VAL Loss: 7096.7241\n",
            "\n",
            "Epoch 51: Validation loss decreased (7096.724121 --> 7034.317383).\n",
            "\t Train_Loss: 4942.5552 Val_Loss: 7034.3174  BEST VAL Loss: 7034.3174\n",
            "\n",
            "Epoch 52: Validation loss decreased (7034.317383 --> 6972.261719).\n",
            "\t Train_Loss: 4890.9102 Val_Loss: 6972.2617  BEST VAL Loss: 6972.2617\n",
            "\n",
            "Epoch 53: Validation loss decreased (6972.261719 --> 6910.529785).\n",
            "\t Train_Loss: 4839.5474 Val_Loss: 6910.5298  BEST VAL Loss: 6910.5298\n",
            "\n",
            "Epoch 54: Validation loss decreased (6910.529785 --> 6849.151855).\n",
            "\t Train_Loss: 4788.5005 Val_Loss: 6849.1519  BEST VAL Loss: 6849.1519\n",
            "\n",
            "Epoch 55: Validation loss decreased (6849.151855 --> 6788.156250).\n",
            "\t Train_Loss: 4737.7959 Val_Loss: 6788.1562  BEST VAL Loss: 6788.1562\n",
            "\n",
            "Epoch 56: Validation loss decreased (6788.156250 --> 6727.562012).\n",
            "\t Train_Loss: 4687.4546 Val_Loss: 6727.5620  BEST VAL Loss: 6727.5620\n",
            "\n",
            "Epoch 57: Validation loss decreased (6727.562012 --> 6667.381348).\n",
            "\t Train_Loss: 4637.4922 Val_Loss: 6667.3813  BEST VAL Loss: 6667.3813\n",
            "\n",
            "Epoch 58: Validation loss decreased (6667.381348 --> 6607.631348).\n",
            "\t Train_Loss: 4587.9209 Val_Loss: 6607.6313  BEST VAL Loss: 6607.6313\n",
            "\n",
            "Epoch 59: Validation loss decreased (6607.631348 --> 6548.316406).\n",
            "\t Train_Loss: 4538.7500 Val_Loss: 6548.3164  BEST VAL Loss: 6548.3164\n",
            "\n",
            "Epoch 60: Validation loss decreased (6548.316406 --> 6489.446777).\n",
            "\t Train_Loss: 4489.9873 Val_Loss: 6489.4468  BEST VAL Loss: 6489.4468\n",
            "\n",
            "Epoch 61: Validation loss decreased (6489.446777 --> 6431.026855).\n",
            "\t Train_Loss: 4441.6372 Val_Loss: 6431.0269  BEST VAL Loss: 6431.0269\n",
            "\n",
            "Epoch 62: Validation loss decreased (6431.026855 --> 6373.058594).\n",
            "\t Train_Loss: 4393.7051 Val_Loss: 6373.0586  BEST VAL Loss: 6373.0586\n",
            "\n",
            "Epoch 63: Validation loss decreased (6373.058594 --> 6315.548340).\n",
            "\t Train_Loss: 4346.1929 Val_Loss: 6315.5483  BEST VAL Loss: 6315.5483\n",
            "\n",
            "Epoch 64: Validation loss decreased (6315.548340 --> 6258.495605).\n",
            "\t Train_Loss: 4299.1025 Val_Loss: 6258.4956  BEST VAL Loss: 6258.4956\n",
            "\n",
            "Epoch 65: Validation loss decreased (6258.495605 --> 6201.902832).\n",
            "\t Train_Loss: 4252.4351 Val_Loss: 6201.9028  BEST VAL Loss: 6201.9028\n",
            "\n",
            "Epoch 66: Validation loss decreased (6201.902832 --> 6145.768555).\n",
            "\t Train_Loss: 4206.1909 Val_Loss: 6145.7686  BEST VAL Loss: 6145.7686\n",
            "\n",
            "Epoch 67: Validation loss decreased (6145.768555 --> 6090.093262).\n",
            "\t Train_Loss: 4160.3691 Val_Loss: 6090.0933  BEST VAL Loss: 6090.0933\n",
            "\n",
            "Epoch 68: Validation loss decreased (6090.093262 --> 6034.875977).\n",
            "\t Train_Loss: 4114.9697 Val_Loss: 6034.8760  BEST VAL Loss: 6034.8760\n",
            "\n",
            "Epoch 69: Validation loss decreased (6034.875977 --> 5980.116699).\n",
            "\t Train_Loss: 4069.9915 Val_Loss: 5980.1167  BEST VAL Loss: 5980.1167\n",
            "\n",
            "Epoch 70: Validation loss decreased (5980.116699 --> 5925.811035).\n",
            "\t Train_Loss: 4025.4319 Val_Loss: 5925.8110  BEST VAL Loss: 5925.8110\n",
            "\n",
            "Epoch 71: Validation loss decreased (5925.811035 --> 5871.958496).\n",
            "\t Train_Loss: 3981.2900 Val_Loss: 5871.9585  BEST VAL Loss: 5871.9585\n",
            "\n",
            "Epoch 72: Validation loss decreased (5871.958496 --> 5818.557617).\n",
            "\t Train_Loss: 3937.5635 Val_Loss: 5818.5576  BEST VAL Loss: 5818.5576\n",
            "\n",
            "Epoch 73: Validation loss decreased (5818.557617 --> 5765.605957).\n",
            "\t Train_Loss: 3894.2498 Val_Loss: 5765.6060  BEST VAL Loss: 5765.6060\n",
            "\n",
            "Epoch 74: Validation loss decreased (5765.605957 --> 5713.099609).\n",
            "\t Train_Loss: 3851.3474 Val_Loss: 5713.0996  BEST VAL Loss: 5713.0996\n",
            "\n",
            "Epoch 75: Validation loss decreased (5713.099609 --> 5661.037598).\n",
            "\t Train_Loss: 3808.8521 Val_Loss: 5661.0376  BEST VAL Loss: 5661.0376\n",
            "\n",
            "Epoch 76: Validation loss decreased (5661.037598 --> 5609.416504).\n",
            "\t Train_Loss: 3766.7625 Val_Loss: 5609.4165  BEST VAL Loss: 5609.4165\n",
            "\n",
            "Epoch 77: Validation loss decreased (5609.416504 --> 5558.231445).\n",
            "\t Train_Loss: 3725.0754 Val_Loss: 5558.2314  BEST VAL Loss: 5558.2314\n",
            "\n",
            "Epoch 78: Validation loss decreased (5558.231445 --> 5507.482422).\n",
            "\t Train_Loss: 3683.7881 Val_Loss: 5507.4824  BEST VAL Loss: 5507.4824\n",
            "\n",
            "Epoch 79: Validation loss decreased (5507.482422 --> 5457.164062).\n",
            "\t Train_Loss: 3642.8970 Val_Loss: 5457.1641  BEST VAL Loss: 5457.1641\n",
            "\n",
            "Epoch 80: Validation loss decreased (5457.164062 --> 5407.275391).\n",
            "\t Train_Loss: 3602.3997 Val_Loss: 5407.2754  BEST VAL Loss: 5407.2754\n",
            "\n",
            "Epoch 81: Validation loss decreased (5407.275391 --> 5357.811523).\n",
            "\t Train_Loss: 3562.2930 Val_Loss: 5357.8115  BEST VAL Loss: 5357.8115\n",
            "\n",
            "Epoch 82: Validation loss decreased (5357.811523 --> 5308.770508).\n",
            "\t Train_Loss: 3522.5735 Val_Loss: 5308.7705  BEST VAL Loss: 5308.7705\n",
            "\n",
            "Epoch 83: Validation loss decreased (5308.770508 --> 5260.146973).\n",
            "\t Train_Loss: 3483.2388 Val_Loss: 5260.1470  BEST VAL Loss: 5260.1470\n",
            "\n",
            "Epoch 84: Validation loss decreased (5260.146973 --> 5211.941406).\n",
            "\t Train_Loss: 3444.2852 Val_Loss: 5211.9414  BEST VAL Loss: 5211.9414\n",
            "\n",
            "Epoch 85: Validation loss decreased (5211.941406 --> 5164.147949).\n",
            "\t Train_Loss: 3405.7104 Val_Loss: 5164.1479  BEST VAL Loss: 5164.1479\n",
            "\n",
            "Epoch 86: Validation loss decreased (5164.147949 --> 5116.763184).\n",
            "\t Train_Loss: 3367.5105 Val_Loss: 5116.7632  BEST VAL Loss: 5116.7632\n",
            "\n",
            "Epoch 87: Validation loss decreased (5116.763184 --> 5069.785645).\n",
            "\t Train_Loss: 3329.6829 Val_Loss: 5069.7856  BEST VAL Loss: 5069.7856\n",
            "\n",
            "Epoch 88: Validation loss decreased (5069.785645 --> 5023.210938).\n",
            "\t Train_Loss: 3292.2239 Val_Loss: 5023.2109  BEST VAL Loss: 5023.2109\n",
            "\n",
            "Epoch 89: Validation loss decreased (5023.210938 --> 4977.036133).\n",
            "\t Train_Loss: 3255.1311 Val_Loss: 4977.0361  BEST VAL Loss: 4977.0361\n",
            "\n",
            "Epoch 90: Validation loss decreased (4977.036133 --> 4931.258301).\n",
            "\t Train_Loss: 3218.4019 Val_Loss: 4931.2583  BEST VAL Loss: 4931.2583\n",
            "\n",
            "Epoch 91: Validation loss decreased (4931.258301 --> 4885.873535).\n",
            "\t Train_Loss: 3182.0317 Val_Loss: 4885.8735  BEST VAL Loss: 4885.8735\n",
            "\n",
            "Epoch 92: Validation loss decreased (4885.873535 --> 4840.880371).\n",
            "\t Train_Loss: 3146.0188 Val_Loss: 4840.8804  BEST VAL Loss: 4840.8804\n",
            "\n",
            "Epoch 93: Validation loss decreased (4840.880371 --> 4796.274902).\n",
            "\t Train_Loss: 3110.3601 Val_Loss: 4796.2749  BEST VAL Loss: 4796.2749\n",
            "\n",
            "Epoch 94: Validation loss decreased (4796.274902 --> 4752.054199).\n",
            "\t Train_Loss: 3075.0532 Val_Loss: 4752.0542  BEST VAL Loss: 4752.0542\n",
            "\n",
            "Epoch 95: Validation loss decreased (4752.054199 --> 4708.214844).\n",
            "\t Train_Loss: 3040.0938 Val_Loss: 4708.2148  BEST VAL Loss: 4708.2148\n",
            "\n",
            "Epoch 96: Validation loss decreased (4708.214844 --> 4664.754395).\n",
            "\t Train_Loss: 3005.4797 Val_Loss: 4664.7544  BEST VAL Loss: 4664.7544\n",
            "\n",
            "Epoch 97: Validation loss decreased (4664.754395 --> 4621.669434).\n",
            "\t Train_Loss: 2971.2080 Val_Loss: 4621.6694  BEST VAL Loss: 4621.6694\n",
            "\n",
            "Epoch 98: Validation loss decreased (4621.669434 --> 4578.958008).\n",
            "\t Train_Loss: 2937.2754 Val_Loss: 4578.9580  BEST VAL Loss: 4578.9580\n",
            "\n",
            "Epoch 99: Validation loss decreased (4578.958008 --> 4536.615234).\n",
            "\t Train_Loss: 2903.6807 Val_Loss: 4536.6152  BEST VAL Loss: 4536.6152\n",
            "\n",
            "Epoch 100: Validation loss decreased (4536.615234 --> 4494.640137).\n",
            "\t Train_Loss: 2870.4194 Val_Loss: 4494.6401  BEST VAL Loss: 4494.6401\n",
            "\n",
            "Epoch 101: Validation loss decreased (4494.640137 --> 4453.029785).\n",
            "\t Train_Loss: 2837.4895 Val_Loss: 4453.0298  BEST VAL Loss: 4453.0298\n",
            "\n",
            "Epoch 102: Validation loss decreased (4453.029785 --> 4411.780762).\n",
            "\t Train_Loss: 2804.8879 Val_Loss: 4411.7808  BEST VAL Loss: 4411.7808\n",
            "\n",
            "Epoch 103: Validation loss decreased (4411.780762 --> 4370.889648).\n",
            "\t Train_Loss: 2772.6125 Val_Loss: 4370.8896  BEST VAL Loss: 4370.8896\n",
            "\n",
            "Epoch 104: Validation loss decreased (4370.889648 --> 4330.355469).\n",
            "\t Train_Loss: 2740.6597 Val_Loss: 4330.3555  BEST VAL Loss: 4330.3555\n",
            "\n",
            "Epoch 105: Validation loss decreased (4330.355469 --> 4290.173828).\n",
            "\t Train_Loss: 2709.0278 Val_Loss: 4290.1738  BEST VAL Loss: 4290.1738\n",
            "\n",
            "Epoch 106: Validation loss decreased (4290.173828 --> 4250.343262).\n",
            "\t Train_Loss: 2677.7134 Val_Loss: 4250.3433  BEST VAL Loss: 4250.3433\n",
            "\n",
            "Epoch 107: Validation loss decreased (4250.343262 --> 4210.859863).\n",
            "\t Train_Loss: 2646.7151 Val_Loss: 4210.8599  BEST VAL Loss: 4210.8599\n",
            "\n",
            "Epoch 108: Validation loss decreased (4210.859863 --> 4171.722168).\n",
            "\t Train_Loss: 2616.0286 Val_Loss: 4171.7222  BEST VAL Loss: 4171.7222\n",
            "\n",
            "Epoch 109: Validation loss decreased (4171.722168 --> 4132.927246).\n",
            "\t Train_Loss: 2585.6531 Val_Loss: 4132.9272  BEST VAL Loss: 4132.9272\n",
            "\n",
            "Epoch 110: Validation loss decreased (4132.927246 --> 4094.472412).\n",
            "\t Train_Loss: 2555.5845 Val_Loss: 4094.4724  BEST VAL Loss: 4094.4724\n",
            "\n",
            "Epoch 111: Validation loss decreased (4094.472412 --> 4056.355469).\n",
            "\t Train_Loss: 2525.8210 Val_Loss: 4056.3555  BEST VAL Loss: 4056.3555\n",
            "\n",
            "Epoch 112: Validation loss decreased (4056.355469 --> 4018.573486).\n",
            "\t Train_Loss: 2496.3606 Val_Loss: 4018.5735  BEST VAL Loss: 4018.5735\n",
            "\n",
            "Epoch 113: Validation loss decreased (4018.573486 --> 3981.123535).\n",
            "\t Train_Loss: 2467.2002 Val_Loss: 3981.1235  BEST VAL Loss: 3981.1235\n",
            "\n",
            "Epoch 114: Validation loss decreased (3981.123535 --> 3944.002686).\n",
            "\t Train_Loss: 2438.3374 Val_Loss: 3944.0027  BEST VAL Loss: 3944.0027\n",
            "\n",
            "Epoch 115: Validation loss decreased (3944.002686 --> 3907.210938).\n",
            "\t Train_Loss: 2409.7693 Val_Loss: 3907.2109  BEST VAL Loss: 3907.2109\n",
            "\n",
            "Epoch 116: Validation loss decreased (3907.210938 --> 3870.743896).\n",
            "\t Train_Loss: 2381.4951 Val_Loss: 3870.7439  BEST VAL Loss: 3870.7439\n",
            "\n",
            "Epoch 117: Validation loss decreased (3870.743896 --> 3834.599365).\n",
            "\t Train_Loss: 2353.5110 Val_Loss: 3834.5994  BEST VAL Loss: 3834.5994\n",
            "\n",
            "Epoch 118: Validation loss decreased (3834.599365 --> 3798.774170).\n",
            "\t Train_Loss: 2325.8149 Val_Loss: 3798.7742  BEST VAL Loss: 3798.7742\n",
            "\n",
            "Epoch 119: Validation loss decreased (3798.774170 --> 3763.268066).\n",
            "\t Train_Loss: 2298.4045 Val_Loss: 3763.2681  BEST VAL Loss: 3763.2681\n",
            "\n",
            "Epoch 120: Validation loss decreased (3763.268066 --> 3728.077393).\n",
            "\t Train_Loss: 2271.2773 Val_Loss: 3728.0774  BEST VAL Loss: 3728.0774\n",
            "\n",
            "Epoch 121: Validation loss decreased (3728.077393 --> 3693.199219).\n",
            "\t Train_Loss: 2244.4319 Val_Loss: 3693.1992  BEST VAL Loss: 3693.1992\n",
            "\n",
            "Epoch 122: Validation loss decreased (3693.199219 --> 3658.632812).\n",
            "\t Train_Loss: 2217.8645 Val_Loss: 3658.6328  BEST VAL Loss: 3658.6328\n",
            "\n",
            "Epoch 123: Validation loss decreased (3658.632812 --> 3624.374268).\n",
            "\t Train_Loss: 2191.5742 Val_Loss: 3624.3743  BEST VAL Loss: 3624.3743\n",
            "\n",
            "Epoch 124: Validation loss decreased (3624.374268 --> 3590.421875).\n",
            "\t Train_Loss: 2165.5576 Val_Loss: 3590.4219  BEST VAL Loss: 3590.4219\n",
            "\n",
            "Epoch 125: Validation loss decreased (3590.421875 --> 3556.773926).\n",
            "\t Train_Loss: 2139.8130 Val_Loss: 3556.7739  BEST VAL Loss: 3556.7739\n",
            "\n",
            "Epoch 126: Validation loss decreased (3556.773926 --> 3523.427734).\n",
            "\t Train_Loss: 2114.3379 Val_Loss: 3523.4277  BEST VAL Loss: 3523.4277\n",
            "\n",
            "Epoch 127: Validation loss decreased (3523.427734 --> 3490.380859).\n",
            "\t Train_Loss: 2089.1304 Val_Loss: 3490.3809  BEST VAL Loss: 3490.3809\n",
            "\n",
            "Epoch 128: Validation loss decreased (3490.380859 --> 3457.631348).\n",
            "\t Train_Loss: 2064.1875 Val_Loss: 3457.6313  BEST VAL Loss: 3457.6313\n",
            "\n",
            "Epoch 129: Validation loss decreased (3457.631348 --> 3425.176514).\n",
            "\t Train_Loss: 2039.5067 Val_Loss: 3425.1765  BEST VAL Loss: 3425.1765\n",
            "\n",
            "Epoch 130: Validation loss decreased (3425.176514 --> 3393.014404).\n",
            "\t Train_Loss: 2015.0857 Val_Loss: 3393.0144  BEST VAL Loss: 3393.0144\n",
            "\n",
            "Epoch 131: Validation loss decreased (3393.014404 --> 3361.143066).\n",
            "\t Train_Loss: 1990.9221 Val_Loss: 3361.1431  BEST VAL Loss: 3361.1431\n",
            "\n",
            "Epoch 132: Validation loss decreased (3361.143066 --> 3329.559326).\n",
            "\t Train_Loss: 1967.0126 Val_Loss: 3329.5593  BEST VAL Loss: 3329.5593\n",
            "\n",
            "Epoch 133: Validation loss decreased (3329.559326 --> 3298.262451).\n",
            "\t Train_Loss: 1943.3544 Val_Loss: 3298.2625  BEST VAL Loss: 3298.2625\n",
            "\n",
            "Epoch 134: Validation loss decreased (3298.262451 --> 3267.248779).\n",
            "\t Train_Loss: 1919.9435 Val_Loss: 3267.2488  BEST VAL Loss: 3267.2488\n",
            "\n",
            "Epoch 135: Validation loss decreased (3267.248779 --> 3236.515137).\n",
            "\t Train_Loss: 1896.7749 Val_Loss: 3236.5151  BEST VAL Loss: 3236.5151\n",
            "\n",
            "Epoch 136: Validation loss decreased (3236.515137 --> 3206.059082).\n",
            "\t Train_Loss: 1873.8429 Val_Loss: 3206.0591  BEST VAL Loss: 3206.0591\n",
            "\n",
            "Epoch 137: Validation loss decreased (3206.059082 --> 3175.875732).\n",
            "\t Train_Loss: 1851.1381 Val_Loss: 3175.8757  BEST VAL Loss: 3175.8757\n",
            "\n",
            "Epoch 138: Validation loss decreased (3175.875732 --> 3145.958740).\n",
            "\t Train_Loss: 1828.6471 Val_Loss: 3145.9587  BEST VAL Loss: 3145.9587\n",
            "\n",
            "Epoch 139: Validation loss decreased (3145.958740 --> 3116.295654).\n",
            "\t Train_Loss: 1806.3481 Val_Loss: 3116.2957  BEST VAL Loss: 3116.2957\n",
            "\n",
            "Epoch 140: Validation loss decreased (3116.295654 --> 3086.861084).\n",
            "\t Train_Loss: 1784.2037 Val_Loss: 3086.8611  BEST VAL Loss: 3086.8611\n",
            "\n",
            "Epoch 141: Validation loss decreased (3086.861084 --> 3057.583740).\n",
            "\t Train_Loss: 1762.1489 Val_Loss: 3057.5837  BEST VAL Loss: 3057.5837\n",
            "\n",
            "Epoch 142: Validation loss decreased (3057.583740 --> 3028.250732).\n",
            "\t Train_Loss: 1740.0688 Val_Loss: 3028.2507  BEST VAL Loss: 3028.2507\n",
            "\n",
            "Epoch 143: Validation loss decreased (3028.250732 --> 2998.194824).\n",
            "\t Train_Loss: 1717.7874 Val_Loss: 2998.1948  BEST VAL Loss: 2998.1948\n",
            "\n",
            "Epoch 144: Validation loss decreased (2998.194824 --> 2965.982178).\n",
            "\t Train_Loss: 1695.1143 Val_Loss: 2965.9822  BEST VAL Loss: 2965.9822\n",
            "\n",
            "Epoch 145: Validation loss decreased (2965.982178 --> 2933.383301).\n",
            "\t Train_Loss: 1672.2172 Val_Loss: 2933.3833  BEST VAL Loss: 2933.3833\n",
            "\n",
            "Epoch 146: Validation loss decreased (2933.383301 --> 2903.734131).\n",
            "\t Train_Loss: 1650.1469 Val_Loss: 2903.7341  BEST VAL Loss: 2903.7341\n",
            "\n",
            "Epoch 147: Validation loss decreased (2903.734131 --> 2874.564697).\n",
            "\t Train_Loss: 1628.7157 Val_Loss: 2874.5647  BEST VAL Loss: 2874.5647\n",
            "\n",
            "Epoch 148: Validation loss decreased (2874.564697 --> 2845.584473).\n",
            "\t Train_Loss: 1607.4576 Val_Loss: 2845.5845  BEST VAL Loss: 2845.5845\n",
            "\n",
            "Epoch 149: Validation loss decreased (2845.584473 --> 2816.814209).\n",
            "\t Train_Loss: 1586.3781 Val_Loss: 2816.8142  BEST VAL Loss: 2816.8142\n",
            "\n",
            "Epoch 150: Validation loss decreased (2816.814209 --> 2788.272705).\n",
            "\t Train_Loss: 1565.4927 Val_Loss: 2788.2727  BEST VAL Loss: 2788.2727\n",
            "\n",
            "Epoch 151: Validation loss decreased (2788.272705 --> 2759.972900).\n",
            "\t Train_Loss: 1544.8147 Val_Loss: 2759.9729  BEST VAL Loss: 2759.9729\n",
            "\n",
            "Epoch 152: Validation loss decreased (2759.972900 --> 2731.923828).\n",
            "\t Train_Loss: 1524.3533 Val_Loss: 2731.9238  BEST VAL Loss: 2731.9238\n",
            "\n",
            "Epoch 153: Validation loss decreased (2731.923828 --> 2704.133789).\n",
            "\t Train_Loss: 1504.1144 Val_Loss: 2704.1338  BEST VAL Loss: 2704.1338\n",
            "\n",
            "Epoch 154: Validation loss decreased (2704.133789 --> 2676.607666).\n",
            "\t Train_Loss: 1484.1038 Val_Loss: 2676.6077  BEST VAL Loss: 2676.6077\n",
            "\n",
            "Epoch 155: Validation loss decreased (2676.607666 --> 2649.350830).\n",
            "\t Train_Loss: 1464.3240 Val_Loss: 2649.3508  BEST VAL Loss: 2649.3508\n",
            "\n",
            "Epoch 156: Validation loss decreased (2649.350830 --> 2622.364746).\n",
            "\t Train_Loss: 1444.7788 Val_Loss: 2622.3647  BEST VAL Loss: 2622.3647\n",
            "\n",
            "Epoch 157: Validation loss decreased (2622.364746 --> 2595.651611).\n",
            "\t Train_Loss: 1425.4685 Val_Loss: 2595.6516  BEST VAL Loss: 2595.6516\n",
            "\n",
            "Epoch 158: Validation loss decreased (2595.651611 --> 2569.213623).\n",
            "\t Train_Loss: 1406.3942 Val_Loss: 2569.2136  BEST VAL Loss: 2569.2136\n",
            "\n",
            "Epoch 159: Validation loss decreased (2569.213623 --> 2543.050537).\n",
            "\t Train_Loss: 1387.5569 Val_Loss: 2543.0505  BEST VAL Loss: 2543.0505\n",
            "\n",
            "Epoch 160: Validation loss decreased (2543.050537 --> 2517.161865).\n",
            "\t Train_Loss: 1368.9553 Val_Loss: 2517.1619  BEST VAL Loss: 2517.1619\n",
            "\n",
            "Epoch 161: Validation loss decreased (2517.161865 --> 2491.548096).\n",
            "\t Train_Loss: 1350.5891 Val_Loss: 2491.5481  BEST VAL Loss: 2491.5481\n",
            "\n",
            "Epoch 162: Validation loss decreased (2491.548096 --> 2466.207520).\n",
            "\t Train_Loss: 1332.4578 Val_Loss: 2466.2075  BEST VAL Loss: 2466.2075\n",
            "\n",
            "Epoch 163: Validation loss decreased (2466.207520 --> 2441.139404).\n",
            "\t Train_Loss: 1314.5593 Val_Loss: 2441.1394  BEST VAL Loss: 2441.1394\n",
            "\n",
            "Epoch 164: Validation loss decreased (2441.139404 --> 2416.341553).\n",
            "\t Train_Loss: 1296.8923 Val_Loss: 2416.3416  BEST VAL Loss: 2416.3416\n",
            "\n",
            "Epoch 165: Validation loss decreased (2416.341553 --> 2391.812988).\n",
            "\t Train_Loss: 1279.4553 Val_Loss: 2391.8130  BEST VAL Loss: 2391.8130\n",
            "\n",
            "Epoch 166: Validation loss decreased (2391.812988 --> 2367.551514).\n",
            "\t Train_Loss: 1262.2471 Val_Loss: 2367.5515  BEST VAL Loss: 2367.5515\n",
            "\n",
            "Epoch 167: Validation loss decreased (2367.551514 --> 2343.556152).\n",
            "\t Train_Loss: 1245.2643 Val_Loss: 2343.5562  BEST VAL Loss: 2343.5562\n",
            "\n",
            "Epoch 168: Validation loss decreased (2343.556152 --> 2319.822510).\n",
            "\t Train_Loss: 1228.5061 Val_Loss: 2319.8225  BEST VAL Loss: 2319.8225\n",
            "\n",
            "Epoch 169: Validation loss decreased (2319.822510 --> 2296.351074).\n",
            "\t Train_Loss: 1211.9694 Val_Loss: 2296.3511  BEST VAL Loss: 2296.3511\n",
            "\n",
            "Epoch 170: Validation loss decreased (2296.351074 --> 2273.137939).\n",
            "\t Train_Loss: 1195.6531 Val_Loss: 2273.1379  BEST VAL Loss: 2273.1379\n",
            "\n",
            "Epoch 171: Validation loss decreased (2273.137939 --> 2250.180908).\n",
            "\t Train_Loss: 1179.5541 Val_Loss: 2250.1809  BEST VAL Loss: 2250.1809\n",
            "\n",
            "Epoch 172: Validation loss decreased (2250.180908 --> 2227.477295).\n",
            "\t Train_Loss: 1163.6703 Val_Loss: 2227.4773  BEST VAL Loss: 2227.4773\n",
            "\n",
            "Epoch 173: Validation loss decreased (2227.477295 --> 2205.025879).\n",
            "\t Train_Loss: 1147.9994 Val_Loss: 2205.0259  BEST VAL Loss: 2205.0259\n",
            "\n",
            "Epoch 174: Validation loss decreased (2205.025879 --> 2182.823486).\n",
            "\t Train_Loss: 1132.5392 Val_Loss: 2182.8235  BEST VAL Loss: 2182.8235\n",
            "\n",
            "Epoch 175: Validation loss decreased (2182.823486 --> 2160.867188).\n",
            "\t Train_Loss: 1117.2874 Val_Loss: 2160.8672  BEST VAL Loss: 2160.8672\n",
            "\n",
            "Epoch 176: Validation loss decreased (2160.867188 --> 2139.154297).\n",
            "\t Train_Loss: 1102.2413 Val_Loss: 2139.1543  BEST VAL Loss: 2139.1543\n",
            "\n",
            "Epoch 177: Validation loss decreased (2139.154297 --> 2117.683350).\n",
            "\t Train_Loss: 1087.3984 Val_Loss: 2117.6833  BEST VAL Loss: 2117.6833\n",
            "\n",
            "Epoch 178: Validation loss decreased (2117.683350 --> 2096.451172).\n",
            "\t Train_Loss: 1072.7571 Val_Loss: 2096.4512  BEST VAL Loss: 2096.4512\n",
            "\n",
            "Epoch 179: Validation loss decreased (2096.451172 --> 2075.455078).\n",
            "\t Train_Loss: 1058.3143 Val_Loss: 2075.4551  BEST VAL Loss: 2075.4551\n",
            "\n",
            "Epoch 180: Validation loss decreased (2075.455078 --> 2054.693359).\n",
            "\t Train_Loss: 1044.0681 Val_Loss: 2054.6934  BEST VAL Loss: 2054.6934\n",
            "\n",
            "Epoch 181: Validation loss decreased (2054.693359 --> 2034.162964).\n",
            "\t Train_Loss: 1030.0161 Val_Loss: 2034.1630  BEST VAL Loss: 2034.1630\n",
            "\n",
            "Epoch 182: Validation loss decreased (2034.162964 --> 2013.861694).\n",
            "\t Train_Loss: 1016.1558 Val_Loss: 2013.8617  BEST VAL Loss: 2013.8617\n",
            "\n",
            "Epoch 183: Validation loss decreased (2013.861694 --> 1993.786987).\n",
            "\t Train_Loss: 1002.4853 Val_Loss: 1993.7870  BEST VAL Loss: 1993.7870\n",
            "\n",
            "Epoch 184: Validation loss decreased (1993.786987 --> 1973.935913).\n",
            "\t Train_Loss: 989.0020 Val_Loss: 1973.9359  BEST VAL Loss: 1973.9359\n",
            "\n",
            "Epoch 185: Validation loss decreased (1973.935913 --> 1954.307007).\n",
            "\t Train_Loss: 975.7036 Val_Loss: 1954.3070  BEST VAL Loss: 1954.3070\n",
            "\n",
            "Epoch 186: Validation loss decreased (1954.307007 --> 1934.897095).\n",
            "\t Train_Loss: 962.5880 Val_Loss: 1934.8971  BEST VAL Loss: 1934.8971\n",
            "\n",
            "Epoch 187: Validation loss decreased (1934.897095 --> 1915.704468).\n",
            "\t Train_Loss: 949.6529 Val_Loss: 1915.7045  BEST VAL Loss: 1915.7045\n",
            "\n",
            "Epoch 188: Validation loss decreased (1915.704468 --> 1896.726196).\n",
            "\t Train_Loss: 936.8966 Val_Loss: 1896.7262  BEST VAL Loss: 1896.7262\n",
            "\n",
            "Epoch 189: Validation loss decreased (1896.726196 --> 1877.960938).\n",
            "\t Train_Loss: 924.3161 Val_Loss: 1877.9609  BEST VAL Loss: 1877.9609\n",
            "\n",
            "Epoch 190: Validation loss decreased (1877.960938 --> 1859.405151).\n",
            "\t Train_Loss: 911.9102 Val_Loss: 1859.4052  BEST VAL Loss: 1859.4052\n",
            "\n",
            "Epoch 191: Validation loss decreased (1859.405151 --> 1841.057251).\n",
            "\t Train_Loss: 899.6760 Val_Loss: 1841.0573  BEST VAL Loss: 1841.0573\n",
            "\n",
            "Epoch 192: Validation loss decreased (1841.057251 --> 1822.914917).\n",
            "\t Train_Loss: 887.6119 Val_Loss: 1822.9149  BEST VAL Loss: 1822.9149\n",
            "\n",
            "Epoch 193: Validation loss decreased (1822.914917 --> 1804.975952).\n",
            "\t Train_Loss: 875.7155 Val_Loss: 1804.9760  BEST VAL Loss: 1804.9760\n",
            "\n",
            "Epoch 194: Validation loss decreased (1804.975952 --> 1787.238281).\n",
            "\t Train_Loss: 863.9850 Val_Loss: 1787.2383  BEST VAL Loss: 1787.2383\n",
            "\n",
            "Epoch 195: Validation loss decreased (1787.238281 --> 1769.699585).\n",
            "\t Train_Loss: 852.4185 Val_Loss: 1769.6996  BEST VAL Loss: 1769.6996\n",
            "\n",
            "Epoch 196: Validation loss decreased (1769.699585 --> 1752.357422).\n",
            "\t Train_Loss: 841.0137 Val_Loss: 1752.3574  BEST VAL Loss: 1752.3574\n",
            "\n",
            "Epoch 197: Validation loss decreased (1752.357422 --> 1735.210327).\n",
            "\t Train_Loss: 829.7686 Val_Loss: 1735.2103  BEST VAL Loss: 1735.2103\n",
            "\n",
            "Epoch 198: Validation loss decreased (1735.210327 --> 1718.256104).\n",
            "\t Train_Loss: 818.6814 Val_Loss: 1718.2561  BEST VAL Loss: 1718.2561\n",
            "\n",
            "Epoch 199: Validation loss decreased (1718.256104 --> 1701.491455).\n",
            "\t Train_Loss: 807.7504 Val_Loss: 1701.4915  BEST VAL Loss: 1701.4915\n",
            "\n",
            "Epoch 200: Validation loss decreased (1701.491455 --> 1684.916382).\n",
            "\t Train_Loss: 796.9729 Val_Loss: 1684.9164  BEST VAL Loss: 1684.9164\n",
            "\n",
            "Epoch 201: Validation loss decreased (1684.916382 --> 1668.528198).\n",
            "\t Train_Loss: 786.3480 Val_Loss: 1668.5282  BEST VAL Loss: 1668.5282\n",
            "\n",
            "Epoch 202: Validation loss decreased (1668.528198 --> 1652.323486).\n",
            "\t Train_Loss: 775.8737 Val_Loss: 1652.3235  BEST VAL Loss: 1652.3235\n",
            "\n",
            "Epoch 203: Validation loss decreased (1652.323486 --> 1636.301758).\n",
            "\t Train_Loss: 765.5474 Val_Loss: 1636.3018  BEST VAL Loss: 1636.3018\n",
            "\n",
            "Epoch 204: Validation loss decreased (1636.301758 --> 1620.461792).\n",
            "\t Train_Loss: 755.3679 Val_Loss: 1620.4618  BEST VAL Loss: 1620.4618\n",
            "\n",
            "Epoch 205: Validation loss decreased (1620.461792 --> 1604.799072).\n",
            "\t Train_Loss: 745.3338 Val_Loss: 1604.7991  BEST VAL Loss: 1604.7991\n",
            "\n",
            "Epoch 206: Validation loss decreased (1604.799072 --> 1589.313599).\n",
            "\t Train_Loss: 735.4421 Val_Loss: 1589.3136  BEST VAL Loss: 1589.3136\n",
            "\n",
            "Epoch 207: Validation loss decreased (1589.313599 --> 1574.003296).\n",
            "\t Train_Loss: 725.6920 Val_Loss: 1574.0033  BEST VAL Loss: 1574.0033\n",
            "\n",
            "Epoch 208: Validation loss decreased (1574.003296 --> 1558.865234).\n",
            "\t Train_Loss: 716.0815 Val_Loss: 1558.8652  BEST VAL Loss: 1558.8652\n",
            "\n",
            "Epoch 209: Validation loss decreased (1558.865234 --> 1543.900146).\n",
            "\t Train_Loss: 706.6083 Val_Loss: 1543.9001  BEST VAL Loss: 1543.9001\n",
            "\n",
            "Epoch 210: Validation loss decreased (1543.900146 --> 1529.103149).\n",
            "\t Train_Loss: 697.2725 Val_Loss: 1529.1031  BEST VAL Loss: 1529.1031\n",
            "\n",
            "Epoch 211: Validation loss decreased (1529.103149 --> 1514.473877).\n",
            "\t Train_Loss: 688.0704 Val_Loss: 1514.4739  BEST VAL Loss: 1514.4739\n",
            "\n",
            "Epoch 212: Validation loss decreased (1514.473877 --> 1500.010620).\n",
            "\t Train_Loss: 679.0012 Val_Loss: 1500.0106  BEST VAL Loss: 1500.0106\n",
            "\n",
            "Epoch 213: Validation loss decreased (1500.010620 --> 1485.711914).\n",
            "\t Train_Loss: 670.0634 Val_Loss: 1485.7119  BEST VAL Loss: 1485.7119\n",
            "\n",
            "Epoch 214: Validation loss decreased (1485.711914 --> 1471.575195).\n",
            "\t Train_Loss: 661.2554 Val_Loss: 1471.5752  BEST VAL Loss: 1471.5752\n",
            "\n",
            "Epoch 215: Validation loss decreased (1471.575195 --> 1457.599487).\n",
            "\t Train_Loss: 652.5751 Val_Loss: 1457.5995  BEST VAL Loss: 1457.5995\n",
            "\n",
            "Epoch 216: Validation loss decreased (1457.599487 --> 1443.782349).\n",
            "\t Train_Loss: 644.0216 Val_Loss: 1443.7823  BEST VAL Loss: 1443.7823\n",
            "\n",
            "Epoch 217: Validation loss decreased (1443.782349 --> 1430.122192).\n",
            "\t Train_Loss: 635.5926 Val_Loss: 1430.1222  BEST VAL Loss: 1430.1222\n",
            "\n",
            "Epoch 218: Validation loss decreased (1430.122192 --> 1416.618164).\n",
            "\t Train_Loss: 627.2867 Val_Loss: 1416.6182  BEST VAL Loss: 1416.6182\n",
            "\n",
            "Epoch 219: Validation loss decreased (1416.618164 --> 1403.267456).\n",
            "\t Train_Loss: 619.1030 Val_Loss: 1403.2675  BEST VAL Loss: 1403.2675\n",
            "\n",
            "Epoch 220: Validation loss decreased (1403.267456 --> 1390.069702).\n",
            "\t Train_Loss: 611.0391 Val_Loss: 1390.0697  BEST VAL Loss: 1390.0697\n",
            "\n",
            "Epoch 221: Validation loss decreased (1390.069702 --> 1377.022705).\n",
            "\t Train_Loss: 603.0944 Val_Loss: 1377.0227  BEST VAL Loss: 1377.0227\n",
            "\n",
            "Epoch 222: Validation loss decreased (1377.022705 --> 1364.124512).\n",
            "\t Train_Loss: 595.2670 Val_Loss: 1364.1245  BEST VAL Loss: 1364.1245\n",
            "\n",
            "Epoch 223: Validation loss decreased (1364.124512 --> 1351.373901).\n",
            "\t Train_Loss: 587.5553 Val_Loss: 1351.3739  BEST VAL Loss: 1351.3739\n",
            "\n",
            "Epoch 224: Validation loss decreased (1351.373901 --> 1338.769531).\n",
            "\t Train_Loss: 579.9578 Val_Loss: 1338.7695  BEST VAL Loss: 1338.7695\n",
            "\n",
            "Epoch 225: Validation loss decreased (1338.769531 --> 1326.309326).\n",
            "\t Train_Loss: 572.4734 Val_Loss: 1326.3093  BEST VAL Loss: 1326.3093\n",
            "\n",
            "Epoch 226: Validation loss decreased (1326.309326 --> 1313.991699).\n",
            "\t Train_Loss: 565.1005 Val_Loss: 1313.9917  BEST VAL Loss: 1313.9917\n",
            "\n",
            "Epoch 227: Validation loss decreased (1313.991699 --> 1301.815308).\n",
            "\t Train_Loss: 557.8374 Val_Loss: 1301.8153  BEST VAL Loss: 1301.8153\n",
            "\n",
            "Epoch 228: Validation loss decreased (1301.815308 --> 1289.779175).\n",
            "\t Train_Loss: 550.6829 Val_Loss: 1289.7792  BEST VAL Loss: 1289.7792\n",
            "\n",
            "Epoch 229: Validation loss decreased (1289.779175 --> 1277.881226).\n",
            "\t Train_Loss: 543.6360 Val_Loss: 1277.8812  BEST VAL Loss: 1277.8812\n",
            "\n",
            "Epoch 230: Validation loss decreased (1277.881226 --> 1266.120850).\n",
            "\t Train_Loss: 536.6949 Val_Loss: 1266.1208  BEST VAL Loss: 1266.1208\n",
            "\n",
            "Epoch 231: Validation loss decreased (1266.120850 --> 1254.495117).\n",
            "\t Train_Loss: 529.8588 Val_Loss: 1254.4951  BEST VAL Loss: 1254.4951\n",
            "\n",
            "Epoch 232: Validation loss decreased (1254.495117 --> 1243.003784).\n",
            "\t Train_Loss: 523.1255 Val_Loss: 1243.0038  BEST VAL Loss: 1243.0038\n",
            "\n",
            "Epoch 233: Validation loss decreased (1243.003784 --> 1231.644531).\n",
            "\t Train_Loss: 516.4943 Val_Loss: 1231.6445  BEST VAL Loss: 1231.6445\n",
            "\n",
            "Epoch 234: Validation loss decreased (1231.644531 --> 1220.416870).\n",
            "\t Train_Loss: 509.9637 Val_Loss: 1220.4169  BEST VAL Loss: 1220.4169\n",
            "\n",
            "Epoch 235: Validation loss decreased (1220.416870 --> 1209.319336).\n",
            "\t Train_Loss: 503.5325 Val_Loss: 1209.3193  BEST VAL Loss: 1209.3193\n",
            "\n",
            "Epoch 236: Validation loss decreased (1209.319336 --> 1198.348999).\n",
            "\t Train_Loss: 497.1996 Val_Loss: 1198.3490  BEST VAL Loss: 1198.3490\n",
            "\n",
            "Epoch 237: Validation loss decreased (1198.348999 --> 1187.506714).\n",
            "\t Train_Loss: 490.9629 Val_Loss: 1187.5067  BEST VAL Loss: 1187.5067\n",
            "\n",
            "Epoch 238: Validation loss decreased (1187.506714 --> 1176.789307).\n",
            "\t Train_Loss: 484.8223 Val_Loss: 1176.7893  BEST VAL Loss: 1176.7893\n",
            "\n",
            "Epoch 239: Validation loss decreased (1176.789307 --> 1166.196655).\n",
            "\t Train_Loss: 478.7756 Val_Loss: 1166.1967  BEST VAL Loss: 1166.1967\n",
            "\n",
            "Epoch 240: Validation loss decreased (1166.196655 --> 1155.726440).\n",
            "\t Train_Loss: 472.8223 Val_Loss: 1155.7264  BEST VAL Loss: 1155.7264\n",
            "\n",
            "Epoch 241: Validation loss decreased (1155.726440 --> 1145.378418).\n",
            "\t Train_Loss: 466.9605 Val_Loss: 1145.3784  BEST VAL Loss: 1145.3784\n",
            "\n",
            "Epoch 242: Validation loss decreased (1145.378418 --> 1135.150757).\n",
            "\t Train_Loss: 461.1898 Val_Loss: 1135.1508  BEST VAL Loss: 1135.1508\n",
            "\n",
            "Epoch 243: Validation loss decreased (1135.150757 --> 1125.041626).\n",
            "\t Train_Loss: 455.5085 Val_Loss: 1125.0416  BEST VAL Loss: 1125.0416\n",
            "\n",
            "Epoch 244: Validation loss decreased (1125.041626 --> 1115.050903).\n",
            "\t Train_Loss: 449.9154 Val_Loss: 1115.0509  BEST VAL Loss: 1115.0509\n",
            "\n",
            "Epoch 245: Validation loss decreased (1115.050903 --> 1105.175781).\n",
            "\t Train_Loss: 444.4097 Val_Loss: 1105.1758  BEST VAL Loss: 1105.1758\n",
            "\n",
            "Epoch 246: Validation loss decreased (1105.175781 --> 1095.416870).\n",
            "\t Train_Loss: 438.9895 Val_Loss: 1095.4169  BEST VAL Loss: 1095.4169\n",
            "\n",
            "Epoch 247: Validation loss decreased (1095.416870 --> 1085.771362).\n",
            "\t Train_Loss: 433.6548 Val_Loss: 1085.7714  BEST VAL Loss: 1085.7714\n",
            "\n",
            "Epoch 248: Validation loss decreased (1085.771362 --> 1076.238525).\n",
            "\t Train_Loss: 428.4034 Val_Loss: 1076.2385  BEST VAL Loss: 1076.2385\n",
            "\n",
            "Epoch 249: Validation loss decreased (1076.238525 --> 1066.817017).\n",
            "\t Train_Loss: 423.2347 Val_Loss: 1066.8170  BEST VAL Loss: 1066.8170\n",
            "\n",
            "Epoch 250: Validation loss decreased (1066.817017 --> 1057.506470).\n",
            "\t Train_Loss: 418.1474 Val_Loss: 1057.5065  BEST VAL Loss: 1057.5065\n",
            "\n",
            "Epoch 251: Validation loss decreased (1057.506470 --> 1048.304565).\n",
            "\t Train_Loss: 413.1409 Val_Loss: 1048.3046  BEST VAL Loss: 1048.3046\n",
            "\n",
            "Epoch 252: Validation loss decreased (1048.304565 --> 1039.210571).\n",
            "\t Train_Loss: 408.2134 Val_Loss: 1039.2106  BEST VAL Loss: 1039.2106\n",
            "\n",
            "Epoch 253: Validation loss decreased (1039.210571 --> 1030.223267).\n",
            "\t Train_Loss: 403.3643 Val_Loss: 1030.2233  BEST VAL Loss: 1030.2233\n",
            "\n",
            "Epoch 254: Validation loss decreased (1030.223267 --> 1021.341797).\n",
            "\t Train_Loss: 398.5924 Val_Loss: 1021.3418  BEST VAL Loss: 1021.3418\n",
            "\n",
            "Epoch 255: Validation loss decreased (1021.341797 --> 1012.564636).\n",
            "\t Train_Loss: 393.8967 Val_Loss: 1012.5646  BEST VAL Loss: 1012.5646\n",
            "\n",
            "Epoch 256: Validation loss decreased (1012.564636 --> 1003.890137).\n",
            "\t Train_Loss: 389.2762 Val_Loss: 1003.8901  BEST VAL Loss: 1003.8901\n",
            "\n",
            "Epoch 257: Validation loss decreased (1003.890137 --> 995.318481).\n",
            "\t Train_Loss: 384.7295 Val_Loss: 995.3185  BEST VAL Loss: 995.3185\n",
            "\n",
            "Epoch 258: Validation loss decreased (995.318481 --> 986.847839).\n",
            "\t Train_Loss: 380.2562 Val_Loss: 986.8478  BEST VAL Loss: 986.8478\n",
            "\n",
            "Epoch 259: Validation loss decreased (986.847839 --> 978.476868).\n",
            "\t Train_Loss: 375.8551 Val_Loss: 978.4769  BEST VAL Loss: 978.4769\n",
            "\n",
            "Epoch 260: Validation loss decreased (978.476868 --> 970.205078).\n",
            "\t Train_Loss: 371.5249 Val_Loss: 970.2051  BEST VAL Loss: 970.2051\n",
            "\n",
            "Epoch 261: Validation loss decreased (970.205078 --> 962.030701).\n",
            "\t Train_Loss: 367.2651 Val_Loss: 962.0307  BEST VAL Loss: 962.0307\n",
            "\n",
            "Epoch 262: Validation loss decreased (962.030701 --> 953.953125).\n",
            "\t Train_Loss: 363.0743 Val_Loss: 953.9531  BEST VAL Loss: 953.9531\n",
            "\n",
            "Epoch 263: Validation loss decreased (953.953125 --> 945.971130).\n",
            "\t Train_Loss: 358.9519 Val_Loss: 945.9711  BEST VAL Loss: 945.9711\n",
            "\n",
            "Epoch 264: Validation loss decreased (945.971130 --> 938.083618).\n",
            "\t Train_Loss: 354.8966 Val_Loss: 938.0836  BEST VAL Loss: 938.0836\n",
            "\n",
            "Epoch 265: Validation loss decreased (938.083618 --> 930.289673).\n",
            "\t Train_Loss: 350.9077 Val_Loss: 930.2897  BEST VAL Loss: 930.2897\n",
            "\n",
            "Epoch 266: Validation loss decreased (930.289673 --> 922.588196).\n",
            "\t Train_Loss: 346.9842 Val_Loss: 922.5882  BEST VAL Loss: 922.5882\n",
            "\n",
            "Epoch 267: Validation loss decreased (922.588196 --> 914.977966).\n",
            "\t Train_Loss: 343.1253 Val_Loss: 914.9780  BEST VAL Loss: 914.9780\n",
            "\n",
            "Epoch 268: Validation loss decreased (914.977966 --> 907.457825).\n",
            "\t Train_Loss: 339.3299 Val_Loss: 907.4578  BEST VAL Loss: 907.4578\n",
            "\n",
            "Epoch 269: Validation loss decreased (907.457825 --> 900.027771).\n",
            "\t Train_Loss: 335.5970 Val_Loss: 900.0278  BEST VAL Loss: 900.0278\n",
            "\n",
            "Epoch 270: Validation loss decreased (900.027771 --> 892.685364).\n",
            "\t Train_Loss: 331.9262 Val_Loss: 892.6854  BEST VAL Loss: 892.6854\n",
            "\n",
            "Epoch 271: Validation loss decreased (892.685364 --> 885.431152).\n",
            "\t Train_Loss: 328.3160 Val_Loss: 885.4312  BEST VAL Loss: 885.4312\n",
            "\n",
            "Epoch 272: Validation loss decreased (885.431152 --> 878.262634).\n",
            "\t Train_Loss: 324.7663 Val_Loss: 878.2626  BEST VAL Loss: 878.2626\n",
            "\n",
            "Epoch 273: Validation loss decreased (878.262634 --> 871.180481).\n",
            "\t Train_Loss: 321.2754 Val_Loss: 871.1805  BEST VAL Loss: 871.1805\n",
            "\n",
            "Epoch 274: Validation loss decreased (871.180481 --> 864.182068).\n",
            "\t Train_Loss: 317.8433 Val_Loss: 864.1821  BEST VAL Loss: 864.1821\n",
            "\n",
            "Epoch 275: Validation loss decreased (864.182068 --> 857.267395).\n",
            "\t Train_Loss: 314.4684 Val_Loss: 857.2674  BEST VAL Loss: 857.2674\n",
            "\n",
            "Epoch 276: Validation loss decreased (857.267395 --> 850.435486).\n",
            "\t Train_Loss: 311.1502 Val_Loss: 850.4355  BEST VAL Loss: 850.4355\n",
            "\n",
            "Epoch 277: Validation loss decreased (850.435486 --> 843.685242).\n",
            "\t Train_Loss: 307.8880 Val_Loss: 843.6852  BEST VAL Loss: 843.6852\n",
            "\n",
            "Epoch 278: Validation loss decreased (843.685242 --> 837.015747).\n",
            "\t Train_Loss: 304.6810 Val_Loss: 837.0157  BEST VAL Loss: 837.0157\n",
            "\n",
            "Epoch 279: Validation loss decreased (837.015747 --> 830.426208).\n",
            "\t Train_Loss: 301.5281 Val_Loss: 830.4262  BEST VAL Loss: 830.4262\n",
            "\n",
            "Epoch 280: Validation loss decreased (830.426208 --> 823.915039).\n",
            "\t Train_Loss: 298.4287 Val_Loss: 823.9150  BEST VAL Loss: 823.9150\n",
            "\n",
            "Epoch 281: Validation loss decreased (823.915039 --> 817.482605).\n",
            "\t Train_Loss: 295.3818 Val_Loss: 817.4826  BEST VAL Loss: 817.4826\n",
            "\n",
            "Epoch 282: Validation loss decreased (817.482605 --> 811.126892).\n",
            "\t Train_Loss: 292.3872 Val_Loss: 811.1269  BEST VAL Loss: 811.1269\n",
            "\n",
            "Epoch 283: Validation loss decreased (811.126892 --> 804.848083).\n",
            "\t Train_Loss: 289.4435 Val_Loss: 804.8481  BEST VAL Loss: 804.8481\n",
            "\n",
            "Epoch 284: Validation loss decreased (804.848083 --> 798.644714).\n",
            "\t Train_Loss: 286.5505 Val_Loss: 798.6447  BEST VAL Loss: 798.6447\n",
            "\n",
            "Epoch 285: Validation loss decreased (798.644714 --> 792.515320).\n",
            "\t Train_Loss: 283.7072 Val_Loss: 792.5153  BEST VAL Loss: 792.5153\n",
            "\n",
            "Epoch 286: Validation loss decreased (792.515320 --> 786.460205).\n",
            "\t Train_Loss: 280.9125 Val_Loss: 786.4602  BEST VAL Loss: 786.4602\n",
            "\n",
            "Epoch 287: Validation loss decreased (786.460205 --> 780.477783).\n",
            "\t Train_Loss: 278.1663 Val_Loss: 780.4778  BEST VAL Loss: 780.4778\n",
            "\n",
            "Epoch 288: Validation loss decreased (780.477783 --> 774.567200).\n",
            "\t Train_Loss: 275.4675 Val_Loss: 774.5672  BEST VAL Loss: 774.5672\n",
            "\n",
            "Epoch 289: Validation loss decreased (774.567200 --> 768.728210).\n",
            "\t Train_Loss: 272.8154 Val_Loss: 768.7282  BEST VAL Loss: 768.7282\n",
            "\n",
            "Epoch 290: Validation loss decreased (768.728210 --> 762.959351).\n",
            "\t Train_Loss: 270.2095 Val_Loss: 762.9594  BEST VAL Loss: 762.9594\n",
            "\n",
            "Epoch 291: Validation loss decreased (762.959351 --> 757.260437).\n",
            "\t Train_Loss: 267.6489 Val_Loss: 757.2604  BEST VAL Loss: 757.2604\n",
            "\n",
            "Epoch 292: Validation loss decreased (757.260437 --> 751.630249).\n",
            "\t Train_Loss: 265.1331 Val_Loss: 751.6302  BEST VAL Loss: 751.6302\n",
            "\n",
            "Epoch 293: Validation loss decreased (751.630249 --> 746.067627).\n",
            "\t Train_Loss: 262.6613 Val_Loss: 746.0676  BEST VAL Loss: 746.0676\n",
            "\n",
            "Epoch 294: Validation loss decreased (746.067627 --> 740.572937).\n",
            "\t Train_Loss: 260.2328 Val_Loss: 740.5729  BEST VAL Loss: 740.5729\n",
            "\n",
            "Epoch 295: Validation loss decreased (740.572937 --> 735.144043).\n",
            "\t Train_Loss: 257.8472 Val_Loss: 735.1440  BEST VAL Loss: 735.1440\n",
            "\n",
            "Epoch 296: Validation loss decreased (735.144043 --> 729.781494).\n",
            "\t Train_Loss: 255.5033 Val_Loss: 729.7815  BEST VAL Loss: 729.7815\n",
            "\n",
            "Epoch 297: Validation loss decreased (729.781494 --> 724.483704).\n",
            "\t Train_Loss: 253.2011 Val_Loss: 724.4837  BEST VAL Loss: 724.4837\n",
            "\n",
            "Epoch 298: Validation loss decreased (724.483704 --> 719.249939).\n",
            "\t Train_Loss: 250.9397 Val_Loss: 719.2499  BEST VAL Loss: 719.2499\n",
            "\n",
            "Epoch 299: Validation loss decreased (719.249939 --> 714.079529).\n",
            "\t Train_Loss: 248.7183 Val_Loss: 714.0795  BEST VAL Loss: 714.0795\n",
            "\n",
            "Epoch 300: Validation loss decreased (714.079529 --> 708.972107).\n",
            "\t Train_Loss: 246.5364 Val_Loss: 708.9721  BEST VAL Loss: 708.9721\n",
            "\n",
            "Epoch 301: Validation loss decreased (708.972107 --> 703.926636).\n",
            "\t Train_Loss: 244.3935 Val_Loss: 703.9266  BEST VAL Loss: 703.9266\n",
            "\n",
            "Epoch 302: Validation loss decreased (703.926636 --> 698.942383).\n",
            "\t Train_Loss: 242.2889 Val_Loss: 698.9424  BEST VAL Loss: 698.9424\n",
            "\n",
            "Epoch 303: Validation loss decreased (698.942383 --> 694.018677).\n",
            "\t Train_Loss: 240.2220 Val_Loss: 694.0187  BEST VAL Loss: 694.0187\n",
            "\n",
            "Epoch 304: Validation loss decreased (694.018677 --> 689.154907).\n",
            "\t Train_Loss: 238.1922 Val_Loss: 689.1549  BEST VAL Loss: 689.1549\n",
            "\n",
            "Epoch 305: Validation loss decreased (689.154907 --> 684.350525).\n",
            "\t Train_Loss: 236.1990 Val_Loss: 684.3505  BEST VAL Loss: 684.3505\n",
            "\n",
            "Epoch 306: Validation loss decreased (684.350525 --> 679.603699).\n",
            "\t Train_Loss: 234.2419 Val_Loss: 679.6037  BEST VAL Loss: 679.6037\n",
            "\n",
            "Epoch 307: Validation loss decreased (679.603699 --> 674.915466).\n",
            "\t Train_Loss: 232.3198 Val_Loss: 674.9155  BEST VAL Loss: 674.9155\n",
            "\n",
            "Epoch 308: Validation loss decreased (674.915466 --> 670.284363).\n",
            "\t Train_Loss: 230.4328 Val_Loss: 670.2844  BEST VAL Loss: 670.2844\n",
            "\n",
            "Epoch 309: Validation loss decreased (670.284363 --> 665.709412).\n",
            "\t Train_Loss: 228.5802 Val_Loss: 665.7094  BEST VAL Loss: 665.7094\n",
            "\n",
            "Epoch 310: Validation loss decreased (665.709412 --> 661.190125).\n",
            "\t Train_Loss: 226.7612 Val_Loss: 661.1901  BEST VAL Loss: 661.1901\n",
            "\n",
            "Epoch 311: Validation loss decreased (661.190125 --> 656.726440).\n",
            "\t Train_Loss: 224.9753 Val_Loss: 656.7264  BEST VAL Loss: 656.7264\n",
            "\n",
            "Epoch 312: Validation loss decreased (656.726440 --> 652.317078).\n",
            "\t Train_Loss: 223.2223 Val_Loss: 652.3171  BEST VAL Loss: 652.3171\n",
            "\n",
            "Epoch 313: Validation loss decreased (652.317078 --> 647.961426).\n",
            "\t Train_Loss: 221.5014 Val_Loss: 647.9614  BEST VAL Loss: 647.9614\n",
            "\n",
            "Epoch 314: Validation loss decreased (647.961426 --> 643.658875).\n",
            "\t Train_Loss: 219.8121 Val_Loss: 643.6589  BEST VAL Loss: 643.6589\n",
            "\n",
            "Epoch 315: Validation loss decreased (643.658875 --> 639.409485).\n",
            "\t Train_Loss: 218.1539 Val_Loss: 639.4095  BEST VAL Loss: 639.4095\n",
            "\n",
            "Epoch 316: Validation loss decreased (639.409485 --> 635.211609).\n",
            "\t Train_Loss: 216.5265 Val_Loss: 635.2116  BEST VAL Loss: 635.2116\n",
            "\n",
            "Epoch 317: Validation loss decreased (635.211609 --> 631.065002).\n",
            "\t Train_Loss: 214.9290 Val_Loss: 631.0650  BEST VAL Loss: 631.0650\n",
            "\n",
            "Epoch 318: Validation loss decreased (631.065002 --> 626.969543).\n",
            "\t Train_Loss: 213.3612 Val_Loss: 626.9695  BEST VAL Loss: 626.9695\n",
            "\n",
            "Epoch 319: Validation loss decreased (626.969543 --> 622.923767).\n",
            "\t Train_Loss: 211.8227 Val_Loss: 622.9238  BEST VAL Loss: 622.9238\n",
            "\n",
            "Epoch 320: Validation loss decreased (622.923767 --> 618.927612).\n",
            "\t Train_Loss: 210.3127 Val_Loss: 618.9276  BEST VAL Loss: 618.9276\n",
            "\n",
            "Epoch 321: Validation loss decreased (618.927612 --> 614.980408).\n",
            "\t Train_Loss: 208.8309 Val_Loss: 614.9804  BEST VAL Loss: 614.9804\n",
            "\n",
            "Epoch 322: Validation loss decreased (614.980408 --> 611.082031).\n",
            "\t Train_Loss: 207.3768 Val_Loss: 611.0820  BEST VAL Loss: 611.0820\n",
            "\n",
            "Epoch 323: Validation loss decreased (611.082031 --> 607.231384).\n",
            "\t Train_Loss: 205.9502 Val_Loss: 607.2314  BEST VAL Loss: 607.2314\n",
            "\n",
            "Epoch 324: Validation loss decreased (607.231384 --> 603.427551).\n",
            "\t Train_Loss: 204.5504 Val_Loss: 603.4276  BEST VAL Loss: 603.4276\n",
            "\n",
            "Epoch 325: Validation loss decreased (603.427551 --> 599.670715).\n",
            "\t Train_Loss: 203.1768 Val_Loss: 599.6707  BEST VAL Loss: 599.6707\n",
            "\n",
            "Epoch 326: Validation loss decreased (599.670715 --> 595.960632).\n",
            "\t Train_Loss: 201.8293 Val_Loss: 595.9606  BEST VAL Loss: 595.9606\n",
            "\n",
            "Epoch 327: Validation loss decreased (595.960632 --> 592.294739).\n",
            "\t Train_Loss: 200.5075 Val_Loss: 592.2947  BEST VAL Loss: 592.2947\n",
            "\n",
            "Epoch 328: Validation loss decreased (592.294739 --> 588.674866).\n",
            "\t Train_Loss: 199.2103 Val_Loss: 588.6749  BEST VAL Loss: 588.6749\n",
            "\n",
            "Epoch 329: Validation loss decreased (588.674866 --> 585.098938).\n",
            "\t Train_Loss: 197.9381 Val_Loss: 585.0989  BEST VAL Loss: 585.0989\n",
            "\n",
            "Epoch 330: Validation loss decreased (585.098938 --> 581.567566).\n",
            "\t Train_Loss: 196.6900 Val_Loss: 581.5676  BEST VAL Loss: 581.5676\n",
            "\n",
            "Epoch 331: Validation loss decreased (581.567566 --> 578.079773).\n",
            "\t Train_Loss: 195.4659 Val_Loss: 578.0798  BEST VAL Loss: 578.0798\n",
            "\n",
            "Epoch 332: Validation loss decreased (578.079773 --> 574.634216).\n",
            "\t Train_Loss: 194.2653 Val_Loss: 574.6342  BEST VAL Loss: 574.6342\n",
            "\n",
            "Epoch 333: Validation loss decreased (574.634216 --> 571.231567).\n",
            "\t Train_Loss: 193.0875 Val_Loss: 571.2316  BEST VAL Loss: 571.2316\n",
            "\n",
            "Epoch 334: Validation loss decreased (571.231567 --> 567.870178).\n",
            "\t Train_Loss: 191.9326 Val_Loss: 567.8702  BEST VAL Loss: 567.8702\n",
            "\n",
            "Epoch 335: Validation loss decreased (567.870178 --> 564.551392).\n",
            "\t Train_Loss: 190.7997 Val_Loss: 564.5514  BEST VAL Loss: 564.5514\n",
            "\n",
            "Epoch 336: Validation loss decreased (564.551392 --> 561.272644).\n",
            "\t Train_Loss: 189.6890 Val_Loss: 561.2726  BEST VAL Loss: 561.2726\n",
            "\n",
            "Epoch 337: Validation loss decreased (561.272644 --> 558.034485).\n",
            "\t Train_Loss: 188.5996 Val_Loss: 558.0345  BEST VAL Loss: 558.0345\n",
            "\n",
            "Epoch 338: Validation loss decreased (558.034485 --> 554.835999).\n",
            "\t Train_Loss: 187.5314 Val_Loss: 554.8360  BEST VAL Loss: 554.8360\n",
            "\n",
            "Epoch 339: Validation loss decreased (554.835999 --> 551.677673).\n",
            "\t Train_Loss: 186.4838 Val_Loss: 551.6777  BEST VAL Loss: 551.6777\n",
            "\n",
            "Epoch 340: Validation loss decreased (551.677673 --> 548.557678).\n",
            "\t Train_Loss: 185.4569 Val_Loss: 548.5577  BEST VAL Loss: 548.5577\n",
            "\n",
            "Epoch 341: Validation loss decreased (548.557678 --> 545.476868).\n",
            "\t Train_Loss: 184.4499 Val_Loss: 545.4769  BEST VAL Loss: 545.4769\n",
            "\n",
            "Epoch 342: Validation loss decreased (545.476868 --> 542.433533).\n",
            "\t Train_Loss: 183.4627 Val_Loss: 542.4335  BEST VAL Loss: 542.4335\n",
            "\n",
            "Epoch 343: Validation loss decreased (542.433533 --> 539.427917).\n",
            "\t Train_Loss: 182.4948 Val_Loss: 539.4279  BEST VAL Loss: 539.4279\n",
            "\n",
            "Epoch 344: Validation loss decreased (539.427917 --> 536.459534).\n",
            "\t Train_Loss: 181.5459 Val_Loss: 536.4595  BEST VAL Loss: 536.4595\n",
            "\n",
            "Epoch 345: Validation loss decreased (536.459534 --> 533.527893).\n",
            "\t Train_Loss: 180.6158 Val_Loss: 533.5279  BEST VAL Loss: 533.5279\n",
            "\n",
            "Epoch 346: Validation loss decreased (533.527893 --> 530.632385).\n",
            "\t Train_Loss: 179.7040 Val_Loss: 530.6324  BEST VAL Loss: 530.6324\n",
            "\n",
            "Epoch 347: Validation loss decreased (530.632385 --> 527.773376).\n",
            "\t Train_Loss: 178.8102 Val_Loss: 527.7734  BEST VAL Loss: 527.7734\n",
            "\n",
            "Epoch 348: Validation loss decreased (527.773376 --> 524.949036).\n",
            "\t Train_Loss: 177.9344 Val_Loss: 524.9490  BEST VAL Loss: 524.9490\n",
            "\n",
            "Epoch 349: Validation loss decreased (524.949036 --> 522.159668).\n",
            "\t Train_Loss: 177.0757 Val_Loss: 522.1597  BEST VAL Loss: 522.1597\n",
            "\n",
            "Epoch 350: Validation loss decreased (522.159668 --> 519.404907).\n",
            "\t Train_Loss: 176.2343 Val_Loss: 519.4049  BEST VAL Loss: 519.4049\n",
            "\n",
            "Epoch 351: Validation loss decreased (519.404907 --> 516.684265).\n",
            "\t Train_Loss: 175.4095 Val_Loss: 516.6843  BEST VAL Loss: 516.6843\n",
            "\n",
            "Epoch 352: Validation loss decreased (516.684265 --> 513.997559).\n",
            "\t Train_Loss: 174.6013 Val_Loss: 513.9976  BEST VAL Loss: 513.9976\n",
            "\n",
            "Epoch 353: Validation loss decreased (513.997559 --> 511.344055).\n",
            "\t Train_Loss: 173.8093 Val_Loss: 511.3441  BEST VAL Loss: 511.3441\n",
            "\n",
            "Epoch 354: Validation loss decreased (511.344055 --> 508.723358).\n",
            "\t Train_Loss: 173.0332 Val_Loss: 508.7234  BEST VAL Loss: 508.7234\n",
            "\n",
            "Epoch 355: Validation loss decreased (508.723358 --> 506.135590).\n",
            "\t Train_Loss: 172.2728 Val_Loss: 506.1356  BEST VAL Loss: 506.1356\n",
            "\n",
            "Epoch 356: Validation loss decreased (506.135590 --> 503.579346).\n",
            "\t Train_Loss: 171.5278 Val_Loss: 503.5793  BEST VAL Loss: 503.5793\n",
            "\n",
            "Epoch 357: Validation loss decreased (503.579346 --> 501.054993).\n",
            "\t Train_Loss: 170.7976 Val_Loss: 501.0550  BEST VAL Loss: 501.0550\n",
            "\n",
            "Epoch 358: Validation loss decreased (501.054993 --> 498.562225).\n",
            "\t Train_Loss: 170.0824 Val_Loss: 498.5622  BEST VAL Loss: 498.5622\n",
            "\n",
            "Epoch 359: Validation loss decreased (498.562225 --> 496.100586).\n",
            "\t Train_Loss: 169.3817 Val_Loss: 496.1006  BEST VAL Loss: 496.1006\n",
            "\n",
            "Epoch 360: Validation loss decreased (496.100586 --> 493.668945).\n",
            "\t Train_Loss: 168.6953 Val_Loss: 493.6689  BEST VAL Loss: 493.6689\n",
            "\n",
            "Epoch 361: Validation loss decreased (493.668945 --> 491.267792).\n",
            "\t Train_Loss: 168.0228 Val_Loss: 491.2678  BEST VAL Loss: 491.2678\n",
            "\n",
            "Epoch 362: Validation loss decreased (491.267792 --> 488.896698).\n",
            "\t Train_Loss: 167.3641 Val_Loss: 488.8967  BEST VAL Loss: 488.8967\n",
            "\n",
            "Epoch 363: Validation loss decreased (488.896698 --> 486.554688).\n",
            "\t Train_Loss: 166.7189 Val_Loss: 486.5547  BEST VAL Loss: 486.5547\n",
            "\n",
            "Epoch 364: Validation loss decreased (486.554688 --> 484.241913).\n",
            "\t Train_Loss: 166.0869 Val_Loss: 484.2419  BEST VAL Loss: 484.2419\n",
            "\n",
            "Epoch 365: Validation loss decreased (484.241913 --> 481.958099).\n",
            "\t Train_Loss: 165.4678 Val_Loss: 481.9581  BEST VAL Loss: 481.9581\n",
            "\n",
            "Epoch 366: Validation loss decreased (481.958099 --> 479.702728).\n",
            "\t Train_Loss: 164.8616 Val_Loss: 479.7027  BEST VAL Loss: 479.7027\n",
            "\n",
            "Epoch 367: Validation loss decreased (479.702728 --> 477.475403).\n",
            "\t Train_Loss: 164.2679 Val_Loss: 477.4754  BEST VAL Loss: 477.4754\n",
            "\n",
            "Epoch 368: Validation loss decreased (477.475403 --> 475.275787).\n",
            "\t Train_Loss: 163.6865 Val_Loss: 475.2758  BEST VAL Loss: 475.2758\n",
            "\n",
            "Epoch 369: Validation loss decreased (475.275787 --> 473.103516).\n",
            "\t Train_Loss: 163.1172 Val_Loss: 473.1035  BEST VAL Loss: 473.1035\n",
            "\n",
            "Epoch 370: Validation loss decreased (473.103516 --> 470.958466).\n",
            "\t Train_Loss: 162.5596 Val_Loss: 470.9585  BEST VAL Loss: 470.9585\n",
            "\n",
            "Epoch 371: Validation loss decreased (470.958466 --> 468.839935).\n",
            "\t Train_Loss: 162.0137 Val_Loss: 468.8399  BEST VAL Loss: 468.8399\n",
            "\n",
            "Epoch 372: Validation loss decreased (468.839935 --> 466.747955).\n",
            "\t Train_Loss: 161.4792 Val_Loss: 466.7480  BEST VAL Loss: 466.7480\n",
            "\n",
            "Epoch 373: Validation loss decreased (466.747955 --> 464.682343).\n",
            "\t Train_Loss: 160.9558 Val_Loss: 464.6823  BEST VAL Loss: 464.6823\n",
            "\n",
            "Epoch 374: Validation loss decreased (464.682343 --> 462.642181).\n",
            "\t Train_Loss: 160.4435 Val_Loss: 462.6422  BEST VAL Loss: 462.6422\n",
            "\n",
            "Epoch 375: Validation loss decreased (462.642181 --> 460.627441).\n",
            "\t Train_Loss: 159.9419 Val_Loss: 460.6274  BEST VAL Loss: 460.6274\n",
            "\n",
            "Epoch 376: Validation loss decreased (460.627441 --> 458.637695).\n",
            "\t Train_Loss: 159.4508 Val_Loss: 458.6377  BEST VAL Loss: 458.6377\n",
            "\n",
            "Epoch 377: Validation loss decreased (458.637695 --> 456.672913).\n",
            "\t Train_Loss: 158.9700 Val_Loss: 456.6729  BEST VAL Loss: 456.6729\n",
            "\n",
            "Epoch 378: Validation loss decreased (456.672913 --> 454.733063).\n",
            "\t Train_Loss: 158.4994 Val_Loss: 454.7331  BEST VAL Loss: 454.7331\n",
            "\n",
            "Epoch 379: Validation loss decreased (454.733063 --> 452.817108).\n",
            "\t Train_Loss: 158.0389 Val_Loss: 452.8171  BEST VAL Loss: 452.8171\n",
            "\n",
            "Epoch 380: Validation loss decreased (452.817108 --> 450.924896).\n",
            "\t Train_Loss: 157.5880 Val_Loss: 450.9249  BEST VAL Loss: 450.9249\n",
            "\n",
            "Epoch 381: Validation loss decreased (450.924896 --> 449.057098).\n",
            "\t Train_Loss: 157.1467 Val_Loss: 449.0571  BEST VAL Loss: 449.0571\n",
            "\n",
            "Epoch 382: Validation loss decreased (449.057098 --> 447.211823).\n",
            "\t Train_Loss: 156.7149 Val_Loss: 447.2118  BEST VAL Loss: 447.2118\n",
            "\n",
            "Epoch 383: Validation loss decreased (447.211823 --> 445.389862).\n",
            "\t Train_Loss: 156.2922 Val_Loss: 445.3899  BEST VAL Loss: 445.3899\n",
            "\n",
            "Epoch 384: Validation loss decreased (445.389862 --> 443.591064).\n",
            "\t Train_Loss: 155.8786 Val_Loss: 443.5911  BEST VAL Loss: 443.5911\n",
            "\n",
            "Epoch 385: Validation loss decreased (443.591064 --> 441.814697).\n",
            "\t Train_Loss: 155.4739 Val_Loss: 441.8147  BEST VAL Loss: 441.8147\n",
            "\n",
            "Epoch 386: Validation loss decreased (441.814697 --> 440.060303).\n",
            "\t Train_Loss: 155.0779 Val_Loss: 440.0603  BEST VAL Loss: 440.0603\n",
            "\n",
            "Epoch 387: Validation loss decreased (440.060303 --> 438.328094).\n",
            "\t Train_Loss: 154.6903 Val_Loss: 438.3281  BEST VAL Loss: 438.3281\n",
            "\n",
            "Epoch 388: Validation loss decreased (438.328094 --> 436.617279).\n",
            "\t Train_Loss: 154.3111 Val_Loss: 436.6173  BEST VAL Loss: 436.6173\n",
            "\n",
            "Epoch 389: Validation loss decreased (436.617279 --> 434.928436).\n",
            "\t Train_Loss: 153.9401 Val_Loss: 434.9284  BEST VAL Loss: 434.9284\n",
            "\n",
            "Epoch 390: Validation loss decreased (434.928436 --> 433.260712).\n",
            "\t Train_Loss: 153.5772 Val_Loss: 433.2607  BEST VAL Loss: 433.2607\n",
            "\n",
            "Epoch 391: Validation loss decreased (433.260712 --> 431.613586).\n",
            "\t Train_Loss: 153.2222 Val_Loss: 431.6136  BEST VAL Loss: 431.6136\n",
            "\n",
            "Epoch 392: Validation loss decreased (431.613586 --> 429.987305).\n",
            "\t Train_Loss: 152.8747 Val_Loss: 429.9873  BEST VAL Loss: 429.9873\n",
            "\n",
            "Epoch 393: Validation loss decreased (429.987305 --> 428.381348).\n",
            "\t Train_Loss: 152.5349 Val_Loss: 428.3813  BEST VAL Loss: 428.3813\n",
            "\n",
            "Epoch 394: Validation loss decreased (428.381348 --> 426.795227).\n",
            "\t Train_Loss: 152.2025 Val_Loss: 426.7952  BEST VAL Loss: 426.7952\n",
            "\n",
            "Epoch 395: Validation loss decreased (426.795227 --> 425.229492).\n",
            "\t Train_Loss: 151.8773 Val_Loss: 425.2295  BEST VAL Loss: 425.2295\n",
            "\n",
            "Epoch 396: Validation loss decreased (425.229492 --> 423.683594).\n",
            "\t Train_Loss: 151.5593 Val_Loss: 423.6836  BEST VAL Loss: 423.6836\n",
            "\n",
            "Epoch 397: Validation loss decreased (423.683594 --> 422.156982).\n",
            "\t Train_Loss: 151.2484 Val_Loss: 422.1570  BEST VAL Loss: 422.1570\n",
            "\n",
            "Epoch 398: Validation loss decreased (422.156982 --> 420.649231).\n",
            "\t Train_Loss: 150.9442 Val_Loss: 420.6492  BEST VAL Loss: 420.6492\n",
            "\n",
            "Epoch 399: Validation loss decreased (420.649231 --> 419.160736).\n",
            "\t Train_Loss: 150.6467 Val_Loss: 419.1607  BEST VAL Loss: 419.1607\n",
            "\n",
            "Epoch 400: Validation loss decreased (419.160736 --> 417.690918).\n",
            "\t Train_Loss: 150.3557 Val_Loss: 417.6909  BEST VAL Loss: 417.6909\n",
            "\n",
            "Epoch 401: Validation loss decreased (417.690918 --> 416.239471).\n",
            "\t Train_Loss: 150.0712 Val_Loss: 416.2395  BEST VAL Loss: 416.2395\n",
            "\n",
            "Epoch 402: Validation loss decreased (416.239471 --> 414.806610).\n",
            "\t Train_Loss: 149.7930 Val_Loss: 414.8066  BEST VAL Loss: 414.8066\n",
            "\n",
            "Epoch 403: Validation loss decreased (414.806610 --> 413.391266).\n",
            "\t Train_Loss: 149.5210 Val_Loss: 413.3913  BEST VAL Loss: 413.3913\n",
            "\n",
            "Epoch 404: Validation loss decreased (413.391266 --> 411.994202).\n",
            "\t Train_Loss: 149.2550 Val_Loss: 411.9942  BEST VAL Loss: 411.9942\n",
            "\n",
            "Epoch 405: Validation loss decreased (411.994202 --> 410.614990).\n",
            "\t Train_Loss: 148.9950 Val_Loss: 410.6150  BEST VAL Loss: 410.6150\n",
            "\n",
            "Epoch 406: Validation loss decreased (410.614990 --> 409.252777).\n",
            "\t Train_Loss: 148.7408 Val_Loss: 409.2528  BEST VAL Loss: 409.2528\n",
            "\n",
            "Epoch 407: Validation loss decreased (409.252777 --> 407.907684).\n",
            "\t Train_Loss: 148.4923 Val_Loss: 407.9077  BEST VAL Loss: 407.9077\n",
            "\n",
            "Epoch 408: Validation loss decreased (407.907684 --> 406.579895).\n",
            "\t Train_Loss: 148.2493 Val_Loss: 406.5799  BEST VAL Loss: 406.5799\n",
            "\n",
            "Epoch 409: Validation loss decreased (406.579895 --> 405.268524).\n",
            "\t Train_Loss: 148.0118 Val_Loss: 405.2685  BEST VAL Loss: 405.2685\n",
            "\n",
            "Epoch 410: Validation loss decreased (405.268524 --> 403.974030).\n",
            "\t Train_Loss: 147.7796 Val_Loss: 403.9740  BEST VAL Loss: 403.9740\n",
            "\n",
            "Epoch 411: Validation loss decreased (403.974030 --> 402.695496).\n",
            "\t Train_Loss: 147.5527 Val_Loss: 402.6955  BEST VAL Loss: 402.6955\n",
            "\n",
            "Epoch 412: Validation loss decreased (402.695496 --> 401.433411).\n",
            "\t Train_Loss: 147.3309 Val_Loss: 401.4334  BEST VAL Loss: 401.4334\n",
            "\n",
            "Epoch 413: Validation loss decreased (401.433411 --> 400.187347).\n",
            "\t Train_Loss: 147.1141 Val_Loss: 400.1873  BEST VAL Loss: 400.1873\n",
            "\n",
            "Epoch 414: Validation loss decreased (400.187347 --> 398.956818).\n",
            "\t Train_Loss: 146.9022 Val_Loss: 398.9568  BEST VAL Loss: 398.9568\n",
            "\n",
            "Epoch 415: Validation loss decreased (398.956818 --> 397.742157).\n",
            "\t Train_Loss: 146.6951 Val_Loss: 397.7422  BEST VAL Loss: 397.7422\n",
            "\n",
            "Epoch 416: Validation loss decreased (397.742157 --> 396.542816).\n",
            "\t Train_Loss: 146.4928 Val_Loss: 396.5428  BEST VAL Loss: 396.5428\n",
            "\n",
            "Epoch 417: Validation loss decreased (396.542816 --> 395.358429).\n",
            "\t Train_Loss: 146.2951 Val_Loss: 395.3584  BEST VAL Loss: 395.3584\n",
            "\n",
            "Epoch 418: Validation loss decreased (395.358429 --> 394.189453).\n",
            "\t Train_Loss: 146.1019 Val_Loss: 394.1895  BEST VAL Loss: 394.1895\n",
            "\n",
            "Epoch 419: Validation loss decreased (394.189453 --> 393.034760).\n",
            "\t Train_Loss: 145.9131 Val_Loss: 393.0348  BEST VAL Loss: 393.0348\n",
            "\n",
            "Epoch 420: Validation loss decreased (393.034760 --> 391.895355).\n",
            "\t Train_Loss: 145.7286 Val_Loss: 391.8954  BEST VAL Loss: 391.8954\n",
            "\n",
            "Epoch 421: Validation loss decreased (391.895355 --> 390.769775).\n",
            "\t Train_Loss: 145.5485 Val_Loss: 390.7698  BEST VAL Loss: 390.7698\n",
            "\n",
            "Epoch 422: Validation loss decreased (390.769775 --> 389.658813).\n",
            "\t Train_Loss: 145.3723 Val_Loss: 389.6588  BEST VAL Loss: 389.6588\n",
            "\n",
            "Epoch 423: Validation loss decreased (389.658813 --> 388.562134).\n",
            "\t Train_Loss: 145.2003 Val_Loss: 388.5621  BEST VAL Loss: 388.5621\n",
            "\n",
            "Epoch 424: Validation loss decreased (388.562134 --> 387.479095).\n",
            "\t Train_Loss: 145.0323 Val_Loss: 387.4791  BEST VAL Loss: 387.4791\n",
            "\n",
            "Epoch 425: Validation loss decreased (387.479095 --> 386.410217).\n",
            "\t Train_Loss: 144.8682 Val_Loss: 386.4102  BEST VAL Loss: 386.4102\n",
            "\n",
            "Epoch 426: Validation loss decreased (386.410217 --> 385.354340).\n",
            "\t Train_Loss: 144.7079 Val_Loss: 385.3543  BEST VAL Loss: 385.3543\n",
            "\n",
            "Epoch 427: Validation loss decreased (385.354340 --> 384.312225).\n",
            "\t Train_Loss: 144.5512 Val_Loss: 384.3122  BEST VAL Loss: 384.3122\n",
            "\n",
            "Epoch 428: Validation loss decreased (384.312225 --> 383.283264).\n",
            "\t Train_Loss: 144.3983 Val_Loss: 383.2833  BEST VAL Loss: 383.2833\n",
            "\n",
            "Epoch 429: Validation loss decreased (383.283264 --> 382.267426).\n",
            "\t Train_Loss: 144.2488 Val_Loss: 382.2674  BEST VAL Loss: 382.2674\n",
            "\n",
            "Epoch 430: Validation loss decreased (382.267426 --> 381.264862).\n",
            "\t Train_Loss: 144.1029 Val_Loss: 381.2649  BEST VAL Loss: 381.2649\n",
            "\n",
            "Epoch 431: Validation loss decreased (381.264862 --> 380.274750).\n",
            "\t Train_Loss: 143.9604 Val_Loss: 380.2747  BEST VAL Loss: 380.2747\n",
            "\n",
            "Epoch 432: Validation loss decreased (380.274750 --> 379.297333).\n",
            "\t Train_Loss: 143.8213 Val_Loss: 379.2973  BEST VAL Loss: 379.2973\n",
            "\n",
            "Epoch 433: Validation loss decreased (379.297333 --> 378.332764).\n",
            "\t Train_Loss: 143.6854 Val_Loss: 378.3328  BEST VAL Loss: 378.3328\n",
            "\n",
            "Epoch 434: Validation loss decreased (378.332764 --> 377.379974).\n",
            "\t Train_Loss: 143.5528 Val_Loss: 377.3800  BEST VAL Loss: 377.3800\n",
            "\n",
            "Epoch 435: Validation loss decreased (377.379974 --> 376.439453).\n",
            "\t Train_Loss: 143.4232 Val_Loss: 376.4395  BEST VAL Loss: 376.4395\n",
            "\n",
            "Epoch 436: Validation loss decreased (376.439453 --> 375.511108).\n",
            "\t Train_Loss: 143.2966 Val_Loss: 375.5111  BEST VAL Loss: 375.5111\n",
            "\n",
            "Epoch 437: Validation loss decreased (375.511108 --> 374.594635).\n",
            "\t Train_Loss: 143.1731 Val_Loss: 374.5946  BEST VAL Loss: 374.5946\n",
            "\n",
            "Epoch 438: Validation loss decreased (374.594635 --> 373.690002).\n",
            "\t Train_Loss: 143.0526 Val_Loss: 373.6900  BEST VAL Loss: 373.6900\n",
            "\n",
            "Epoch 439: Validation loss decreased (373.690002 --> 372.796631).\n",
            "\t Train_Loss: 142.9349 Val_Loss: 372.7966  BEST VAL Loss: 372.7966\n",
            "\n",
            "Epoch 440: Validation loss decreased (372.796631 --> 371.915039).\n",
            "\t Train_Loss: 142.8199 Val_Loss: 371.9150  BEST VAL Loss: 371.9150\n",
            "\n",
            "Epoch 441: Validation loss decreased (371.915039 --> 371.044342).\n",
            "\t Train_Loss: 142.7078 Val_Loss: 371.0443  BEST VAL Loss: 371.0443\n",
            "\n",
            "Epoch 442: Validation loss decreased (371.044342 --> 370.185516).\n",
            "\t Train_Loss: 142.5983 Val_Loss: 370.1855  BEST VAL Loss: 370.1855\n",
            "\n",
            "Epoch 443: Validation loss decreased (370.185516 --> 369.337280).\n",
            "\t Train_Loss: 142.4914 Val_Loss: 369.3373  BEST VAL Loss: 369.3373\n",
            "\n",
            "Epoch 444: Validation loss decreased (369.337280 --> 368.499847).\n",
            "\t Train_Loss: 142.3871 Val_Loss: 368.4998  BEST VAL Loss: 368.4998\n",
            "\n",
            "Epoch 445: Validation loss decreased (368.499847 --> 367.673431).\n",
            "\t Train_Loss: 142.2853 Val_Loss: 367.6734  BEST VAL Loss: 367.6734\n",
            "\n",
            "Epoch 446: Validation loss decreased (367.673431 --> 366.857178).\n",
            "\t Train_Loss: 142.1860 Val_Loss: 366.8572  BEST VAL Loss: 366.8572\n",
            "\n",
            "Epoch 447: Validation loss decreased (366.857178 --> 366.051849).\n",
            "\t Train_Loss: 142.0890 Val_Loss: 366.0518  BEST VAL Loss: 366.0518\n",
            "\n",
            "Epoch 448: Validation loss decreased (366.051849 --> 365.256927).\n",
            "\t Train_Loss: 141.9943 Val_Loss: 365.2569  BEST VAL Loss: 365.2569\n",
            "\n",
            "Epoch 449: Validation loss decreased (365.256927 --> 364.472260).\n",
            "\t Train_Loss: 141.9020 Val_Loss: 364.4723  BEST VAL Loss: 364.4723\n",
            "\n",
            "Epoch 450: Validation loss decreased (364.472260 --> 363.697479).\n",
            "\t Train_Loss: 141.8120 Val_Loss: 363.6975  BEST VAL Loss: 363.6975\n",
            "\n",
            "Epoch 451: Validation loss decreased (363.697479 --> 362.932861).\n",
            "\t Train_Loss: 141.7240 Val_Loss: 362.9329  BEST VAL Loss: 362.9329\n",
            "\n",
            "Epoch 452: Validation loss decreased (362.932861 --> 362.177795).\n",
            "\t Train_Loss: 141.6383 Val_Loss: 362.1778  BEST VAL Loss: 362.1778\n",
            "\n",
            "Epoch 453: Validation loss decreased (362.177795 --> 361.432831).\n",
            "\t Train_Loss: 141.5545 Val_Loss: 361.4328  BEST VAL Loss: 361.4328\n",
            "\n",
            "Epoch 454: Validation loss decreased (361.432831 --> 360.697510).\n",
            "\t Train_Loss: 141.4729 Val_Loss: 360.6975  BEST VAL Loss: 360.6975\n",
            "\n",
            "Epoch 455: Validation loss decreased (360.697510 --> 359.971527).\n",
            "\t Train_Loss: 141.3933 Val_Loss: 359.9715  BEST VAL Loss: 359.9715\n",
            "\n",
            "Epoch 456: Validation loss decreased (359.971527 --> 359.254700).\n",
            "\t Train_Loss: 141.3156 Val_Loss: 359.2547  BEST VAL Loss: 359.2547\n",
            "\n",
            "Epoch 457: Validation loss decreased (359.254700 --> 358.547363).\n",
            "\t Train_Loss: 141.2397 Val_Loss: 358.5474  BEST VAL Loss: 358.5474\n",
            "\n",
            "Epoch 458: Validation loss decreased (358.547363 --> 357.849274).\n",
            "\t Train_Loss: 141.1658 Val_Loss: 357.8493  BEST VAL Loss: 357.8493\n",
            "\n",
            "Epoch 459: Validation loss decreased (357.849274 --> 357.160400).\n",
            "\t Train_Loss: 141.0936 Val_Loss: 357.1604  BEST VAL Loss: 357.1604\n",
            "\n",
            "Epoch 460: Validation loss decreased (357.160400 --> 356.480469).\n",
            "\t Train_Loss: 141.0233 Val_Loss: 356.4805  BEST VAL Loss: 356.4805\n",
            "\n",
            "Epoch 461: Validation loss decreased (356.480469 --> 355.809021).\n",
            "\t Train_Loss: 140.9548 Val_Loss: 355.8090  BEST VAL Loss: 355.8090\n",
            "\n",
            "Epoch 462: Validation loss decreased (355.809021 --> 355.146393).\n",
            "\t Train_Loss: 140.8878 Val_Loss: 355.1464  BEST VAL Loss: 355.1464\n",
            "\n",
            "Epoch 463: Validation loss decreased (355.146393 --> 354.492676).\n",
            "\t Train_Loss: 140.8226 Val_Loss: 354.4927  BEST VAL Loss: 354.4927\n",
            "\n",
            "Epoch 464: Validation loss decreased (354.492676 --> 353.847229).\n",
            "\t Train_Loss: 140.7590 Val_Loss: 353.8472  BEST VAL Loss: 353.8472\n",
            "\n",
            "Epoch 465: Validation loss decreased (353.847229 --> 353.210358).\n",
            "\t Train_Loss: 140.6970 Val_Loss: 353.2104  BEST VAL Loss: 353.2104\n",
            "\n",
            "Epoch 466: Validation loss decreased (353.210358 --> 352.581543).\n",
            "\t Train_Loss: 140.6365 Val_Loss: 352.5815  BEST VAL Loss: 352.5815\n",
            "\n",
            "Epoch 467: Validation loss decreased (352.581543 --> 351.960938).\n",
            "\t Train_Loss: 140.5775 Val_Loss: 351.9609  BEST VAL Loss: 351.9609\n",
            "\n",
            "Epoch 468: Validation loss decreased (351.960938 --> 351.348572).\n",
            "\t Train_Loss: 140.5199 Val_Loss: 351.3486  BEST VAL Loss: 351.3486\n",
            "\n",
            "Epoch 469: Validation loss decreased (351.348572 --> 350.743958).\n",
            "\t Train_Loss: 140.4639 Val_Loss: 350.7440  BEST VAL Loss: 350.7440\n",
            "\n",
            "Epoch 470: Validation loss decreased (350.743958 --> 350.147705).\n",
            "\t Train_Loss: 140.4091 Val_Loss: 350.1477  BEST VAL Loss: 350.1477\n",
            "\n",
            "Epoch 471: Validation loss decreased (350.147705 --> 349.558929).\n",
            "\t Train_Loss: 140.3559 Val_Loss: 349.5589  BEST VAL Loss: 349.5589\n",
            "\n",
            "Epoch 472: Validation loss decreased (349.558929 --> 348.978180).\n",
            "\t Train_Loss: 140.3039 Val_Loss: 348.9782  BEST VAL Loss: 348.9782\n",
            "\n",
            "Epoch 473: Validation loss decreased (348.978180 --> 348.404510).\n",
            "\t Train_Loss: 140.2532 Val_Loss: 348.4045  BEST VAL Loss: 348.4045\n",
            "\n",
            "Epoch 474: Validation loss decreased (348.404510 --> 347.838959).\n",
            "\t Train_Loss: 140.2035 Val_Loss: 347.8390  BEST VAL Loss: 347.8390\n",
            "\n",
            "Epoch 475: Validation loss decreased (347.838959 --> 347.283081).\n",
            "\t Train_Loss: 140.1543 Val_Loss: 347.2831  BEST VAL Loss: 347.2831\n",
            "\n",
            "Epoch 476: Validation loss decreased (347.283081 --> 347.035858).\n",
            "\t Train_Loss: 140.0954 Val_Loss: 347.0359  BEST VAL Loss: 347.0359\n",
            "\n",
            "Epoch 477: Validation loss did not decrease\n",
            "\t Train_Loss: 139.5410 Val_Loss: 362.9410  BEST VAL Loss: 347.0359\n",
            "\n",
            "Epoch 478: Validation loss did not decrease\n",
            "\t Train_Loss: 136.9857 Val_Loss: 349.5437  BEST VAL Loss: 347.0359\n",
            "\n",
            "Epoch 479: Validation loss did not decrease\n",
            "\t Train_Loss: 136.5643 Val_Loss: 359.0014  BEST VAL Loss: 347.0359\n",
            "\n",
            "Epoch 480: Validation loss did not decrease\n",
            "\t Train_Loss: 134.5106 Val_Loss: 363.7475  BEST VAL Loss: 347.0359\n",
            "\n",
            "Epoch 481: Validation loss did not decrease\n",
            "\t Train_Loss: 133.4487 Val_Loss: 347.6232  BEST VAL Loss: 347.0359\n",
            "\n",
            "Epoch 482: Validation loss did not decrease\n",
            "\t Train_Loss: 132.8660 Val_Loss: 353.7204  BEST VAL Loss: 347.0359\n",
            "\n",
            "Epoch 483: Validation loss did not decrease\n",
            "\t Train_Loss: 129.2355 Val_Loss: 370.4583  BEST VAL Loss: 347.0359\n",
            "\n",
            "Epoch 484: Validation loss decreased (347.035858 --> 345.199432).\n",
            "\t Train_Loss: 131.2874 Val_Loss: 345.1994  BEST VAL Loss: 345.1994\n",
            "\n",
            "Epoch 485: Validation loss decreased (345.199432 --> 343.306732).\n",
            "\t Train_Loss: 128.8227 Val_Loss: 343.3067  BEST VAL Loss: 343.3067\n",
            "\n",
            "Epoch 486: Validation loss did not decrease\n",
            "\t Train_Loss: 128.2220 Val_Loss: 353.6920  BEST VAL Loss: 343.3067\n",
            "\n",
            "Epoch 487: Validation loss did not decrease\n",
            "\t Train_Loss: 125.2745 Val_Loss: 353.3609  BEST VAL Loss: 343.3067\n",
            "\n",
            "Epoch 488: Validation loss decreased (343.306732 --> 342.255096).\n",
            "\t Train_Loss: 124.8465 Val_Loss: 342.2551  BEST VAL Loss: 342.2551\n",
            "\n",
            "Epoch 489: Validation loss decreased (342.255096 --> 341.150635).\n",
            "\t Train_Loss: 124.1628 Val_Loss: 341.1506  BEST VAL Loss: 341.1506\n",
            "\n",
            "Epoch 490: Validation loss did not decrease\n",
            "\t Train_Loss: 123.6317 Val_Loss: 345.6472  BEST VAL Loss: 341.1506\n",
            "\n",
            "Epoch 491: Validation loss did not decrease\n",
            "\t Train_Loss: 122.1386 Val_Loss: 349.5491  BEST VAL Loss: 341.1506\n",
            "\n",
            "Epoch 492: Validation loss decreased (341.150635 --> 338.829437).\n",
            "\t Train_Loss: 122.9022 Val_Loss: 338.8294  BEST VAL Loss: 338.8294\n",
            "\n",
            "Epoch 493: Validation loss decreased (338.829437 --> 336.637421).\n",
            "\t Train_Loss: 121.2716 Val_Loss: 336.6374  BEST VAL Loss: 336.6374\n",
            "\n",
            "Epoch 494: Validation loss did not decrease\n",
            "\t Train_Loss: 121.8428 Val_Loss: 338.2962  BEST VAL Loss: 336.6374\n",
            "\n",
            "Epoch 495: Validation loss did not decrease\n",
            "\t Train_Loss: 119.3922 Val_Loss: 348.1617  BEST VAL Loss: 336.6374\n",
            "\n",
            "Epoch 496: Validation loss did not decrease\n",
            "\t Train_Loss: 116.6353 Val_Loss: 402.4029  BEST VAL Loss: 336.6374\n",
            "\n",
            "Epoch 497: Validation loss decreased (336.637421 --> 334.746918).\n",
            "\t Train_Loss: 131.1802 Val_Loss: 334.7469  BEST VAL Loss: 334.7469\n",
            "\n",
            "Epoch 498: Validation loss decreased (334.746918 --> 330.745209).\n",
            "\t Train_Loss: 120.3286 Val_Loss: 330.7452  BEST VAL Loss: 330.7452\n",
            "\n",
            "Epoch 499: Validation loss did not decrease\n",
            "\t Train_Loss: 120.8751 Val_Loss: 332.5455  BEST VAL Loss: 330.7452\n",
            "\n",
            "Epoch 500: Validation loss did not decrease\n",
            "\t Train_Loss: 116.4324 Val_Loss: 346.3131  BEST VAL Loss: 330.7452\n",
            "\n",
            "Epoch 501: Validation loss did not decrease\n",
            "\t Train_Loss: 122.0430 Val_Loss: 342.5054  BEST VAL Loss: 330.7452\n",
            "\n",
            "Epoch 502: Validation loss decreased (330.745209 --> 330.083221).\n",
            "\t Train_Loss: 121.0111 Val_Loss: 330.0832  BEST VAL Loss: 330.0832\n",
            "\n",
            "Epoch 503: Validation loss decreased (330.083221 --> 326.390869).\n",
            "\t Train_Loss: 117.0879 Val_Loss: 326.3909  BEST VAL Loss: 326.3909\n",
            "\n",
            "Epoch 504: Validation loss decreased (326.390869 --> 325.194031).\n",
            "\t Train_Loss: 119.0859 Val_Loss: 325.1940  BEST VAL Loss: 325.1940\n",
            "\n",
            "Epoch 505: Validation loss decreased (325.194031 --> 324.955170).\n",
            "\t Train_Loss: 119.6922 Val_Loss: 324.9552  BEST VAL Loss: 324.9552\n",
            "\n",
            "Epoch 506: Validation loss did not decrease\n",
            "\t Train_Loss: 116.9175 Val_Loss: 327.9591  BEST VAL Loss: 324.9552\n",
            "\n",
            "Epoch 507: Validation loss did not decrease\n",
            "\t Train_Loss: 114.9177 Val_Loss: 333.6747  BEST VAL Loss: 324.9552\n",
            "\n",
            "Epoch 508: Validation loss did not decrease\n",
            "\t Train_Loss: 114.7112 Val_Loss: 332.8213  BEST VAL Loss: 324.9552\n",
            "\n",
            "Epoch 509: Validation loss did not decrease\n",
            "\t Train_Loss: 111.4800 Val_Loss: 327.5971  BEST VAL Loss: 324.9552\n",
            "\n",
            "Epoch 510: Validation loss did not decrease\n",
            "\t Train_Loss: 107.9567 Val_Loss: 327.2418  BEST VAL Loss: 324.9552\n",
            "\n",
            "Epoch 511: Validation loss did not decrease\n",
            "\t Train_Loss: 108.1242 Val_Loss: 331.7850  BEST VAL Loss: 324.9552\n",
            "\n",
            "Epoch 512: Validation loss did not decrease\n",
            "\t Train_Loss: 110.7109 Val_Loss: 329.4700  BEST VAL Loss: 324.9552\n",
            "\n",
            "Epoch 513: Validation loss decreased (324.955170 --> 323.331390).\n",
            "\t Train_Loss: 110.0468 Val_Loss: 323.3314  BEST VAL Loss: 323.3314\n",
            "\n",
            "Epoch 514: Validation loss decreased (323.331390 --> 321.639313).\n",
            "\t Train_Loss: 106.6823 Val_Loss: 321.6393  BEST VAL Loss: 321.6393\n",
            "\n",
            "Epoch 515: Validation loss did not decrease\n",
            "\t Train_Loss: 104.7651 Val_Loss: 324.7415  BEST VAL Loss: 321.6393\n",
            "\n",
            "Epoch 516: Validation loss decreased (321.639313 --> 315.814850).\n",
            "\t Train_Loss: 105.8290 Val_Loss: 315.8148  BEST VAL Loss: 315.8148\n",
            "\n",
            "Epoch 517: Validation loss decreased (315.814850 --> 314.062256).\n",
            "\t Train_Loss: 104.3191 Val_Loss: 314.0623  BEST VAL Loss: 314.0623\n",
            "\n",
            "Epoch 518: Validation loss did not decrease\n",
            "\t Train_Loss: 105.4788 Val_Loss: 315.6837  BEST VAL Loss: 314.0623\n",
            "\n",
            "Epoch 519: Validation loss did not decrease\n",
            "\t Train_Loss: 99.6516 Val_Loss: 390.1400  BEST VAL Loss: 314.0623\n",
            "\n",
            "Epoch 520: Validation loss did not decrease\n",
            "\t Train_Loss: 123.7159 Val_Loss: 324.3045  BEST VAL Loss: 314.0623\n",
            "\n",
            "Epoch 521: Validation loss decreased (314.062256 --> 310.575470).\n",
            "\t Train_Loss: 99.6708 Val_Loss: 310.5755  BEST VAL Loss: 310.5755\n",
            "\n",
            "Epoch 522: Validation loss decreased (310.575470 --> 309.931610).\n",
            "\t Train_Loss: 101.1446 Val_Loss: 309.9316  BEST VAL Loss: 309.9316\n",
            "\n",
            "Epoch 523: Validation loss did not decrease\n",
            "\t Train_Loss: 101.4855 Val_Loss: 316.2587  BEST VAL Loss: 309.9316\n",
            "\n",
            "Epoch 524: Validation loss did not decrease\n",
            "\t Train_Loss: 102.2066 Val_Loss: 310.2143  BEST VAL Loss: 309.9316\n",
            "\n",
            "Epoch 525: Validation loss decreased (309.931610 --> 306.277100).\n",
            "\t Train_Loss: 99.2821 Val_Loss: 306.2771  BEST VAL Loss: 306.2771\n",
            "\n",
            "Epoch 526: Validation loss decreased (306.277100 --> 303.564423).\n",
            "\t Train_Loss: 96.7425 Val_Loss: 303.5644  BEST VAL Loss: 303.5644\n",
            "\n",
            "Epoch 527: Validation loss decreased (303.564423 --> 302.290039).\n",
            "\t Train_Loss: 96.3995 Val_Loss: 302.2900  BEST VAL Loss: 302.2900\n",
            "\n",
            "Epoch 528: Validation loss decreased (302.290039 --> 301.575104).\n",
            "\t Train_Loss: 95.7696 Val_Loss: 301.5751  BEST VAL Loss: 301.5751\n",
            "\n",
            "Epoch 529: Validation loss decreased (301.575104 --> 300.608734).\n",
            "\t Train_Loss: 96.4075 Val_Loss: 300.6087  BEST VAL Loss: 300.6087\n",
            "\n",
            "Epoch 530: Validation loss decreased (300.608734 --> 299.210785).\n",
            "\t Train_Loss: 96.1862 Val_Loss: 299.2108  BEST VAL Loss: 299.2108\n",
            "\n",
            "Epoch 531: Validation loss decreased (299.210785 --> 298.028656).\n",
            "\t Train_Loss: 95.0248 Val_Loss: 298.0287  BEST VAL Loss: 298.0287\n",
            "\n",
            "Epoch 532: Validation loss decreased (298.028656 --> 297.312836).\n",
            "\t Train_Loss: 94.1693 Val_Loss: 297.3128  BEST VAL Loss: 297.3128\n",
            "\n",
            "Epoch 533: Validation loss decreased (297.312836 --> 297.064209).\n",
            "\t Train_Loss: 93.0474 Val_Loss: 297.0642  BEST VAL Loss: 297.0642\n",
            "\n",
            "Epoch 534: Validation loss decreased (297.064209 --> 296.279022).\n",
            "\t Train_Loss: 92.8772 Val_Loss: 296.2790  BEST VAL Loss: 296.2790\n",
            "\n",
            "Epoch 535: Validation loss decreased (296.279022 --> 295.094696).\n",
            "\t Train_Loss: 92.1867 Val_Loss: 295.0947  BEST VAL Loss: 295.0947\n",
            "\n",
            "Epoch 536: Validation loss decreased (295.094696 --> 293.225922).\n",
            "\t Train_Loss: 92.0632 Val_Loss: 293.2259  BEST VAL Loss: 293.2259\n",
            "\n",
            "Epoch 537: Validation loss decreased (293.225922 --> 292.254517).\n",
            "\t Train_Loss: 93.4493 Val_Loss: 292.2545  BEST VAL Loss: 292.2545\n",
            "\n",
            "Epoch 538: Validation loss decreased (292.254517 --> 291.891418).\n",
            "\t Train_Loss: 91.5279 Val_Loss: 291.8914  BEST VAL Loss: 291.8914\n",
            "\n",
            "Epoch 539: Validation loss decreased (291.891418 --> 290.098297).\n",
            "\t Train_Loss: 92.5580 Val_Loss: 290.0983  BEST VAL Loss: 290.0983\n",
            "\n",
            "Epoch 540: Validation loss decreased (290.098297 --> 288.644562).\n",
            "\t Train_Loss: 89.8162 Val_Loss: 288.6446  BEST VAL Loss: 288.6446\n",
            "\n",
            "Epoch 541: Validation loss decreased (288.644562 --> 287.677399).\n",
            "\t Train_Loss: 90.6977 Val_Loss: 287.6774  BEST VAL Loss: 287.6774\n",
            "\n",
            "Epoch 542: Validation loss decreased (287.677399 --> 286.948242).\n",
            "\t Train_Loss: 88.8695 Val_Loss: 286.9482  BEST VAL Loss: 286.9482\n",
            "\n",
            "Epoch 543: Validation loss decreased (286.948242 --> 286.342926).\n",
            "\t Train_Loss: 87.9679 Val_Loss: 286.3429  BEST VAL Loss: 286.3429\n",
            "\n",
            "Epoch 544: Validation loss did not decrease\n",
            "\t Train_Loss: 82.4940 Val_Loss: 303.1367  BEST VAL Loss: 286.3429\n",
            "\n",
            "Epoch 545: Validation loss decreased (286.342926 --> 284.115631).\n",
            "\t Train_Loss: 87.9353 Val_Loss: 284.1156  BEST VAL Loss: 284.1156\n",
            "\n",
            "Epoch 546: Validation loss decreased (284.115631 --> 282.728729).\n",
            "\t Train_Loss: 85.6920 Val_Loss: 282.7287  BEST VAL Loss: 282.7287\n",
            "\n",
            "Epoch 547: Validation loss did not decrease\n",
            "\t Train_Loss: 83.2045 Val_Loss: 283.4196  BEST VAL Loss: 282.7287\n",
            "\n",
            "Epoch 548: Validation loss decreased (282.728729 --> 282.145691).\n",
            "\t Train_Loss: 85.0272 Val_Loss: 282.1457  BEST VAL Loss: 282.1457\n",
            "\n",
            "Epoch 549: Validation loss decreased (282.145691 --> 280.176575).\n",
            "\t Train_Loss: 83.7027 Val_Loss: 280.1766  BEST VAL Loss: 280.1766\n",
            "\n",
            "Epoch 550: Validation loss decreased (280.176575 --> 278.620087).\n",
            "\t Train_Loss: 79.7526 Val_Loss: 278.6201  BEST VAL Loss: 278.6201\n",
            "\n",
            "Epoch 551: Validation loss decreased (278.620087 --> 277.464813).\n",
            "\t Train_Loss: 78.0197 Val_Loss: 277.4648  BEST VAL Loss: 277.4648\n",
            "\n",
            "Epoch 552: Validation loss decreased (277.464813 --> 276.799774).\n",
            "\t Train_Loss: 78.1491 Val_Loss: 276.7998  BEST VAL Loss: 276.7998\n",
            "\n",
            "Epoch 553: Validation loss decreased (276.799774 --> 276.447723).\n",
            "\t Train_Loss: 77.4370 Val_Loss: 276.4477  BEST VAL Loss: 276.4477\n",
            "\n",
            "Epoch 554: Validation loss decreased (276.447723 --> 275.375336).\n",
            "\t Train_Loss: 77.5430 Val_Loss: 275.3753  BEST VAL Loss: 275.3753\n",
            "\n",
            "Epoch 555: Validation loss decreased (275.375336 --> 274.378448).\n",
            "\t Train_Loss: 76.3250 Val_Loss: 274.3784  BEST VAL Loss: 274.3784\n",
            "\n",
            "Epoch 556: Validation loss did not decrease\n",
            "\t Train_Loss: 73.5703 Val_Loss: 276.0011  BEST VAL Loss: 274.3784\n",
            "\n",
            "Epoch 557: Validation loss decreased (274.378448 --> 271.882294).\n",
            "\t Train_Loss: 74.6124 Val_Loss: 271.8823  BEST VAL Loss: 271.8823\n",
            "\n",
            "Epoch 558: Validation loss decreased (271.882294 --> 270.490570).\n",
            "\t Train_Loss: 73.4203 Val_Loss: 270.4906  BEST VAL Loss: 270.4906\n",
            "\n",
            "Epoch 559: Validation loss decreased (270.490570 --> 269.445312).\n",
            "\t Train_Loss: 71.8429 Val_Loss: 269.4453  BEST VAL Loss: 269.4453\n",
            "\n",
            "Epoch 560: Validation loss decreased (269.445312 --> 268.431335).\n",
            "\t Train_Loss: 71.8006 Val_Loss: 268.4313  BEST VAL Loss: 268.4313\n",
            "\n",
            "Epoch 561: Validation loss decreased (268.431335 --> 267.039124).\n",
            "\t Train_Loss: 72.3599 Val_Loss: 267.0391  BEST VAL Loss: 267.0391\n",
            "\n",
            "Epoch 562: Validation loss decreased (267.039124 --> 265.550049).\n",
            "\t Train_Loss: 71.8250 Val_Loss: 265.5500  BEST VAL Loss: 265.5500\n",
            "\n",
            "Epoch 563: Validation loss decreased (265.550049 --> 264.216644).\n",
            "\t Train_Loss: 70.8028 Val_Loss: 264.2166  BEST VAL Loss: 264.2166\n",
            "\n",
            "Epoch 564: Validation loss decreased (264.216644 --> 263.154480).\n",
            "\t Train_Loss: 69.6279 Val_Loss: 263.1545  BEST VAL Loss: 263.1545\n",
            "\n",
            "Epoch 565: Validation loss decreased (263.154480 --> 262.120605).\n",
            "\t Train_Loss: 69.1960 Val_Loss: 262.1206  BEST VAL Loss: 262.1206\n",
            "\n",
            "Epoch 566: Validation loss decreased (262.120605 --> 260.841553).\n",
            "\t Train_Loss: 68.1928 Val_Loss: 260.8416  BEST VAL Loss: 260.8416\n",
            "\n",
            "Epoch 567: Validation loss decreased (260.841553 --> 259.633728).\n",
            "\t Train_Loss: 68.7419 Val_Loss: 259.6337  BEST VAL Loss: 259.6337\n",
            "\n",
            "Epoch 568: Validation loss decreased (259.633728 --> 258.350555).\n",
            "\t Train_Loss: 68.2925 Val_Loss: 258.3506  BEST VAL Loss: 258.3506\n",
            "\n",
            "Epoch 569: Validation loss decreased (258.350555 --> 257.203522).\n",
            "\t Train_Loss: 67.6195 Val_Loss: 257.2035  BEST VAL Loss: 257.2035\n",
            "\n",
            "Epoch 570: Validation loss decreased (257.203522 --> 256.200317).\n",
            "\t Train_Loss: 67.4917 Val_Loss: 256.2003  BEST VAL Loss: 256.2003\n",
            "\n",
            "Epoch 571: Validation loss decreased (256.200317 --> 255.158737).\n",
            "\t Train_Loss: 66.8698 Val_Loss: 255.1587  BEST VAL Loss: 255.1587\n",
            "\n",
            "Epoch 572: Validation loss decreased (255.158737 --> 253.979660).\n",
            "\t Train_Loss: 66.8257 Val_Loss: 253.9797  BEST VAL Loss: 253.9797\n",
            "\n",
            "Epoch 573: Validation loss decreased (253.979660 --> 252.829468).\n",
            "\t Train_Loss: 65.7880 Val_Loss: 252.8295  BEST VAL Loss: 252.8295\n",
            "\n",
            "Epoch 574: Validation loss decreased (252.829468 --> 251.884811).\n",
            "\t Train_Loss: 65.3924 Val_Loss: 251.8848  BEST VAL Loss: 251.8848\n",
            "\n",
            "Epoch 575: Validation loss decreased (251.884811 --> 251.050156).\n",
            "\t Train_Loss: 64.1706 Val_Loss: 251.0502  BEST VAL Loss: 251.0502\n",
            "\n",
            "Epoch 576: Validation loss decreased (251.050156 --> 249.957504).\n",
            "\t Train_Loss: 64.1169 Val_Loss: 249.9575  BEST VAL Loss: 249.9575\n",
            "\n",
            "Epoch 577: Validation loss decreased (249.957504 --> 248.744827).\n",
            "\t Train_Loss: 63.4167 Val_Loss: 248.7448  BEST VAL Loss: 248.7448\n",
            "\n",
            "Epoch 578: Validation loss decreased (248.744827 --> 247.730957).\n",
            "\t Train_Loss: 63.3041 Val_Loss: 247.7310  BEST VAL Loss: 247.7310\n",
            "\n",
            "Epoch 579: Validation loss decreased (247.730957 --> 246.961472).\n",
            "\t Train_Loss: 62.9832 Val_Loss: 246.9615  BEST VAL Loss: 246.9615\n",
            "\n",
            "Epoch 580: Validation loss decreased (246.961472 --> 246.104034).\n",
            "\t Train_Loss: 62.4462 Val_Loss: 246.1040  BEST VAL Loss: 246.1040\n",
            "\n",
            "Epoch 581: Validation loss decreased (246.104034 --> 244.929688).\n",
            "\t Train_Loss: 62.0838 Val_Loss: 244.9297  BEST VAL Loss: 244.9297\n",
            "\n",
            "Epoch 582: Validation loss decreased (244.929688 --> 243.941208).\n",
            "\t Train_Loss: 61.5898 Val_Loss: 243.9412  BEST VAL Loss: 243.9412\n",
            "\n",
            "Epoch 583: Validation loss decreased (243.941208 --> 243.167633).\n",
            "\t Train_Loss: 61.3153 Val_Loss: 243.1676  BEST VAL Loss: 243.1676\n",
            "\n",
            "Epoch 584: Validation loss decreased (243.167633 --> 242.011093).\n",
            "\t Train_Loss: 60.9620 Val_Loss: 242.0111  BEST VAL Loss: 242.0111\n",
            "\n",
            "Epoch 585: Validation loss decreased (242.011093 --> 240.697800).\n",
            "\t Train_Loss: 60.5091 Val_Loss: 240.6978  BEST VAL Loss: 240.6978\n",
            "\n",
            "Epoch 586: Validation loss decreased (240.697800 --> 239.629913).\n",
            "\t Train_Loss: 60.0290 Val_Loss: 239.6299  BEST VAL Loss: 239.6299\n",
            "\n",
            "Epoch 587: Validation loss decreased (239.629913 --> 238.516586).\n",
            "\t Train_Loss: 59.5816 Val_Loss: 238.5166  BEST VAL Loss: 238.5166\n",
            "\n",
            "Epoch 588: Validation loss decreased (238.516586 --> 237.319748).\n",
            "\t Train_Loss: 59.1812 Val_Loss: 237.3197  BEST VAL Loss: 237.3197\n",
            "\n",
            "Epoch 589: Validation loss decreased (237.319748 --> 236.265244).\n",
            "\t Train_Loss: 58.6792 Val_Loss: 236.2652  BEST VAL Loss: 236.2652\n",
            "\n",
            "Epoch 590: Validation loss decreased (236.265244 --> 235.424850).\n",
            "\t Train_Loss: 58.2808 Val_Loss: 235.4249  BEST VAL Loss: 235.4249\n",
            "\n",
            "Epoch 591: Validation loss decreased (235.424850 --> 234.491501).\n",
            "\t Train_Loss: 57.8724 Val_Loss: 234.4915  BEST VAL Loss: 234.4915\n",
            "\n",
            "Epoch 592: Validation loss decreased (234.491501 --> 233.416092).\n",
            "\t Train_Loss: 57.3826 Val_Loss: 233.4161  BEST VAL Loss: 233.4161\n",
            "\n",
            "Epoch 593: Validation loss decreased (233.416092 --> 232.279709).\n",
            "\t Train_Loss: 56.9512 Val_Loss: 232.2797  BEST VAL Loss: 232.2797\n",
            "\n",
            "Epoch 594: Validation loss decreased (232.279709 --> 231.181061).\n",
            "\t Train_Loss: 56.5163 Val_Loss: 231.1811  BEST VAL Loss: 231.1811\n",
            "\n",
            "Epoch 595: Validation loss decreased (231.181061 --> 230.300201).\n",
            "\t Train_Loss: 56.1972 Val_Loss: 230.3002  BEST VAL Loss: 230.3002\n",
            "\n",
            "Epoch 596: Validation loss decreased (230.300201 --> 229.331985).\n",
            "\t Train_Loss: 55.8733 Val_Loss: 229.3320  BEST VAL Loss: 229.3320\n",
            "\n",
            "Epoch 597: Validation loss decreased (229.331985 --> 228.229721).\n",
            "\t Train_Loss: 55.5698 Val_Loss: 228.2297  BEST VAL Loss: 228.2297\n",
            "\n",
            "Epoch 598: Validation loss decreased (228.229721 --> 227.304108).\n",
            "\t Train_Loss: 55.2116 Val_Loss: 227.3041  BEST VAL Loss: 227.3041\n",
            "\n",
            "Epoch 599: Validation loss decreased (227.304108 --> 226.485580).\n",
            "\t Train_Loss: 54.8241 Val_Loss: 226.4856  BEST VAL Loss: 226.4856\n",
            "\n",
            "Epoch 600: Validation loss decreased (226.485580 --> 225.547394).\n",
            "\t Train_Loss: 54.5068 Val_Loss: 225.5474  BEST VAL Loss: 225.5474\n",
            "\n",
            "Epoch 601: Validation loss decreased (225.547394 --> 224.658600).\n",
            "\t Train_Loss: 54.1238 Val_Loss: 224.6586  BEST VAL Loss: 224.6586\n",
            "\n",
            "Epoch 602: Validation loss decreased (224.658600 --> 223.763672).\n",
            "\t Train_Loss: 53.8406 Val_Loss: 223.7637  BEST VAL Loss: 223.7637\n",
            "\n",
            "Epoch 603: Validation loss decreased (223.763672 --> 222.829056).\n",
            "\t Train_Loss: 53.5304 Val_Loss: 222.8291  BEST VAL Loss: 222.8291\n",
            "\n",
            "Epoch 604: Validation loss decreased (222.829056 --> 221.941895).\n",
            "\t Train_Loss: 53.2629 Val_Loss: 221.9419  BEST VAL Loss: 221.9419\n",
            "\n",
            "Epoch 605: Validation loss decreased (221.941895 --> 220.982178).\n",
            "\t Train_Loss: 52.8964 Val_Loss: 220.9822  BEST VAL Loss: 220.9822\n",
            "\n",
            "Epoch 606: Validation loss decreased (220.982178 --> 220.007278).\n",
            "\t Train_Loss: 52.5728 Val_Loss: 220.0073  BEST VAL Loss: 220.0073\n",
            "\n",
            "Epoch 607: Validation loss decreased (220.007278 --> 219.114304).\n",
            "\t Train_Loss: 52.2645 Val_Loss: 219.1143  BEST VAL Loss: 219.1143\n",
            "\n",
            "Epoch 608: Validation loss decreased (219.114304 --> 218.205368).\n",
            "\t Train_Loss: 51.9962 Val_Loss: 218.2054  BEST VAL Loss: 218.2054\n",
            "\n",
            "Epoch 609: Validation loss decreased (218.205368 --> 217.318115).\n",
            "\t Train_Loss: 51.7014 Val_Loss: 217.3181  BEST VAL Loss: 217.3181\n",
            "\n",
            "Epoch 610: Validation loss decreased (217.318115 --> 216.433014).\n",
            "\t Train_Loss: 51.4098 Val_Loss: 216.4330  BEST VAL Loss: 216.4330\n",
            "\n",
            "Epoch 611: Validation loss decreased (216.433014 --> 215.502441).\n",
            "\t Train_Loss: 51.1150 Val_Loss: 215.5024  BEST VAL Loss: 215.5024\n",
            "\n",
            "Epoch 612: Validation loss decreased (215.502441 --> 214.659775).\n",
            "\t Train_Loss: 50.8053 Val_Loss: 214.6598  BEST VAL Loss: 214.6598\n",
            "\n",
            "Epoch 613: Validation loss decreased (214.659775 --> 213.827148).\n",
            "\t Train_Loss: 50.5232 Val_Loss: 213.8271  BEST VAL Loss: 213.8271\n",
            "\n",
            "Epoch 614: Validation loss decreased (213.827148 --> 212.974365).\n",
            "\t Train_Loss: 50.2427 Val_Loss: 212.9744  BEST VAL Loss: 212.9744\n",
            "\n",
            "Epoch 615: Validation loss decreased (212.974365 --> 212.119247).\n",
            "\t Train_Loss: 49.9818 Val_Loss: 212.1192  BEST VAL Loss: 212.1192\n",
            "\n",
            "Epoch 616: Validation loss decreased (212.119247 --> 211.252426).\n",
            "\t Train_Loss: 49.6862 Val_Loss: 211.2524  BEST VAL Loss: 211.2524\n",
            "\n",
            "Epoch 617: Validation loss decreased (211.252426 --> 210.491074).\n",
            "\t Train_Loss: 49.4046 Val_Loss: 210.4911  BEST VAL Loss: 210.4911\n",
            "\n",
            "Epoch 618: Validation loss decreased (210.491074 --> 209.775146).\n",
            "\t Train_Loss: 49.1180 Val_Loss: 209.7751  BEST VAL Loss: 209.7751\n",
            "\n",
            "Epoch 619: Validation loss decreased (209.775146 --> 209.048340).\n",
            "\t Train_Loss: 48.8521 Val_Loss: 209.0483  BEST VAL Loss: 209.0483\n",
            "\n",
            "Epoch 620: Validation loss decreased (209.048340 --> 208.319138).\n",
            "\t Train_Loss: 48.5940 Val_Loss: 208.3191  BEST VAL Loss: 208.3191\n",
            "\n",
            "Epoch 621: Validation loss decreased (208.319138 --> 207.450195).\n",
            "\t Train_Loss: 48.3347 Val_Loss: 207.4502  BEST VAL Loss: 207.4502\n",
            "\n",
            "Epoch 622: Validation loss decreased (207.450195 --> 206.713577).\n",
            "\t Train_Loss: 48.0772 Val_Loss: 206.7136  BEST VAL Loss: 206.7136\n",
            "\n",
            "Epoch 623: Validation loss decreased (206.713577 --> 205.788681).\n",
            "\t Train_Loss: 47.8292 Val_Loss: 205.7887  BEST VAL Loss: 205.7887\n",
            "\n",
            "Epoch 624: Validation loss decreased (205.788681 --> 205.146347).\n",
            "\t Train_Loss: 47.5928 Val_Loss: 205.1463  BEST VAL Loss: 205.1463\n",
            "\n",
            "Epoch 625: Validation loss decreased (205.146347 --> 204.191162).\n",
            "\t Train_Loss: 47.3666 Val_Loss: 204.1912  BEST VAL Loss: 204.1912\n",
            "\n",
            "Epoch 626: Validation loss decreased (204.191162 --> 203.635132).\n",
            "\t Train_Loss: 47.1649 Val_Loss: 203.6351  BEST VAL Loss: 203.6351\n",
            "\n",
            "Epoch 627: Validation loss decreased (203.635132 --> 202.542374).\n",
            "\t Train_Loss: 46.9972 Val_Loss: 202.5424  BEST VAL Loss: 202.5424\n",
            "\n",
            "Epoch 628: Validation loss decreased (202.542374 --> 202.099960).\n",
            "\t Train_Loss: 46.8347 Val_Loss: 202.1000  BEST VAL Loss: 202.1000\n",
            "\n",
            "Epoch 629: Validation loss decreased (202.099960 --> 201.005844).\n",
            "\t Train_Loss: 46.5700 Val_Loss: 201.0058  BEST VAL Loss: 201.0058\n",
            "\n",
            "Epoch 630: Validation loss decreased (201.005844 --> 200.554367).\n",
            "\t Train_Loss: 46.2430 Val_Loss: 200.5544  BEST VAL Loss: 200.5544\n",
            "\n",
            "Epoch 631: Validation loss decreased (200.554367 --> 199.701202).\n",
            "\t Train_Loss: 45.8711 Val_Loss: 199.7012  BEST VAL Loss: 199.7012\n",
            "\n",
            "Epoch 632: Validation loss decreased (199.701202 --> 199.041321).\n",
            "\t Train_Loss: 45.5518 Val_Loss: 199.0413  BEST VAL Loss: 199.0413\n",
            "\n",
            "Epoch 633: Validation loss decreased (199.041321 --> 198.396500).\n",
            "\t Train_Loss: 45.2943 Val_Loss: 198.3965  BEST VAL Loss: 198.3965\n",
            "\n",
            "Epoch 634: Validation loss decreased (198.396500 --> 197.453079).\n",
            "\t Train_Loss: 45.0910 Val_Loss: 197.4531  BEST VAL Loss: 197.4531\n",
            "\n",
            "Epoch 635: Validation loss decreased (197.453079 --> 197.112411).\n",
            "\t Train_Loss: 44.9487 Val_Loss: 197.1124  BEST VAL Loss: 197.1124\n",
            "\n",
            "Epoch 636: Validation loss decreased (197.112411 --> 195.852951).\n",
            "\t Train_Loss: 44.8827 Val_Loss: 195.8530  BEST VAL Loss: 195.8530\n",
            "\n",
            "Epoch 637: Validation loss decreased (195.852951 --> 195.839188).\n",
            "\t Train_Loss: 45.0528 Val_Loss: 195.8392  BEST VAL Loss: 195.8392\n",
            "\n",
            "Epoch 638: Validation loss decreased (195.839188 --> 194.137070).\n",
            "\t Train_Loss: 45.0154 Val_Loss: 194.1371  BEST VAL Loss: 194.1371\n",
            "\n",
            "Epoch 639: Validation loss decreased (194.137070 --> 193.731812).\n",
            "\t Train_Loss: 44.9359 Val_Loss: 193.7318  BEST VAL Loss: 193.7318\n",
            "\n",
            "Epoch 640: Validation loss decreased (193.731812 --> 192.880890).\n",
            "\t Train_Loss: 44.0047 Val_Loss: 192.8809  BEST VAL Loss: 192.8809\n",
            "\n",
            "Epoch 641: Validation loss decreased (192.880890 --> 192.100250).\n",
            "\t Train_Loss: 43.5292 Val_Loss: 192.1003  BEST VAL Loss: 192.1003\n",
            "\n",
            "Epoch 642: Validation loss did not decrease\n",
            "\t Train_Loss: 43.6664 Val_Loss: 192.2985  BEST VAL Loss: 192.1003\n",
            "\n",
            "Epoch 643: Validation loss decreased (192.100250 --> 190.840378).\n",
            "\t Train_Loss: 43.6459 Val_Loss: 190.8404  BEST VAL Loss: 190.8404\n",
            "\n",
            "Epoch 644: Validation loss decreased (190.840378 --> 190.548874).\n",
            "\t Train_Loss: 43.3389 Val_Loss: 190.5489  BEST VAL Loss: 190.5489\n",
            "\n",
            "Epoch 645: Validation loss decreased (190.548874 --> 189.753159).\n",
            "\t Train_Loss: 42.7077 Val_Loss: 189.7532  BEST VAL Loss: 189.7532\n",
            "\n",
            "Epoch 646: Validation loss decreased (189.753159 --> 188.727295).\n",
            "\t Train_Loss: 42.4517 Val_Loss: 188.7273  BEST VAL Loss: 188.7273\n",
            "\n",
            "Epoch 647: Validation loss decreased (188.727295 --> 188.656525).\n",
            "\t Train_Loss: 42.5250 Val_Loss: 188.6565  BEST VAL Loss: 188.6565\n",
            "\n",
            "Epoch 648: Validation loss decreased (188.656525 --> 187.356674).\n",
            "\t Train_Loss: 42.3960 Val_Loss: 187.3567  BEST VAL Loss: 187.3567\n",
            "\n",
            "Epoch 649: Validation loss decreased (187.356674 --> 186.866959).\n",
            "\t Train_Loss: 42.0259 Val_Loss: 186.8670  BEST VAL Loss: 186.8670\n",
            "\n",
            "Epoch 650: Validation loss decreased (186.866959 --> 186.325592).\n",
            "\t Train_Loss: 41.5798 Val_Loss: 186.3256  BEST VAL Loss: 186.3256\n",
            "\n",
            "Epoch 651: Validation loss decreased (186.325592 --> 185.280869).\n",
            "\t Train_Loss: 41.4805 Val_Loss: 185.2809  BEST VAL Loss: 185.2809\n",
            "\n",
            "Epoch 652: Validation loss decreased (185.280869 --> 185.122269).\n",
            "\t Train_Loss: 41.4946 Val_Loss: 185.1223  BEST VAL Loss: 185.1223\n",
            "\n",
            "Epoch 653: Validation loss decreased (185.122269 --> 184.130112).\n",
            "\t Train_Loss: 41.1743 Val_Loss: 184.1301  BEST VAL Loss: 184.1301\n",
            "\n",
            "Epoch 654: Validation loss decreased (184.130112 --> 183.625931).\n",
            "\t Train_Loss: 40.7851 Val_Loss: 183.6259  BEST VAL Loss: 183.6259\n",
            "\n",
            "Epoch 655: Validation loss decreased (183.625931 --> 183.328644).\n",
            "\t Train_Loss: 40.5309 Val_Loss: 183.3286  BEST VAL Loss: 183.3286\n",
            "\n",
            "Epoch 656: Validation loss decreased (183.328644 --> 182.216232).\n",
            "\t Train_Loss: 40.4458 Val_Loss: 182.2162  BEST VAL Loss: 182.2162\n",
            "\n",
            "Epoch 657: Validation loss decreased (182.216232 --> 181.929001).\n",
            "\t Train_Loss: 40.3991 Val_Loss: 181.9290  BEST VAL Loss: 181.9290\n",
            "\n",
            "Epoch 658: Validation loss decreased (181.929001 --> 180.780258).\n",
            "\t Train_Loss: 40.1486 Val_Loss: 180.7803  BEST VAL Loss: 180.7803\n",
            "\n",
            "Epoch 659: Validation loss decreased (180.780258 --> 180.342636).\n",
            "\t Train_Loss: 39.8538 Val_Loss: 180.3426  BEST VAL Loss: 180.3426\n",
            "\n",
            "Epoch 660: Validation loss decreased (180.342636 --> 179.847198).\n",
            "\t Train_Loss: 39.5200 Val_Loss: 179.8472  BEST VAL Loss: 179.8472\n",
            "\n",
            "Epoch 661: Validation loss decreased (179.847198 --> 179.140305).\n",
            "\t Train_Loss: 39.3203 Val_Loss: 179.1403  BEST VAL Loss: 179.1403\n",
            "\n",
            "Epoch 662: Validation loss decreased (179.140305 --> 179.057205).\n",
            "\t Train_Loss: 39.2701 Val_Loss: 179.0572  BEST VAL Loss: 179.0572\n",
            "\n",
            "Epoch 663: Validation loss decreased (179.057205 --> 177.853058).\n",
            "\t Train_Loss: 39.2020 Val_Loss: 177.8531  BEST VAL Loss: 177.8531\n",
            "\n",
            "Epoch 664: Validation loss decreased (177.853058 --> 177.773560).\n",
            "\t Train_Loss: 39.1697 Val_Loss: 177.7736  BEST VAL Loss: 177.7736\n",
            "\n",
            "Epoch 665: Validation loss decreased (177.773560 --> 176.674622).\n",
            "\t Train_Loss: 38.8824 Val_Loss: 176.6746  BEST VAL Loss: 176.6746\n",
            "\n",
            "Epoch 666: Validation loss decreased (176.674622 --> 176.374832).\n",
            "\t Train_Loss: 38.5205 Val_Loss: 176.3748  BEST VAL Loss: 176.3748\n",
            "\n",
            "Epoch 667: Validation loss decreased (176.374832 --> 175.752731).\n",
            "\t Train_Loss: 38.2347 Val_Loss: 175.7527  BEST VAL Loss: 175.7527\n",
            "\n",
            "Epoch 668: Validation loss decreased (175.752731 --> 175.314819).\n",
            "\t Train_Loss: 37.9453 Val_Loss: 175.3148  BEST VAL Loss: 175.3148\n",
            "\n",
            "Epoch 669: Validation loss decreased (175.314819 --> 174.867783).\n",
            "\t Train_Loss: 37.7713 Val_Loss: 174.8678  BEST VAL Loss: 174.8678\n",
            "\n",
            "Epoch 670: Validation loss decreased (174.867783 --> 174.287460).\n",
            "\t Train_Loss: 37.5702 Val_Loss: 174.2875  BEST VAL Loss: 174.2875\n",
            "\n",
            "Epoch 671: Validation loss decreased (174.287460 --> 173.748581).\n",
            "\t Train_Loss: 37.3877 Val_Loss: 173.7486  BEST VAL Loss: 173.7486\n",
            "\n",
            "Epoch 672: Validation loss decreased (173.748581 --> 173.179382).\n",
            "\t Train_Loss: 37.2032 Val_Loss: 173.1794  BEST VAL Loss: 173.1794\n",
            "\n",
            "Epoch 673: Validation loss decreased (173.179382 --> 172.759399).\n",
            "\t Train_Loss: 37.0003 Val_Loss: 172.7594  BEST VAL Loss: 172.7594\n",
            "\n",
            "Epoch 674: Validation loss decreased (172.759399 --> 172.397537).\n",
            "\t Train_Loss: 36.8438 Val_Loss: 172.3975  BEST VAL Loss: 172.3975\n",
            "\n",
            "Epoch 675: Validation loss decreased (172.397537 --> 172.021957).\n",
            "\t Train_Loss: 36.6251 Val_Loss: 172.0220  BEST VAL Loss: 172.0220\n",
            "\n",
            "Epoch 676: Validation loss decreased (172.021957 --> 171.243546).\n",
            "\t Train_Loss: 36.4881 Val_Loss: 171.2435  BEST VAL Loss: 171.2435\n",
            "\n",
            "Epoch 677: Validation loss decreased (171.243546 --> 170.962479).\n",
            "\t Train_Loss: 36.3533 Val_Loss: 170.9625  BEST VAL Loss: 170.9625\n",
            "\n",
            "Epoch 678: Validation loss decreased (170.962479 --> 169.823257).\n",
            "\t Train_Loss: 36.5477 Val_Loss: 169.8233  BEST VAL Loss: 169.8233\n",
            "\n",
            "Epoch 679: Validation loss did not decrease\n",
            "\t Train_Loss: 37.8492 Val_Loss: 170.9094  BEST VAL Loss: 169.8233\n",
            "\n",
            "Epoch 680: Validation loss decreased (169.823257 --> 168.143951).\n",
            "\t Train_Loss: 40.5072 Val_Loss: 168.1440  BEST VAL Loss: 168.1440\n",
            "\n",
            "Epoch 681: Validation loss decreased (168.143951 --> 167.831924).\n",
            "\t Train_Loss: 41.8594 Val_Loss: 167.8319  BEST VAL Loss: 167.8319\n",
            "\n",
            "Epoch 682: Validation loss did not decrease\n",
            "\t Train_Loss: 35.6878 Val_Loss: 168.9212  BEST VAL Loss: 167.8319\n",
            "\n",
            "Epoch 683: Validation loss decreased (167.831924 --> 166.216629).\n",
            "\t Train_Loss: 41.8638 Val_Loss: 166.2166  BEST VAL Loss: 166.2166\n",
            "\n",
            "Epoch 684: Validation loss decreased (166.216629 --> 165.395126).\n",
            "\t Train_Loss: 37.1120 Val_Loss: 165.3951  BEST VAL Loss: 165.3951\n",
            "\n",
            "Epoch 685: Validation loss decreased (165.395126 --> 165.248505).\n",
            "\t Train_Loss: 38.1459 Val_Loss: 165.2485  BEST VAL Loss: 165.2485\n",
            "\n",
            "Epoch 686: Validation loss decreased (165.248505 --> 164.883316).\n",
            "\t Train_Loss: 37.4303 Val_Loss: 164.8833  BEST VAL Loss: 164.8833\n",
            "\n",
            "Epoch 687: Validation loss decreased (164.883316 --> 164.280075).\n",
            "\t Train_Loss: 36.5431 Val_Loss: 164.2801  BEST VAL Loss: 164.2801\n",
            "\n",
            "Epoch 688: Validation loss did not decrease\n",
            "\t Train_Loss: 36.6853 Val_Loss: 164.4561  BEST VAL Loss: 164.2801\n",
            "\n",
            "Epoch 689: Validation loss did not decrease\n",
            "\t Train_Loss: 36.1376 Val_Loss: 165.7170  BEST VAL Loss: 164.2801\n",
            "\n",
            "Epoch 690: Validation loss did not decrease\n",
            "\t Train_Loss: 35.6206 Val_Loss: 165.6124  BEST VAL Loss: 164.2801\n",
            "\n",
            "Epoch 691: Validation loss decreased (164.280075 --> 163.830276).\n",
            "\t Train_Loss: 35.2156 Val_Loss: 163.8303  BEST VAL Loss: 163.8303\n",
            "\n",
            "Epoch 692: Validation loss decreased (163.830276 --> 163.468796).\n",
            "\t Train_Loss: 35.3781 Val_Loss: 163.4688  BEST VAL Loss: 163.4688\n",
            "\n",
            "Epoch 693: Validation loss did not decrease\n",
            "\t Train_Loss: 34.8046 Val_Loss: 164.6943  BEST VAL Loss: 163.4688\n",
            "\n",
            "Epoch 694: Validation loss did not decrease\n",
            "\t Train_Loss: 34.7567 Val_Loss: 163.6066  BEST VAL Loss: 163.4688\n",
            "\n",
            "Epoch 695: Validation loss decreased (163.468796 --> 162.130051).\n",
            "\t Train_Loss: 33.9544 Val_Loss: 162.1301  BEST VAL Loss: 162.1301\n",
            "\n",
            "Epoch 696: Validation loss decreased (162.130051 --> 161.362289).\n",
            "\t Train_Loss: 34.3947 Val_Loss: 161.3623  BEST VAL Loss: 161.3623\n",
            "\n",
            "Epoch 697: Validation loss decreased (161.362289 --> 161.238235).\n",
            "\t Train_Loss: 33.6036 Val_Loss: 161.2382  BEST VAL Loss: 161.2382\n",
            "\n",
            "Epoch 698: Validation loss decreased (161.238235 --> 160.401291).\n",
            "\t Train_Loss: 34.1730 Val_Loss: 160.4013  BEST VAL Loss: 160.4013\n",
            "\n",
            "Epoch 699: Validation loss decreased (160.401291 --> 159.917953).\n",
            "\t Train_Loss: 33.1431 Val_Loss: 159.9180  BEST VAL Loss: 159.9180\n",
            "\n",
            "Epoch 700: Validation loss did not decrease\n",
            "\t Train_Loss: 33.6265 Val_Loss: 160.1777  BEST VAL Loss: 159.9180\n",
            "\n",
            "Epoch 701: Validation loss did not decrease\n",
            "\t Train_Loss: 32.7597 Val_Loss: 160.9814  BEST VAL Loss: 159.9180\n",
            "\n",
            "Epoch 702: Validation loss decreased (159.917953 --> 159.854767).\n",
            "\t Train_Loss: 33.3057 Val_Loss: 159.8548  BEST VAL Loss: 159.8548\n",
            "\n",
            "Epoch 703: Validation loss decreased (159.854767 --> 159.125610).\n",
            "\t Train_Loss: 32.4660 Val_Loss: 159.1256  BEST VAL Loss: 159.1256\n",
            "\n",
            "Epoch 704: Validation loss decreased (159.125610 --> 158.992340).\n",
            "\t Train_Loss: 32.8463 Val_Loss: 158.9923  BEST VAL Loss: 158.9923\n",
            "\n",
            "Epoch 705: Validation loss decreased (158.992340 --> 158.426270).\n",
            "\t Train_Loss: 32.2502 Val_Loss: 158.4263  BEST VAL Loss: 158.4263\n",
            "\n",
            "Epoch 706: Validation loss decreased (158.426270 --> 157.389221).\n",
            "\t Train_Loss: 32.2622 Val_Loss: 157.3892  BEST VAL Loss: 157.3892\n",
            "\n",
            "Epoch 707: Validation loss decreased (157.389221 --> 156.829895).\n",
            "\t Train_Loss: 32.0348 Val_Loss: 156.8299  BEST VAL Loss: 156.8299\n",
            "\n",
            "Epoch 708: Validation loss decreased (156.829895 --> 156.670746).\n",
            "\t Train_Loss: 31.8079 Val_Loss: 156.6707  BEST VAL Loss: 156.6707\n",
            "\n",
            "Epoch 709: Validation loss decreased (156.670746 --> 156.410507).\n",
            "\t Train_Loss: 31.7769 Val_Loss: 156.4105  BEST VAL Loss: 156.4105\n",
            "\n",
            "Epoch 710: Validation loss decreased (156.410507 --> 156.195099).\n",
            "\t Train_Loss: 31.3961 Val_Loss: 156.1951  BEST VAL Loss: 156.1951\n",
            "\n",
            "Epoch 711: Validation loss decreased (156.195099 --> 156.106552).\n",
            "\t Train_Loss: 31.4372 Val_Loss: 156.1066  BEST VAL Loss: 156.1066\n",
            "\n",
            "Epoch 712: Validation loss decreased (156.106552 --> 155.938797).\n",
            "\t Train_Loss: 31.0562 Val_Loss: 155.9388  BEST VAL Loss: 155.9388\n",
            "\n",
            "Epoch 713: Validation loss decreased (155.938797 --> 155.077438).\n",
            "\t Train_Loss: 31.1151 Val_Loss: 155.0774  BEST VAL Loss: 155.0774\n",
            "\n",
            "Epoch 714: Validation loss decreased (155.077438 --> 154.369217).\n",
            "\t Train_Loss: 30.7615 Val_Loss: 154.3692  BEST VAL Loss: 154.3692\n",
            "\n",
            "Epoch 715: Validation loss decreased (154.369217 --> 153.967575).\n",
            "\t Train_Loss: 30.7254 Val_Loss: 153.9676  BEST VAL Loss: 153.9676\n",
            "\n",
            "Epoch 716: Validation loss decreased (153.967575 --> 153.523590).\n",
            "\t Train_Loss: 30.4836 Val_Loss: 153.5236  BEST VAL Loss: 153.5236\n",
            "\n",
            "Epoch 717: Validation loss decreased (153.523590 --> 153.015839).\n",
            "\t Train_Loss: 30.3203 Val_Loss: 153.0158  BEST VAL Loss: 153.0158\n",
            "\n",
            "Epoch 718: Validation loss decreased (153.015839 --> 152.568604).\n",
            "\t Train_Loss: 30.2499 Val_Loss: 152.5686  BEST VAL Loss: 152.5686\n",
            "\n",
            "Epoch 719: Validation loss decreased (152.568604 --> 152.137772).\n",
            "\t Train_Loss: 29.9744 Val_Loss: 152.1378  BEST VAL Loss: 152.1378\n",
            "\n",
            "Epoch 720: Validation loss decreased (152.137772 --> 151.643570).\n",
            "\t Train_Loss: 29.9515 Val_Loss: 151.6436  BEST VAL Loss: 151.6436\n",
            "\n",
            "Epoch 721: Validation loss decreased (151.643570 --> 151.249313).\n",
            "\t Train_Loss: 29.7319 Val_Loss: 151.2493  BEST VAL Loss: 151.2493\n",
            "\n",
            "Epoch 722: Validation loss decreased (151.249313 --> 150.900055).\n",
            "\t Train_Loss: 29.5640 Val_Loss: 150.9001  BEST VAL Loss: 150.9001\n",
            "\n",
            "Epoch 723: Validation loss decreased (150.900055 --> 150.457703).\n",
            "\t Train_Loss: 29.5010 Val_Loss: 150.4577  BEST VAL Loss: 150.4577\n",
            "\n",
            "Epoch 724: Validation loss decreased (150.457703 --> 150.075989).\n",
            "\t Train_Loss: 29.2590 Val_Loss: 150.0760  BEST VAL Loss: 150.0760\n",
            "\n",
            "Epoch 725: Validation loss decreased (150.075989 --> 149.728043).\n",
            "\t Train_Loss: 29.1300 Val_Loss: 149.7280  BEST VAL Loss: 149.7280\n",
            "\n",
            "Epoch 726: Validation loss decreased (149.728043 --> 149.362305).\n",
            "\t Train_Loss: 29.0384 Val_Loss: 149.3623  BEST VAL Loss: 149.3623\n",
            "\n",
            "Epoch 727: Validation loss decreased (149.362305 --> 148.909180).\n",
            "\t Train_Loss: 28.8363 Val_Loss: 148.9092  BEST VAL Loss: 148.9092\n",
            "\n",
            "Epoch 728: Validation loss decreased (148.909180 --> 148.423950).\n",
            "\t Train_Loss: 28.7042 Val_Loss: 148.4240  BEST VAL Loss: 148.4240\n",
            "\n",
            "Epoch 729: Validation loss decreased (148.423950 --> 148.045914).\n",
            "\t Train_Loss: 28.6145 Val_Loss: 148.0459  BEST VAL Loss: 148.0459\n",
            "\n",
            "Epoch 730: Validation loss decreased (148.045914 --> 147.677521).\n",
            "\t Train_Loss: 28.4366 Val_Loss: 147.6775  BEST VAL Loss: 147.6775\n",
            "\n",
            "Epoch 731: Validation loss decreased (147.677521 --> 147.251099).\n",
            "\t Train_Loss: 28.2746 Val_Loss: 147.2511  BEST VAL Loss: 147.2511\n",
            "\n",
            "Epoch 732: Validation loss decreased (147.251099 --> 146.816040).\n",
            "\t Train_Loss: 28.1825 Val_Loss: 146.8160  BEST VAL Loss: 146.8160\n",
            "\n",
            "Epoch 733: Validation loss decreased (146.816040 --> 146.296448).\n",
            "\t Train_Loss: 28.0489 Val_Loss: 146.2964  BEST VAL Loss: 146.2964\n",
            "\n",
            "Epoch 734: Validation loss decreased (146.296448 --> 145.869736).\n",
            "\t Train_Loss: 27.8775 Val_Loss: 145.8697  BEST VAL Loss: 145.8697\n",
            "\n",
            "Epoch 735: Validation loss decreased (145.869736 --> 145.545975).\n",
            "\t Train_Loss: 27.7530 Val_Loss: 145.5460  BEST VAL Loss: 145.5460\n",
            "\n",
            "Epoch 736: Validation loss decreased (145.545975 --> 145.136459).\n",
            "\t Train_Loss: 27.6581 Val_Loss: 145.1365  BEST VAL Loss: 145.1365\n",
            "\n",
            "Epoch 737: Validation loss decreased (145.136459 --> 144.748901).\n",
            "\t Train_Loss: 27.5277 Val_Loss: 144.7489  BEST VAL Loss: 144.7489\n",
            "\n",
            "Epoch 738: Validation loss decreased (144.748901 --> 144.277420).\n",
            "\t Train_Loss: 27.3703 Val_Loss: 144.2774  BEST VAL Loss: 144.2774\n",
            "\n",
            "Epoch 739: Validation loss decreased (144.277420 --> 143.811691).\n",
            "\t Train_Loss: 27.2313 Val_Loss: 143.8117  BEST VAL Loss: 143.8117\n",
            "\n",
            "Epoch 740: Validation loss decreased (143.811691 --> 143.472000).\n",
            "\t Train_Loss: 27.1243 Val_Loss: 143.4720  BEST VAL Loss: 143.4720\n",
            "\n",
            "Epoch 741: Validation loss decreased (143.472000 --> 143.069046).\n",
            "\t Train_Loss: 27.0159 Val_Loss: 143.0690  BEST VAL Loss: 143.0690\n",
            "\n",
            "Epoch 742: Validation loss decreased (143.069046 --> 142.721039).\n",
            "\t Train_Loss: 26.8778 Val_Loss: 142.7210  BEST VAL Loss: 142.7210\n",
            "\n",
            "Epoch 743: Validation loss decreased (142.721039 --> 142.313339).\n",
            "\t Train_Loss: 26.7363 Val_Loss: 142.3133  BEST VAL Loss: 142.3133\n",
            "\n",
            "Epoch 744: Validation loss decreased (142.313339 --> 141.866165).\n",
            "\t Train_Loss: 26.6144 Val_Loss: 141.8662  BEST VAL Loss: 141.8662\n",
            "\n",
            "Epoch 745: Validation loss decreased (141.866165 --> 141.515732).\n",
            "\t Train_Loss: 26.5068 Val_Loss: 141.5157  BEST VAL Loss: 141.5157\n",
            "\n",
            "Epoch 746: Validation loss decreased (141.515732 --> 141.068375).\n",
            "\t Train_Loss: 26.3931 Val_Loss: 141.0684  BEST VAL Loss: 141.0684\n",
            "\n",
            "Epoch 747: Validation loss decreased (141.068375 --> 140.667221).\n",
            "\t Train_Loss: 26.2621 Val_Loss: 140.6672  BEST VAL Loss: 140.6672\n",
            "\n",
            "Epoch 748: Validation loss decreased (140.667221 --> 140.220444).\n",
            "\t Train_Loss: 26.1291 Val_Loss: 140.2204  BEST VAL Loss: 140.2204\n",
            "\n",
            "Epoch 749: Validation loss decreased (140.220444 --> 139.785263).\n",
            "\t Train_Loss: 26.0066 Val_Loss: 139.7853  BEST VAL Loss: 139.7853\n",
            "\n",
            "Epoch 750: Validation loss decreased (139.785263 --> 139.443634).\n",
            "\t Train_Loss: 25.8959 Val_Loss: 139.4436  BEST VAL Loss: 139.4436\n",
            "\n",
            "Epoch 751: Validation loss decreased (139.443634 --> 139.050690).\n",
            "\t Train_Loss: 25.7839 Val_Loss: 139.0507  BEST VAL Loss: 139.0507\n",
            "\n",
            "Epoch 752: Validation loss decreased (139.050690 --> 138.686523).\n",
            "\t Train_Loss: 25.6617 Val_Loss: 138.6865  BEST VAL Loss: 138.6865\n",
            "\n",
            "Epoch 753: Validation loss decreased (138.686523 --> 138.273270).\n",
            "\t Train_Loss: 25.5364 Val_Loss: 138.2733  BEST VAL Loss: 138.2733\n",
            "\n",
            "Epoch 754: Validation loss decreased (138.273270 --> 137.855148).\n",
            "\t Train_Loss: 25.4183 Val_Loss: 137.8551  BEST VAL Loss: 137.8551\n",
            "\n",
            "Epoch 755: Validation loss decreased (137.855148 --> 137.514862).\n",
            "\t Train_Loss: 25.3085 Val_Loss: 137.5149  BEST VAL Loss: 137.5149\n",
            "\n",
            "Epoch 756: Validation loss decreased (137.514862 --> 137.118774).\n",
            "\t Train_Loss: 25.1964 Val_Loss: 137.1188  BEST VAL Loss: 137.1188\n",
            "\n",
            "Epoch 757: Validation loss decreased (137.118774 --> 136.740036).\n",
            "\t Train_Loss: 25.0777 Val_Loss: 136.7400  BEST VAL Loss: 136.7400\n",
            "\n",
            "Epoch 758: Validation loss decreased (136.740036 --> 136.331406).\n",
            "\t Train_Loss: 24.9590 Val_Loss: 136.3314  BEST VAL Loss: 136.3314\n",
            "\n",
            "Epoch 759: Validation loss decreased (136.331406 --> 135.916122).\n",
            "\t Train_Loss: 24.8466 Val_Loss: 135.9161  BEST VAL Loss: 135.9161\n",
            "\n",
            "Epoch 760: Validation loss decreased (135.916122 --> 135.569870).\n",
            "\t Train_Loss: 24.7374 Val_Loss: 135.5699  BEST VAL Loss: 135.5699\n",
            "\n",
            "Epoch 761: Validation loss decreased (135.569870 --> 135.166519).\n",
            "\t Train_Loss: 24.6249 Val_Loss: 135.1665  BEST VAL Loss: 135.1665\n",
            "\n",
            "Epoch 762: Validation loss decreased (135.166519 --> 134.762543).\n",
            "\t Train_Loss: 24.5094 Val_Loss: 134.7625  BEST VAL Loss: 134.7625\n",
            "\n",
            "Epoch 763: Validation loss decreased (134.762543 --> 134.371567).\n",
            "\t Train_Loss: 24.3964 Val_Loss: 134.3716  BEST VAL Loss: 134.3716\n",
            "\n",
            "Epoch 764: Validation loss decreased (134.371567 --> 133.976425).\n",
            "\t Train_Loss: 24.2873 Val_Loss: 133.9764  BEST VAL Loss: 133.9764\n",
            "\n",
            "Epoch 765: Validation loss decreased (133.976425 --> 133.625412).\n",
            "\t Train_Loss: 24.1767 Val_Loss: 133.6254  BEST VAL Loss: 133.6254\n",
            "\n",
            "Epoch 766: Validation loss decreased (133.625412 --> 133.224274).\n",
            "\t Train_Loss: 24.0634 Val_Loss: 133.2243  BEST VAL Loss: 133.2243\n",
            "\n",
            "Epoch 767: Validation loss decreased (133.224274 --> 132.818039).\n",
            "\t Train_Loss: 23.9495 Val_Loss: 132.8180  BEST VAL Loss: 132.8180\n",
            "\n",
            "Epoch 768: Validation loss decreased (132.818039 --> 132.443756).\n",
            "\t Train_Loss: 23.8376 Val_Loss: 132.4438  BEST VAL Loss: 132.4438\n",
            "\n",
            "Epoch 769: Validation loss decreased (132.443756 --> 132.046021).\n",
            "\t Train_Loss: 23.7260 Val_Loss: 132.0460  BEST VAL Loss: 132.0460\n",
            "\n",
            "Epoch 770: Validation loss decreased (132.046021 --> 131.664948).\n",
            "\t Train_Loss: 23.6116 Val_Loss: 131.6649  BEST VAL Loss: 131.6649\n",
            "\n",
            "Epoch 771: Validation loss decreased (131.664948 --> 131.263580).\n",
            "\t Train_Loss: 23.4950 Val_Loss: 131.2636  BEST VAL Loss: 131.2636\n",
            "\n",
            "Epoch 772: Validation loss decreased (131.263580 --> 130.870346).\n",
            "\t Train_Loss: 23.3781 Val_Loss: 130.8703  BEST VAL Loss: 130.8703\n",
            "\n",
            "Epoch 773: Validation loss decreased (130.870346 --> 130.485123).\n",
            "\t Train_Loss: 23.2616 Val_Loss: 130.4851  BEST VAL Loss: 130.4851\n",
            "\n",
            "Epoch 774: Validation loss decreased (130.485123 --> 130.068924).\n",
            "\t Train_Loss: 23.1451 Val_Loss: 130.0689  BEST VAL Loss: 130.0689\n",
            "\n",
            "Epoch 775: Validation loss decreased (130.068924 --> 129.681381).\n",
            "\t Train_Loss: 23.0276 Val_Loss: 129.6814  BEST VAL Loss: 129.6814\n",
            "\n",
            "Epoch 776: Validation loss decreased (129.681381 --> 129.266022).\n",
            "\t Train_Loss: 22.9093 Val_Loss: 129.2660  BEST VAL Loss: 129.2660\n",
            "\n",
            "Epoch 777: Validation loss decreased (129.266022 --> 128.856857).\n",
            "\t Train_Loss: 22.7908 Val_Loss: 128.8569  BEST VAL Loss: 128.8569\n",
            "\n",
            "Epoch 778: Validation loss decreased (128.856857 --> 128.426590).\n",
            "\t Train_Loss: 22.6724 Val_Loss: 128.4266  BEST VAL Loss: 128.4266\n",
            "\n",
            "Epoch 779: Validation loss decreased (128.426590 --> 128.019836).\n",
            "\t Train_Loss: 22.5543 Val_Loss: 128.0198  BEST VAL Loss: 128.0198\n",
            "\n",
            "Epoch 780: Validation loss decreased (128.019836 --> 127.590630).\n",
            "\t Train_Loss: 22.4361 Val_Loss: 127.5906  BEST VAL Loss: 127.5906\n",
            "\n",
            "Epoch 781: Validation loss decreased (127.590630 --> 127.156250).\n",
            "\t Train_Loss: 22.3181 Val_Loss: 127.1562  BEST VAL Loss: 127.1562\n",
            "\n",
            "Epoch 782: Validation loss decreased (127.156250 --> 126.709877).\n",
            "\t Train_Loss: 22.2006 Val_Loss: 126.7099  BEST VAL Loss: 126.7099\n",
            "\n",
            "Epoch 783: Validation loss decreased (126.709877 --> 126.263916).\n",
            "\t Train_Loss: 22.0838 Val_Loss: 126.2639  BEST VAL Loss: 126.2639\n",
            "\n",
            "Epoch 784: Validation loss decreased (126.263916 --> 125.837547).\n",
            "\t Train_Loss: 21.9722 Val_Loss: 125.8375  BEST VAL Loss: 125.8375\n",
            "\n",
            "Epoch 785: Validation loss decreased (125.837547 --> 125.252663).\n",
            "\t Train_Loss: 21.8755 Val_Loss: 125.2527  BEST VAL Loss: 125.2527\n",
            "\n",
            "Epoch 786: Validation loss decreased (125.252663 --> 124.800560).\n",
            "\t Train_Loss: 21.7520 Val_Loss: 124.8006  BEST VAL Loss: 124.8006\n",
            "\n",
            "Epoch 787: Validation loss decreased (124.800560 --> 124.417168).\n",
            "\t Train_Loss: 21.5863 Val_Loss: 124.4172  BEST VAL Loss: 124.4172\n",
            "\n",
            "Epoch 788: Validation loss decreased (124.417168 --> 123.805519).\n",
            "\t Train_Loss: 21.5285 Val_Loss: 123.8055  BEST VAL Loss: 123.8055\n",
            "\n",
            "Epoch 789: Validation loss decreased (123.805519 --> 123.108521).\n",
            "\t Train_Loss: 21.3331 Val_Loss: 123.1085  BEST VAL Loss: 123.1085\n",
            "\n",
            "Epoch 790: Validation loss decreased (123.108521 --> 122.671631).\n",
            "\t Train_Loss: 21.2813 Val_Loss: 122.6716  BEST VAL Loss: 122.6716\n",
            "\n",
            "Epoch 791: Validation loss decreased (122.671631 --> 121.821449).\n",
            "\t Train_Loss: 21.5032 Val_Loss: 121.8214  BEST VAL Loss: 121.8214\n",
            "\n",
            "Epoch 792: Validation loss decreased (121.821449 --> 121.766479).\n",
            "\t Train_Loss: 21.5084 Val_Loss: 121.7665  BEST VAL Loss: 121.7665\n",
            "\n",
            "Epoch 793: Validation loss decreased (121.766479 --> 121.367935).\n",
            "\t Train_Loss: 21.8673 Val_Loss: 121.3679  BEST VAL Loss: 121.3679\n",
            "\n",
            "Epoch 794: Validation loss decreased (121.367935 --> 121.198074).\n",
            "\t Train_Loss: 24.4179 Val_Loss: 121.1981  BEST VAL Loss: 121.1981\n",
            "\n",
            "Epoch 795: Validation loss decreased (121.198074 --> 121.089676).\n",
            "\t Train_Loss: 26.9432 Val_Loss: 121.0897  BEST VAL Loss: 121.0897\n",
            "\n",
            "Epoch 796: Validation loss did not decrease\n",
            "\t Train_Loss: 22.3913 Val_Loss: 124.3128  BEST VAL Loss: 121.0897\n",
            "\n",
            "Epoch 797: Validation loss did not decrease\n",
            "\t Train_Loss: 31.5307 Val_Loss: 121.2710  BEST VAL Loss: 121.0897\n",
            "\n",
            "Epoch 798: Validation loss decreased (121.089676 --> 120.537025).\n",
            "\t Train_Loss: 28.1037 Val_Loss: 120.5370  BEST VAL Loss: 120.5370\n",
            "\n",
            "Epoch 799: Validation loss decreased (120.537025 --> 119.769279).\n",
            "\t Train_Loss: 25.7568 Val_Loss: 119.7693  BEST VAL Loss: 119.7693\n",
            "\n",
            "Epoch 800: Validation loss decreased (119.769279 --> 119.489296).\n",
            "\t Train_Loss: 24.0358 Val_Loss: 119.4893  BEST VAL Loss: 119.4893\n",
            "\n",
            "Epoch 801: Validation loss did not decrease\n",
            "\t Train_Loss: 24.7076 Val_Loss: 119.9159  BEST VAL Loss: 119.4893\n",
            "\n",
            "Epoch 802: Validation loss did not decrease\n",
            "\t Train_Loss: 25.8825 Val_Loss: 121.1242  BEST VAL Loss: 119.4893\n",
            "\n",
            "Epoch 803: Validation loss did not decrease\n",
            "\t Train_Loss: 23.1066 Val_Loss: 120.3381  BEST VAL Loss: 119.4893\n",
            "\n",
            "Epoch 804: Validation loss decreased (119.489296 --> 118.983086).\n",
            "\t Train_Loss: 24.9452 Val_Loss: 118.9831  BEST VAL Loss: 118.9831\n",
            "\n",
            "Epoch 805: Validation loss decreased (118.983086 --> 118.630615).\n",
            "\t Train_Loss: 23.3337 Val_Loss: 118.6306  BEST VAL Loss: 118.6306\n",
            "\n",
            "Epoch 806: Validation loss did not decrease\n",
            "\t Train_Loss: 23.9809 Val_Loss: 119.1256  BEST VAL Loss: 118.6306\n",
            "\n",
            "Epoch 807: Validation loss did not decrease\n",
            "\t Train_Loss: 21.6580 Val_Loss: 119.4486  BEST VAL Loss: 118.6306\n",
            "\n",
            "Epoch 808: Validation loss decreased (118.630615 --> 118.004051).\n",
            "\t Train_Loss: 23.3421 Val_Loss: 118.0041  BEST VAL Loss: 118.0041\n",
            "\n",
            "Epoch 809: Validation loss did not decrease\n",
            "\t Train_Loss: 21.6955 Val_Loss: 119.4386  BEST VAL Loss: 118.0041\n",
            "\n",
            "Epoch 810: Validation loss decreased (118.004051 --> 117.013916).\n",
            "\t Train_Loss: 22.5295 Val_Loss: 117.0139  BEST VAL Loss: 117.0139\n",
            "\n",
            "Epoch 811: Validation loss decreased (117.013916 --> 115.693138).\n",
            "\t Train_Loss: 21.8573 Val_Loss: 115.6931  BEST VAL Loss: 115.6931\n",
            "\n",
            "Epoch 812: Validation loss decreased (115.693138 --> 115.295067).\n",
            "\t Train_Loss: 21.6629 Val_Loss: 115.2951  BEST VAL Loss: 115.2951\n",
            "\n",
            "Epoch 813: Validation loss did not decrease\n",
            "\t Train_Loss: 22.0693 Val_Loss: 115.4598  BEST VAL Loss: 115.2951\n",
            "\n",
            "Epoch 814: Validation loss decreased (115.295067 --> 114.795364).\n",
            "\t Train_Loss: 21.0210 Val_Loss: 114.7954  BEST VAL Loss: 114.7954\n",
            "\n",
            "Epoch 815: Validation loss decreased (114.795364 --> 113.856102).\n",
            "\t Train_Loss: 20.4689 Val_Loss: 113.8561  BEST VAL Loss: 113.8561\n",
            "\n",
            "Epoch 816: Validation loss decreased (113.856102 --> 113.310890).\n",
            "\t Train_Loss: 20.6552 Val_Loss: 113.3109  BEST VAL Loss: 113.3109\n",
            "\n",
            "Epoch 817: Validation loss decreased (113.310890 --> 113.118492).\n",
            "\t Train_Loss: 20.5018 Val_Loss: 113.1185  BEST VAL Loss: 113.1185\n",
            "\n",
            "Epoch 818: Validation loss decreased (113.118492 --> 112.536560).\n",
            "\t Train_Loss: 20.5899 Val_Loss: 112.5366  BEST VAL Loss: 112.5366\n",
            "\n",
            "Epoch 819: Validation loss decreased (112.536560 --> 112.090797).\n",
            "\t Train_Loss: 19.8325 Val_Loss: 112.0908  BEST VAL Loss: 112.0908\n",
            "\n",
            "Epoch 820: Validation loss did not decrease\n",
            "\t Train_Loss: 19.6601 Val_Loss: 112.7153  BEST VAL Loss: 112.0908\n",
            "\n",
            "Epoch 821: Validation loss did not decrease\n",
            "\t Train_Loss: 19.8302 Val_Loss: 112.6129  BEST VAL Loss: 112.0908\n",
            "\n",
            "Epoch 822: Validation loss decreased (112.090797 --> 111.902344).\n",
            "\t Train_Loss: 19.7649 Val_Loss: 111.9023  BEST VAL Loss: 111.9023\n",
            "\n",
            "Epoch 823: Validation loss decreased (111.902344 --> 111.553528).\n",
            "\t Train_Loss: 19.2977 Val_Loss: 111.5535  BEST VAL Loss: 111.5535\n",
            "\n",
            "Epoch 824: Validation loss decreased (111.553528 --> 111.198341).\n",
            "\t Train_Loss: 18.8016 Val_Loss: 111.1983  BEST VAL Loss: 111.1983\n",
            "\n",
            "Epoch 825: Validation loss decreased (111.198341 --> 110.341484).\n",
            "\t Train_Loss: 19.1663 Val_Loss: 110.3415  BEST VAL Loss: 110.3415\n",
            "\n",
            "Epoch 826: Validation loss decreased (110.341484 --> 110.116287).\n",
            "\t Train_Loss: 19.1361 Val_Loss: 110.1163  BEST VAL Loss: 110.1163\n",
            "\n",
            "Epoch 827: Validation loss did not decrease\n",
            "\t Train_Loss: 18.7051 Val_Loss: 110.2678  BEST VAL Loss: 110.1163\n",
            "\n",
            "Epoch 828: Validation loss decreased (110.116287 --> 109.730461).\n",
            "\t Train_Loss: 18.5369 Val_Loss: 109.7305  BEST VAL Loss: 109.7305\n",
            "\n",
            "Epoch 829: Validation loss decreased (109.730461 --> 109.126541).\n",
            "\t Train_Loss: 18.4939 Val_Loss: 109.1265  BEST VAL Loss: 109.1265\n",
            "\n",
            "Epoch 830: Validation loss decreased (109.126541 --> 108.823265).\n",
            "\t Train_Loss: 18.5613 Val_Loss: 108.8233  BEST VAL Loss: 108.8233\n",
            "\n",
            "Epoch 831: Validation loss decreased (108.823265 --> 108.261917).\n",
            "\t Train_Loss: 18.3225 Val_Loss: 108.2619  BEST VAL Loss: 108.2619\n",
            "\n",
            "Epoch 832: Validation loss decreased (108.261917 --> 107.672928).\n",
            "\t Train_Loss: 18.0864 Val_Loss: 107.6729  BEST VAL Loss: 107.6729\n",
            "\n",
            "Epoch 833: Validation loss did not decrease\n",
            "\t Train_Loss: 18.0987 Val_Loss: 107.8491  BEST VAL Loss: 107.6729\n",
            "\n",
            "Epoch 834: Validation loss decreased (107.672928 --> 107.352295).\n",
            "\t Train_Loss: 18.1428 Val_Loss: 107.3523  BEST VAL Loss: 107.3523\n",
            "\n",
            "Epoch 835: Validation loss decreased (107.352295 --> 106.642921).\n",
            "\t Train_Loss: 17.8610 Val_Loss: 106.6429  BEST VAL Loss: 106.6429\n",
            "\n",
            "Epoch 836: Validation loss decreased (106.642921 --> 106.482323).\n",
            "\t Train_Loss: 17.8154 Val_Loss: 106.4823  BEST VAL Loss: 106.4823\n",
            "\n",
            "Epoch 837: Validation loss decreased (106.482323 --> 106.299553).\n",
            "\t Train_Loss: 17.7672 Val_Loss: 106.2996  BEST VAL Loss: 106.2996\n",
            "\n",
            "Epoch 838: Validation loss decreased (106.299553 --> 105.894974).\n",
            "\t Train_Loss: 17.7290 Val_Loss: 105.8950  BEST VAL Loss: 105.8950\n",
            "\n",
            "Epoch 839: Validation loss decreased (105.894974 --> 105.697388).\n",
            "\t Train_Loss: 17.6245 Val_Loss: 105.6974  BEST VAL Loss: 105.6974\n",
            "\n",
            "Epoch 840: Validation loss decreased (105.697388 --> 105.374489).\n",
            "\t Train_Loss: 17.4467 Val_Loss: 105.3745  BEST VAL Loss: 105.3745\n",
            "\n",
            "Epoch 841: Validation loss decreased (105.374489 --> 104.685158).\n",
            "\t Train_Loss: 17.4557 Val_Loss: 104.6852  BEST VAL Loss: 104.6852\n",
            "\n",
            "Epoch 842: Validation loss decreased (104.685158 --> 104.428421).\n",
            "\t Train_Loss: 17.3994 Val_Loss: 104.4284  BEST VAL Loss: 104.4284\n",
            "\n",
            "Epoch 843: Validation loss did not decrease\n",
            "\t Train_Loss: 17.2623 Val_Loss: 104.4491  BEST VAL Loss: 104.4284\n",
            "\n",
            "Epoch 844: Validation loss decreased (104.428421 --> 104.074852).\n",
            "\t Train_Loss: 17.2131 Val_Loss: 104.0749  BEST VAL Loss: 104.0749\n",
            "\n",
            "Epoch 845: Validation loss decreased (104.074852 --> 103.829811).\n",
            "\t Train_Loss: 17.1561 Val_Loss: 103.8298  BEST VAL Loss: 103.8298\n",
            "\n",
            "Epoch 846: Validation loss decreased (103.829811 --> 103.798538).\n",
            "\t Train_Loss: 17.1114 Val_Loss: 103.7985  BEST VAL Loss: 103.7985\n",
            "\n",
            "Epoch 847: Validation loss decreased (103.798538 --> 103.159973).\n",
            "\t Train_Loss: 17.0240 Val_Loss: 103.1600  BEST VAL Loss: 103.1600\n",
            "\n",
            "Epoch 848: Validation loss decreased (103.159973 --> 102.472923).\n",
            "\t Train_Loss: 16.9144 Val_Loss: 102.4729  BEST VAL Loss: 102.4729\n",
            "\n",
            "Epoch 849: Validation loss decreased (102.472923 --> 102.148048).\n",
            "\t Train_Loss: 16.8995 Val_Loss: 102.1480  BEST VAL Loss: 102.1480\n",
            "\n",
            "Epoch 850: Validation loss decreased (102.148048 --> 101.628395).\n",
            "\t Train_Loss: 16.8279 Val_Loss: 101.6284  BEST VAL Loss: 101.6284\n",
            "\n",
            "Epoch 851: Validation loss decreased (101.628395 --> 101.448563).\n",
            "\t Train_Loss: 16.7466 Val_Loss: 101.4486  BEST VAL Loss: 101.4486\n",
            "\n",
            "Epoch 852: Validation loss did not decrease\n",
            "\t Train_Loss: 16.6941 Val_Loss: 101.6551  BEST VAL Loss: 101.4486\n",
            "\n",
            "Epoch 853: Validation loss decreased (101.448563 --> 101.402794).\n",
            "\t Train_Loss: 16.6495 Val_Loss: 101.4028  BEST VAL Loss: 101.4028\n",
            "\n",
            "Epoch 854: Validation loss decreased (101.402794 --> 100.938141).\n",
            "\t Train_Loss: 16.5815 Val_Loss: 100.9381  BEST VAL Loss: 100.9381\n",
            "\n",
            "Epoch 855: Validation loss decreased (100.938141 --> 100.677330).\n",
            "\t Train_Loss: 16.5069 Val_Loss: 100.6773  BEST VAL Loss: 100.6773\n",
            "\n",
            "Epoch 856: Validation loss decreased (100.677330 --> 100.235313).\n",
            "\t Train_Loss: 16.4629 Val_Loss: 100.2353  BEST VAL Loss: 100.2353\n",
            "\n",
            "Epoch 857: Validation loss decreased (100.235313 --> 99.990067).\n",
            "\t Train_Loss: 16.4086 Val_Loss: 99.9901  BEST VAL Loss: 99.9901\n",
            "\n",
            "Epoch 858: Validation loss did not decrease\n",
            "\t Train_Loss: 16.3489 Val_Loss: 100.0152  BEST VAL Loss: 99.9901\n",
            "\n",
            "Epoch 859: Validation loss decreased (99.990067 --> 99.628334).\n",
            "\t Train_Loss: 16.2890 Val_Loss: 99.6283  BEST VAL Loss: 99.6283\n",
            "\n",
            "Epoch 860: Validation loss decreased (99.628334 --> 99.238655).\n",
            "\t Train_Loss: 16.2347 Val_Loss: 99.2387  BEST VAL Loss: 99.2387\n",
            "\n",
            "Epoch 861: Validation loss decreased (99.238655 --> 99.040604).\n",
            "\t Train_Loss: 16.1924 Val_Loss: 99.0406  BEST VAL Loss: 99.0406\n",
            "\n",
            "Epoch 862: Validation loss decreased (99.040604 --> 98.590424).\n",
            "\t Train_Loss: 16.1319 Val_Loss: 98.5904  BEST VAL Loss: 98.5904\n",
            "\n",
            "Epoch 863: Validation loss decreased (98.590424 --> 98.255981).\n",
            "\t Train_Loss: 16.0679 Val_Loss: 98.2560  BEST VAL Loss: 98.2560\n",
            "\n",
            "Epoch 864: Validation loss decreased (98.255981 --> 98.053993).\n",
            "\t Train_Loss: 16.0204 Val_Loss: 98.0540  BEST VAL Loss: 98.0540\n",
            "\n",
            "Epoch 865: Validation loss decreased (98.053993 --> 97.649200).\n",
            "\t Train_Loss: 15.9625 Val_Loss: 97.6492  BEST VAL Loss: 97.6492\n",
            "\n",
            "Epoch 866: Validation loss decreased (97.649200 --> 97.461632).\n",
            "\t Train_Loss: 15.8866 Val_Loss: 97.4616  BEST VAL Loss: 97.4616\n",
            "\n",
            "Epoch 867: Validation loss decreased (97.461632 --> 97.330421).\n",
            "\t Train_Loss: 15.7993 Val_Loss: 97.3304  BEST VAL Loss: 97.3304\n",
            "\n",
            "Epoch 868: Validation loss decreased (97.330421 --> 96.803055).\n",
            "\t Train_Loss: 15.7313 Val_Loss: 96.8031  BEST VAL Loss: 96.8031\n",
            "\n",
            "Epoch 869: Validation loss decreased (96.803055 --> 96.584541).\n",
            "\t Train_Loss: 15.6607 Val_Loss: 96.5845  BEST VAL Loss: 96.5845\n",
            "\n",
            "Epoch 870: Validation loss did not decrease\n",
            "\t Train_Loss: 15.5793 Val_Loss: 96.5984  BEST VAL Loss: 96.5845\n",
            "\n",
            "Epoch 871: Validation loss decreased (96.584541 --> 96.295547).\n",
            "\t Train_Loss: 15.5168 Val_Loss: 96.2955  BEST VAL Loss: 96.2955\n",
            "\n",
            "Epoch 872: Validation loss decreased (96.295547 --> 96.227135).\n",
            "\t Train_Loss: 15.4588 Val_Loss: 96.2271  BEST VAL Loss: 96.2271\n",
            "\n",
            "Epoch 873: Validation loss decreased (96.227135 --> 95.760826).\n",
            "\t Train_Loss: 15.3945 Val_Loss: 95.7608  BEST VAL Loss: 95.7608\n",
            "\n",
            "Epoch 874: Validation loss decreased (95.760826 --> 95.268700).\n",
            "\t Train_Loss: 15.3244 Val_Loss: 95.2687  BEST VAL Loss: 95.2687\n",
            "\n",
            "Epoch 875: Validation loss decreased (95.268700 --> 95.173706).\n",
            "\t Train_Loss: 15.2751 Val_Loss: 95.1737  BEST VAL Loss: 95.1737\n",
            "\n",
            "Epoch 876: Validation loss decreased (95.173706 --> 94.911461).\n",
            "\t Train_Loss: 15.2160 Val_Loss: 94.9115  BEST VAL Loss: 94.9115\n",
            "\n",
            "Epoch 877: Validation loss decreased (94.911461 --> 94.890686).\n",
            "\t Train_Loss: 15.1489 Val_Loss: 94.8907  BEST VAL Loss: 94.8907\n",
            "\n",
            "Epoch 878: Validation loss did not decrease\n",
            "\t Train_Loss: 15.0901 Val_Loss: 94.9233  BEST VAL Loss: 94.8907\n",
            "\n",
            "Epoch 879: Validation loss decreased (94.890686 --> 94.580315).\n",
            "\t Train_Loss: 15.0359 Val_Loss: 94.5803  BEST VAL Loss: 94.5803\n",
            "\n",
            "Epoch 880: Validation loss decreased (94.580315 --> 94.318619).\n",
            "\t Train_Loss: 14.9730 Val_Loss: 94.3186  BEST VAL Loss: 94.3186\n",
            "\n",
            "Epoch 881: Validation loss decreased (94.318619 --> 93.742165).\n",
            "\t Train_Loss: 14.9100 Val_Loss: 93.7422  BEST VAL Loss: 93.7422\n",
            "\n",
            "Epoch 882: Validation loss decreased (93.742165 --> 93.159355).\n",
            "\t Train_Loss: 14.8450 Val_Loss: 93.1594  BEST VAL Loss: 93.1594\n",
            "\n",
            "Epoch 883: Validation loss decreased (93.159355 --> 92.989159).\n",
            "\t Train_Loss: 14.7922 Val_Loss: 92.9892  BEST VAL Loss: 92.9892\n",
            "\n",
            "Epoch 884: Validation loss decreased (92.989159 --> 92.625824).\n",
            "\t Train_Loss: 14.7338 Val_Loss: 92.6258  BEST VAL Loss: 92.6258\n",
            "\n",
            "Epoch 885: Validation loss decreased (92.625824 --> 92.221336).\n",
            "\t Train_Loss: 14.6683 Val_Loss: 92.2213  BEST VAL Loss: 92.2213\n",
            "\n",
            "Epoch 886: Validation loss decreased (92.221336 --> 91.877586).\n",
            "\t Train_Loss: 14.6112 Val_Loss: 91.8776  BEST VAL Loss: 91.8776\n",
            "\n",
            "Epoch 887: Validation loss decreased (91.877586 --> 91.453896).\n",
            "\t Train_Loss: 14.5614 Val_Loss: 91.4539  BEST VAL Loss: 91.4539\n",
            "\n",
            "Epoch 888: Validation loss decreased (91.453896 --> 91.123039).\n",
            "\t Train_Loss: 14.5024 Val_Loss: 91.1230  BEST VAL Loss: 91.1230\n",
            "\n",
            "Epoch 889: Validation loss decreased (91.123039 --> 90.736084).\n",
            "\t Train_Loss: 14.4419 Val_Loss: 90.7361  BEST VAL Loss: 90.7361\n",
            "\n",
            "Epoch 890: Validation loss decreased (90.736084 --> 90.279129).\n",
            "\t Train_Loss: 14.3850 Val_Loss: 90.2791  BEST VAL Loss: 90.2791\n",
            "\n",
            "Epoch 891: Validation loss decreased (90.279129 --> 90.108170).\n",
            "\t Train_Loss: 14.3304 Val_Loss: 90.1082  BEST VAL Loss: 90.1082\n",
            "\n",
            "Epoch 892: Validation loss decreased (90.108170 --> 89.732414).\n",
            "\t Train_Loss: 14.2757 Val_Loss: 89.7324  BEST VAL Loss: 89.7324\n",
            "\n",
            "Epoch 893: Validation loss decreased (89.732414 --> 89.633339).\n",
            "\t Train_Loss: 14.2268 Val_Loss: 89.6333  BEST VAL Loss: 89.6333\n",
            "\n",
            "Epoch 894: Validation loss decreased (89.633339 --> 88.949585).\n",
            "\t Train_Loss: 14.1752 Val_Loss: 88.9496  BEST VAL Loss: 88.9496\n",
            "\n",
            "Epoch 895: Validation loss decreased (88.949585 --> 88.760948).\n",
            "\t Train_Loss: 14.1196 Val_Loss: 88.7609  BEST VAL Loss: 88.7609\n",
            "\n",
            "Epoch 896: Validation loss decreased (88.760948 --> 88.279488).\n",
            "\t Train_Loss: 14.0470 Val_Loss: 88.2795  BEST VAL Loss: 88.2795\n",
            "\n",
            "Epoch 897: Validation loss decreased (88.279488 --> 87.601700).\n",
            "\t Train_Loss: 13.9908 Val_Loss: 87.6017  BEST VAL Loss: 87.6017\n",
            "\n",
            "Epoch 898: Validation loss decreased (87.601700 --> 87.117363).\n",
            "\t Train_Loss: 13.9377 Val_Loss: 87.1174  BEST VAL Loss: 87.1174\n",
            "\n",
            "Epoch 899: Validation loss decreased (87.117363 --> 86.392593).\n",
            "\t Train_Loss: 13.8918 Val_Loss: 86.3926  BEST VAL Loss: 86.3926\n",
            "\n",
            "Epoch 900: Validation loss decreased (86.392593 --> 86.088440).\n",
            "\t Train_Loss: 13.8410 Val_Loss: 86.0884  BEST VAL Loss: 86.0884\n",
            "\n",
            "Epoch 901: Validation loss decreased (86.088440 --> 85.546349).\n",
            "\t Train_Loss: 13.7737 Val_Loss: 85.5463  BEST VAL Loss: 85.5463\n",
            "\n",
            "Epoch 902: Validation loss decreased (85.546349 --> 85.072960).\n",
            "\t Train_Loss: 13.7148 Val_Loss: 85.0730  BEST VAL Loss: 85.0730\n",
            "\n",
            "Epoch 903: Validation loss decreased (85.072960 --> 84.681633).\n",
            "\t Train_Loss: 13.6588 Val_Loss: 84.6816  BEST VAL Loss: 84.6816\n",
            "\n",
            "Epoch 904: Validation loss decreased (84.681633 --> 84.249603).\n",
            "\t Train_Loss: 13.6110 Val_Loss: 84.2496  BEST VAL Loss: 84.2496\n",
            "\n",
            "Epoch 905: Validation loss decreased (84.249603 --> 83.883667).\n",
            "\t Train_Loss: 13.5603 Val_Loss: 83.8837  BEST VAL Loss: 83.8837\n",
            "\n",
            "Epoch 906: Validation loss decreased (83.883667 --> 83.454903).\n",
            "\t Train_Loss: 13.5091 Val_Loss: 83.4549  BEST VAL Loss: 83.4549\n",
            "\n",
            "Epoch 907: Validation loss decreased (83.454903 --> 83.106850).\n",
            "\t Train_Loss: 13.4553 Val_Loss: 83.1068  BEST VAL Loss: 83.1068\n",
            "\n",
            "Epoch 908: Validation loss decreased (83.106850 --> 82.808311).\n",
            "\t Train_Loss: 13.4035 Val_Loss: 82.8083  BEST VAL Loss: 82.8083\n",
            "\n",
            "Epoch 909: Validation loss decreased (82.808311 --> 82.554428).\n",
            "\t Train_Loss: 13.3531 Val_Loss: 82.5544  BEST VAL Loss: 82.5544\n",
            "\n",
            "Epoch 910: Validation loss decreased (82.554428 --> 82.350388).\n",
            "\t Train_Loss: 13.3037 Val_Loss: 82.3504  BEST VAL Loss: 82.3504\n",
            "\n",
            "Epoch 911: Validation loss decreased (82.350388 --> 82.163643).\n",
            "\t Train_Loss: 13.2583 Val_Loss: 82.1636  BEST VAL Loss: 82.1636\n",
            "\n",
            "Epoch 912: Validation loss decreased (82.163643 --> 81.881615).\n",
            "\t Train_Loss: 13.2124 Val_Loss: 81.8816  BEST VAL Loss: 81.8816\n",
            "\n",
            "Epoch 913: Validation loss decreased (81.881615 --> 81.610085).\n",
            "\t Train_Loss: 13.1650 Val_Loss: 81.6101  BEST VAL Loss: 81.6101\n",
            "\n",
            "Epoch 914: Validation loss decreased (81.610085 --> 81.404198).\n",
            "\t Train_Loss: 13.1169 Val_Loss: 81.4042  BEST VAL Loss: 81.4042\n",
            "\n",
            "Epoch 915: Validation loss decreased (81.404198 --> 81.254883).\n",
            "\t Train_Loss: 13.0694 Val_Loss: 81.2549  BEST VAL Loss: 81.2549\n",
            "\n",
            "Epoch 916: Validation loss decreased (81.254883 --> 81.058769).\n",
            "\t Train_Loss: 13.0234 Val_Loss: 81.0588  BEST VAL Loss: 81.0588\n",
            "\n",
            "Epoch 917: Validation loss decreased (81.058769 --> 80.814476).\n",
            "\t Train_Loss: 12.9787 Val_Loss: 80.8145  BEST VAL Loss: 80.8145\n",
            "\n",
            "Epoch 918: Validation loss decreased (80.814476 --> 80.603516).\n",
            "\t Train_Loss: 12.9349 Val_Loss: 80.6035  BEST VAL Loss: 80.6035\n",
            "\n",
            "Epoch 919: Validation loss decreased (80.603516 --> 80.438065).\n",
            "\t Train_Loss: 12.8915 Val_Loss: 80.4381  BEST VAL Loss: 80.4381\n",
            "\n",
            "Epoch 920: Validation loss decreased (80.438065 --> 80.265663).\n",
            "\t Train_Loss: 12.8487 Val_Loss: 80.2657  BEST VAL Loss: 80.2657\n",
            "\n",
            "Epoch 921: Validation loss decreased (80.265663 --> 80.074928).\n",
            "\t Train_Loss: 12.8048 Val_Loss: 80.0749  BEST VAL Loss: 80.0749\n",
            "\n",
            "Epoch 922: Validation loss decreased (80.074928 --> 79.889648).\n",
            "\t Train_Loss: 12.7615 Val_Loss: 79.8896  BEST VAL Loss: 79.8896\n",
            "\n",
            "Epoch 923: Validation loss decreased (79.889648 --> 79.718895).\n",
            "\t Train_Loss: 12.7183 Val_Loss: 79.7189  BEST VAL Loss: 79.7189\n",
            "\n",
            "Epoch 924: Validation loss decreased (79.718895 --> 79.539421).\n",
            "\t Train_Loss: 12.6764 Val_Loss: 79.5394  BEST VAL Loss: 79.5394\n",
            "\n",
            "Epoch 925: Validation loss decreased (79.539421 --> 79.365173).\n",
            "\t Train_Loss: 12.6346 Val_Loss: 79.3652  BEST VAL Loss: 79.3652\n",
            "\n",
            "Epoch 926: Validation loss decreased (79.365173 --> 79.224388).\n",
            "\t Train_Loss: 12.5936 Val_Loss: 79.2244  BEST VAL Loss: 79.2244\n",
            "\n",
            "Epoch 927: Validation loss decreased (79.224388 --> 79.049454).\n",
            "\t Train_Loss: 12.5508 Val_Loss: 79.0495  BEST VAL Loss: 79.0495\n",
            "\n",
            "Epoch 928: Validation loss decreased (79.049454 --> 78.850227).\n",
            "\t Train_Loss: 12.4875 Val_Loss: 78.8502  BEST VAL Loss: 78.8502\n",
            "\n",
            "Epoch 929: Validation loss decreased (78.850227 --> 78.540405).\n",
            "\t Train_Loss: 12.4721 Val_Loss: 78.5404  BEST VAL Loss: 78.5404\n",
            "\n",
            "Epoch 930: Validation loss did not decrease\n",
            "\t Train_Loss: 12.4393 Val_Loss: 78.6136  BEST VAL Loss: 78.5404\n",
            "\n",
            "Epoch 931: Validation loss decreased (78.540405 --> 78.514847).\n",
            "\t Train_Loss: 12.4858 Val_Loss: 78.5148  BEST VAL Loss: 78.5148\n",
            "\n",
            "Epoch 932: Validation loss decreased (78.514847 --> 78.066093).\n",
            "\t Train_Loss: 12.5289 Val_Loss: 78.0661  BEST VAL Loss: 78.0661\n",
            "\n",
            "Epoch 933: Validation loss decreased (78.066093 --> 77.631378).\n",
            "\t Train_Loss: 12.2942 Val_Loss: 77.6314  BEST VAL Loss: 77.6314\n",
            "\n",
            "Epoch 934: Validation loss decreased (77.631378 --> 77.572197).\n",
            "\t Train_Loss: 12.3192 Val_Loss: 77.5722  BEST VAL Loss: 77.5722\n",
            "\n",
            "Epoch 935: Validation loss decreased (77.572197 --> 77.478691).\n",
            "\t Train_Loss: 12.2354 Val_Loss: 77.4787  BEST VAL Loss: 77.4787\n",
            "\n",
            "Epoch 936: Validation loss decreased (77.478691 --> 77.342705).\n",
            "\t Train_Loss: 12.3104 Val_Loss: 77.3427  BEST VAL Loss: 77.3427\n",
            "\n",
            "Epoch 937: Validation loss decreased (77.342705 --> 77.030144).\n",
            "\t Train_Loss: 12.1603 Val_Loss: 77.0301  BEST VAL Loss: 77.0301\n",
            "\n",
            "Epoch 938: Validation loss did not decrease\n",
            "\t Train_Loss: 12.3750 Val_Loss: 77.0990  BEST VAL Loss: 77.0301\n",
            "\n",
            "Epoch 939: Validation loss decreased (77.030144 --> 77.015739).\n",
            "\t Train_Loss: 12.3229 Val_Loss: 77.0157  BEST VAL Loss: 77.0157\n",
            "\n",
            "Epoch 940: Validation loss decreased (77.015739 --> 76.625427).\n",
            "\t Train_Loss: 12.5518 Val_Loss: 76.6254  BEST VAL Loss: 76.6254\n",
            "\n",
            "Epoch 941: Validation loss decreased (76.625427 --> 76.246597).\n",
            "\t Train_Loss: 12.0499 Val_Loss: 76.2466  BEST VAL Loss: 76.2466\n",
            "\n",
            "Epoch 942: Validation loss decreased (76.246597 --> 76.075615).\n",
            "\t Train_Loss: 12.3958 Val_Loss: 76.0756  BEST VAL Loss: 76.0756\n",
            "\n",
            "Epoch 943: Validation loss decreased (76.075615 --> 76.020203).\n",
            "\t Train_Loss: 12.1772 Val_Loss: 76.0202  BEST VAL Loss: 76.0202\n",
            "\n",
            "Epoch 944: Validation loss decreased (76.020203 --> 75.879471).\n",
            "\t Train_Loss: 12.5754 Val_Loss: 75.8795  BEST VAL Loss: 75.8795\n",
            "\n",
            "Epoch 945: Validation loss decreased (75.879471 --> 75.770546).\n",
            "\t Train_Loss: 11.9746 Val_Loss: 75.7705  BEST VAL Loss: 75.7705\n",
            "\n",
            "Epoch 946: Validation loss decreased (75.770546 --> 75.633781).\n",
            "\t Train_Loss: 11.8929 Val_Loss: 75.6338  BEST VAL Loss: 75.6338\n",
            "\n",
            "Epoch 947: Validation loss decreased (75.633781 --> 75.362061).\n",
            "\t Train_Loss: 11.9310 Val_Loss: 75.3621  BEST VAL Loss: 75.3621\n",
            "\n",
            "Epoch 948: Validation loss decreased (75.362061 --> 75.211510).\n",
            "\t Train_Loss: 11.9994 Val_Loss: 75.2115  BEST VAL Loss: 75.2115\n",
            "\n",
            "Epoch 949: Validation loss decreased (75.211510 --> 74.972092).\n",
            "\t Train_Loss: 12.1345 Val_Loss: 74.9721  BEST VAL Loss: 74.9721\n",
            "\n",
            "Epoch 950: Validation loss decreased (74.972092 --> 74.908569).\n",
            "\t Train_Loss: 11.7051 Val_Loss: 74.9086  BEST VAL Loss: 74.9086\n",
            "\n",
            "Epoch 951: Validation loss decreased (74.908569 --> 74.861351).\n",
            "\t Train_Loss: 11.7197 Val_Loss: 74.8614  BEST VAL Loss: 74.8614\n",
            "\n",
            "Epoch 952: Validation loss decreased (74.861351 --> 74.807816).\n",
            "\t Train_Loss: 11.9731 Val_Loss: 74.8078  BEST VAL Loss: 74.8078\n",
            "\n",
            "Epoch 953: Validation loss decreased (74.807816 --> 74.624138).\n",
            "\t Train_Loss: 12.0298 Val_Loss: 74.6241  BEST VAL Loss: 74.6241\n",
            "\n",
            "Epoch 954: Validation loss decreased (74.624138 --> 74.159889).\n",
            "\t Train_Loss: 12.1091 Val_Loss: 74.1599  BEST VAL Loss: 74.1599\n",
            "\n",
            "Epoch 955: Validation loss decreased (74.159889 --> 73.838112).\n",
            "\t Train_Loss: 11.6168 Val_Loss: 73.8381  BEST VAL Loss: 73.8381\n",
            "\n",
            "Epoch 956: Validation loss decreased (73.838112 --> 73.763283).\n",
            "\t Train_Loss: 11.5404 Val_Loss: 73.7633  BEST VAL Loss: 73.7633\n",
            "\n",
            "Epoch 957: Validation loss did not decrease\n",
            "\t Train_Loss: 11.9545 Val_Loss: 73.8627  BEST VAL Loss: 73.7633\n",
            "\n",
            "Epoch 958: Validation loss did not decrease\n",
            "\t Train_Loss: 11.9052 Val_Loss: 74.0113  BEST VAL Loss: 73.7633\n",
            "\n",
            "Epoch 959: Validation loss did not decrease\n",
            "\t Train_Loss: 11.7661 Val_Loss: 73.7644  BEST VAL Loss: 73.7633\n",
            "\n",
            "Epoch 960: Validation loss decreased (73.763283 --> 73.521362).\n",
            "\t Train_Loss: 11.4891 Val_Loss: 73.5214  BEST VAL Loss: 73.5214\n",
            "\n",
            "Epoch 961: Validation loss decreased (73.521362 --> 73.234459).\n",
            "\t Train_Loss: 11.5653 Val_Loss: 73.2345  BEST VAL Loss: 73.2345\n",
            "\n",
            "Epoch 962: Validation loss decreased (73.234459 --> 73.232216).\n",
            "\t Train_Loss: 11.5822 Val_Loss: 73.2322  BEST VAL Loss: 73.2322\n",
            "\n",
            "Epoch 963: Validation loss decreased (73.232216 --> 73.185577).\n",
            "\t Train_Loss: 11.3088 Val_Loss: 73.1856  BEST VAL Loss: 73.1856\n",
            "\n",
            "Epoch 964: Validation loss decreased (73.185577 --> 73.092896).\n",
            "\t Train_Loss: 11.2589 Val_Loss: 73.0929  BEST VAL Loss: 73.0929\n",
            "\n",
            "Epoch 965: Validation loss decreased (73.092896 --> 72.922989).\n",
            "\t Train_Loss: 11.2969 Val_Loss: 72.9230  BEST VAL Loss: 72.9230\n",
            "\n",
            "Epoch 966: Validation loss decreased (72.922989 --> 72.708588).\n",
            "\t Train_Loss: 11.2281 Val_Loss: 72.7086  BEST VAL Loss: 72.7086\n",
            "\n",
            "Epoch 967: Validation loss decreased (72.708588 --> 72.536858).\n",
            "\t Train_Loss: 11.1603 Val_Loss: 72.5369  BEST VAL Loss: 72.5369\n",
            "\n",
            "Epoch 968: Validation loss decreased (72.536858 --> 72.321686).\n",
            "\t Train_Loss: 11.1119 Val_Loss: 72.3217  BEST VAL Loss: 72.3217\n",
            "\n",
            "Epoch 969: Validation loss decreased (72.321686 --> 72.173294).\n",
            "\t Train_Loss: 11.1097 Val_Loss: 72.1733  BEST VAL Loss: 72.1733\n",
            "\n",
            "Epoch 970: Validation loss did not decrease\n",
            "\t Train_Loss: 11.0966 Val_Loss: 72.1879  BEST VAL Loss: 72.1733\n",
            "\n",
            "Epoch 971: Validation loss did not decrease\n",
            "\t Train_Loss: 11.0184 Val_Loss: 72.1961  BEST VAL Loss: 72.1733\n",
            "\n",
            "Epoch 972: Validation loss decreased (72.173294 --> 71.982803).\n",
            "\t Train_Loss: 10.9936 Val_Loss: 71.9828  BEST VAL Loss: 71.9828\n",
            "\n",
            "Epoch 973: Validation loss decreased (71.982803 --> 71.699158).\n",
            "\t Train_Loss: 10.9704 Val_Loss: 71.6992  BEST VAL Loss: 71.6992\n",
            "\n",
            "Epoch 974: Validation loss decreased (71.699158 --> 71.514572).\n",
            "\t Train_Loss: 10.9410 Val_Loss: 71.5146  BEST VAL Loss: 71.5146\n",
            "\n",
            "Epoch 975: Validation loss decreased (71.514572 --> 71.449120).\n",
            "\t Train_Loss: 10.9438 Val_Loss: 71.4491  BEST VAL Loss: 71.4491\n",
            "\n",
            "Epoch 976: Validation loss did not decrease\n",
            "\t Train_Loss: 10.8468 Val_Loss: 71.4823  BEST VAL Loss: 71.4491\n",
            "\n",
            "Epoch 977: Validation loss decreased (71.449120 --> 71.348320).\n",
            "\t Train_Loss: 10.8328 Val_Loss: 71.3483  BEST VAL Loss: 71.3483\n",
            "\n",
            "Epoch 978: Validation loss decreased (71.348320 --> 71.065819).\n",
            "\t Train_Loss: 10.8390 Val_Loss: 71.0658  BEST VAL Loss: 71.0658\n",
            "\n",
            "Epoch 979: Validation loss decreased (71.065819 --> 70.835785).\n",
            "\t Train_Loss: 10.7769 Val_Loss: 70.8358  BEST VAL Loss: 70.8358\n",
            "\n",
            "Epoch 980: Validation loss decreased (70.835785 --> 70.766579).\n",
            "\t Train_Loss: 10.7682 Val_Loss: 70.7666  BEST VAL Loss: 70.7666\n",
            "\n",
            "Epoch 981: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7034 Val_Loss: 70.7937  BEST VAL Loss: 70.7666\n",
            "\n",
            "Epoch 982: Validation loss decreased (70.766579 --> 70.696831).\n",
            "\t Train_Loss: 10.6783 Val_Loss: 70.6968  BEST VAL Loss: 70.6968\n",
            "\n",
            "Epoch 983: Validation loss decreased (70.696831 --> 70.480774).\n",
            "\t Train_Loss: 10.6751 Val_Loss: 70.4808  BEST VAL Loss: 70.4808\n",
            "\n",
            "Epoch 984: Validation loss decreased (70.480774 --> 70.368912).\n",
            "\t Train_Loss: 10.6404 Val_Loss: 70.3689  BEST VAL Loss: 70.3689\n",
            "\n",
            "Epoch 985: Validation loss decreased (70.368912 --> 70.245567).\n",
            "\t Train_Loss: 10.6119 Val_Loss: 70.2456  BEST VAL Loss: 70.2456\n",
            "\n",
            "Epoch 986: Validation loss decreased (70.245567 --> 70.160812).\n",
            "\t Train_Loss: 10.5770 Val_Loss: 70.1608  BEST VAL Loss: 70.1608\n",
            "\n",
            "Epoch 987: Validation loss decreased (70.160812 --> 69.974022).\n",
            "\t Train_Loss: 10.5281 Val_Loss: 69.9740  BEST VAL Loss: 69.9740\n",
            "\n",
            "Epoch 988: Validation loss decreased (69.974022 --> 69.854698).\n",
            "\t Train_Loss: 10.5002 Val_Loss: 69.8547  BEST VAL Loss: 69.8547\n",
            "\n",
            "Epoch 989: Validation loss decreased (69.854698 --> 69.801086).\n",
            "\t Train_Loss: 10.4683 Val_Loss: 69.8011  BEST VAL Loss: 69.8011\n",
            "\n",
            "Epoch 990: Validation loss decreased (69.801086 --> 69.641258).\n",
            "\t Train_Loss: 10.4504 Val_Loss: 69.6413  BEST VAL Loss: 69.6413\n",
            "\n",
            "Epoch 991: Validation loss decreased (69.641258 --> 69.544823).\n",
            "\t Train_Loss: 10.4244 Val_Loss: 69.5448  BEST VAL Loss: 69.5448\n",
            "\n",
            "Epoch 992: Validation loss decreased (69.544823 --> 69.346077).\n",
            "\t Train_Loss: 10.4374 Val_Loss: 69.3461  BEST VAL Loss: 69.3461\n",
            "\n",
            "Epoch 993: Validation loss did not decrease\n",
            "\t Train_Loss: 10.5027 Val_Loss: 69.3633  BEST VAL Loss: 69.3461\n",
            "\n",
            "Epoch 994: Validation loss decreased (69.346077 --> 69.137383).\n",
            "\t Train_Loss: 10.6956 Val_Loss: 69.1374  BEST VAL Loss: 69.1374\n",
            "\n",
            "Epoch 995: Validation loss decreased (69.137383 --> 69.079796).\n",
            "\t Train_Loss: 11.0750 Val_Loss: 69.0798  BEST VAL Loss: 69.0798\n",
            "\n",
            "Epoch 996: Validation loss decreased (69.079796 --> 68.721024).\n",
            "\t Train_Loss: 11.0792 Val_Loss: 68.7210  BEST VAL Loss: 68.7210\n",
            "\n",
            "Epoch 997: Validation loss decreased (68.721024 --> 68.604286).\n",
            "\t Train_Loss: 11.0372 Val_Loss: 68.6043  BEST VAL Loss: 68.6043\n",
            "\n",
            "Epoch 998: Validation loss decreased (68.604286 --> 68.515739).\n",
            "\t Train_Loss: 10.3730 Val_Loss: 68.5157  BEST VAL Loss: 68.5157\n",
            "\n",
            "Epoch 999: Validation loss decreased (68.515739 --> 68.344887).\n",
            "\t Train_Loss: 10.7369 Val_Loss: 68.3449  BEST VAL Loss: 68.3449\n",
            "\n",
            "Epoch 1000: Validation loss decreased (68.344887 --> 68.080162).\n",
            "\t Train_Loss: 10.8662 Val_Loss: 68.0802  BEST VAL Loss: 68.0802\n",
            "\n",
            "Epoch 1001: Validation loss decreased (68.080162 --> 67.842560).\n",
            "\t Train_Loss: 10.2324 Val_Loss: 67.8426  BEST VAL Loss: 67.8426\n",
            "\n",
            "Epoch 1002: Validation loss did not decrease\n",
            "\t Train_Loss: 10.5786 Val_Loss: 68.0251  BEST VAL Loss: 67.8426\n",
            "\n",
            "Epoch 1003: Validation loss decreased (67.842560 --> 67.499382).\n",
            "\t Train_Loss: 11.0806 Val_Loss: 67.4994  BEST VAL Loss: 67.4994\n",
            "\n",
            "Epoch 1004: Validation loss decreased (67.499382 --> 67.336143).\n",
            "\t Train_Loss: 10.2952 Val_Loss: 67.3361  BEST VAL Loss: 67.3361\n",
            "\n",
            "Epoch 1005: Validation loss did not decrease\n",
            "\t Train_Loss: 10.2487 Val_Loss: 67.3810  BEST VAL Loss: 67.3361\n",
            "\n",
            "Epoch 1006: Validation loss decreased (67.336143 --> 67.157959).\n",
            "\t Train_Loss: 10.6956 Val_Loss: 67.1580  BEST VAL Loss: 67.1580\n",
            "\n",
            "Epoch 1007: Validation loss decreased (67.157959 --> 67.056694).\n",
            "\t Train_Loss: 10.1956 Val_Loss: 67.0567  BEST VAL Loss: 67.0567\n",
            "\n",
            "Epoch 1008: Validation loss decreased (67.056694 --> 67.021133).\n",
            "\t Train_Loss: 10.1548 Val_Loss: 67.0211  BEST VAL Loss: 67.0211\n",
            "\n",
            "Epoch 1009: Validation loss did not decrease\n",
            "\t Train_Loss: 10.3949 Val_Loss: 67.0668  BEST VAL Loss: 67.0211\n",
            "\n",
            "Epoch 1010: Validation loss decreased (67.021133 --> 67.004509).\n",
            "\t Train_Loss: 10.0232 Val_Loss: 67.0045  BEST VAL Loss: 67.0045\n",
            "\n",
            "Epoch 1011: Validation loss decreased (67.004509 --> 66.885193).\n",
            "\t Train_Loss: 10.1633 Val_Loss: 66.8852  BEST VAL Loss: 66.8852\n",
            "\n",
            "Epoch 1012: Validation loss decreased (66.885193 --> 66.617424).\n",
            "\t Train_Loss: 10.2627 Val_Loss: 66.6174  BEST VAL Loss: 66.6174\n",
            "\n",
            "Epoch 1013: Validation loss decreased (66.617424 --> 66.544312).\n",
            "\t Train_Loss: 9.9081 Val_Loss: 66.5443  BEST VAL Loss: 66.5443\n",
            "\n",
            "Epoch 1014: Validation loss did not decrease\n",
            "\t Train_Loss: 10.1010 Val_Loss: 66.6027  BEST VAL Loss: 66.5443\n",
            "\n",
            "Epoch 1015: Validation loss decreased (66.544312 --> 66.342400).\n",
            "\t Train_Loss: 10.2419 Val_Loss: 66.3424  BEST VAL Loss: 66.3424\n",
            "\n",
            "Epoch 1016: Validation loss decreased (66.342400 --> 66.066513).\n",
            "\t Train_Loss: 9.8456 Val_Loss: 66.0665  BEST VAL Loss: 66.0665\n",
            "\n",
            "Epoch 1017: Validation loss did not decrease\n",
            "\t Train_Loss: 10.0221 Val_Loss: 66.3099  BEST VAL Loss: 66.0665\n",
            "\n",
            "Epoch 1018: Validation loss did not decrease\n",
            "\t Train_Loss: 10.1068 Val_Loss: 66.1488  BEST VAL Loss: 66.0665\n",
            "\n",
            "Epoch 1019: Validation loss decreased (66.066513 --> 66.057083).\n",
            "\t Train_Loss: 9.7457 Val_Loss: 66.0571  BEST VAL Loss: 66.0571\n",
            "\n",
            "Epoch 1020: Validation loss did not decrease\n",
            "\t Train_Loss: 9.9814 Val_Loss: 66.2239  BEST VAL Loss: 66.0571\n",
            "\n",
            "Epoch 1021: Validation loss decreased (66.057083 --> 65.813431).\n",
            "\t Train_Loss: 10.0302 Val_Loss: 65.8134  BEST VAL Loss: 65.8134\n",
            "\n",
            "Epoch 1022: Validation loss decreased (65.813431 --> 65.714836).\n",
            "\t Train_Loss: 9.7065 Val_Loss: 65.7148  BEST VAL Loss: 65.7148\n",
            "\n",
            "Epoch 1023: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8635 Val_Loss: 65.7449  BEST VAL Loss: 65.7148\n",
            "\n",
            "Epoch 1024: Validation loss decreased (65.714836 --> 65.365189).\n",
            "\t Train_Loss: 9.9431 Val_Loss: 65.3652  BEST VAL Loss: 65.3652\n",
            "\n",
            "Epoch 1025: Validation loss decreased (65.365189 --> 65.186913).\n",
            "\t Train_Loss: 9.5828 Val_Loss: 65.1869  BEST VAL Loss: 65.1869\n",
            "\n",
            "Epoch 1026: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8329 Val_Loss: 65.4016  BEST VAL Loss: 65.1869\n",
            "\n",
            "Epoch 1027: Validation loss decreased (65.186913 --> 64.916687).\n",
            "\t Train_Loss: 10.0136 Val_Loss: 64.9167  BEST VAL Loss: 64.9167\n",
            "\n",
            "Epoch 1028: Validation loss decreased (64.916687 --> 64.686600).\n",
            "\t Train_Loss: 9.5016 Val_Loss: 64.6866  BEST VAL Loss: 64.6866\n",
            "\n",
            "Epoch 1029: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8280 Val_Loss: 65.0863  BEST VAL Loss: 64.6866\n",
            "\n",
            "Epoch 1030: Validation loss decreased (64.686600 --> 64.528229).\n",
            "\t Train_Loss: 10.0520 Val_Loss: 64.5282  BEST VAL Loss: 64.5282\n",
            "\n",
            "Epoch 1031: Validation loss decreased (64.528229 --> 64.353447).\n",
            "\t Train_Loss: 9.5020 Val_Loss: 64.3534  BEST VAL Loss: 64.3534\n",
            "\n",
            "Epoch 1032: Validation loss did not decrease\n",
            "\t Train_Loss: 10.2394 Val_Loss: 64.7437  BEST VAL Loss: 64.3534\n",
            "\n",
            "Epoch 1033: Validation loss decreased (64.353447 --> 63.631042).\n",
            "\t Train_Loss: 10.6064 Val_Loss: 63.6310  BEST VAL Loss: 63.6310\n",
            "\n",
            "Epoch 1034: Validation loss did not decrease\n",
            "\t Train_Loss: 9.6543 Val_Loss: 63.7556  BEST VAL Loss: 63.6310\n",
            "\n",
            "Epoch 1035: Validation loss did not decrease\n",
            "\t Train_Loss: 10.9310 Val_Loss: 64.3723  BEST VAL Loss: 63.6310\n",
            "\n",
            "Epoch 1036: Validation loss did not decrease\n",
            "\t Train_Loss: 12.2806 Val_Loss: 63.9776  BEST VAL Loss: 63.6310\n",
            "\n",
            "Epoch 1037: Validation loss did not decrease\n",
            "\t Train_Loss: 10.1746 Val_Loss: 65.9169  BEST VAL Loss: 63.6310\n",
            "\n",
            "Epoch 1038: Validation loss decreased (63.631042 --> 63.229065).\n",
            "\t Train_Loss: 15.8953 Val_Loss: 63.2291  BEST VAL Loss: 63.2291\n",
            "\n",
            "Epoch 1039: Validation loss did not decrease\n",
            "\t Train_Loss: 12.6655 Val_Loss: 66.0627  BEST VAL Loss: 63.2291\n",
            "\n",
            "Epoch 1040: Validation loss did not decrease\n",
            "\t Train_Loss: 11.9469 Val_Loss: 64.7940  BEST VAL Loss: 63.2291\n",
            "\n",
            "Epoch 1041: Validation loss did not decrease\n",
            "\t Train_Loss: 11.7194 Val_Loss: 64.8076  BEST VAL Loss: 63.2291\n",
            "\n",
            "Epoch 1042: Validation loss decreased (63.229065 --> 62.754242).\n",
            "\t Train_Loss: 12.0361 Val_Loss: 62.7542  BEST VAL Loss: 62.7542\n",
            "\n",
            "Epoch 1043: Validation loss decreased (62.754242 --> 62.179615).\n",
            "\t Train_Loss: 10.5648 Val_Loss: 62.1796  BEST VAL Loss: 62.1796\n",
            "\n",
            "Epoch 1044: Validation loss did not decrease\n",
            "\t Train_Loss: 12.3847 Val_Loss: 62.4256  BEST VAL Loss: 62.1796\n",
            "\n",
            "Epoch 1045: Validation loss decreased (62.179615 --> 62.145973).\n",
            "\t Train_Loss: 11.2588 Val_Loss: 62.1460  BEST VAL Loss: 62.1460\n",
            "\n",
            "Epoch 1046: Validation loss did not decrease\n",
            "\t Train_Loss: 11.4813 Val_Loss: 64.6239  BEST VAL Loss: 62.1460\n",
            "\n",
            "Epoch 1047: Validation loss did not decrease\n",
            "\t Train_Loss: 10.9909 Val_Loss: 65.3867  BEST VAL Loss: 62.1460\n",
            "\n",
            "Epoch 1048: Validation loss did not decrease\n",
            "\t Train_Loss: 11.2214 Val_Loss: 63.6973  BEST VAL Loss: 62.1460\n",
            "\n",
            "Epoch 1049: Validation loss did not decrease\n",
            "\t Train_Loss: 10.8350 Val_Loss: 62.6426  BEST VAL Loss: 62.1460\n",
            "\n",
            "Epoch 1050: Validation loss did not decrease\n",
            "\t Train_Loss: 11.9597 Val_Loss: 62.3537  BEST VAL Loss: 62.1460\n",
            "\n",
            "Epoch 1051: Validation loss decreased (62.145973 --> 61.680004).\n",
            "\t Train_Loss: 11.3351 Val_Loss: 61.6800  BEST VAL Loss: 61.6800\n",
            "\n",
            "Epoch 1052: Validation loss did not decrease\n",
            "\t Train_Loss: 10.5870 Val_Loss: 62.4053  BEST VAL Loss: 61.6800\n",
            "\n",
            "Epoch 1053: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1633 Val_Loss: 62.3955  BEST VAL Loss: 61.6800\n",
            "\n",
            "Epoch 1054: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7468 Val_Loss: 62.1278  BEST VAL Loss: 61.6800\n",
            "\n",
            "Epoch 1055: Validation loss did not decrease\n",
            "\t Train_Loss: 10.5530 Val_Loss: 63.0751  BEST VAL Loss: 61.6800\n",
            "\n",
            "Epoch 1056: Validation loss did not decrease\n",
            "\t Train_Loss: 10.2800 Val_Loss: 63.3464  BEST VAL Loss: 61.6800\n",
            "\n",
            "Epoch 1057: Validation loss did not decrease\n",
            "\t Train_Loss: 10.1207 Val_Loss: 62.1985  BEST VAL Loss: 61.6800\n",
            "\n",
            "Epoch 1058: Validation loss did not decrease\n",
            "\t Train_Loss: 10.1974 Val_Loss: 63.7169  BEST VAL Loss: 61.6800\n",
            "\n",
            "Epoch 1059: Validation loss decreased (61.680004 --> 61.140369).\n",
            "\t Train_Loss: 9.8730 Val_Loss: 61.1404  BEST VAL Loss: 61.1404\n",
            "\n",
            "Epoch 1060: Validation loss decreased (61.140369 --> 60.661091).\n",
            "\t Train_Loss: 9.4479 Val_Loss: 60.6611  BEST VAL Loss: 60.6611\n",
            "\n",
            "Epoch 1061: Validation loss did not decrease\n",
            "\t Train_Loss: 9.9136 Val_Loss: 60.7383  BEST VAL Loss: 60.6611\n",
            "\n",
            "Epoch 1062: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8735 Val_Loss: 61.0922  BEST VAL Loss: 60.6611\n",
            "\n",
            "Epoch 1063: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8108 Val_Loss: 61.2228  BEST VAL Loss: 60.6611\n",
            "\n",
            "Epoch 1064: Validation loss did not decrease\n",
            "\t Train_Loss: 9.2560 Val_Loss: 61.4187  BEST VAL Loss: 60.6611\n",
            "\n",
            "Epoch 1065: Validation loss did not decrease\n",
            "\t Train_Loss: 9.2603 Val_Loss: 61.4460  BEST VAL Loss: 60.6611\n",
            "\n",
            "Epoch 1066: Validation loss did not decrease\n",
            "\t Train_Loss: 9.1828 Val_Loss: 62.0886  BEST VAL Loss: 60.6611\n",
            "\n",
            "Epoch 1067: Validation loss did not decrease\n",
            "\t Train_Loss: 9.4078 Val_Loss: 62.3544  BEST VAL Loss: 60.6611\n",
            "\n",
            "Epoch 1068: Validation loss did not decrease\n",
            "\t Train_Loss: 9.3595 Val_Loss: 62.0750  BEST VAL Loss: 60.6611\n",
            "\n",
            "Epoch 1069: Validation loss did not decrease\n",
            "\t Train_Loss: 9.5243 Val_Loss: 61.3043  BEST VAL Loss: 60.6611\n",
            "\n",
            "Epoch 1070: Validation loss did not decrease\n",
            "\t Train_Loss: 9.3545 Val_Loss: 60.8577  BEST VAL Loss: 60.6611\n",
            "\n",
            "Epoch 1071: Validation loss did not decrease\n",
            "\t Train_Loss: 9.4971 Val_Loss: 60.7803  BEST VAL Loss: 60.6611\n",
            "\n",
            "Epoch 1072: Validation loss decreased (60.661091 --> 60.487896).\n",
            "\t Train_Loss: 9.3053 Val_Loss: 60.4879  BEST VAL Loss: 60.4879\n",
            "\n",
            "Epoch 1073: Validation loss decreased (60.487896 --> 60.097485).\n",
            "\t Train_Loss: 9.2574 Val_Loss: 60.0975  BEST VAL Loss: 60.0975\n",
            "\n",
            "Epoch 1074: Validation loss did not decrease\n",
            "\t Train_Loss: 9.1129 Val_Loss: 60.2017  BEST VAL Loss: 60.0975\n",
            "\n",
            "Epoch 1075: Validation loss did not decrease\n",
            "\t Train_Loss: 9.0721 Val_Loss: 60.5227  BEST VAL Loss: 60.0975\n",
            "\n",
            "Epoch 1076: Validation loss did not decrease\n",
            "\t Train_Loss: 9.0059 Val_Loss: 60.6870  BEST VAL Loss: 60.0975\n",
            "\n",
            "Epoch 1077: Validation loss did not decrease\n",
            "\t Train_Loss: 9.0299 Val_Loss: 60.3482  BEST VAL Loss: 60.0975\n",
            "\n",
            "Epoch 1078: Validation loss did not decrease\n",
            "\t Train_Loss: 8.7516 Val_Loss: 60.1126  BEST VAL Loss: 60.0975\n",
            "\n",
            "Epoch 1079: Validation loss did not decrease\n",
            "\t Train_Loss: 8.8092 Val_Loss: 60.6296  BEST VAL Loss: 60.0975\n",
            "\n",
            "Epoch 1080: Validation loss did not decrease\n",
            "\t Train_Loss: 8.9417 Val_Loss: 60.5383  BEST VAL Loss: 60.0975\n",
            "\n",
            "Epoch 1081: Validation loss did not decrease\n",
            "\t Train_Loss: 8.5478 Val_Loss: 60.3242  BEST VAL Loss: 60.0975\n",
            "\n",
            "Epoch 1082: Validation loss did not decrease\n",
            "\t Train_Loss: 8.9144 Val_Loss: 61.2535  BEST VAL Loss: 60.0975\n",
            "\n",
            "Epoch 1083: Validation loss did not decrease\n",
            "\t Train_Loss: 8.6316 Val_Loss: 61.3751  BEST VAL Loss: 60.0975\n",
            "\n",
            "Epoch 1084: Validation loss did not decrease\n",
            "\t Train_Loss: 8.8617 Val_Loss: 60.4711  BEST VAL Loss: 60.0975\n",
            "\n",
            "Epoch 1085: Validation loss did not decrease\n",
            "\t Train_Loss: 9.1471 Val_Loss: 60.5337  BEST VAL Loss: 60.0975\n",
            "\n",
            "Epoch 1086: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4431 Val_Loss: 60.7433  BEST VAL Loss: 60.0975\n",
            "\n",
            "Epoch 1087: Validation loss did not decrease\n",
            "\t Train_Loss: 8.7922 Val_Loss: 62.0587  BEST VAL Loss: 60.0975\n",
            "\n",
            "Epoch 1088: Validation loss decreased (60.097485 --> 60.034374).\n",
            "\t Train_Loss: 8.9312 Val_Loss: 60.0344  BEST VAL Loss: 60.0344\n",
            "\n",
            "Epoch 1089: Validation loss decreased (60.034374 --> 59.467541).\n",
            "\t Train_Loss: 8.4270 Val_Loss: 59.4675  BEST VAL Loss: 59.4675\n",
            "\n",
            "Epoch 1090: Validation loss decreased (59.467541 --> 58.982952).\n",
            "\t Train_Loss: 8.5611 Val_Loss: 58.9830  BEST VAL Loss: 58.9830\n",
            "\n",
            "Epoch 1091: Validation loss decreased (58.982952 --> 58.764565).\n",
            "\t Train_Loss: 8.4685 Val_Loss: 58.7646  BEST VAL Loss: 58.7646\n",
            "\n",
            "Epoch 1092: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4061 Val_Loss: 58.9598  BEST VAL Loss: 58.7646\n",
            "\n",
            "Epoch 1093: Validation loss decreased (58.764565 --> 58.747925).\n",
            "\t Train_Loss: 8.4924 Val_Loss: 58.7479  BEST VAL Loss: 58.7479\n",
            "\n",
            "Epoch 1094: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2517 Val_Loss: 58.8230  BEST VAL Loss: 58.7479\n",
            "\n",
            "Epoch 1095: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1860 Val_Loss: 59.1800  BEST VAL Loss: 58.7479\n",
            "\n",
            "Epoch 1096: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2959 Val_Loss: 58.9504  BEST VAL Loss: 58.7479\n",
            "\n",
            "Epoch 1097: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1598 Val_Loss: 58.8891  BEST VAL Loss: 58.7479\n",
            "\n",
            "Epoch 1098: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1032 Val_Loss: 58.8537  BEST VAL Loss: 58.7479\n",
            "\n",
            "Epoch 1099: Validation loss decreased (58.747925 --> 58.142117).\n",
            "\t Train_Loss: 8.1704 Val_Loss: 58.1421  BEST VAL Loss: 58.1421\n",
            "\n",
            "Epoch 1100: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1967 Val_Loss: 58.3595  BEST VAL Loss: 58.1421\n",
            "\n",
            "Epoch 1101: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1441 Val_Loss: 58.4187  BEST VAL Loss: 58.1421\n",
            "\n",
            "Epoch 1102: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0633 Val_Loss: 58.3826  BEST VAL Loss: 58.1421\n",
            "\n",
            "Epoch 1103: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1964 Val_Loss: 58.8063  BEST VAL Loss: 58.1421\n",
            "\n",
            "Epoch 1104: Validation loss decreased (58.142117 --> 58.075806).\n",
            "\t Train_Loss: 8.2849 Val_Loss: 58.0758  BEST VAL Loss: 58.0758\n",
            "\n",
            "Epoch 1105: Validation loss decreased (58.075806 --> 58.041725).\n",
            "\t Train_Loss: 8.0856 Val_Loss: 58.0417  BEST VAL Loss: 58.0417\n",
            "\n",
            "Epoch 1106: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9387 Val_Loss: 58.3908  BEST VAL Loss: 58.0417\n",
            "\n",
            "Epoch 1107: Validation loss decreased (58.041725 --> 57.725220).\n",
            "\t Train_Loss: 8.1523 Val_Loss: 57.7252  BEST VAL Loss: 57.7252\n",
            "\n",
            "Epoch 1108: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1422 Val_Loss: 57.9728  BEST VAL Loss: 57.7252\n",
            "\n",
            "Epoch 1109: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0309 Val_Loss: 58.0040  BEST VAL Loss: 57.7252\n",
            "\n",
            "Epoch 1110: Validation loss decreased (57.725220 --> 57.722908).\n",
            "\t Train_Loss: 7.9197 Val_Loss: 57.7229  BEST VAL Loss: 57.7229\n",
            "\n",
            "Epoch 1111: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1672 Val_Loss: 58.2168  BEST VAL Loss: 57.7229\n",
            "\n",
            "Epoch 1112: Validation loss decreased (57.722908 --> 57.550171).\n",
            "\t Train_Loss: 7.9848 Val_Loss: 57.5502  BEST VAL Loss: 57.5502\n",
            "\n",
            "Epoch 1113: Validation loss decreased (57.550171 --> 57.506508).\n",
            "\t Train_Loss: 7.8437 Val_Loss: 57.5065  BEST VAL Loss: 57.5065\n",
            "\n",
            "Epoch 1114: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7898 Val_Loss: 57.6491  BEST VAL Loss: 57.5065\n",
            "\n",
            "Epoch 1115: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7789 Val_Loss: 57.5651  BEST VAL Loss: 57.5065\n",
            "\n",
            "Epoch 1116: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8199 Val_Loss: 57.7181  BEST VAL Loss: 57.5065\n",
            "\n",
            "Epoch 1117: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8447 Val_Loss: 58.0443  BEST VAL Loss: 57.5065\n",
            "\n",
            "Epoch 1118: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8498 Val_Loss: 57.6460  BEST VAL Loss: 57.5065\n",
            "\n",
            "Epoch 1119: Validation loss decreased (57.506508 --> 57.394299).\n",
            "\t Train_Loss: 7.7677 Val_Loss: 57.3943  BEST VAL Loss: 57.3943\n",
            "\n",
            "Epoch 1120: Validation loss decreased (57.394299 --> 57.208271).\n",
            "\t Train_Loss: 7.7288 Val_Loss: 57.2083  BEST VAL Loss: 57.2083\n",
            "\n",
            "Epoch 1121: Validation loss decreased (57.208271 --> 56.950012).\n",
            "\t Train_Loss: 7.6821 Val_Loss: 56.9500  BEST VAL Loss: 56.9500\n",
            "\n",
            "Epoch 1122: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6455 Val_Loss: 57.1031  BEST VAL Loss: 56.9500\n",
            "\n",
            "Epoch 1123: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6123 Val_Loss: 57.3103  BEST VAL Loss: 56.9500\n",
            "\n",
            "Epoch 1124: Validation loss decreased (56.950012 --> 56.738869).\n",
            "\t Train_Loss: 7.6636 Val_Loss: 56.7389  BEST VAL Loss: 56.7389\n",
            "\n",
            "Epoch 1125: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7436 Val_Loss: 56.9048  BEST VAL Loss: 56.7389\n",
            "\n",
            "Epoch 1126: Validation loss decreased (56.738869 --> 56.292286).\n",
            "\t Train_Loss: 7.8345 Val_Loss: 56.2923  BEST VAL Loss: 56.2923\n",
            "\n",
            "Epoch 1127: Validation loss decreased (56.292286 --> 56.235744).\n",
            "\t Train_Loss: 7.6387 Val_Loss: 56.2357  BEST VAL Loss: 56.2357\n",
            "\n",
            "Epoch 1128: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7171 Val_Loss: 56.6100  BEST VAL Loss: 56.2357\n",
            "\n",
            "Epoch 1129: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0391 Val_Loss: 56.7897  BEST VAL Loss: 56.2357\n",
            "\n",
            "Epoch 1130: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8161 Val_Loss: 56.4120  BEST VAL Loss: 56.2357\n",
            "\n",
            "Epoch 1131: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5238 Val_Loss: 57.1424  BEST VAL Loss: 56.2357\n",
            "\n",
            "Epoch 1132: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6748 Val_Loss: 56.5731  BEST VAL Loss: 56.2357\n",
            "\n",
            "Epoch 1133: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8263 Val_Loss: 56.7261  BEST VAL Loss: 56.2357\n",
            "\n",
            "Epoch 1134: Validation loss decreased (56.235744 --> 55.823597).\n",
            "\t Train_Loss: 7.9851 Val_Loss: 55.8236  BEST VAL Loss: 55.8236\n",
            "\n",
            "Epoch 1135: Validation loss decreased (55.823597 --> 55.630085).\n",
            "\t Train_Loss: 8.1169 Val_Loss: 55.6301  BEST VAL Loss: 55.6301\n",
            "\n",
            "Epoch 1136: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9535 Val_Loss: 56.2451  BEST VAL Loss: 55.6301\n",
            "\n",
            "Epoch 1137: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6701 Val_Loss: 56.4418  BEST VAL Loss: 55.6301\n",
            "\n",
            "Epoch 1138: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8642 Val_Loss: 56.0838  BEST VAL Loss: 55.6301\n",
            "\n",
            "Epoch 1139: Validation loss decreased (55.630085 --> 55.472267).\n",
            "\t Train_Loss: 7.5771 Val_Loss: 55.4723  BEST VAL Loss: 55.4723\n",
            "\n",
            "Epoch 1140: Validation loss decreased (55.472267 --> 55.337414).\n",
            "\t Train_Loss: 7.4992 Val_Loss: 55.3374  BEST VAL Loss: 55.3374\n",
            "\n",
            "Epoch 1141: Validation loss decreased (55.337414 --> 54.808285).\n",
            "\t Train_Loss: 7.7901 Val_Loss: 54.8083  BEST VAL Loss: 54.8083\n",
            "\n",
            "Epoch 1142: Validation loss decreased (54.808285 --> 54.784573).\n",
            "\t Train_Loss: 7.4700 Val_Loss: 54.7846  BEST VAL Loss: 54.7846\n",
            "\n",
            "Epoch 1143: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5057 Val_Loss: 55.0779  BEST VAL Loss: 54.7846\n",
            "\n",
            "Epoch 1144: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5061 Val_Loss: 55.1481  BEST VAL Loss: 54.7846\n",
            "\n",
            "Epoch 1145: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5555 Val_Loss: 55.8198  BEST VAL Loss: 54.7846\n",
            "\n",
            "Epoch 1146: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8051 Val_Loss: 55.3215  BEST VAL Loss: 54.7846\n",
            "\n",
            "Epoch 1147: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8075 Val_Loss: 55.1541  BEST VAL Loss: 54.7846\n",
            "\n",
            "Epoch 1148: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4729 Val_Loss: 55.3368  BEST VAL Loss: 54.7846\n",
            "\n",
            "Epoch 1149: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4257 Val_Loss: 55.8600  BEST VAL Loss: 54.7846\n",
            "\n",
            "Epoch 1150: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9142 Val_Loss: 55.5707  BEST VAL Loss: 54.7846\n",
            "\n",
            "Epoch 1151: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4634 Val_Loss: 55.1955  BEST VAL Loss: 54.7846\n",
            "\n",
            "Epoch 1152: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5908 Val_Loss: 56.0809  BEST VAL Loss: 54.7846\n",
            "\n",
            "Epoch 1153: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7248 Val_Loss: 55.6888  BEST VAL Loss: 54.7846\n",
            "\n",
            "Epoch 1154: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2465 Val_Loss: 54.9782  BEST VAL Loss: 54.7846\n",
            "\n",
            "Epoch 1155: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6715 Val_Loss: 56.4462  BEST VAL Loss: 54.7846\n",
            "\n",
            "Epoch 1156: Validation loss did not decrease\n",
            "\t Train_Loss: 9.0204 Val_Loss: 55.4626  BEST VAL Loss: 54.7846\n",
            "\n",
            "Epoch 1157: Validation loss did not decrease\n",
            "\t Train_Loss: 10.9915 Val_Loss: 55.8213  BEST VAL Loss: 54.7846\n",
            "\n",
            "Epoch 1158: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8380 Val_Loss: 56.5566  BEST VAL Loss: 54.7846\n",
            "\n",
            "Epoch 1159: Validation loss decreased (54.784573 --> 54.127483).\n",
            "\t Train_Loss: 10.2009 Val_Loss: 54.1275  BEST VAL Loss: 54.1275\n",
            "\n",
            "Epoch 1160: Validation loss decreased (54.127483 --> 52.951477).\n",
            "\t Train_Loss: 9.0364 Val_Loss: 52.9515  BEST VAL Loss: 52.9515\n",
            "\n",
            "Epoch 1161: Validation loss did not decrease\n",
            "\t Train_Loss: 9.3158 Val_Loss: 53.7109  BEST VAL Loss: 52.9515\n",
            "\n",
            "Epoch 1162: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2905 Val_Loss: 54.1183  BEST VAL Loss: 52.9515\n",
            "\n",
            "Epoch 1163: Validation loss did not decrease\n",
            "\t Train_Loss: 8.6708 Val_Loss: 53.7282  BEST VAL Loss: 52.9515\n",
            "\n",
            "Epoch 1164: Validation loss did not decrease\n",
            "\t Train_Loss: 8.9492 Val_Loss: 53.4990  BEST VAL Loss: 52.9515\n",
            "\n",
            "Epoch 1165: Validation loss did not decrease\n",
            "\t Train_Loss: 8.8209 Val_Loss: 53.5738  BEST VAL Loss: 52.9515\n",
            "\n",
            "Epoch 1166: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3493 Val_Loss: 54.0448  BEST VAL Loss: 52.9515\n",
            "\n",
            "Epoch 1167: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3866 Val_Loss: 54.6952  BEST VAL Loss: 52.9515\n",
            "\n",
            "Epoch 1168: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3172 Val_Loss: 54.0718  BEST VAL Loss: 52.9515\n",
            "\n",
            "Epoch 1169: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0045 Val_Loss: 53.1920  BEST VAL Loss: 52.9515\n",
            "\n",
            "Epoch 1170: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6564 Val_Loss: 53.3666  BEST VAL Loss: 52.9515\n",
            "\n",
            "Epoch 1171: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5294 Val_Loss: 54.0360  BEST VAL Loss: 52.9515\n",
            "\n",
            "Epoch 1172: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7102 Val_Loss: 54.5850  BEST VAL Loss: 52.9515\n",
            "\n",
            "Epoch 1173: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7428 Val_Loss: 55.2845  BEST VAL Loss: 52.9515\n",
            "\n",
            "Epoch 1174: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6025 Val_Loss: 55.2677  BEST VAL Loss: 52.9515\n",
            "\n",
            "Epoch 1175: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5846 Val_Loss: 54.8141  BEST VAL Loss: 52.9515\n",
            "\n",
            "Epoch 1176: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3844 Val_Loss: 54.4274  BEST VAL Loss: 52.9515\n",
            "\n",
            "Epoch 1177: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3567 Val_Loss: 54.0645  BEST VAL Loss: 52.9515\n",
            "\n",
            "Epoch 1178: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2236 Val_Loss: 54.0732  BEST VAL Loss: 52.9515\n",
            "\n",
            "Epoch 1179: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2147 Val_Loss: 53.9089  BEST VAL Loss: 52.9515\n",
            "\n",
            "Epoch 1180: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1035 Val_Loss: 53.2451  BEST VAL Loss: 52.9515\n",
            "\n",
            "Epoch 1181: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0303 Val_Loss: 53.0415  BEST VAL Loss: 52.9515\n",
            "\n",
            "Epoch 1182: Validation loss decreased (52.951477 --> 52.925354).\n",
            "\t Train_Loss: 7.0756 Val_Loss: 52.9254  BEST VAL Loss: 52.9254\n",
            "\n",
            "Epoch 1183: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1076 Val_Loss: 52.9618  BEST VAL Loss: 52.9254\n",
            "\n",
            "Epoch 1184: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0701 Val_Loss: 53.7620  BEST VAL Loss: 52.9254\n",
            "\n",
            "Epoch 1185: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0991 Val_Loss: 53.4265  BEST VAL Loss: 52.9254\n",
            "\n",
            "Epoch 1186: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1332 Val_Loss: 54.2771  BEST VAL Loss: 52.9254\n",
            "\n",
            "Epoch 1187: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1789 Val_Loss: 54.7066  BEST VAL Loss: 52.9254\n",
            "\n",
            "Epoch 1188: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9524 Val_Loss: 58.9006  BEST VAL Loss: 52.9254\n",
            "\n",
            "Epoch 1189: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4739 Val_Loss: 55.2233  BEST VAL Loss: 52.9254\n",
            "\n",
            "Epoch 1190: Validation loss did not decrease\n",
            "\t Train_Loss: 9.3586 Val_Loss: 54.0873  BEST VAL Loss: 52.9254\n",
            "\n",
            "Epoch 1191: Validation loss decreased (52.925354 --> 52.401028).\n",
            "\t Train_Loss: 8.5166 Val_Loss: 52.4010  BEST VAL Loss: 52.4010\n",
            "\n",
            "Epoch 1192: Validation loss did not decrease\n",
            "\t Train_Loss: 9.5863 Val_Loss: 55.4738  BEST VAL Loss: 52.4010\n",
            "\n",
            "Epoch 1193: Validation loss did not decrease\n",
            "\t Train_Loss: 8.6729 Val_Loss: 55.4704  BEST VAL Loss: 52.4010\n",
            "\n",
            "Epoch 1194: Validation loss decreased (52.401028 --> 50.718960).\n",
            "\t Train_Loss: 9.2665 Val_Loss: 50.7190  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1195: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8422 Val_Loss: 53.8757  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1196: Validation loss did not decrease\n",
            "\t Train_Loss: 8.7583 Val_Loss: 53.1809  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1197: Validation loss did not decrease\n",
            "\t Train_Loss: 9.2453 Val_Loss: 55.0839  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1198: Validation loss did not decrease\n",
            "\t Train_Loss: 9.0620 Val_Loss: 54.2739  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1199: Validation loss did not decrease\n",
            "\t Train_Loss: 9.4053 Val_Loss: 53.6282  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1200: Validation loss did not decrease\n",
            "\t Train_Loss: 8.6515 Val_Loss: 54.5038  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1201: Validation loss did not decrease\n",
            "\t Train_Loss: 8.9266 Val_Loss: 52.9625  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1202: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2025 Val_Loss: 52.2994  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1203: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0478 Val_Loss: 52.9186  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1204: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3463 Val_Loss: 53.0003  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1205: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4395 Val_Loss: 52.9912  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1206: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3456 Val_Loss: 53.1018  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1207: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3932 Val_Loss: 53.0509  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1208: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8343 Val_Loss: 53.0004  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1209: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4973 Val_Loss: 52.3479  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1210: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1550 Val_Loss: 51.7516  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1211: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6476 Val_Loss: 52.8862  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1212: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4796 Val_Loss: 53.2897  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1213: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4595 Val_Loss: 53.1798  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1214: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6856 Val_Loss: 53.1033  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1215: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3663 Val_Loss: 53.1190  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1216: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0690 Val_Loss: 51.9644  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1217: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9119 Val_Loss: 50.9572  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1218: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0439 Val_Loss: 51.1938  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1219: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0055 Val_Loss: 51.4450  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1220: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8641 Val_Loss: 51.6847  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1221: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9384 Val_Loss: 52.1144  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1222: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7629 Val_Loss: 52.7944  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1223: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6728 Val_Loss: 56.3833  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1224: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0315 Val_Loss: 53.7101  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1225: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3944 Val_Loss: 53.8144  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1226: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2858 Val_Loss: 54.7681  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1227: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1258 Val_Loss: 53.8795  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1228: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5017 Val_Loss: 53.3128  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1229: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6076 Val_Loss: 53.5811  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1230: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9730 Val_Loss: 51.7597  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1231: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9884 Val_Loss: 53.5093  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1232: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3234 Val_Loss: 53.1756  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1233: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2824 Val_Loss: 51.9988  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1234: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1723 Val_Loss: 51.2931  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1235: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3547 Val_Loss: 51.9513  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1236: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9427 Val_Loss: 52.0291  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1237: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8558 Val_Loss: 51.3210  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1238: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0170 Val_Loss: 51.6625  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1239: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8532 Val_Loss: 52.5380  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1240: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9011 Val_Loss: 52.5413  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1241: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6844 Val_Loss: 52.0576  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1242: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9819 Val_Loss: 51.9868  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1243: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6083 Val_Loss: 51.9410  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1244: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6139 Val_Loss: 51.5217  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1245: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5590 Val_Loss: 50.7562  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1246: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7738 Val_Loss: 51.8387  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1247: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3206 Val_Loss: 52.3341  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1248: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5483 Val_Loss: 52.1749  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1249: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4280 Val_Loss: 52.1577  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1250: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5510 Val_Loss: 52.2595  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1251: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3509 Val_Loss: 52.2017  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1252: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4493 Val_Loss: 52.1277  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1253: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2975 Val_Loss: 51.7404  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1254: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3548 Val_Loss: 51.9233  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1255: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2229 Val_Loss: 52.0202  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1256: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2511 Val_Loss: 51.6525  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1257: Validation loss did not decrease\n",
            "\t Train_Loss: 6.1969 Val_Loss: 52.1390  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1258: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0828 Val_Loss: 52.2027  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1259: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0595 Val_Loss: 51.9180  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1260: Validation loss did not decrease\n",
            "\t Train_Loss: 6.1207 Val_Loss: 52.6700  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1261: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2767 Val_Loss: 52.3237  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1262: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2210 Val_Loss: 51.8763  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1263: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2154 Val_Loss: 52.0034  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1264: Validation loss did not decrease\n",
            "\t Train_Loss: 6.1034 Val_Loss: 52.2562  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1265: Validation loss did not decrease\n",
            "\t Train_Loss: 6.1669 Val_Loss: 51.4742  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1266: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0577 Val_Loss: 51.2006  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1267: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0420 Val_Loss: 51.6160  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1268: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0290 Val_Loss: 51.8316  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1269: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0216 Val_Loss: 51.7756  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1270: Validation loss did not decrease\n",
            "\t Train_Loss: 5.9691 Val_Loss: 51.5352  BEST VAL Loss: 50.7190\n",
            "\n",
            "Epoch 1271: Validation loss decreased (50.718960 --> 50.547749).\n",
            "\t Train_Loss: 6.0242 Val_Loss: 50.5477  BEST VAL Loss: 50.5477\n",
            "\n",
            "Epoch 1272: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0843 Val_Loss: 52.3171  BEST VAL Loss: 50.5477\n",
            "\n",
            "Epoch 1273: Validation loss did not decrease\n",
            "\t Train_Loss: 6.1041 Val_Loss: 52.2426  BEST VAL Loss: 50.5477\n",
            "\n",
            "Epoch 1274: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0475 Val_Loss: 51.6559  BEST VAL Loss: 50.5477\n",
            "\n",
            "Epoch 1275: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2257 Val_Loss: 51.8967  BEST VAL Loss: 50.5477\n",
            "\n",
            "Epoch 1276: Validation loss did not decrease\n",
            "\t Train_Loss: 5.9749 Val_Loss: 51.4944  BEST VAL Loss: 50.5477\n",
            "\n",
            "Epoch 1277: Validation loss decreased (50.547749 --> 50.343456).\n",
            "\t Train_Loss: 6.0386 Val_Loss: 50.3435  BEST VAL Loss: 50.3435\n",
            "\n",
            "Epoch 1278: Validation loss did not decrease\n",
            "\t Train_Loss: 5.9660 Val_Loss: 50.9422  BEST VAL Loss: 50.3435\n",
            "\n",
            "Epoch 1279: Validation loss did not decrease\n",
            "\t Train_Loss: 5.8598 Val_Loss: 51.2395  BEST VAL Loss: 50.3435\n",
            "\n",
            "Epoch 1280: Validation loss did not decrease\n",
            "\t Train_Loss: 5.8585 Val_Loss: 51.1351  BEST VAL Loss: 50.3435\n",
            "\n",
            "Epoch 1281: Validation loss did not decrease\n",
            "\t Train_Loss: 5.8060 Val_Loss: 51.0524  BEST VAL Loss: 50.3435\n",
            "\n",
            "Epoch 1282: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7715 Val_Loss: 51.0826  BEST VAL Loss: 50.3435\n",
            "\n",
            "Epoch 1283: Validation loss did not decrease\n",
            "\t Train_Loss: 5.8168 Val_Loss: 50.4365  BEST VAL Loss: 50.3435\n",
            "\n",
            "Epoch 1284: Validation loss did not decrease\n",
            "\t Train_Loss: 5.9127 Val_Loss: 52.0851  BEST VAL Loss: 50.3435\n",
            "\n",
            "Epoch 1285: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0900 Val_Loss: 51.6751  BEST VAL Loss: 50.3435\n",
            "\n",
            "Epoch 1286: Validation loss did not decrease\n",
            "\t Train_Loss: 5.9314 Val_Loss: 51.0546  BEST VAL Loss: 50.3435\n",
            "\n",
            "Epoch 1287: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2474 Val_Loss: 50.7229  BEST VAL Loss: 50.3435\n",
            "\n",
            "Epoch 1288: Validation loss did not decrease\n",
            "\t Train_Loss: 6.1114 Val_Loss: 50.6313  BEST VAL Loss: 50.3435\n",
            "\n",
            "Epoch 1289: Validation loss decreased (50.343456 --> 49.562817).\n",
            "\t Train_Loss: 5.9699 Val_Loss: 49.5628  BEST VAL Loss: 49.5628\n",
            "\n",
            "Epoch 1290: Validation loss decreased (49.562817 --> 47.885597).\n",
            "\t Train_Loss: 5.9150 Val_Loss: 47.8856  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1291: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5088 Val_Loss: 49.7119  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1292: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5579 Val_Loss: 49.3829  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1293: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0410 Val_Loss: 49.6195  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1294: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3276 Val_Loss: 49.4737  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1295: Validation loss did not decrease\n",
            "\t Train_Loss: 6.1654 Val_Loss: 48.4971  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1296: Validation loss did not decrease\n",
            "\t Train_Loss: 6.1567 Val_Loss: 49.8947  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1297: Validation loss did not decrease\n",
            "\t Train_Loss: 6.1638 Val_Loss: 50.2376  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1298: Validation loss did not decrease\n",
            "\t Train_Loss: 5.9579 Val_Loss: 49.8221  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1299: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0246 Val_Loss: 48.8741  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1300: Validation loss did not decrease\n",
            "\t Train_Loss: 5.9411 Val_Loss: 49.9298  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1301: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7590 Val_Loss: 50.6173  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1302: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7966 Val_Loss: 50.5320  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1303: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7216 Val_Loss: 50.9144  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1304: Validation loss did not decrease\n",
            "\t Train_Loss: 5.9091 Val_Loss: 50.2574  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1305: Validation loss did not decrease\n",
            "\t Train_Loss: 5.8231 Val_Loss: 50.7204  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1306: Validation loss did not decrease\n",
            "\t Train_Loss: 5.8591 Val_Loss: 51.0321  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1307: Validation loss did not decrease\n",
            "\t Train_Loss: 5.8310 Val_Loss: 50.5184  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1308: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7242 Val_Loss: 49.6694  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1309: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7480 Val_Loss: 50.0254  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1310: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7063 Val_Loss: 49.9445  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1311: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7075 Val_Loss: 49.6308  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1312: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6627 Val_Loss: 49.3623  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1313: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6909 Val_Loss: 49.6256  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1314: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6710 Val_Loss: 49.4796  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1315: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6043 Val_Loss: 49.0253  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1316: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6336 Val_Loss: 49.0956  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1317: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5841 Val_Loss: 49.6287  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1318: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5630 Val_Loss: 50.3604  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1319: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4876 Val_Loss: 50.5883  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1320: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5251 Val_Loss: 51.0828  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1321: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4998 Val_Loss: 50.6358  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1322: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4602 Val_Loss: 49.9264  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1323: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4572 Val_Loss: 48.9212  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1324: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7725 Val_Loss: 51.4465  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1325: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5080 Val_Loss: 49.0389  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1326: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4701 Val_Loss: 49.7274  BEST VAL Loss: 47.8856\n",
            "\n",
            "Epoch 1327: Validation loss decreased (47.885597 --> 47.567825).\n",
            "\t Train_Loss: 13.0750 Val_Loss: 47.5678  BEST VAL Loss: 47.5678\n",
            "\n",
            "Epoch 1328: Validation loss decreased (47.567825 --> 45.556751).\n",
            "\t Train_Loss: 11.6576 Val_Loss: 45.5568  BEST VAL Loss: 45.5568\n",
            "\n",
            "Epoch 1329: Validation loss did not decrease\n",
            "\t Train_Loss: 11.6288 Val_Loss: 48.3607  BEST VAL Loss: 45.5568\n",
            "\n",
            "Epoch 1330: Validation loss did not decrease\n",
            "\t Train_Loss: 10.1639 Val_Loss: 48.9047  BEST VAL Loss: 45.5568\n",
            "\n",
            "Epoch 1331: Validation loss did not decrease\n",
            "\t Train_Loss: 12.1965 Val_Loss: 49.1633  BEST VAL Loss: 45.5568\n",
            "\n",
            "Epoch 1332: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5941 Val_Loss: 46.6056  BEST VAL Loss: 45.5568\n",
            "\n",
            "Epoch 1333: Validation loss decreased (45.556751 --> 43.387974).\n",
            "\t Train_Loss: 9.8631 Val_Loss: 43.3880  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1334: Validation loss did not decrease\n",
            "\t Train_Loss: 8.7211 Val_Loss: 47.6013  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1335: Validation loss did not decrease\n",
            "\t Train_Loss: 9.3162 Val_Loss: 45.0897  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1336: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9259 Val_Loss: 45.5752  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1337: Validation loss did not decrease\n",
            "\t Train_Loss: 9.2718 Val_Loss: 49.6390  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1338: Validation loss did not decrease\n",
            "\t Train_Loss: 9.0223 Val_Loss: 47.8695  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1339: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4820 Val_Loss: 49.0541  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1340: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4746 Val_Loss: 48.2281  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1341: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4887 Val_Loss: 48.6368  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1342: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9770 Val_Loss: 50.9805  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1343: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2120 Val_Loss: 45.3610  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1344: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6133 Val_Loss: 45.3116  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1345: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4645 Val_Loss: 45.2091  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1346: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1678 Val_Loss: 48.4318  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1347: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2497 Val_Loss: 47.1993  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1348: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0856 Val_Loss: 45.4591  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1349: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5031 Val_Loss: 46.8088  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1350: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8416 Val_Loss: 47.4074  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1351: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6215 Val_Loss: 46.7343  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1352: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5267 Val_Loss: 45.7193  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1353: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4930 Val_Loss: 45.4403  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1354: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5936 Val_Loss: 45.5644  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1355: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0375 Val_Loss: 44.8446  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1356: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7241 Val_Loss: 45.3983  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1357: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8783 Val_Loss: 47.0814  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1358: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3340 Val_Loss: 46.8930  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1359: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6849 Val_Loss: 45.3724  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1360: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2241 Val_Loss: 44.7024  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1361: Validation loss did not decrease\n",
            "\t Train_Loss: 6.1815 Val_Loss: 44.5854  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1362: Validation loss did not decrease\n",
            "\t Train_Loss: 6.1188 Val_Loss: 44.9444  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1363: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0791 Val_Loss: 45.2361  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1364: Validation loss did not decrease\n",
            "\t Train_Loss: 5.9671 Val_Loss: 44.4779  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1365: Validation loss did not decrease\n",
            "\t Train_Loss: 5.9713 Val_Loss: 44.2294  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1366: Validation loss did not decrease\n",
            "\t Train_Loss: 5.8168 Val_Loss: 43.8226  BEST VAL Loss: 43.3880\n",
            "\n",
            "Epoch 1367: Validation loss decreased (43.387974 --> 43.039001).\n",
            "\t Train_Loss: 6.0061 Val_Loss: 43.0390  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1368: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7559 Val_Loss: 43.6683  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1369: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7182 Val_Loss: 44.5028  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1370: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6755 Val_Loss: 44.4767  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1371: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7988 Val_Loss: 44.5325  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1372: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6066 Val_Loss: 46.0927  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1373: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5337 Val_Loss: 47.9043  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1374: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5099 Val_Loss: 48.3991  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1375: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5126 Val_Loss: 48.6396  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1376: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5449 Val_Loss: 48.5004  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1377: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4501 Val_Loss: 47.9724  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1378: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4054 Val_Loss: 47.7228  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1379: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4382 Val_Loss: 47.6608  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1380: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3798 Val_Loss: 47.6515  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1381: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3967 Val_Loss: 47.7384  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1382: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3829 Val_Loss: 47.9487  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1383: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2959 Val_Loss: 47.8493  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1384: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3097 Val_Loss: 47.4588  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1385: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3189 Val_Loss: 47.3239  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1386: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3043 Val_Loss: 47.5572  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1387: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2621 Val_Loss: 47.7022  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1388: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2487 Val_Loss: 47.7566  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1389: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2395 Val_Loss: 47.8111  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1390: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2360 Val_Loss: 47.7398  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1391: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1973 Val_Loss: 47.6108  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1392: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2004 Val_Loss: 47.6114  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1393: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2107 Val_Loss: 47.4977  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1394: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1824 Val_Loss: 47.6064  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1395: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1619 Val_Loss: 47.9014  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1396: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1620 Val_Loss: 47.8679  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1397: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1529 Val_Loss: 47.7293  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1398: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1405 Val_Loss: 47.7712  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1399: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1252 Val_Loss: 47.7677  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1400: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1181 Val_Loss: 47.9170  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1401: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1156 Val_Loss: 48.1541  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1402: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1026 Val_Loss: 48.1689  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1403: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0928 Val_Loss: 48.0939  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1404: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0906 Val_Loss: 48.2012  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1405: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0781 Val_Loss: 48.0852  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1406: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0683 Val_Loss: 48.0747  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1407: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0596 Val_Loss: 48.2557  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1408: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0535 Val_Loss: 48.2793  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1409: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0464 Val_Loss: 48.2800  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1410: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0342 Val_Loss: 48.2695  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1411: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0270 Val_Loss: 48.0617  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1412: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0204 Val_Loss: 48.0728  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1413: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0093 Val_Loss: 48.1626  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1414: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0022 Val_Loss: 48.1504  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1415: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9968 Val_Loss: 48.1169  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1416: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9912 Val_Loss: 47.9316  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1417: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9848 Val_Loss: 48.1218  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1418: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9786 Val_Loss: 47.9569  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1419: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9787 Val_Loss: 48.0720  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1420: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9904 Val_Loss: 47.6823  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1421: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9965 Val_Loss: 48.2037  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1422: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9830 Val_Loss: 47.9136  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1423: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9467 Val_Loss: 47.9920  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1424: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9325 Val_Loss: 48.1667  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1425: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9387 Val_Loss: 47.8003  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1426: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9469 Val_Loss: 48.2301  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1427: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9353 Val_Loss: 47.9071  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1428: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9113 Val_Loss: 48.1013  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1429: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9000 Val_Loss: 48.2323  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1430: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8978 Val_Loss: 47.9381  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1431: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8973 Val_Loss: 48.1822  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1432: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8920 Val_Loss: 47.9010  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1433: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8801 Val_Loss: 48.1272  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1434: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8696 Val_Loss: 48.0201  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1435: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8608 Val_Loss: 47.9160  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1436: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8554 Val_Loss: 48.0353  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1437: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8519 Val_Loss: 47.9177  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1438: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8502 Val_Loss: 48.2349  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1439: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8515 Val_Loss: 47.8118  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1440: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8526 Val_Loss: 48.2040  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1441: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8536 Val_Loss: 47.7438  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1442: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8497 Val_Loss: 48.2649  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1443: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8443 Val_Loss: 47.7976  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1444: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8204 Val_Loss: 48.0076  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1445: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8061 Val_Loss: 47.7712  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1446: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7963 Val_Loss: 47.7783  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1447: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7894 Val_Loss: 47.8295  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1448: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7845 Val_Loss: 47.7186  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1449: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7816 Val_Loss: 47.9650  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1450: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7813 Val_Loss: 47.6406  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1451: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7850 Val_Loss: 48.1258  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1452: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7952 Val_Loss: 47.5127  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1453: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8112 Val_Loss: 48.2107  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1454: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8304 Val_Loss: 47.2410  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1455: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8436 Val_Loss: 48.1371  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1456: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8399 Val_Loss: 47.2339  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1457: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8076 Val_Loss: 47.8832  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1458: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7576 Val_Loss: 47.5006  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1459: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7255 Val_Loss: 47.4541  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1460: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7202 Val_Loss: 47.7926  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1461: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7355 Val_Loss: 47.1546  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1462: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7621 Val_Loss: 47.8996  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1463: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8006 Val_Loss: 46.8783  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1464: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8265 Val_Loss: 48.0210  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1465: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8215 Val_Loss: 46.9952  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1466: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7432 Val_Loss: 47.5166  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1467: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6848 Val_Loss: 47.6181  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1468: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6853 Val_Loss: 46.8819  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1469: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7329 Val_Loss: 47.6912  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1470: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7945 Val_Loss: 46.4745  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1471: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8007 Val_Loss: 47.6938  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1472: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7734 Val_Loss: 47.0396  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1473: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6918 Val_Loss: 47.2215  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1474: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6504 Val_Loss: 47.4533  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1475: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6919 Val_Loss: 46.3725  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1476: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7326 Val_Loss: 47.4320  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1477: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7245 Val_Loss: 46.8221  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1478: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6852 Val_Loss: 47.1924  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1479: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6590 Val_Loss: 47.2467  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1480: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6389 Val_Loss: 47.0003  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1481: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6800 Val_Loss: 47.7093  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1482: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6401 Val_Loss: 47.1896  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1483: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6230 Val_Loss: 47.0411  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1484: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5982 Val_Loss: 46.7514  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1485: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5938 Val_Loss: 46.8877  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1486: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5715 Val_Loss: 47.4436  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1487: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5787 Val_Loss: 47.2389  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1488: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5741 Val_Loss: 47.7501  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1489: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5950 Val_Loss: 47.0153  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1490: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6268 Val_Loss: 48.3291  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1491: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7221 Val_Loss: 47.0683  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1492: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7091 Val_Loss: 47.5170  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1493: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5555 Val_Loss: 47.1120  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1494: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5365 Val_Loss: 46.7324  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1495: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5860 Val_Loss: 48.5614  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1496: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8845 Val_Loss: 45.5913  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1497: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3803 Val_Loss: 47.3645  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1498: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4297 Val_Loss: 46.8362  BEST VAL Loss: 43.0390\n",
            "\n",
            "Epoch 1499: Validation loss decreased (43.039001 --> 42.727581).\n",
            "\t Train_Loss: 8.4583 Val_Loss: 42.7276  BEST VAL Loss: 42.7276\n",
            "\n",
            "Epoch 1500: Validation loss did not decrease\n",
            "\t Train_Loss: 9.1327 Val_Loss: 43.4494  BEST VAL Loss: 42.7276\n",
            "\n",
            "Epoch 1501: Validation loss did not decrease\n",
            "\t Train_Loss: 9.7339 Val_Loss: 54.4121  BEST VAL Loss: 42.7276\n",
            "\n",
            "Epoch 1502: Validation loss did not decrease\n",
            "\t Train_Loss: 13.7158 Val_Loss: 45.5891  BEST VAL Loss: 42.7276\n",
            "\n",
            "Epoch 1503: Validation loss decreased (42.727581 --> 41.396938).\n",
            "\t Train_Loss: 10.3393 Val_Loss: 41.3969  BEST VAL Loss: 41.3969\n",
            "\n",
            "Epoch 1504: Validation loss decreased (41.396938 --> 39.530209).\n",
            "\t Train_Loss: 9.3452 Val_Loss: 39.5302  BEST VAL Loss: 39.5302\n",
            "\n",
            "Epoch 1505: Validation loss decreased (39.530209 --> 38.117985).\n",
            "\t Train_Loss: 10.3250 Val_Loss: 38.1180  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1506: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5705 Val_Loss: 39.5000  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1507: Validation loss did not decrease\n",
            "\t Train_Loss: 8.5940 Val_Loss: 42.4406  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1508: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4010 Val_Loss: 47.5508  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1509: Validation loss did not decrease\n",
            "\t Train_Loss: 9.5235 Val_Loss: 46.0127  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1510: Validation loss did not decrease\n",
            "\t Train_Loss: 9.2832 Val_Loss: 41.9631  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1511: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1594 Val_Loss: 40.0484  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1512: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5254 Val_Loss: 39.3381  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1513: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4996 Val_Loss: 40.1695  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1514: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6358 Val_Loss: 40.6807  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1515: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4815 Val_Loss: 42.0999  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1516: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7585 Val_Loss: 42.9513  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1517: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2510 Val_Loss: 43.0088  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1518: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7931 Val_Loss: 43.3389  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1519: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5122 Val_Loss: 45.2930  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1520: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8292 Val_Loss: 46.3669  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1521: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8876 Val_Loss: 44.3384  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1522: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8150 Val_Loss: 44.5828  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1523: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2867 Val_Loss: 43.1570  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1524: Validation loss did not decrease\n",
            "\t Train_Loss: 5.9679 Val_Loss: 41.3037  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1525: Validation loss did not decrease\n",
            "\t Train_Loss: 5.9942 Val_Loss: 40.7417  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1526: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3433 Val_Loss: 40.4069  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1527: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3670 Val_Loss: 40.6833  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1528: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0134 Val_Loss: 42.4274  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1529: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5829 Val_Loss: 45.5844  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1530: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7535 Val_Loss: 46.3974  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1531: Validation loss did not decrease\n",
            "\t Train_Loss: 5.8430 Val_Loss: 46.0399  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1532: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7385 Val_Loss: 46.9636  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1533: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5416 Val_Loss: 47.3507  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1534: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5748 Val_Loss: 45.5421  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1535: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7291 Val_Loss: 45.9948  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1536: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4685 Val_Loss: 46.4720  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1537: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5394 Val_Loss: 43.3600  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1538: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2896 Val_Loss: 42.0186  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1539: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4772 Val_Loss: 45.3409  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1540: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4151 Val_Loss: 45.9274  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1541: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3578 Val_Loss: 43.9631  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1542: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2549 Val_Loss: 43.6972  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1543: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1827 Val_Loss: 45.6968  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1544: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2632 Val_Loss: 45.6425  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1545: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0312 Val_Loss: 45.2559  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1546: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1009 Val_Loss: 45.7512  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1547: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0491 Val_Loss: 45.1405  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1548: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0017 Val_Loss: 43.5147  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1549: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9019 Val_Loss: 43.0422  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1550: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8962 Val_Loss: 43.3851  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1551: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8864 Val_Loss: 42.8829  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1552: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8335 Val_Loss: 42.2595  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1553: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7986 Val_Loss: 43.3691  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1554: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7491 Val_Loss: 43.4598  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1555: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7522 Val_Loss: 42.5271  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1556: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7282 Val_Loss: 42.8247  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1557: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7000 Val_Loss: 43.4456  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1558: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7033 Val_Loss: 42.8910  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1559: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6897 Val_Loss: 41.9975  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1560: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6805 Val_Loss: 42.1754  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1561: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6585 Val_Loss: 42.5114  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1562: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6416 Val_Loss: 41.9879  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1563: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6347 Val_Loss: 41.8459  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1564: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6191 Val_Loss: 42.2023  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1565: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6041 Val_Loss: 42.3074  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1566: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5671 Val_Loss: 42.7074  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1567: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5903 Val_Loss: 44.3709  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1568: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6866 Val_Loss: 43.2809  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1569: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7231 Val_Loss: 42.4468  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1570: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7399 Val_Loss: 42.5943  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1571: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6525 Val_Loss: 43.5026  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1572: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5732 Val_Loss: 44.3250  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1573: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5954 Val_Loss: 43.6995  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1574: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9686 Val_Loss: 45.4371  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1575: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7782 Val_Loss: 43.1217  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1576: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9988 Val_Loss: 42.1465  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1577: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0969 Val_Loss: 42.5722  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1578: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9211 Val_Loss: 42.7523  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1579: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6341 Val_Loss: 42.9125  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1580: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7748 Val_Loss: 43.8122  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1581: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8358 Val_Loss: 42.9902  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1582: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6807 Val_Loss: 42.5237  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1583: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8130 Val_Loss: 43.0093  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1584: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6089 Val_Loss: 43.3889  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1585: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8112 Val_Loss: 41.8091  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1586: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7849 Val_Loss: 42.7892  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1587: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6741 Val_Loss: 44.7767  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1588: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6016 Val_Loss: 43.7252  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1589: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5926 Val_Loss: 42.9100  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1590: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6092 Val_Loss: 43.2126  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1591: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5989 Val_Loss: 42.6681  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1592: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5176 Val_Loss: 42.0493  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1593: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4547 Val_Loss: 43.1389  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1594: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4778 Val_Loss: 44.1060  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1595: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5399 Val_Loss: 44.3664  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1596: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4338 Val_Loss: 43.5400  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1597: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4351 Val_Loss: 44.5737  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1598: Validation loss did not decrease\n",
            "\t Train_Loss: 4.3695 Val_Loss: 45.1526  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1599: Validation loss did not decrease\n",
            "\t Train_Loss: 4.3932 Val_Loss: 44.4870  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1600: Validation loss did not decrease\n",
            "\t Train_Loss: 4.3855 Val_Loss: 44.9500  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1601: Validation loss did not decrease\n",
            "\t Train_Loss: 4.3497 Val_Loss: 45.6383  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1602: Validation loss did not decrease\n",
            "\t Train_Loss: 4.3340 Val_Loss: 44.9517  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1603: Validation loss did not decrease\n",
            "\t Train_Loss: 4.3245 Val_Loss: 45.0072  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1604: Validation loss did not decrease\n",
            "\t Train_Loss: 4.3164 Val_Loss: 45.4944  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1605: Validation loss did not decrease\n",
            "\t Train_Loss: 4.3192 Val_Loss: 45.0355  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1606: Validation loss did not decrease\n",
            "\t Train_Loss: 4.2905 Val_Loss: 45.1662  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1607: Validation loss did not decrease\n",
            "\t Train_Loss: 4.2838 Val_Loss: 45.4626  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1608: Validation loss did not decrease\n",
            "\t Train_Loss: 4.2784 Val_Loss: 44.8007  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1609: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4931 Val_Loss: 47.1984  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1610: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8173 Val_Loss: 45.8159  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1611: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5012 Val_Loss: 46.2311  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1612: Validation loss did not decrease\n",
            "\t Train_Loss: 14.8330 Val_Loss: 40.2536  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1613: Validation loss did not decrease\n",
            "\t Train_Loss: 8.7936 Val_Loss: 42.2201  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1614: Validation loss did not decrease\n",
            "\t Train_Loss: 9.0930 Val_Loss: 45.6743  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1615: Validation loss did not decrease\n",
            "\t Train_Loss: 9.2538 Val_Loss: 45.4503  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1616: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8512 Val_Loss: 54.1408  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1617: Validation loss did not decrease\n",
            "\t Train_Loss: 16.4660 Val_Loss: 41.8574  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1618: Validation loss did not decrease\n",
            "\t Train_Loss: 11.2183 Val_Loss: 41.0004  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1619: Validation loss did not decrease\n",
            "\t Train_Loss: 12.0611 Val_Loss: 46.6531  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1620: Validation loss did not decrease\n",
            "\t Train_Loss: 12.2182 Val_Loss: 47.5286  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1621: Validation loss did not decrease\n",
            "\t Train_Loss: 9.6968 Val_Loss: 47.0279  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1622: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4325 Val_Loss: 46.0104  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1623: Validation loss did not decrease\n",
            "\t Train_Loss: 9.1406 Val_Loss: 48.3406  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1624: Validation loss did not decrease\n",
            "\t Train_Loss: 8.7341 Val_Loss: 49.2546  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1625: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4362 Val_Loss: 47.6724  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1626: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8894 Val_Loss: 47.7888  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1627: Validation loss did not decrease\n",
            "\t Train_Loss: 8.5640 Val_Loss: 52.6841  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1628: Validation loss did not decrease\n",
            "\t Train_Loss: 12.4752 Val_Loss: 47.1008  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1629: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7054 Val_Loss: 49.1116  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1630: Validation loss did not decrease\n",
            "\t Train_Loss: 12.4246 Val_Loss: 52.9802  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1631: Validation loss did not decrease\n",
            "\t Train_Loss: 10.3030 Val_Loss: 52.1903  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1632: Validation loss did not decrease\n",
            "\t Train_Loss: 9.7144 Val_Loss: 51.6128  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1633: Validation loss did not decrease\n",
            "\t Train_Loss: 10.1850 Val_Loss: 50.4443  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1634: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0112 Val_Loss: 51.3327  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1635: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2658 Val_Loss: 51.8865  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1636: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8352 Val_Loss: 51.4281  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1637: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0657 Val_Loss: 50.2005  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1638: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9037 Val_Loss: 49.1502  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1639: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6437 Val_Loss: 49.0831  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1640: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4512 Val_Loss: 49.4958  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1641: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4231 Val_Loss: 49.7365  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1642: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5902 Val_Loss: 49.3790  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1643: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4615 Val_Loss: 48.6958  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1644: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1156 Val_Loss: 53.1234  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1645: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3731 Val_Loss: 48.1791  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1646: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7107 Val_Loss: 47.9870  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1647: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6208 Val_Loss: 47.8894  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1648: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6081 Val_Loss: 47.8268  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1649: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4510 Val_Loss: 47.2082  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1650: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4567 Val_Loss: 46.4846  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1651: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3366 Val_Loss: 46.6203  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1652: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3660 Val_Loss: 46.7864  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1653: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3486 Val_Loss: 46.2920  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1654: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2311 Val_Loss: 45.7845  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1655: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2884 Val_Loss: 45.8702  BEST VAL Loss: 38.1180\n",
            "\n",
            "Epoch 1656: Validation loss did not decrease\n",
            "Early stopped at epoch : 1656\n"
          ]
        }
      ],
      "source": [
        "GRU_best_model, train_losses, val_losses = trainer(GRU_model, X_train, y_train, X_val, y_val, optimizer, criterion, n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "jaUYwd9vFHuC",
        "outputId": "95cab696-7e85-464e-cb5a-6ac771b3e84d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(train_losses, label = 'Train_loss')\n",
        "plt.plot(val_losses, label = 'validation_loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jcdsct5Y9a8"
      },
      "source": [
        "---\n",
        "### 5.4 Evaluate model on validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "PHWWy8uCY7Vg"
      },
      "outputs": [],
      "source": [
        "val_predict_GRU = GRU_best_model(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "wThiG7D-ZBA_",
        "outputId": "0caeb71a-b13f-47b3-d7e4-af73ce756636"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABIgAAAHQCAYAAADKyVH+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3hT1f8H8He6995ltJQySssoe8jeCgqoIIKAG8U9EEQRfyo4QUVB1C8q4BYREBGULbullNIBlELponuvNLm/P9Jckq4kbdIk7fv1PH0gycm55+be3Hvzued8jkQQBAFERERERERERNRuWRi7AUREREREREREZFwMEBERERERERERtXMMEBERERERERERtXMMEBERERERERERtXMMEBERERERERERtXMMEBERERERERERtXMMEBERERERERERtXMMEBERERERERERtXMMEBERERERERERtXMMEFGbFRQUBIlEAolEgkOHDhm7OWZj9OjR4uf2zTffGLs5pIU33nhD3GYLFy40dnNa3aFDh8T1DwoKMnZziHSycOFCcf994403Gixz7do1sYxEImndBuqovZ9DVLfTtWvXjN0cIiKRsY5P5nQOIwaIyASoXhw35689XoASESmpBsiauuire4FW98/CwgIuLi4IDg7GjBkz8Omnn6KwsLBV14WIzEdNTQ0OHDiA5cuXY+TIkejatSvc3NxgY2MDLy8vdOvWDXfeeSdWrVqFEydOQBAErerVdN3n5OSEDh06YPz48VixYgWSkpK0bnPd46WuNxC/+eYbvf/IVr3J09CftbU1PD090atXL8yfPx8//PADqqqqmt1miUSCd955p1ntGzBggNGXQ0SGxQARkYljTyhqT3j33XgEQUBJSQmuXbuGHTt24Omnn0bHjh3xxRdfGLtpZATtvWciNU4mk+G7775Djx49MG7cOKxevRpHjx5FcnIyioqKIJVKkZeXh8uXL2Pnzp144403MGzYMAQFBeGdd95BaWlpi5ZfVlaG9PR0/Pvvv3j77bfRo0cPLFq0CCUlJXpaQ9NSU1OD/Px8xMfHY+vWrZg7dy5CQkLw999/N7vODz74AEVFRXpspXGXQ0T6Y2XsBhCpcnd3x6BBg3R6T2BgoIFaQ0TUdg0cOBAeHh7iY0EQkJ+fj7i4OFRWVgIASktL8fjjjyM7OxuvvfaasZpKRCYiPz8fd999Nw4ePFjvtcDAQPj6+sLNzQ2FhYXIzs5GWlqa+HpqaipeffVVfPTRR7hy5Qrc3Nw0Li88PLzedV5JSQkSExORn58vPvfNN9/g6tWr2LdvH2xtbZu/gkZmZ2eHUaNGqT0nlUqRlZWFxMREyOVyAEB6ejpuv/12/P7775g2bZrOyykoKMAHH3yA//u//9NLu429HCLSHwaIyKT07t0be/fuNXYz2jX2UiJzM3r0aK2HLtAt7733HkaPHl3v+fLycqxfvx4rVqyAVCoFAKxcuRKTJk3SOYBP+hEUFGQ2+zjPIW1XRkYGRo0ahStXrojPde7cGS+//DLuuOMOdOrUqd57srKysG/fPmzZsgX//PMPACAvL08MQmvywgsvNNiDTRAE7NmzB0888QRSU1MBAEeOHMG6deuwdOnSZqydafD19W30OvjmzZtYtWoVNmzYAEDRk+uhhx7C1atX4eTkpPOy1q1bh6effhre3t4tarOpLIeI9INDzIiIiEjk4OCAl19+GZs3bxafEwQBq1evNmKriMiYampqMHv2bLXg0LPPPotLly7hiSeeaDA4BAB+fn544IEHsH//fpw6dQojRozQS3skEgluv/12HDp0CM7OzuLz69atM5tgqq58fX3x+eefY8mSJeJzOTk52LJli9Z1ODs7w9fXF4Cih+iaNWv03s7WXA4R6R8DRERERFTP/fffj/79+4uP//nnH7FHERG1L2+//TaOHTsmPl6+fDnWrl0LGxsbresYNGgQDh06hP/7v/+DlZV+BjEEBwdj0aJF4uOsrCzEx8frpW5TtXLlSlhY3PoJd+DAAa3fa2Njg+XLl4uPN2zYgIyMDL22rzWXQ0T6xwARkYrr16/jnXfewciRI9GhQwfY2trC09MTffv2xYsvvtisi46amhr8/PPPWLBgAXr06AEPDw9YW1vDw8MDAwcOxBNPPIE///wTMplMfI/qbEPXr18Xnx8zZkyDM1zUHSbS2LTfCQkJWLp0Kfr27Qtvb29YWFjUmxa8OVMUFxYW4rPPPsP06dPRpUsXODs7w9bWFn5+fhg9ejRWrFiBs2fP6vrR1dPYNJlXr17FK6+8gt69e8Pd3R1OTk4ICwvD888/j8uXL2tVd0PJkXNycvDRRx9hxIgR6NChA6ytrZtMnrxz504sWLAAoaGhcHFxgaOjI4KDgzFr1ix89913qKmp0Wl95XI5tm7dismTJyMgIAB2dnbo3Lkzpk6dip9++kltn9GkOVPBNydJbUVFBTZv3ozZs2cjNDRUnNHG29sbw4cPxwsvvIBDhw6p3eFVbZuq4ODgBvf3um1pzroVFBRg7dq1GDduHDp06AA7Ozt4enoiIiICzzzzDE6fPq1VPY19RidPnsTChQvRrVs3ODg4wN3dHQMHDsSbb75pVgk7p0yZIv6/tLS0RYnDVWe4UT1m/ffff1iwYAG6d+8OR0dHeHp6YtCgQVizZo1Ws6i15Hin6vTp03jhhRfQr18/+Pj4iMew2267DatXr0Zubq5O61tVVYUNGzZg1KhR8PHxgb29PUJCQnD33Xfjr7/+0qmu5k4RHBMTg+XLl2Pw4MEICAiAra0tnJycEBoailmzZmHDhg3IyclRe4/yHLBq1SrxuW+//bbRGZbq7hPNOYccOnQIjz/+OMLCwuDu7g57e3vxWLdhwwaUlZVpVU9D7SouLsYnn3yCYcOGwdfXF3Z2dujYsSPmzJmj0w/rlsjKysJbb72FAQMGwNvbGw4ODggNDcVjjz2G6OjoJt87fPhwcZ10GTpVUVEBNzc38b0///xzs9tfUlKCdevWiY8HDhyotn/owtLSEitWrICXl1ez21NX3V5JKSkpeqvbFHl5eaFHjx7iY13X97HHHkPHjh0BKPaTt956S6/ta+3lNEV1puQ33ngDgOLa6pdffsEdd9yBzp07w9bWFt7e3pg+fXqjx4QTJ05g/vz5CAoKgq2tLTw8PDBixAhs2rRJzAulrfT0dLz99tsYPnw4/P39YWtrCx8fH/Tv3x/Lli1DQkKCzuuZnJyMF198EWFhYXBycoK7uzt69+6NpUuX4urVqzrXp0rf50YyAwKRkS1YsEAAIAAQRo0apbd6O3fuLNZ78ODBJstKpVJh2bJlgq2trfiehv4sLS2F5557TqipqdGqDfv27RO6devWZJ0NrXtKSopW72nsczt48KD4WufOnQVBEITVq1cLVlZW9d6rfF1p1KhR4mubN2/WuI7r1q0T3NzctGrnypUrtfrcGlP3cxEEQdiyZYtgb2/f6DLt7OyETz/9VGPdqu9JSUkR9uzZI3h7ezdYZ0pKitp7k5OThWHDhmlc/x49eggnT57Ual3T09OF4cOHN1nf2LFjhZycHGHlypXicwsWLGiwvob2CU20qVfVtm3bhICAAK32BdX6VNum63ubs25bt24VPD09NS7n/vvvF0pLS3X6jKqrq4Vnn322yXr9/PyE2NhYje3UVt3Pr+7+qVT3+6PpuCgIgvDFF1+ovefEiRPNbufmzZvVjllSqVTjZxUQECAcOnSoyXpbcrwTBEHIzs4WZs2apXF/cHNzE7799lut1jU+Pl7o1atXk/XNmTNHKC0tVTsHNnaMbOjY15Ts7Gzh7rvvFiQSicb1srGxERITE8X3qp4DtPmru7/pcg7JyckR7rjjDo3LCAwMFP7880+N6123XWfOnBGCgoKarHvJkiWCXC7XWLe26rbh77//Fjw8PBpdvoWFhbBs2bJG2/DNN9+oHTukUqlW7fjuu+/E93l5eQlVVVXNXqd169aptXnXrl3NrksbqsvS5jpk//79au/ZunVro2XrHi+1OQ6qUj2ONXW81YXqeUTb87Pq9UHXrl21brOnp6cgCIKwadMm8Tlra2vh6tWrWrWvf//+Rl+OLuoeX/Py8oSJEyc2eUxYs2aN+H6ZTCYsWbKkyfLjxo0TKioqtGrPhx9+KDg6OjZZn5WVlfDcc89p/V3fsGFDk9fB9vb2wpYtWwRBqH98aoo+z426nsPIuJikmtq9yspK3H333fjzzz/F5ywsLBAWFgZvb2+UlpYiNjYWVVVVkMlkWLt2LW7cuIGff/65yTu5X375JRYvXqzWy8PBwQE9evSAm5sbiouLkZiYKE73qnqn3N7eHpMmTQIAHD58WEzmWHfWIaXevXs3uY7vv/8+li1bBgCwtbVFeHg4nJ2dcePGDZ16oaiSy+V46KGH6t0h9vLyQkhICBwcHJCbm4vExERxWIo2vQF0sXv3bsyfPx+A4q5kREQEXF1dkZKSIiatrKysxFNPPQWZTIZnnnlGq3qPHz+OBQsWoKamBhKJBD179oSvry9yc3Pr9SJLSkrC2LFj1bpOK3sw2djYICEhAXl5eQCAxMREjBs3Drt3724wObBSfn4+JkyYoLYsGxsbREREwNHREZcuXUJWVhYOHDiA6dOnY+zYsVqtlyG9/vrr9WYpcXV1FXtTFRQUICEhQdyXVfcFDw8PcX9XnbZ35MiRsLe3r7esiIiIZrfzk08+qbcfdOzYEV26dEFxcTEuXLgg9vTatm0brl69ir///lstx0VTFi9ejK+//hoA4Onpie7du8PS0hJxcXEoKCgAoOhNMHnyZCQkJMDFxaXZ69Iaqqur1R7rMpxEk2XLlom9EpycnNCrVy9YWVkhISFBnJ0oIyMDU6dOxf79+zFs2DCt6tXleJeSkoKJEyeq5VWxt7dHr1694OLigps3byI+Ph6CIKCwsBALFixAUVERnnrqqUaXn5KSgnHjxiEzM1N8ztHREb169YK1tbW4fj/++CPkcnmD+3hLXLlyBZMmTap317hbt27w9/dHTU0NUlNTcePGDQCKbVxRUSGWGzRoEOzs7HDlyhUkJycDAAICAhr93jW3/Tdv3sTYsWPVjnPK7eXo6IjLly+Ln2F6ejruvPNObNmyBXPmzNGq/vj4eMyZMwclJSWQSCTo1asXvL29kZOTg4sXL4q9GNevX4/OnTvjxRdfbNZ6NCU6Ohr33XcfqqurIZFIxOuKtLQ0cZ+Ty+VYvXo1KioqsHbt2np13HvvvXj22WdRWFiIrKws7N69G3fddZfGZX/11Vfi/+fPn9+i7+6uXbvE/wcEBKj1LDQFynOskrbHa3Omus7NWd9Fixbh3XffRXJyMqRSKVatWqV1jz9TXI42ampqcOedd4pDJbt06YJOnTqhsLAQsbGxYk+gV155BZ07d8acOXOwePFibNq0CcCtnltyuRwxMTEoLy8HAPz777945pln8MUXXzS5/BdeeAEfffSR2nNdu3ZFhw4dkJubKx6XampqsHbtWly9ehW//vprk8MxN27ciMWLF6s9p7ymKSoqwoULF1BRUYEHHngA7u7uWn9Whjg3khkxbnyKyPg9iB577DGxnI2NjbBq1SohLy9PrUxpaanwf//3f4KlpaVYdt26dY3W+e+//woWFhZqdz+3bNlS7w6DTCYTTpw4ITzxxBPCkCFDWrQeqlTvkNnb2wtWVlaClZWV8NZbbwklJSVqZa9cuaL2WNu7v6p3egAIgwcPFg4dOiTIZDK1chUVFcIff/whTJ8+XXj22We1an9j6t6B8PLyEgAI9913n5CZmVnvM+jSpYvaHZnz5883Wrdqvc7OzmK9qampauUyMjKE8vJyQRAEobq6Wujbt6/a/vPuu+8KZWVlYnmpVCp8++23gqurq1jO19dXyMnJabQt8+bNq3eHOz8/X3xdJpMJ27dvF3x8fNQ+B8A4PYjq3lHt0aOHsHPnznp3v6qrq4V///1XmDdvnjBr1qwG61KtR9s7s9qu24kTJ9S+w6GhofV6p2RnZwsPPvigWjsefPDBRutU/YyUvZI6dOgg7NixQ+27IJVKhTVr1qj16FixYoVW66eJIXsQ1b1zeuPGjWa3U3U/8fDwECQSiWBlZSW88847at+Z6upq4csvv1S7yxoUFKRWRlVzj3eVlZVCnz59xPf6+/sLW7ZsqdfT4saNG8KcOXPEctbW1sKZM2cabItcLhdGjhwplrW0tBTefPNNtZ5oyvVzcnKq9/1taQ+isrIyISwsTCxnYWEhPPPMM0JaWlq9smlpacK6deuEkJAQ4dy5c/Ve17UHoZK255Dbb79dLCeRSIQXX3xRKCgoEF+Xy+XCrl271Hol2tvbC0lJSY3WqfoZKb+PDz30kJCRkaFWLiEhQYiIiBDLOjo6CkVFRVqvY1MaOkeNHz++3rn23LlzQmRkpFr5xnpJqX4P77jjDo1tuHTpklq9Fy9ebPb6SKVSte/ivffe2+y6tKXadm16ED3xxBNq70lOTm60bFvoQZSVlaV2jTl//nyt26zs2SMIit60qseqhIQEje3TtQeRIZajC9XfGMqefAMGDBCioqLUyl25ckXo3bu3WDYkJET44YcfBACCj4+P8Ouvv6qd04uKioTZs2erHWubOjb9/PPPavvNwIEDhZiYGLUy165dq9ej8s0332y0zoSEBMHGxkZt39m/f79amaysLGHu3Ln1zjVN7buGODeyB5F54RYiozNmgOjAgQNiGVtbW41DGVRPcq6urvV+fAiCIFRVVQkdOnQQy3Xr1k1IT0/X2N6G6tJ2PepqaMhOU12uVWlzcR8bG6t2cTJjxgyhurpaY92NraO2Ghp619SF0Y0bNwQ/Pz+x7NixYxstW7feRx55RGN7Pv74Y7X3/PDDD42W/e+//9RO5I8//niD5U6fPq1W54svvthonTExMfW6Krd2gCg7O1utDcOGDdPqh1Zj+0JzLry1Xbd+/fqplcvKymq07JNPPqnWlsaGBtYNlPr4+AjXr19vtN6nnnpKLNuxY0et1k8TQwWIqqqqhMDAQLF8YGBgi9pZ94cVAOGrr75qtPzff/+tdpxp7EK5uce7119/XSwfHBxcL4hQ1yOPPKLxWPLLL7+otWP9+vWN1rdv3z619QNaHiB66aWX1H6w/PTTT02ukyAoAgANDY8wZIDojz/+UFuf1atXN1pfUlKS2hCtyZMnN1q27n7wyiuvNFo2NTVVcHBwEMt+/fXXWq9jU+q2YcyYMY2eH4uKitQCel27dm1wqNmFCxfEMpaWlhqvKV555RWx/NChQ1u0PqrLBiC89957LapPG6rL0xQgunLlito5qG/fvk2WbwsBItUbmwCEHTt2aN1m1cCNTCZTGwrbWPCvpQEifS9HF6q/MQAIffr0afT648qVK4K1tbVY1sbGRnBycmo0oFVdXS2EhoaK5V9//fUGy1VVValdi0ZGRjY6fF0mkwnTp08Xy1pbWzd6Y0Y1yO7t7d3kvlj3xmNT+64hzo0MEJkXbiEyuroHb13+mjqRahNYmTx5ssYfH3VNmTJFfM/GjRvrvf7111+rXcjVvUuhK30EiG6//Xatl6fNxf38+fPFMp06dRKKi4u1rr8l6p5gPD091e44N+Tbb79Ve8+lS5caLKdaxtfXV2PuGblcLnTv3l18z4wZMzS2/+WXXxbLOzo6CoWFhfXKPPzww2KZoKAgobKyssk6V61apdb21g4Qvfbaa2IZZ2fnej2udNWcC29t1u348eNqdf/xxx9N1llRUaH23Zs3b16D5eoGiL777rsm601OTlYr39LPSxAMFyCq23voqaeealE76/6waipgq7Ro0SKxfMeOHev1UBSE5h3vysrK1IIOR44c0eo9qrmrVPP2KI0fP158fdiwYTqtH9CyAFFhYaHY+xFAi3tsGjJApJoDpH///hpzAH3++edieYlEotVxvFu3bhpzeDzwwANi+aZ6CupCtQ3W1tb1eg7VdfjwYbX37Nu3r8FyQ4cOFcu8/fbbjdYnlUoFf39/sWxLA1+qN9IACNu2bWtRfdpQXV5j+5BcLhd2794tdOzYUW3f2Lt3b5N1m3OAKDMzs15waPjw4Rq/P40FbgRBELZv3672+dXt1VK3fc0JEOl7Obqo+xtDUx7ISZMmqZVXzUfUkHfeeUcsO27cuAbLfP/992rr3lCPTVVZWVlqx/KGehunpqaq3WD44osvmqyzoKCgXi60hvZdQ50bGSAyL5zFjNqtnJwcMd+JtbU1nnzySa3ed//994v/b2i2gx9++EH8/9SpUxEZGdnClrbco48+qre6pFIpfvvtN/HxM888Y7Tx/vPmzYObm1uTZe677z61vE07duzQWO/cuXPh6OjYZJnExEQkJSWJj7XJb/T000+LU9OWlZXhn3/+qVfmjz/+EP//8MMPw9bWtsk6H3/8cVhaWmpctqGo7u8LFy4UZywxNarbPTg4GNOnT2+yvJ2dHR5//HHx8c6dOzXOVOLi4qIxP0qXLl0QEBAgPk5MTGyyfGsSBAH5+fnYu3cvJk6ciPXr14uvubi44JVXXtHr8rTJVbBkyRLx/zdu3EBUVJTG92hzvNuzZ4+Y5ygyMhK33Xabxvc4ODhgxowZ4uO6x/+SkhK155544gmNdaquX0vt3r0bJSUlABTnNH1vL30pLS1VO/Y99dRTGmdmW7RoEVxdXQEo9tOdO3dqXM6DDz6ocSp11dmvDPFdnDp1KkJCQposM3LkSLX8To2do1T36//9739qM0Gq2rNnj5i7ydnZGbNnz9ax1eqU3xMl5XZoSmJiIiZPntzk38svv6zV8j/88MN67x0xYgS8vLxwxx13iLm0LC0tsX79ejGfnbm6efNmvfUdP348wsPDERgYqJbnpm/fvvj11191mtmwrhkzZqB///4AFN+tFStWtHgdjLmcpvTq1QuDBw9ussygQYPE/0skEjz44INNlletr7HZx1S/06NGjULfvn2brNPX1xdz585t8P1Kqtckzs7OeOCBB5qs083NTe33S2MMcW4k88Mk1WRS3N3d1Q7Omvj6+jZ7WceOHRMvsPr06dNg8ueGhIeHi/+vOz1tTU0NTpw4IT6eNWtWs9unT3WngG2JqKgoMTEfYNx1nDx5ssYy1tbWGD9+vDjF75kzZzS+R5vP69SpU+L/HR0dtTqJBgYGol+/fuKP3FOnTql9fteuXVObclqbC13l1KjaTsuuT1lZWWoJDE1lf2+I6vbSZr8BgDvuuENMdqxMKh8WFtZo+f79+8Pa2lpjvYGBgWJSc30nbtfFmDFjtCpnb2+P3377TS2w1VIWFhaYMGGCxnKRkZHw8fFBdnY2AMX3d+DAgU2+R5vv79GjR8X/65Lkvanjf1RUlFoQUZvvb931awnVdRo+fHiLzo+GdPbsWbXPSZuEx3Z2dhg/frx4c0L1+9yYoUOHaiwTGBgo/t8Q30VtjzVTpkzBhQsXADR+jpo9ezaeffZZFBUVITk5GYcPH25wsgNlknwAmDNnjsabHZpUVVWpPdZ00wJQfJaqEw40RDlhgSZxcXGIi4trssysWbOwcuXKFk1gYCoqKys1fnZ+fn5Yvnw5HnvsMb1MHPDWW2+J38Pdu3fj5MmTGDJkSIvrNdZyGqMpOAQoPlulLl26wNvbW+vyjR1DVI9X2iZ4v+OOO8RgYHx8PEpKStRuxqoeJ0aOHAk7OzuNdU6ZMgWffvppk2UMcW4k88MAEZmU3r17Y+/eva2yLNULjtTUVK0v5FRne8nNzVV77caNGygrKxMfK++WGJObm5vWwS9tqN4h8fT0ROfOnfVWt65UT0hN6dWrl/j/y5cvayyv6Y4vALXASK9evcSeQZpERESIASLVOhp6rNrupvTq1csoAaK6d8tMYX9vjOpnq+2PiB49esDKykqc1ezKlStNBohULxSb4uDgIP5fNdhqisaNG4dPP/0UPXv21Gu9wcHBWv9w7dWrlxhA0fT91fZ4p3r83717t/jjXJP09HTx/3WP/6r7mK+vL7y8vLSqU3X9WkL1+2gu30UfHx/4+Pho9b6IiAgxQFT3WNkQbb6Phv4u6vMcZW9vj/nz54s9+7766qt6AaLMzEzs2bNHfPzwww/r2OL66vbSLS4ubnGd+nbq1Kl6PZ3asps3b+LcuXMt6jmkStkrSzm714oVKxrs4Wwuy2mMNkFz1WOCNscmTceQmpoaXL9+XXys7fWHajm5XI6UlBS1GYtVj4HNOc40xhDnRjI/DBBRu6U6RWh2drbGOzYNKSoqUntc9wJF052H1qDv4V+q62js9fP09NS5nDZ3ibX5zFTr0bYdANR+NCqnPW/osYODg9bTR+uyfH1S3Rfs7Ozg5ORklHZooznby8rKCm5ubuLFTt3tVVdz7uQ2NkykNQwcOFAtmGJhYQEnJyd4enqib9++GDt2LEJDQw2ybF32WV2+v9oe71SP/4mJic0aXlT3+K+6fzR3/VrClI7NTTHEsbMhun4fDfFdbM45qri4GIIgNPjj/9FHHxUDRL/99hvWr1+vFsD59ttvxYB2RESETj2yG1M34KrNZz9kyJAGP8+FCxfi22+/1Wn5mzdvxsKFC8XHZWVlSE1Nxc6dO7F27VrcvHkTaWlpmDp1Kg4ePKiXdTamzp0749q1a+JjuVyOzMxMxMfH49NPP8WuXbsgCAI2b96MkpIS/PLLL3pZ7ttvv41Ro0YBUEzbfvDgQa17mZrichqi6zFBH72z6p6ztD0m1L3B0NT1YnOOM40xxLmRzA9zEFG7pdrTp7nqXgA1pyu2oWnbs0Vbquto7PXT9uSt2s6626gh2nxmqvXochGhWrZuW6qrq5tVp7G2gyntC5oYYnuZu/feew979+4V//bs2YOff/4ZGzZswGOPPWaw4BDQ/P1b0zbQ9ninj+N/3ZxUxv7+msv3sT19F5tzjpLL5ZBKpQ2Wi4iIEIfkVFZWYtu2bWqv/+9//xP/r4/eQwAQFBSk9vjixYt6qbe5HB0d0bNnTyxduhTR0dFiL+by8nLMnTsXpaWlTb6/7jBgXfelukPjtBlW3BIWFhYIDAzEhAkTsHPnTrz11lvia7/++is2btyol+WMHDlSbdjvq6++qpd6jbUcU1F3/9L2mFC3nD6uF7U5Lxji3EjmhwEiardUEy3efvvtEBSz+un8p6puV+y2GEVXXUdjr58yIasu5VxcXPSybNX9R9t21C1bd39RbZumi9zG6tQnmUzW5Ouq7S8pKTFqbxhNDLG9qPmauw0M8f19//33m3XsP3TokFqdqm1r7vq1hCkdm5vSnr6LzTlH2draNvmDTzVZtWq+ocOHD4vD02xtbTFv3jxdm9sgPz8/tWHX2uTxay0BAQH44YcfxMBwcnIy3nnnnSbfUzfJti7n2obKu7u76/T+lnr11VfV8tgsXbpUb0N6VINPJ06cwJ9//qmXeo21HFNQd39rzjEBaPp6sbl1NsQQ50YyPwwQUbulOhZZH/kfgPo5D7TJd2NuVNcxLS1N60SThpCSkqJzOW3zXWiiOoRD23YAigvYhuoA1NtWU1ODtLQ0rerUZvmqPzgauztdl6bhPKr7glwuV1s3U9Oc7ZWTk6N2QWXKw3bMjeoQCk0M8f01xPFftW1paWniUB9NdDl+NEX1+2jK5x7V75Eun1NTx05TZYhz1OzZs8UfcefOncO5c+cAqAeLZs6cqdfcg6q5jk6cOKGWb8TYhg4dqjaD08cffyzO4taQukN3VPPDaEP12GVvb6+Wg6a1fPLJJ+IMfcXFxXj77bf1Uu+gQYNw5513io9fe+01g9z4aa3lmAInJye1dAHaHhPqXk81db2o7flUm2Ub4txI5ocBImq3VGdOOH/+vF4CHR4eHmrDMo4cOdLiOlWHTJjCCVT1c6upqcHx48eN1hZtEzOrlouMjNTLslXruXbtmlYnUplMhrNnzzbaloiICLUp67VZP0EQ1OpsjGpulsLCQq32JU0zx0RERKhdHOtjf1fNu6HP/V31s9Z2v1GdeUQikaBfv356a097V1RUhKSkJI3lSkpK1HIg6Ov7q3ocO3nypF7qVN0/qqqqEBsbq/E9ddevJVTX6ejRoy3+/hjq3KO6DaurqxETE6PV+1S/j/raDwzNEOcoBwcHtd5BX331FYqKivDrr7+Kz+lreJmS6lTfMplMb8Oa9GXlypViwKS8vBxr1qxptKyfn59aMPX8+fM6LUu1vKbpyg2la9euanmZNm7c2GRQTBf/93//J56Hz507JyaG17fWWo4pUD03NOf6w93dvd5Qz+bUqU05Q5wbyfwwQETt1uDBg8W7cNXV1fjhhx/0Uq/q2Opvv/1W694ajVGd6Ud1BjVjCQgIUJvJ6csvvzRaW3766SeNZVJSUtROitpMga2NQYMGib1yBEHQqi379+9XCyTddtttaq87ODiozT70888/a6zz8OHDWl0YduzYUfx/eXm5xt4+OTk5OHHiRJNlrK2t1e4s62NfMNT+rvpZ79+/X6su+Vu3bhX/Hx4ebjbDWsyFNt+Z3377TTyGWlpaajV1uTZUp6A/fvy4VrNiaRIaGqp291Wb76/q+rWU6rknNTUV+/bta1F9hvouhoaGqv1A1+bcm5iYKM7+CNQ/dpqqX375RWM+jtLSUrUhNtqco1SHmX3//ff4+uuvxW3UpUsXvSf9HTZsGIYPHy4+/uCDDxAfH6/XZbREUFAQ5s+fLz7+8ssvkZWV1Wh51c/4zz//1Po7ePPmTbXzoupn0tqWLVsmBsUqKyvx3nvv6aXeiIgIzJkzR3z8+uuvGySnTGstxxSoHq+0PearXn+MGDGiXtJ61TovXLig1Y2GH3/8UWMZQ5wbyfwwQETtlo2NDZ588knx8YoVK3Dz5s0W1/vkk0+KB/LU1FS8+eabLapP9ULaVA7US5YsEf//008/teo0paoOHjyocdkrVqwQ7357eHhg2rRpelm2q6srZs2aJT5evXp1k9P/1tTUYPny5eLjvn37NninWPUi95dffmny7rogCHjttde0aq+bmxuCg4PV6m7Km2++qVXyTtV94eTJk2rDHJrDUPv7nDlzxN5O1dXVeOONN5osf+bMGbXP6KGHHtJbW0hh7dq1yMnJafT1yspK/N///Z/4ePLkyVpNU6yNQYMGYdiwYQAUPSKefPLJFv84kUgkaj07PvvsM2RkZDRavu76tdTAgQPVZnB65plnWjR1uyHPPYsWLRL/v3HjRqSmpjZZ/uWXXxb/7+PjgzvuuEOv7TGU5ORktcTRDVm9erWY18bKykqr3EG9e/fG4MGDASh6hKom+n3wwQf1Nv25qnXr1ok3RSorK3HXXXfh6tWrel9Ocy1fvlzsgVtRUYEPPvig0bKqPaLy8vLEmeE0efPNN8XcfBKJRK2e1talSxfMnTtXfLxp0ya9DQlatWqV+FkmJCRoFVgw5eUYm+rxLisrCx9//HGT5X/77Te1HkQNXX9MmDBB7RitKdn3/v37cfjwYY1tNcS5kcwPA0TUrj3//PMIDAwEAGRkZGD06NEah9UAih/C9957L/bv31/vtbCwMCxYsEB8/NZbb+Htt99uMuFvRkYGPvvsswZfUw0ibN682SSSjy5atAg9e/YEoAhSzJw5U2OSwaioKIN0IZ47d26j2+y9997D999/Lz5+7rnn9Dq7z8svvyzewcvMzMTMmTMbDBJVV1dj0aJFYq4IQBG4asiCBQvg7+8PQJHXZ+bMmQ3mSJDJZHj66adx7Ngxrds7Y8YM8f/vvfceLl261GC5Tz75pNH9sa4pU6ao3a1evHixxh9Ely9fxjfffNPga6r7+4YNG/Q2W5GbmxueeOIJ8fFnn33W6DpeunQJM2fOFC+KAgIC1C7wSD8KCwtx1113NThtdmVlJebOnSv+AJVIJFi6dKlel//ee++J3999+/Zh5syZalPFN6S6uhrbt2/HkCFDGhyW/PTTT4uByNLSUtx1111q0wYrVVZW4v7779f7D+w1a9aIQ8OSkpIwceLEJnsYSqVSbN68ucEcFqrfxZiYGBw8eFBv7VyyZInYg7e8vBx33HFHgz0+BEHAsmXLsGvXLvG5l19+WS/TT7eWp59+utGkrT/88IPacKgFCxbUy2XYGNVeRMp90dLS0mDHqgEDBmDt2rXi48uXL2Po0KH4+uuvtcojFRcXh+joaIO0DVAMu5o9e7b4eOPGjY32FJ08ebLa/v3KK6802aNREAR8+OGH2LBhg/jcjBkzxOsgY1m+fLn4fS8vL8eHH36ol3pDQ0PVrmMNldOstZZjbN27d8fdd98tPl6+fDl27NjRYNmTJ0+qBR779OnTYEDcysoKL7zwgvh4+/btjd5wiIuLw/333691ew1xbiTzYmXsBhCpio2NxeTJk3V6z7Bhw/D66683a3menp747bffMGbMGFRUVCAxMVE8GE+ePBldunSBo6MjiouLcePGDURHR+Pvv/8Wf7A3dvdo/fr1OH36tNgFe8WKFdi6dSvuv/9+9O3bF25ubiguLsbFixfx77//4t9//0WvXr3UejQp3XfffeJMAjExMQgMDERkZCTc3d3Fu4Th4eFqs0IYmp2dHX766ScMGzYMpaWlKCkpwR133IGxY8di5syZCA0Nhb29PXJycnDu3Dn8+eefOHfuHJ555hm1Xjctde+99+Lnn3/GwIED8fDDD2PChAlwdXVFSkoKvvvuO7UfNOHh4Wp3ofWhb9++eO2117By5UoAELfj448/jgEDBsDa2hoXLlzAF198gYSEBPF99913X6Ofg7OzM9avXy++npKSgt69e+Pxxx/HyJEj4ejoiMTERHz11VeIioqCra0tJk+ejD/++ENje5988kl8/vnnqKysRGFhIQYPHoxnn30Ww4YNg5WVFS5duoStW7fi2LFjcHBwwKRJk/D7779rrHfbtm3o378/MjMzIZVK8dBDD2HDhg2YPXs2evXqBWdnZ+Tn5yM2NhZ///03/vvvP0yfPl0th4LS3LlzxaE5e/fuhb+/P/r27as2Y8fYsWPx9NNPa2xXXW+++Sb27Nkjfi+XLFmC33//HfPmzUNwcDCKi4tx4MABbNq0Sex5YWFhga+//lpvs2eRQmRkJIqKinD8+HGEh4dj8eLFGDhwIKysrBAbG4uNGzeqBTAfeeQRvQ8rGj58OD788EM888wzAIA//vgDnTt3xpw5czBq1CgEBATAysoKhYWFuHz5Ms6ePYu9e/c2mby9U6dO+L//+z/xwv3MmTPi+g0ePLje+nl4eCAyMlJvvTDHjBmD1157DatWrQIA/PfffwgNDcXcuXMxduxY+Pv7o6amBqmpqTh+/Dh27NiB3NxcteC1Us+ePdG3b1/ExMRAEASMHTsWvXv3RseOHcUfD4Ci54KuycMDAgLwySefiD8OL1y4gF69euGxxx7DiBEj4ODggEuXLuF///uf2p30ESNG4LnnnmvOR2MUynPUuHHjMH/+fEybNg3e3t5IT0/HL7/8onZ89ff312mY0Jw5c/D888+r3TSaMmUKAgIC9LoOqp544glUVlbipZdeglwuR3Z2Nh5++GGsXLkSkyZNwrBhw+Dr6wt3d3dUVlYiLy8PCQkJ2LdvH06cOKGWy8oQx9RXX30VP/zwAwRBQFlZGT766KMGZzWTSCT48ccf0b9/f5SUlKC6uhpz5szB+vXrxcCPs7MzioqKEBsbi59++kkt91BgYCC++OILvbdfV927d8c999wjBrc+//xzvPzyy/D09Gxx3a+//jq2bt2qNp26IbTWcozts88+w9GjR3Hz5k1IpVLMmDEDs2bNwqxZsxAYGIjc3Fzs2bMH3377rRhwtbOzw3fffaeWm1LVs88+ix9++EEMvL7++uv4559/sGDBAoSEhKCoqAj//PMPvvzyS1RWVmL27NlaDe02xLmRzIxAZGQLFiwQADT7784772yw3s6dO4tlDh482GQbzpw5IwQGBuq87L/++qvROnNzc4Vhw4ZpXVefPn0arevVV19t8r2jRo1SK3/w4EHxtc6dOze57nWNGjVKfO/mzZubLBsVFSX4+flpvY7PPPOMTm2pKyUlRa2+goICITw8XONyg4ODhbS0tCbrVi2fkpKiU7teeOEFrT+DmTNnClVVVRrr/OCDDzTWZWFhIWzatElYuXKl+NyCBQuarPfzzz/XWK+tra3w22+/6VTv1atXhe7du7f4eysIgjBv3rwm31u3Lbrs7xkZGVrtMwAEa2tr4YcffmiyPl0+IyVdvmPaUF3/pvbfut8fTcdFfdu8ebPaMevMmTOCm5ubxu1w++23C9XV1Y3W25LjnbJdtra2Wu+7yr+KiopG63zqqae0+p7t3r1b7Ry4cuXKBuuru+00efvttwWJRKL1upw7d67BerTZRnX3N132748//ljrdg4fPlwoLCxssj5tvgeqWrrvaGrDpUuXhDFjxmhcN09PTyE2NlbnZT3xxBNq9ezYsUMv66DJ3r17hS5duuj8nQEgODs7C++8845QWVnZaP2q5XU9Rt59991qy8rPz2+0bFRUlNChQwed2h8eHi5cvXpVpzZponoe0XU/jI2NVfsOrVixol4Z1WOvp6en1nUvWbKk3vr379+/0fKttRxdaHN8VVX3PKWJLsfm+Ph4rX9rODs7a3WOzszMFEJDQ7XabwsLC3U6Rurz3KjrOYyMi0PMiKDoOh0fH48333xTY/dud3d33Hvvvdi1a5daUtC6PD09cfjwYWzcuFEt90tdFhYWGDp0qFp+mrreeustHDhwAPPmzUP37t3h5ORkkBwDuoqMjER8fDxefvnlJhP42tnZYcaMGVrlVtCFm5sbTpw4gQcffLDBoWNWVlZYuHAhoqKixKGEhvDBBx9gz549Tc5oEhQUhG+++Qa//vqrVsMjXnjhBezZswchISENvh4aGoo///wTjzzyiE5tXbx4Mb7//vtG9/PIyEgcO3YMM2fO1Kne4OBgnDt3DqtXr27yO2RlZYUJEyY02FtOacuWLdi+fTvuvvtusRefvvZ3f39/nDp1CitXroS7u3uDZSwsLDBlyhRER0erJdEk/RowYADOnDmjluhclaurK9asWYM//vgD1tbWBmvHwoULkZCQgIceekgtMXNDgoKCsGTJEpw5cwZ2dnaNlvvkk0/w7bffavye3X777S1qe2OWL1+O06dPY9KkSY3efQYUPSGWLl3a6HFmwIABiIuLw6uvvoohQ4bAw8NDrfdQSz399NM4fvx4k73DfH198eGHH+LgwYPisDRzYW1tjb///hsvvfQSnJyc6r0ukUgwffp0xMTEICIiQuf6+/TpI/7f39/fYPtTXZMmTUJSUhK++uorDB8+XOP309bWFiNHjsSXX36JtLQ0LFu2TK/DvVWpDt8uKSnBunXrGi0bGRmJCxcuYNWqVRqv/YKCgrB27VqcPn26yWu61hYREYG77rpLfPzpp5/qrSfHq6++qjZTqaG01nKMrWfPnoiNjcXTTz/d6LnG2toa9913H+Li4ho9N6ry8/PD6dOn8dBDDzX4PbS1tcWDDz6IEydO6Hz8NMS5kcyDRBBMYN5sIhMTGxuL8+fPIycnB+Xl5XByckJgYCB69OiBXr16qU3/q634+HhERUUhOzsblZWVcHV1RUhICAYOHAgvLy8DrEXrkslkOHnyJBITE8XEsx4eHujRowcGDhwIe3v7Fi/j2rVrahdmqoev/Px8HDx4EDdu3IBUKkXHjh0xfvz4Vv9sk5OTceLECdy8eRMymQze3t6IjIxUu5DXhSAIOHHiBC5cuID8/Hz4+voiLCxMbSrS5pBKpTh69CguXryI0tJS+Pv7o1+/fs1uZ902R0dH48KFC8jJyUFNTQ3c3NzQrVs3DBw40GSGa9XU1OD48eNITExEXl4eHBwcEBgYiFGjRsHb29vYzWtzvvnmGzE/yqhRo9Tysly5cgWnTp1CRkYGbG1tERISgnHjxrX6hWZ1dTVOnTqFS5cuIS8vDzKZDC4uLujcuTPCw8PrTTWsiUwmw+HDh5GQkICSkhLxe9a7d2/DrEADCgoKcOTIEaSlpaGgoAD29vYIDAxE79691WakNLb09HQcPXoUmZmZqKqqgre3N3r16oVBgwY165xrasrKyvDvv/8iNTUVZWVl4rFGdYZJXY0ZM0b8Hr3yyitYvXq1nlqrm9LSUpw8eRKZmZnIzc1FRUUFXFxc4OHhgW7duqF3795mkTfq4sWLiImJQW5uLkpLS+Hi4gJvb28MGDAAXbt2NXbzqA2prKzEkSNHcPXqVeTn58PFxQWdOnXC6NGjm32NlJeXh3/++QepqamwtrZGx44dMWbMGHh4eLS4vfo+N5JpY4CIiMxGUwEiIjJtTQWIiEg3ly5dQvfu3QEoeiJdvny50Z5gRERE2jL/WzJERERERO2IakLriRMnMjhERER6wQAREREREZGZ2L59OzZv3iw+1vcMnURE1H5xmnsiIiIiIhMVFxeHFStWQC6XIyUlBXFxceJrkydPxtixY43YOiIiaksYICIiIiIiMlG5ubn4448/6j3fsWNHfPXVV0ZoERERtVUcYkZEREREZAasrKzEKaXPnj2LwMBAYzeJiIjaEM5iBkAulyMjIwPOzs6QSCTGbg4RERERERERkV4IgoCSkhIEBATAwqLxfkIcYgYgIyMDHTt2NHYziIiIiIiIiIgM4saNG+jQoUOjrzNABMDZ2RmA4sNycXHRe/1SqRT79u3DxIkTYW1trff6SXfcJqaF28M0cbuYJm4X08LtYZq4XUwPt4lp4fYwTdwupqetbJPi4mJ07NhRjH00hgEiQBxW5uLiYrAAkYODA1xcXMx6p2pLuE1MC7eHaeJ2MU3cLqaF28M0cbuYHm4T08LtYZq4XUxPW9smmlLqMEk1EREREREREVE7xwAREREREREREVE7xwAREREREREREVE7xwAREREREREREVE7xwAREREREREREVE7Z9RZzI4cOYL3338fUVFRyMzMxO+//4677rpLfP2NN97Ajz/+iBs3bsDGxgb9+/fH22+/jcGDB4tl8vPz8dRTT2HXrl2wsLDArFmz8PHHH8PJyclg7a6pqUF1dbXW5aVSKaytrVFeXt4mMp+3BaawTWxsbGBlxYkEiYiIiIiIyPiM+uu0rKwMffr0wYMPPoiZM2fWe71bt25Yv349unTpgoqKCqxduxYTJ07ElStX4O3tDQC4//77kZmZif3790MqlWLRokV49NFH8f333+u9vYIgIDU1FXl5eRAEQaf3+vr64sqVK3pvEzWfsbeJRCKBp6cnOnXqpHG6QSIiIiIiIiJDMmqAaMqUKZgyZUqjr8+dO1ft8UcffYSvv/4asbGxGDduHBISErB3716cOXMGAwYMAAB8+umnmDp1Kj744AMEBATotb15eXnIzc1FQEAAXFxc+KOemk0QBBQXFyMjIwOOjo7w8vIydpOIiIiIiIioHTOb8S3V1dXYtGkTXF1d0adPHwDAiRMn4ObmJgaHAGD8+PGwsLDAqVOnMGPGDL0tXxAEpKenw8PDA/7+/nqrl9ovR0dHVFRU4Pr165DJZPD29oaFBdOCERERERERUesz+QDR7t27MWfOHJSXl8Pf3x/79+8Xe1tkZWXBx8dHrbyVlRU8PDyQlZXVaJ1VVVWoqqoSHxcXFwNQ5KWRSqUNvkcqlaKmpgbu7u4tXSUikYeHBwoKCvDzzz+jV69eGDZsGCwtLY3drFan/N419v0j4+B2MU3cLqaF28M0cbuYHm4T08LtYZq4XUxPW9km2rbf5ANEY8aMQUxMDHJzc/Hll1/i3nvvxalTp+oFhnSxevVqrFq1qt7z+/btg4ODQ4Pvsba2hq+vL5NMk14p96eSkhLs3LkTycnJLdq3zd3+/fuN3QRqALeLaeJ2MS3cHqaJ28X0cJuYFm4P08TtYhrkApBcLEGxVILLv/6DEBcBFmaaZaa8vFyrciYfIHJ0dETXrl3RtWtXDBkyBKGhofj666+xbNky+Pn5ITs7W618TU0N8vPz4efn12idy5Ytw/PPPy8+Li4uRseOHTFx4kS4uLg0+J7y8nJcuXKFeYdIr5T7U7du3QAAgYGBmDBhgjGbZBRSqRT79+/HhAkTGIQ1IdwuponbxbRwe5gmbhfTw21iWrg9TBO3i+n4++JNrN6TiKziWyOP/FxssWJqD0zq5WvEljWPctSUJiYfIKpLLpeLw8OGDh2KwsJCREVFoX///gCAAwcOQC6XY/DgwY3WYWtrC1tb23rPW1tbN/pF5BeUDEkikcDe3h5FRUXtel9r6jtIxsPtYpq4XUwLt4dp4nYxPdwmpoXbwzRxuxjX3rhMPPXjedSdt/xmcRWe+vE8NsyLxORw88pLrO3+ZNQAUWlpqdo04ykpKYiJiYGHhwc8PT3x9ttvY/r06fD390dubi4+++wzpKen45577gEA9OzZE5MnT8YjjzyCjRs3QiqVYsmSJZgzZ47eZzAjMjSJRAKZTGbsZhAREREREbVLMrmAVbvi6wWHAEAAIAGwalc8JoT5wdJcx5s1wahTJp09exb9+vVDv379AADPP/88+vXrh9dffx2WlpZITEzErFmz0K1bN0ybNg15eXk4evQoevXqJdaxbds29OjRA+PGjcPUqVMxYsQIbNq0yVirRCZm9OjRCAoKMnYziIiIiIiIyMSdTslHZlFlo68LADKLKnE6Jb/1GtWKjNqDaPTo0RCEhmJzCtu3b9dYh4eHB77//nt9NosMICYmBjt27MDChQsZsCEiIiIiIiKTk13SeHCoOeXMjVF7ENEtMrmAE8l5+CMmHSeS8yCTNx44M0cxMTFYtWoVrl27ZuymEBEREREREdXj42yn13LmxuySVLdFe+MysWpXvFpXNn9XO6ycFmZ2ya+IiIiIiIiIzNGgYA/4u9ohq6iywTxEEgB+rnYYFOzR2k1rFexBZGR74zKxeGt0vXGOWUWVWLw1GnvjMo3SrpKSEqxYsQKDBw+Gl5cXbG1t0bVrV7zyyisoLy9XKysIAr788ksMHjwYTk5OcHJyQkREBF5//XUAwBtvvIFFixYBAMaMGQOJRAKJRIKFCxeKr0skkgZ7FwUFBWH06NFqz/3000+YPn06OnXqBFtbW3h5eeGuu+5CbGys3j8HIiIiIiIiah8sLSRYOS2swdeUKalXTgtrkwmqAfYgajFBEFAhbd7MUzK5gJU7LzaZIf2NnfEY3tWrWTugvbUlJJLm7bjp6en46quvMGvWLMydOxdWVlY4fPgw3nvvPZw7dw5///23WHb+/PnYtm0bBg8ejFdffRVubm5ITEzEr7/+ijfffBMzZ85EZmYmNm3ahOXLl6Nnz54AgJCQkGa1bf369fD09MSjjz4KPz8/JCcnY9OmTRg+fDiio6MRGhrarHqJiIiIiIiofZsc7o8N8yLx/M/nUV5967e+XzsY5cMAUQtVSGUIe/1vzQWbQQCQVVyJiDf2Nev98W9OgoNN8zZxly5dcOPGDVhbW4vPPfnkk3jttdfw1ltv4fTp0xg0aBB+/vlnbNu2DfPmzcO3334LC4tbndLkcjkAoHfv3hg6dCg2bdqECRMm1OsRpKu9e/fC0dFR7bkHHngAffv2xdq1a/H555+3qH4iIiIiIiJqvyaH++PTA5dxMaMEI/3keHTqIAzt6tNmew4pcYgZNcjGxkYMDtXU1KCgoAC5ubkYP348AODUqVMAgG3btgEAPvjgA7XgEIB6j/VFGRwSBAHFxcXIzc2Ft7c3unfvLraLiIiIiIiIqDkqpTIkZZUCAMYEyDE42KPNB4cA9iBqMXtrS8S/OalZ7z2dko+Fm89oLPfNooHNSoJlb23ZnGaJPv/8c2zcuBEXL14UewMpFRQUAAAuX74Mf39/+Pr6tmhZujh37hxee+01HDp0CGVlZWqvBQcHt1o7iIiIiIiIqO1JyCxGjVyAh6M13G1qjN2cVsMAUQtJJJJmD+O6LdRbqwzpt4V6t3q08qOPPsILL7yAiRMn4umnn0ZAQABsbGyQnp6OhQsX1gsYtURTeZJqatS/jKmpqRg5ciRcXFzw2muvoXv37nB0dIREIsGzzz6L0tJSvbWLiIiIiIiI2p/zNwoBABGBrpBIKozbmFbEAJERKTOkL94aDQmgFiQydob0LVu2ICgoCH/99ZfaULG9e/eqlevWrRv++OMP3Lx5s8leRE0FgTw8FL2j8vPzERQUJD5fWVmJzMxMdO3aVXzu999/R2lpKXbu3IkxY8ao1ZOXlwdbW1ut1o+IiIiIiIioIbFpRQCAPoGuQGWWkVvTepiDyMiUGdL9XO3UnvdztcOGeZFGy5BuaamYAU0QboWtampqsGbNGrVy999/PwDg5ZdfrterSPW9Tk5OABRBoLq6desGAPjnn3/Unl+7dm29Oi0tLevVDQBffvklsrLazxeXiIiIiIiIDON8WiEAIKKDi3Eb0srYg8gETA73x4QwP5xOyUd2SSV8nO0wyMhJsO6++24sW7YMU6ZMwcyZM1FcXIzvv/9ebVYzALjnnnswe/ZsfPfdd7h8+TKmT58Od3d3XLp0CX///Tfi4uIAAAMHDoSFhQXefvttFBQUwNHREcHBwRg8eDDGjx+P7t274/XXX0deXh6Cg4Nx7NgxnDx5El5eXmrLmzJlChwcHDB//nwsWbIE7u7u+O+//7Bnzx6EhITUG5JGREREREREpK3iSimScxS5biMCXXHqipEb1IoYIDIRlhYSDA3xNHYzRC+99BIEQcDXX3+NZ555Bn5+fpg9ezYWLVqEsLAwtbLff/89brvtNnz99dd48803YWlpieDgYNxzzz1imU6dOuF///sf3n33XSxevBhSqRQLFizA4MGDYWlpiZ07d+Lpp5/Gp59+ChsbG0ycOBGHDx/G8OHD1ZYVEhKCv/76C8uXL8c777wDS0tLDB8+HIcPH8aSJUtw7dq11vh4iIiIiIiIqA2Kqx1e1sHdHp6ONkZuTetigIgaZGlpiWXLlmHZsmX1Xqs7vMvCwgJPPvkknnzyySbrXLBgARYsWNDga926dauX3whAgwGfkSNH4tixY/WeP3TokFbPERERERERETXkvDL/UAc34zbECJiDiIiIiIiIiIgIQGxt/qHeHVyN2xAjYICIiIiIiIiIiAi3prjv09HNqO0wBgaIiIiIiIiIiKjdyympQkZRJSQSIDyQPYiIiIiIiIiIiNod5fCyrt5OcLJtfymbGSAiIiIiIiIionavPQ8vAxggIiIiIiIiIiJSmcGs/Q0vAxggIiIiIiIiIqJ2ThAElRnM3IzaFmNhgIiIiIiIiIiI2rUb+RUoKJfCxtICPfydjd0co2CAiIiIiIiIiIjatfO1vYd6+jvD1srSuI0xEgaIiIiIiIiIiKhda+/DywAGiIiIiIiIiIionTt/ozZBdTudwQxggIiIiIiIiIiI2jGZXEBcRvuewQxggIjMwLVr1yCRSPDGG280+ZwpWbhwISQSibGbQURERERERBpcyS5FebUMjjaW6OLtZOzmGA0DRNTuXLt2DW+88QZiYmKM3RQiIiIiIiIysvM3CgEA4YGusLRovzf6rYzdAKollwHXjwOlNwEnX6DzMMCifWZO10bnzp1RUVEBKyvdd+Fr165h1apVCAoKQt++ffXfOCIiIiIiIjIbyhnM+rbj/EMAA0SmIX4nsHcpUJxx6zmXAGDyu0DYdOO1qwVKSkrg7OxssPolEgns7OwMVj8RERERERG1D7FpivxD7XkGM4BDzIwvfifw8wPqwSEAKM5UPB+/0yjN+uabbyCRSPDPP//gjTfeQOfOnWFra4vevXvjxx9/VCsbFBSE0aNH49y5c5g0aRJcXV3Ru3dv8fXLly9j/vz58Pf3h42NDYKCgvDSSy+hrKys3nKPHTuG4cOHw97eHr6+vliyZAlKS0vrlWsqB9Fvv/2G0aNHw83NDQ4ODujevTuefvppVFdX45tvvsGYMWMAAIsWLYJEIoFEIsHo0aPF9wuCgA0bNqB///5wcHCAk5MTxowZg4MHD9ZbVmVlJV566SUEBATA3t4egwYNwr59+7T9mImIiIiIiMiIKqUyJGQWAwB6t+ME1QB7ELWcIADS8ua9Vy4D/noZgNBQxQAkip5FXUY3b7iZtQPQwkTJS5cuRVlZGZ544gkAwObNm3HfffehsrISCxcuFMulpqZi7NixuOeeezBr1iwxqBMVFYWxY8fCzc0Njz32GAIDA3H+/Hl88skn+O+//3D48GFYW1sDAE6dOoXx48fD2dkZS5cuhZubG3788Uc88MADWrf31VdfxTvvvIOwsDA899xz8Pf3R3JyMn777Te8+eabGDlyJJYvX4533nkHjz76KG677TYAgK+vr1jH/Pnz8cMPP+Duu+/GokWLUFVVhW3btmHChAnYvn07pk+/1avrvvvuw44dOzBt2jRMmjQJycnJmDlzJoKDg5v9mRMREREREVHrSMgsRo1cgKejDTq42xu7OUbFAFFLScuBdwIMVLmg6Fm0pmPz3r48A7BxbFELcnNzERsbC1dXRST18ccfR+/evfH8889j9uzZsLdXfIFSUlLw5Zdf4uGHH1Z7/4MPPgh/f3+cOXNGbcjZuHHjMHPmTGzbtk0MND333HOQy+X477//0K1bNwDAE088gREjRmjV1tOnT+Odd97BmDFjsGfPHrUhaGvWrAEAuLm5YcKECXjnnXcwdOhQzJs3T62O33//Hdu2bcMXX3yBRx99VHz+mWeewZAhQ/DMM89g2rRpkEgk2LdvH3bs2IEFCxbgm2++EcuOHDkSM2bM0KrNREREREREZDy3hpe5tvuZqDnEjJq0ePFiMTgEAK6urnj88cdRUFCAQ4cOic97eHhg0aJFau+9cOECYmNjMXfuXFRVVSE3N1f8GzFiBBwdHcXhWNnZ2Thx4gTuvPNOMTgEADY2Nnjuuee0auu2bdsAAKtXr66Xn0g5lEyTrVu3wtnZGXfddZdaewsLCzFt2jRcu3YNly9fBgDs2LEDAPDSSy+p1XHXXXehe/fuWrWZiIiIiIiIjEc5g1l7zz8EsAdRy1k7KHrqNMf148C2uzWXu/9XxaxmurJ20P09dfTs2bPec2FhYQCAq1evis+FhITA0lJ9GFxCQgIAYOXKlVi5cmWD9d+8eVOtrh49ejS6PE0uX74MiUSCPn36aFW+IQkJCSgpKVEbclbXzZs30a1bN1y9ehUWFhZqAS2lnj17IikpqdntICIiIiIiIsPjDGa3MEDUUhJJ84dxhYxVzFZWnImG8xBJFK+HjDX5Ke8dHOoHowRBsU4vvPACJk+e3OD73N3d9doObXsKNUYQBHh7e+P7779vtEx4eHiz6yciIiIiIiLTUFwpxdVcxeRJ7T1BNcAAkXFZWCqmsv/5AQASqAeJaoMck9cYNTiUkJCAO++8U+25+Ph4AECXLl2afG9oaCgAwNLSEuPHj2+yrDKpc2JiYr3XlMvTpFu3bvjrr79w/vx5DBo0qNFyTQWQQkNDcenSJQwZMgROTk5NLq9Lly6Qy+W4dOkSevXqpfaasvcUERERERERmaa4tCIIAhDoZg9PJ1tjN8fomIPI2MKmA/d+B7j4qz/vEqB4Pmx6w+9rJRs2bEBRUZH4uKioCBs3boSbmxtGjRrV5Hv79euH8PBwbNy4UW04mlJNTQ3y8/MBKGYRGzJkCP744w9cunRJLFNdXY21a9dq1da5c+cCAJYvX47q6up6ryt7NCkDP8plq3rggQcgl8uxbNmyBpehHBIHQAycvf/++2plduzYweFlREREREREJu58bYJqDi9TYA8iUxA2HehxuyInUelNwMlXkXPIBIaVeXl5YfDgwWIC6s2bNyM1NRVfffVVg8PKVEkkEmzZsgVjx45F79698eCDD6JXr14oLy/HlStXsH37dqxevVqcxeyjjz7C6NGjMXz4cDz55JPiNPc1NTVatXXQoEFYunQp3n33XURGRmL27Nnw8/NDSkoKfv31V5w+fRpubm4ICwuDs7MzPv/8czg4OMDNzQ0+Pj4YO3asOLX9+vXrER0djTvuuANeXl5IS0vDiRMncOXKFTHYNWnSJEybNg3ffvst8vPzMXnyZCQnJ+OLL75AeHg44uLimv/BExERERERkUHF1uYf4vAyBQaITIWFJRB8m7FbUc+7776Lo0eP4rPPPhOTM2/btk3sraNJ3759ce7cOaxevRo7d+7Exo0b4ezsjKCgICxcuBDjxo0Tyw4dOhT79+/HK6+8gjVr1sDV1RV33303Fi9ejIiICK2Wt2bNGvTp0wfr16/He++9B7lcjo4dO2Lq1KliQMve3h4//vgjVqxYgWeffRZVVVUYNWoUxo4dCwD43//+hzFjxmDTpk1YvXo1qqur4efnh8jISKxevVpteT/99BNWrFiBbdu2Yf/+/YiIiMD27dvx/fffM0BERERERERkwjiDmToGiKhJVlZWWLVqFVatWtVomWvXrjVZR+fOnbFx40atljdy5EgcP3683vPK4WFKQUFB9Z5Tuu+++3Dfffc1uZypU6di6tSpjb4+f/58zJ8/X2N77e3t8eGHH+LDDz9Ue37ixIn45ptvNL6fiIiIiIiIWl9OSRUyiiohkQAR7EEEgDmIiIiIiIiIiKidUQ4v6+rtBCdb9p0BGCAiIiIiIiIionaGw8vqY4CIiIiIiIiIiNqVWzOYcXiZEgNE1KCFCxdCEASMHj3a2E0hIiIiIiIi0htBEFRmMHMzaltMCQNERERERERERNRupBVUoKBcCmtLCXr4Oxu7OSaDASIiIiIiIiIiajdiavMPhfm7wNbK0riNMSEMEOmosanViZqD+xMREREREVHr4vCyhjFApCVra2sAgFQqNXJLqC1R7k81NTVGbgkREREREVH7oExQ3bsDE1SrYoBIS1ZWVrCyskJ+fr6xm0JtSH5+PmQyGWQymbGbQkRERERE1ObJ5ALi0pUzmLkZtzEmxsrYDTAXEokEgYGBuH79OjIzM+Hi4gKJRGLsZpGZEgQBxcXFKCgoQE5ODgBAJpPBxsbGyC0jIiIiIiJqu65kl6K8WgZHG0t08XYydnNMCgNEOvD09ERpaSnS09ORkZFh7OaQmRMEAUVFRSgqKoIgCKiqqkJgYKCxm0VERERERNRmna/NPxQe6ApLC3b6UMUAkQ4kEgmCgoJQXl6Oo0ePAgAcHR1hZdX0xyiXy5Geno7AwEBYWHBUnykw9jYRBAFSqRQymQxSqRT5+flwd3dHSEhIq7eFiIiIiIiovThfO4NZHw4vq4cBombo2bMn5HI5oqOjkZubqzF/jFwuF3scMUBkGkxlm0gkElhZWaFLly4YMmQI/Pz8jNYWIiIiIiKiti62NkF1H85gVg8DRM0gkUgQHh6Onj17orCwUOMMVDU1NTh48CDGjBmjsbcRtQ5T2ia2trZwdXVlTisiIiIiIiIDqpTKkJhVDIAzmDWE0YoWsLS0hKenp8ZyUqkUzs7O8PHxgbW1dSu0jDThNiEiIiIiImpfEjKLIZUJ8HC0QQd3e2M3x+RwvBMRERERERERtXm3hpdxBEdDGCAiIiIiIiIiojZPOYNZb+YfahADRERERERERETU5t2awYz5hxrCABERERERERERtWkllVJczS0DwB5EjWGAiIiIiIiIiIjatAvpRRAEINDNHl5OtsZujkligIiIiIiIiIiI2rTzN2oTVHN4WaMYICIiIiIiIiKiNi22NkF1Hw4vaxQDRERERERERETUpimnuGf+ocYxQEREREREREREbVZOSRXSCysgkQARHTjErDEMEBERERERERFRm6UcXtbV2wlOtlbGbYwJY4CIiIiIiIiIiNqs8xxephUGiIiIiIiIiIiozTp/oxAAZzDThAEiIiIiIiIiImqTBEHgDGZaYoCIiIiIiIiIiNqktIIKFJRLYW0pQQ9/Z2M3x6QxQEREREREREREbVJM7fCynv4usLWyNG5jTBwDRERERERERETUJnF4mfYYIKJ2RyYXcColH1G5EpxKyYdMLhi7SURERERERGQAt2YwY4JqTayM3QCi1rQ3LhOrdsUjs6gSgCW+u3wW/q52WDktDJPD/Y3dPCIiIiIiItITmVxAXLoiQNSno5txG2MGjNqD6MiRI5g2bRoCAgIgkUiwY8cO8TWpVIqlS5ciIiICjo6OCAgIwAMPPICMjAy1OvLz83H//ffDxcUFbm5ueOihh1BaWtrKa0LmYG9cJhZvja4NDt2SVVSJxVujsTcu00gtIyIiIiIiIn27kl2K8moZHG0sEeLtZOzmmDyjBojKysrQp08ffPbZZ/VeKy8vR3R0NF577TVER0dj+/btSEpKwvTp09XK3X///bh48SL279+P3bt348iRI3j00UdbaxXITMjkAlbtikdDg8mUz63aFc/hZkRERERERG3E+dr8Q+GBrrC0kBi3MWbAqEPMpkyZgilTpjT4mqurK/bv36/23Pr16zFo0CCkpqaiU6dOSEhIwN69e3HmzBkMGDAAAPDpp59i6tSp+OCDDxAQEGDwdSDzcDolv17PIVUCgMyiSpxOycfQEM/WaxgREREREREZhJigmsPLtGJWSaqLioogkUjg5uYGADhx4gTc3NzE4BAAjB8/HhYWFjh16pSRWkmmKLuk8eBQc8oRERERERGRaTt/ozb/EGcw04rZJKmurKzE0qVLcd9998HFxQUAkJWVBR8fH7VyVlZW8PDwQFZWVqN1VVVVoaqqSnxcXFwMQJH3SCqV6r3tyjoNUTdpx9NBu13d08GK28kI+B0xTdwuponbxbRwe5gmbhfTw21iWrg9TBO3i35V1ciRmKX4rR/m59isz7WtbBNt228WASKpVIp7770XgiBgw4YNLa5v9erVWLVqVb3n9+3bBwcHhxbX35i6Q+ao9cgFwM3GEoXVANDQ2FMBLtZATvxJ7Elo5caRiN8R08TtYpq4XUwLt4dp4nYxPdwmpoXbwzRxu+jH9RJAKrOCo5WA88cPIrYFKYjMfZuUl5drVc7kA0TK4ND169dx4MABsfcQAPj5+SE7O1utfE1NDfLz8+Hn59doncuWLcPzzz8vPi4uLkbHjh0xceJEtfr1uQ779+/HhAkTYG1trff6STvWQTfx1I/nG0xUDUjg6mSH8ROHw87aspVbRvyOmCZuF9PE7WJauD1ME7eL6eE2MS3cHqaJ20W/tpxMBeISMSDYG7ffHtmsOtrKNlGOmtLEpANEyuDQ5cuXcfDgQXh6qicPHjp0KAoLCxEVFYX+/fsDAA4cOAC5XI7Bgwc3Wq+trS1sbW3rPW9tbW3QjW7o+qlpd/TtAFhY4Knvz6kFiXycbVFWVYMbBZV4888kvHd3b0gkzHBvDPyOmCZuF9PE7WJauD1ME7eL6eE2MS3cHqaJ20U/4jJLAAB9O7m3+PM0922ibduNGiAqLS3FlStXxMcpKSmIiYmBh4cH/P39cffddyM6Ohq7d++GTCYT8wp5eHjAxsYGPXv2xOTJk/HII49g48aNkEqlWLJkCebMmcMZzKhB3k62EAA42VpiRsdqTBo5GEO7+uBEch4e+N8p/BKVhr6d3HD/4M7GbioRERERERE1U2xabYLqjq5Gbon5MOosZmfPnkW/fv3Qr18/AMDzzz+Pfv364fXXX0d6ejp27tyJtLQ09O3bF/7+/uLf8ePHxTq2bduGHj16YNy4cZg6dSpGjBiBTZs2GWuVyMQdvpQDABjT3RsDvAUMDvaApYUEI0K98NKkHgCAN3ZeRHRqgTGbSURERERERM1UUilFck4pAKA3ZzDTmlF7EI0ePRqC0HBGGABNvqbk4eGB77//Xp/NojbsyGVFgGhkqBeQkab22uOjuuD8jULsvZiFJ7ZGY9dTI+DtXH8oIhEREREREZmuC+lFEAQg0M0eXk78Tacto/YgImpNOSVViEtXJOca0dWz3usSiQTv39MbId6OyCquxFM/RKNGJm/tZhIREREREVELcHhZ8zBARO3G0dreQ70CXBqNIjvbWeOL+f3haGOJk1fz8e7exNZsIhEREREREbXQ+RuFADi8TFcMEFG7caQ2/9Cobt5Nluvq44wP7ukDAPjyaAp2nc8weNuIiIiIiIhIP8QeRAwQ6YQBImoX5HIBRy7nAtAcIAKAKRH+eGxUFwDA0t9icelmiUHbR0RERERERC2XU1KF9MIKSCRARAcOMdMFA0TULsRlFCG/rBpOtlaI7Oyu1Xtemtgdw0I8UV4tw2NbolBcKTVwK4mIiIiIiKglYtMKAQAh3k5wsjXqvFxmhwEiahcOJymGlw0L8YS1pXa7vZWlBT69rx8CXO2QkluGF34+D7lc88x6REREREREZBznObys2RggonZBOb39qO6ah5ep8nSyxYZ5/WFjaYH98Tfx+aErhmgeERERERER6YGyBxFnMNMdA0TU5hVVSBGdWggAGBmqW4AIAPp0dMObd/YCAHy4/xIO1ya7JiIiIiIiItMhCAJnMGsBBoiozTt+JRcyuYAu3o7o6OHQrDrmDOqEOQM7QhCAZ348hxv55XpuJREREREREbVEWkEFCsqlsLaUoKe/s7GbY3YYIKI2TxxepsXsZU15Y3ov9OngisJyKR7fGoVKqUwfzSMiIiIiIiI9OF87vKynvwtsrSyN2xgzxAARtWmCIIgJqlsaILKztsSGef3h4WiDixnFePX3OAgCk1YTERERERGZglvDy5h/qDkYIKI27Up2KTKKKmFjZYHBwZ4tri/AzR7r7+sHCwnwW3Qatp5K1UMriYiIiIiIqKU4g1nLMEBEbZoyofTgYA/Y2+ini+Gwrl54eXIPAMCbuy4i6nqBXuolIiIiIiKi5pHJBcSl1waIOroZtzFmigEiatOUAaKWDi+r67GRXTAl3A9SmYAntkUhp6RKr/UTETVEJhdwKiUfUbkSnErJh0zOYa5EREREgGL0SHm1DA42lgjxdjJ2c8ySlbEbQGQoFdUynErJB6D/AJFEIsH79/TB5exSXMkuxZLvo7H14cGwtmTMlYgMY29cJlbtikdmUSUAS3x3+Sz8Xe2wcloYJof7G7t5REREREalTFAdEegKSwuJcRtjpvhrltqsUyl5qK6RI8DVDl199B9BdrK1wsZ5/eFka4VTKflY81ei3pdBRAQogkOLt0bXBoduySqqxOKt0dgbl2mklhERERGZhtjaABGHlzUfA0TUZonDy7p7QyIxTAS5q48TPrinDwDg62Mp2Hk+wyDLIaL2SyYXsGpXPBoaTKZ8btWueA43IyIionbt/A1F/iHOYNZ8DBBRm2Wo/EN1TQ73w+LRIQCApb/GIimrxKDLI6L25XRKfr2eQ6oEAJlFlThdO6SWiIiIqL2pqpEhMasYAGcwawkGiKhNupFfjqs5ZbC0kGBYVy+DL+/Fid0xoqsXKqQyPL41CkUVUoMvk4jah+ySxoNDzSlHRERE1NYkZJZAKhPg4WiDDu72xm6O2WKAiNqkI5cVvYciO7nBxc7a4MuztJDgk/v6IdDNHim5ZXjh5xhIa+Q4kZyHP2LScSI5j8M/iKhZfJzt9FqOiIiIqK1R5h/q3cHVYOlF2gPOYkZt0uGk1hlepsrD0QYb5kXi7o0n8E9CNvr9336UVtWIr3O2ISJqjkHBHvB3tUNWUWWDeYgkAPxc7TAo2KO1m0ZERERkEmJuFALg8LKWYg8ianOkMjmOJ+cBAEa2YoAIAHp3cMPsAR0BQC04BHC2ISJqHksLCVZOC2s0OAQAK6eFcTpXIiIiardi0xQJqvt0ZILqlmCAiNqc6OsFKK2qgYejDcIDWvcAIZML+CfhZoOvcbYhImquyeH+uKd/h3rP+7naYcO8SPZMJCIionarpFKK5JxSAIob9tR8DBBRm6OcvWxkqBcsWvmOOmcbIiJDqayRAwAm9PSBMuT82+JhDA4RERFRu3YhvQiCAAS62cPLydbYzTFrDBBRmyMGiFp5eBnA2YaIyHCirxcAAO4f3BGdnRTPHak93hERERG1Vxxepj8MEFGbklNShYsZxQCA20JbP0DE2YaIyBCyiyuRXlgBiQToHeiKXu6K3kQHErON3DIiIiIi47o1g5mbUdvRFjBARG3K0drp7cMDXeDt3PrdC5WzDTU2sE0CxWxmnG2IiHQRnVoIAOjm4wxnOyuEuSmGmB27kouqGpkRW0ZERERkXOdvKHoQ9e7AHkQtxQARtSm38g+1fu8h4NZsQwDqBYk42xARNde5G4rhZZGd3QAAHRwBH2dblFfLmNOMzIZMLuBEch7+iEnHieQ8TthAREQtlltaJfayjghkgKilrIzdACJ9kcsFHL2cCwAYZYT8Q0qTw/2xYV4kVu2KV0tY7eFog7dnhDOhLBHp7Nz1QgBAv47uAACJBBjVzQu/RKXj34RsowypJdLF3rjMeudFf1c7rJwWxvMiERE1m3J4WYi3E5ztrI3bmDaAPYiozYjLKEJ+WTWcbK0Q2dndqG2ZHO6PY0vH4odHhqBvbbK0hcODeBFMRDqTyuSITS8EAPTr5CY+P7qbFwDgYFI2BIE9Mch07Y3LxOKt0fVm+cwqqsTirdHYG5dppJYREZG5i+HwMr1igIjajMNJiuFlw7t6wtrS+Lu2pYUEQ0M8Ma1PIIBbMxAREekiKasElVI5nO2sEOLtJD4/LMQTNpYWuJ5Xjqu5ZUZsIVHjZHIBq3bFo6EQpvK5VbviOdyMiIiaRdmDqG9HN6O2o60w/q9oIj0x5vT2TRkYpOjNFHW9AHJeABORjqJTFcHlvh3dYKGSv8zJ1gqDuygS3h/kbGZkok6n5NXrOaRKAJBZVMlcWkREpDNBEMQp7jmDmX4wQERtQlGFFOduFAIwXoLqxvT0d4G9tSWKK2twJafU2M0hIjNzrnYGs8hO9YfOjunuA4DT3ZNpqZHJcSI5D2/svIjFW6O1ek92SeNBJDJ/MrmAUyn5iMqV4FRKPnuMEZFepBVUIL+sGtaWEvT0dzZ2c9oEJqmmNuH4lVzI5AJCvB3R0cPB2M1RY21pgX6d3HA8OQ9nruWjmy8PXkSkPWUPItX8Q0pje/jgzd3xOJ2Sj+JKKVyYnJGMpKpGhuNX8rA3Lgv7E24iv6xap/f7ONsZqGVkbOoJyi3x3eWzTFBORC0mkwv4JeoGAKCjuwOsLNj3RR/4KVKbYKrDy5QG1CbNjrrGPEREpL280ipczysHcGsGM1VBXo7o4uWIGrmAY7WzOBK1lrKqGvwZm4mnfjiH/v/3DxZ9cwY/nb2B/LJquNpbY1ZkB3wxrz/8XGwhaaQOCRSzmQ0K9mjNplMrYYJyIjKEvXGZGPHuAXzy7xUAwNXcMox49wCPKXrAHkRk9gRBwJHaAJExp7dvSv8gxYXvWSaqJiIdKIeXhXg7wtWh4d5BY3v44OqxFBxIzMbUCN6Np+aRyQWcTslHdkklfJwVARtLi/phnYKyavyTcBN/X8zCkcu5qK6Ri6/5uthiYpgfJof7YVCwhzhhhAABi7dGQwI0mKx65bSwBpdF5k1TgnIJFAnKJ4T5cfsTkdaUgee6xxZl4HnDvEj2TmwBBojI7F3JLkVGUSVsrSwwpIunsZvToMhObrCQAKn55cguroSPC7vSE5Fm524ogsoN5R9SGtvDB18dS8GhpGzI5YJaImsibagPAVJQHQKUVVSJffFZ2BuXVS9/TGdPB0zu5YdJ4X7o28Gtwf1vcrg/NsyLrLcMe2tLrJ3dhxfybdTplHytE5QPDTHN6zciMi0MPBseA0Rk9pTDywYFe8DO2tLIrWmYs501uvu5ICGzGGevF/AuPxFpJfp6IQCgXxMBogFBHnCytUJuaTUupBehD6d5JR00dic2s6gSj2+NRpCnA67VDnNU6uHnjMnhip5C3X2dIZFovgifHO6PCWF+OJ2Sj4NJN7HpSAp8XWwZHGrDtE08zgTlRKQtBp4NjwEiMnuHTXx4mdLAIHckZBbjzLV8BoiISCOZXMD5tEIAQGRnt0bL2VhZ4LZQL/wVl4UDidkMEJHWmroTq6QMDkV2csPkcD9M6uWHzp6OzVqepYUEQ0M8ERbggi+PpuBaXrk4pI3aHh9nWy3LcfsTkXYYeDY8Jqkms1ZRLcOplHwAwOjuph0g6q9MVM08RESkhaSsEpRXy+Bka4VQn6ZnPxzTg9Pdk+403YlV+vz+SGx/YjgeHRnS7OCQKld7a/TwcwEAnEnhObEtyiutwrfHrzVZhgnKiUhX2gaUGXhuPgaIyKydTMlDdY0cAa52CPF2MnZzmjSgNlH1xYxilFfXGLk1RGTqlPmH+nR01TiOXhkgv5BehOxi3jUj7Wh7h1Uqk2supKNBQYqbJmeu5eu9bjKufRezMGndEey9eBPKQ1djRzAmKCciXQwK9oC/qx1nxjQgBojIrImzl3X31ioHgjEFutkjwNUOMrmAmNqZiYiIGiPmH2pgevu6fJzt0KeDKwDgUFKOIZtFbYgx78QOClbkhjidwgBRW1FUIcXzP8fg0S1RyC2tRjdfJ+xcMgIb50XCz1V9H3J3sOZMQ0SkM0sLCVZOC2vwNeUvQQaeW4YBIjJr5pJ/SInT3RORtsQZzJrIP6SKw8xIV8o7sY0x5J3YgcGKwGdCVjGKKqR6r59a15FLOZi09gi2R6dDIgEeG9UFu54agfBAV0wO98expWOx9cEBCHFW9EabM6gjg0NE1CyTw/3x0ey+9Z73c7Vj4FkPGCAis3UjvxxXc8pgaSHBsK5exm6OVgZ0Zpd6ItKssLwaV3PKAAB9tehBBCimuweAY1dyUV2j/yFB1PYY806sj7Mdgr0cIQhANG+amK2yqhqs2HEBD/zvNLKKKxHk6YBfHx+KZVN6wtbq1syylhYSDA72QD8vRUr0ixklxmoyEbUBQZ4OAAA3e2t8PKcvfnhkCI4tHcvgkB4wQERmS9l7KLKTG1zsrI3cGu0MqM25cC61EDJ5U/PGEFF7du5GIQAg2MsRHo42Wr0nPMAVXk62KK2qYRCatDY53B+hPvVz+LXGndiBtefEUxxmZpZOp+RjysdHsfVkKgBgwdDO2PPMbejfufEeZx0dFdc+celFEAReBxFR81y6qQgyhwe64s6+gRga4slhZXrCae7JbB0xs+FlANDDzwVOtlYorapBUlYJwgJcjN0kIjJB52p7VPTTYcp6CwsJxnT3xi9Rafg3IRvDzaRnJRlXSaUUKbmK3mrrZveBRCKBj7NiWJmhL7YHBnng57NpDGiamUqpDB/uS8JXx1IgCECAqx3ev6ePVsecAAdFb6L8smpkFFUi0M2+FVpMRG3NpZulAIBuvk3P8kq6Yw8iMkvVNXIcT84DAIzq5mPk1mjP0kKCfp3cAABnr/OCmIgapuxB1K+zdsPLlMb1VBwPDyYxDxFp53hyHmrkAoK9HHFXvw6teid2cG2i6ti0QlRKZQZfHrVcbFoh7vj0GL48qggO3dO/A/Y+N1LrgLSNJRDq7QgAuJBWZMimElEbpuxB1M3XtGexNkcMEJFZik4tQGlVDTwdbdDLzHrhDKjten32GnMuEFF9cpWZDnXpQQQAI0K9YW0pQUpumdgrhKgpylnvjNEbt6OHPXxdbCGVCTjH2T1NWnWNHB/tv4QZnx/HlexSeDnZ4qsHBuD9e/roPMy/V6Diui0unQEiImoeMUDkxx5E+sYAEZkl5fCy20K9YGFm402VeYjOsks9ETXgSk4pSqpqYG9tiR46Xvg42VqJM05xNjPSRBAEow7Xlkgk4nT3HGZmupKySjDj8//wyb+XIZMLuKO3P/Y/NxLjw3ybVV947Y29CwwQEVEzFJZX42ZxFQA0mEOPWoYBIjJL4vT23c0n/5BS345usLSQIKOoEhmFFcZuDhGZGOWMTr07uMLKUvfT9JjutcPMGCAiDZJzSpFeWAEbKwsM7qL/qey1Maj2pslpJqo2OTK5gI2HkzHt02O4mFEMNwdrfHpfP6yfGwl3LZPnN0TZ85uJqomoOZT5hwLd7OFsJhMVmRMGiMjs5JRU4WJGMQDgtlDzCxA52lohzF9xcXSWU/sSUR3KoTaROuYfUlJOd38qJQ+lVTX6aha1QcrhZYODPeBgY5x5SwbW9niLTi1AjUxulDa0dzK5gBPJefgjJh0nkvMgkwtIyS3DPRuPY81fiaiWyTGuhw/2PTcS0/oEtHh5Pf2cYWkhQV5ZNTKLKvWwBkTUnjD/kGFxFjMyO0cvKy5owwNd4OVka+TWNM+AIHdcSC/C2Wv5mK6Hiy0iajuiU3WfwUxVF28nBHk64FpeOY5dzjHoNOVk3g6bwGyg3Xyc4WpvjaIKKS5mFKNPM/d7ap69cZlYtSteLVDjYmeFCqkMUpkAJ1srvD4tDPf07wCJRD9D+u2sLRHq44TErBJcSC9CAGcyIyId3AoQMf+QIbAHEZkdU7igbSkmqiaihhRVSHE5W9F1ul+n5vUgAoCxPRS5QZiHiBpTUS3DqdphXcY8n1pYSDCQw8yMYm9cJhZvja7Xi6e4sgZSmYDuvk7Y++xtuHdAR70Fh5TCA10BABeZh4iIdMQAkWExQERmRS4XcPRyLgBgpBkOL1NSJqpOzCpGSaXUyK0hIlMRm1YIQDG7k7dz83tIKoeZHUzKgVzOHB9U38mUPFTXyBHoZo+uRk7yqUysfpqJqluNTC5g1a54NHV0KK6sgb+rYXr3RNQGiJiomoh0pcxB1J0zmBkEA0RkVuIyipBfVg0nW6tm5+cwBb4udujoYQ+5AE7tS0Si6OuFAIDIFvQeAhQ/uB1tLNVythGpOlybf2hkN2+99w7R1cAgRYDozLV8BjRbyemUfI35fzKLKg3WqytcDBAVM1E1EWktt7QK+WXVkEiAEG/mIDIEBojIrCgvaId39YR1M2b3MSXiMDMmqiaiWudutCz/kJKNlQVGhHoB4DAzapgpDdcOD3SFvbUlCsuluJJTauzmtAvZJdolh9a2nK7C/F1gIVH82FNOV03UHA0lWae261KWYnhZZw8H2NtYGrk1bZN5/8Kmdkd5QTvSBC5oW0o5zOwsu9QTERRDaFs6g5kq5TCzA4k3W1wXtS3X88qQklsGKwsJhnX1NHZzYG1pgcjObgCYh6i1+Djb6bWcruxtLBHqoxgewmFm1Fx74zIx4t0DuO/Lk3jmxxjc9+VJjHj3APbGZRq7aWQgSbX5h0KZf8hgGCAis1FUIcW5G4UAzDv/kJKyB1HMjUJO7UtESMkrQ1GFFLZWFujh59Li+sZ0VwSIzqcVIaeEd+jpliO1N1siO7vDxc7ayK1RUA4zY4CodQwK9oC/qx0aG1woAeDvaifmhzKEcOYhohZoLMl6VlElFm+NZpCojRLzDzFAZDAMEJHZOH4lFzK5gBBvR3T0cDB2c1os1McJLnZWKK+WISGzxNjNISIji64dbtq7gytsrFp+evZxsRMTwR5K4jAzusWUhpcpDVIJEDEnjeFZWkiwclpYg68pg0Yrp4XB0sJw+akiAhWB8DgGiEhHTSVZVz63alc8h5u1QeIMZkxQbTAMEJHZuHVB62PkluiHhYUE/WuHkZzhMLM2gePgqSWUPSRbMr19XWPE2cwYICKFqhoZjifnAQBGdzedAFG/Tu6wspAgq7gSaQUVxm5OuzA53B9rZvWu97yfqx02zIvE5HB/gy4/ogN7EFHzaEqyLsCwSdbJOARBUJningmqDcXK2A0g0oYgCCr5h7yM3Br9GRDkgYNJOYi6XoAHRwQbuznUAnvjMrFqV7zaBYu/qx1WTgsz+EU2tQ3KHkSRndz0VufYHj745N/LOHopF1KZ3OyT+1PLRV0rQHm1DN7Otgjzb/lQRn2xt7FERAdXnEstxOmU/DbRU9gcBHs5AgC8nGzw2h1h8HFWDCszZM8hpTB/V1hIgJySKtwsroSvi2HyHVHbY+wk62QcWcWVKKmsgZWFBF28GCAyFF4pklm4kl2KzKJK2FpZYEgX4yfU1BfVHkTsUm++OA6eWqq0qka8K6bPHkS9A13h5WSDkqoa9lQkAMAh5c2WUONPb1+XMt8N7/q3npRcRT6Pnv4uuLNvIIaGeLZKcAhQBAW7+ih+5F1IYy8i0p6xk6yTcSTVzmAW5OWol6H41DB+smQWlL2HBnfxhJ1125nSsE8HN1hbSpBdUsUu9WaK4+BJH2JvFEIuAIFu9nq9i25hIRGH5R5I4DAzAg4n1Q7XNqHhZUrKPEQMZraeq7llAIAutT2JWhsTVVNzKJOsN8XQSdap9V1mgupWwQARmQVxeFlo2xleBijunvUKUFwcnb3OC2JzxHHwpA/K/EN99Ti8TGlcz9oAEfMQtXuZRRVIulkCiQS4ravpnU8HdPaARKIIWnDmvdaRkqMIEAUbKUCkTKTPRNWki6aSrCv17+zear3hqHXcmuKew8sMiQEiMnkV1TKcqv1xbUoJNfVlYJBymFmBkVtCzaH1OPhijoOnxt3KP6S/4WVKI0K9YGUhwdWcMlzPK9N7/WQ+lNPb9+ngBndHGyO3pj5XB2vxzjB7EbWOlNoeRMHexvnBFcEeRNRMk8P9G0xU7GKnSLG7OzYTXx292trNIgO6XBsgYg8iw2KAiEzeyZQ8VNfIEehmjxAjXcAYUv/Oiu6vUQwQmSVtx7e/+3ci1v1zCVeySw3cIjI3giCozGDmpvf6XeysMbB26M6BRPYias9McXr7upiHqPXI5AKu55UDMN4Qs7AAF1hIgOySKt5IIZ0UVUiRXNsD7uPZffHxnL744ZEhOPf6RLw4sRsA4K0/E/D9qVRjNpP0RC4XcKl2iBmnuDcsBojI5CnzJYzs5mVyCTX1QZmoOulmCYrKpUZuDelKOQ5e056ZUViJdf9cxviPDmPKx0fx2cErSK29MKf27XpeOfLLqmFjaYFeAYaZVWps7XT3DBC1XzUyOY5ezgVg2r1xGSBqPekFFaiWyWFjZYEAN3ujtMHBxkq8+cdeRKSLo5dzIJML6OrjhDv7BaolWX9yTFc8PioEAPDqjgvYcS7dyK2llkorqECFVAYbSwt05iyXBsUAEZm8I5dN/45nS3g724pj/6NT2YvI3CjHwTeUglpS+7f23j746N4+GNPdG1YWEiRkFuP9v5Mw8v2DmL7+GL48chUZhUxS3l6du6H43vcKdIGtlWGS8I+pDRCdupqPsqoagyyDTNu5G4UoqayBm4M1endwM3ZzGqVMVJ2QVYziSt40MaSrtTOYBXk6GDVXC4eZUXMoJ14YV3t+UyWRSLB0cnc8MLQzBAF44Zfz+PtiVms3kfRIOdNriI8TrCwZwjAkfrpk0m7kl+NqThksLSQYZoIJNfVF2YuIiarN0+Rwf4T51+/54edqhw3zIjEjsgNmRnbA5kWDcHbFeLw7KwIjunrBQgLEphXh7T0JGLbmAGZtOI7N/6U02c1eJhdwIjkPf8Sk40RyHmdHawOirxcCAPp11H/+IaUQb0d08nBAtUyOY1dyDbYcMl3K3ri3hXqbdOJWHxc7dPZ0gCAAUdd508SQxPxDRhpephTORNWkI5lcwMHaiRfGNhAgAhRBojem9cKsyA6QyQU89f05HK296UzmR5mguqG8U6RfVsZuAFFTlPkSIju5wcXO2sitMZyBQe74NSqNiarNVGF5tXhn46N7+8DSQgIfZ8X0qnV/iLk52GD2wE6YPbATckur8FdcFnafz8Dpa/mIul6AqOsFeHN3PAYHe+CO3gGYEu4HTydbAMDeuEys2hWvNmuav6sdVk4Lw+Rw/9ZbYdIrZQ+iyM5uBluGRCLB2B4++Ob4NRxMzMakXn4GWxaZJnPIP6Q0KMgD1/PKcTolH2O6N/zjj1ruVoDIuD+4IjqwBxHpJuZGAQrKpXCxsxJvsjbEwkKCd2dFoLy6Bn/FZeGR785iy0ODxbx8ZD4uiwEi5h8yNAaIyKQdMaML2pZQJqo+f6MQ1TWKfABkPvbF30SNXEAPP2fMjOyg9fu8nGwxf0hnzB/SGTeLK/FnbCZ2x2YgOrUQJ6/m4+TVfKzceRHDQjwR5OmIrSev1xvKllVUicVbo7FhXiSDRGaovLoGCZmKi55+BpjBTJUYIErKhiAIbTKnGzUst7RK/PE9MtT0e+MODPbAL1FpOMM8RAalDBAZK0G1Upi/CyQS4GZxFbJLKrWe/IHar39rh5eN7u6jcbiRlaUFPp7TDxVbzuJQUg4e3HwG3z8yRAxMknlIqk1QzRnMDI+/QslkVdfIcTw5DwAwqlvbvoMY4u0IdwdrVNXIcTGDd9DMzV8XMgEAUyOaH6DxdbHDgyOCsf2J4Ti2dAyWTemBiEBXyOQCjl7OxZYGgkMAxOdW7YrncDMzdCGtCDK5AF8XWwS4GvZH0eAuHnCwscTN4ipczCg26LLItCiHVYT5u8DHxfR/fA+uTVR9Pq0QlVKZkVvTdl3NUU5xb9wAkaPtrUTVHGZG2lBOuNDY8LK6bKwssHFefwwO9kBJVQ0e+N8psec3mb4amRzJObUzmDFAZHBa9SCaOXOm1hVu37692Y0hUhWdWoDSqhp4OtoYbGYfUyGRSNC/swf+SbiJs9cKDN6TgPSnqEIq5nSZGqGfYTsd3B3w2KgQPDYqBNdyy/D5oSv4+Wxao+UFAJlFlTidko+hIZ56aQO1jujUQgCK/EOG7tFja2WJ4V29sD/+Jg4mZot5P6jtU+YfMuXZy1R18nCAj7MtskuqEHOjEEO68Limb5VSGTKKFJMjGDsHEaBIVH0luxQX0ooxtoevsZtDJiy9sAKJWSWwkOg2wsDO2hJfLxyI+786hfM3CnH/V6fwy2NDEWQC+z817Xp+Oapr5LC3tkQHd+PMuNieaNWDyNXVVes/In1R5ku4LdQLFiacUFNfBgQxUbU5+if+JqQyAd18ndDVR/93NYK8HDFcywTt2SWNJ7cm03Qu1fD5h1Qp77b+y+nu2w25XMCR2untzWW4tkQiwcDaXkQcZmYY1/PKIQiAi50VPB1tjN0cMWDNPESkibL3UP/O7nDXcd91srXCt4sGooefM3JKqnD/V6c4i6wZuJSl6O0V6uvULn4TGptWPYg2b95skIUfOXIE77//PqKiopCZmYnff/8dd911l/j69u3bsXHjRkRFRSE/Px/nzp1D37591eqorKzECy+8gB9//BFVVVWYNGkSPv/8c/j68u6DuRPzD5nJHc+WGqgMEF0rYH4QM/JXnGJ42RQD5v/RNh8D8zaYF0EQbvUgaqVeg8qEv+fTCpFXWiUmQKe260J6EfLLquFka4XIJpK5mprBwR74MzYTp68xQGQIV2uHawR7O5nE9UYEZzIjLR1IuAkAze5p5uZggy0PDcbsL07gam4Z5n11Cj89NhTezjwfmqpLNzm8rDU1KwdRTU0N/vnnH3zxxRcoKVFE9DIyMlBaWqpTPWVlZejTpw8+++yzRl8fMWIE3n333UbreO6557Br1y788ssvOHz4MDIyMnQaEkemKbukUsyRcVto+wgQhQe6wsbKAnll1biWV27s5pAWiiulOHJJObzMcAGiQcEe8He1Q2OX8BIoZjMbFMxZOcxJWkEFckurYGUhEX8cGZqfqx16BbhAEIBDSZzutz1Q9sYd3tUT1hqSuZoS5SxDUdcLUCOTG7k1bc9VE0lQrdQrQJGoOqu4EjklVcZuDpmoimqZmJ9U2/xDDfF2tsXWhwcj0M0eV3PLMP/rUygsr9ZXM0nPLnGK+1al85XC9evXERERgTvvvBNPPvkkcnIUFx7vvvsuXnzxRZ3qmjJlCt566y3MmDGjwdfnz5+P119/HePHj2/w9aKiInz99df46KOPMHbsWPTv3x+bN2/G8ePHcfLkSd1WjEzK0dof3eGBLvBqJ3e4ba0s0bv2R+JZ3jE1CwcSslEtkyPE29GgJy1LCwlWTgsDgEaDRCunhcGS3W7NyrkbhQCAsAAX2FlbttpylRfVB5I4zKw9uDW9vXlN9tDd1xkudlYor5YxqboB3Jri3jQCRI62VmKwir2IqDHHk3NRVSNHoJt9i6+7Atzsse3hwfB2tkViVgkWbD6D0qoaPbWU9OkSp7hvVTpPc//MM89gwIABOH/+PDw9byUNnDFjBh555BG9Nk6TqKgoSKVStQBSjx490KlTJ5w4cQJDhgxp8H1VVVWoqrp1d6K4WHHhIZVKIZVK9d5OZZ2GqLutOpSk6D56W4hnu9omkZ1ccfZ6AU6n5OGuPvpJeGwOTHV7aPJnbAYAYFKYL2pqDHtRMa67Fz6d0wdv7UlEVrH63dVXp3THuO5eev/8zHW7mIuzKYq7oH06uOr0Gbd0u9zW1QOfHlAM4y2vrDKrXiWmyJS/J0UVUjHP1bAubibZxqb07+yGg0m5OJmcgzA/3QIZprxdTIFyiFknd7tW+4w0bZNe/i5IzilDTGo+RoSYz3BIc2WO35H98VkAgDHdvfRy3RXoaoNvF/TH/f87g/M3CvHg5tP4an4k7G1a76ZNXea4XQypqkYuBrS7eNob5XNpK9tE2/brHCA6evQojh8/Dhsb9aRgQUFBSE9P17W6FsnKyoKNjQ3c3NzUnvf19UVWVlaj71u9ejVWrVpV7/l9+/bBwcFB380U7d+/32B1tyVyATgQbwlAAuu8y9iz57LBlmVq20TIlwCwxJH4NOyxuW7s5rQ6U9seTamUAQcTFfupU8El7NlzqVWWuzQMSC6WoFgKHMmU4FqpBU7HJsC78KLBlmlO28WcHI5T7D/ITcGePVd1fn9zt4tcABytLFFSWYMNP+9FV84voRem+D05lyeBXLCEr72A88cP4ryxG6Qj5wrFOXH3qUT4FcU3qw5T3C6mIClDcfxJT4zGnhutsEBBDs/SJARKCxG9PQF5Tt0BiXpw2qJIsb0PnLuMLhVJrdAoAsznOyIIwN7ztdddRdewZ0+K3up+KARYH2+J09cKMPvT/Xi4uxxWRr53Yi7bxdAyyoAauRXsLAVEHzsAY6ZMM/dtUl6uXQoTnQNEcrkcMpms3vNpaWlwdjaPbl/Lli3D888/Lz4uLi5Gx44dMXHiRLi46H86dalUiv3792PChAmwtrbWe/1tiUwu4KezaSirSYCdtQUemTXJIEMvTHWbDC2vxperD+FmhQRDRo2HhwnMLNIaTHV7NGV3bCZqTl9AkKcDHr57uFGSfO6KzcTzv1xAQrkj1k8Zofc2mON2MRdVUhlePH0AgIBF00eho7v2Nyf0sV0OV1zAjvOZKHcPwdTJ3dVflMsguXECKL0JOPlC6DgUsDDe3VRTZ8rfk6O/XwSQjtv7BWHqlO4ay5sa/xuF2LnpNNKqbDF58midZq8x5e1ibIXlUpSdOAgAuH/6RDja6vxzQCeSxN2w3LcckpIM8TnBOQCyie9A6HGH+JzXtXzs+PoscmT2mDp1lEHbROb3HUnILEHhyROwt7bAknvHwVbPvw/6Xy/Aom+jkFAI7Cvxw7p7e8PKCD1szW27GNru2Ewg9gLCAt1x++2DWr8BchlkKccQd+IfhA8dD8vgEWZ7TaQcNaWJzmeEiRMnYt26ddi0aRMAxVSkpaWlWLlyJaZOnaprdS3i5+eH6upqFBYWqvUiunnzJvz8Gh+eY2trC1vb+nltrK2tDfpFNHT95m5vXCZW7YpHZpFiqu5KqRwTP/4PK6eFYbKBZogy6DaRy4Drx8UfWeg8TOMBxcfVGl19nHAluxSxGaWYENa+ZuMzp+/IvgRFXo+pEf71elS2lskRAVjxRzxuFFQgLqsMkQaaCcuctou5iM0ogVQmwMvJBsHeLs0K7rVku4zv5Ycd5zNx+HIeXpumUkf8TmDvUqD41g85uAQAk98FwqY3a1nthal9TwRBwNErinx+Y3r6mlTbtNW3kyfsrC1QUC5FamEVQpuRf8LUtospSCtSDC/zdbGFm5O9YRcWvxP4bREAQe1pSUkmrH5bBNz7nXhs6dPJszZRdRWKquTtJgelsZnLd+TIFcWw7OFdveHkoP9ZW4d29cGXDwzAQ9+cxd/x2Xh1ZwI+uLuP0aZVN5ftYmjJuRUAgO5+zq3/edReE1kXZ2AAAFzfYNbXRNp+fjqHRT/88EP8999/CAsLQ2VlJebOnSsOL2tqtjFD6N+/P6ytrfHvv/+KzyUlJSE1NRVDhw5t1bZQy+yNy8TirdFicEgpq6gSi7dGY2/tVOJmI34nsC4c+PYO4LeHFP+uC1c8r8GA2mmIz15nompTVV5dg4O1CX4NOXuZJg42VphYG0TcGZOhoTSZkujrhQAU09sbo/fZbaHesLSQ4Ep2KW7k13Y5jt8J/PyAenAIAIozFc9rcfwi05GYVYKbxVWws7YQZwQzNzZWFmLg+1QKz4n6Iubz8DLwjEBymSLgXCc4pFD73N5XFOUAONlaiUmzLzBRNdXxb6LiumtcT8Ml3L8t1Bvr5/aDpYUE26PTsXLnRQhCQ/svtRajJahux9dEOgeIOnTogPPnz2P58uV47rnn0K9fP6xZswbnzp2Dj49uX9jS0lLExMQgJiYGAJCSkoKYmBikpqYCAPLz8xETE4P4eMW486SkJMTExIj5hVxdXfHQQw/h+eefx8GDBxEVFYVFixZh6NChjSaoJtMjkwtYtSu+qcsHrNoVD5ncTA7QLTygDKi9kD97rcBQLaQWOpiYg0qpHJ08HNArQP/DUnVxZ99AAMDu2AxOBW1Gzt1QfL/7dXIzyvJd7a3FYPSBxGydf8iR6VPOXja0i6d2Q7XlMiDlKHDhV8W/JrKtlcGtM5zdU2/EGcy8DTyD2fXj9a+F1AhAcbqiXK2I2tlc49IYIKJb8kqrEFM78+eY7oadkXFiLz98eE8fSCTAlpPX8e7eJNRIpbj43584u3sTLv73J2QGnpiEblEGiLq3ZoConV8TNWvQsZWVFebNm9fihZ89exZjxowRHyvzAi1YsADffPMNdu7ciUWLFomvz5kzBwCwcuVKvPHGGwCAtWvXwsLCArNmzUJVVRUmTZqEzz//vMVto9ZzOiW/Xs8hVQKAzKJKnE7Jx9AQz0bLmQSNBxSJ4oDS4/ZGh5spf7RdSCtCpVTWqtNfk3b21PZomxLhZ5TeH6pGhHrBw9EGuaXVOJ6ch5HdvI3aHtKOsgeRoYYFamNsDx+cSsnHgcRsLPC/of0PueDbWq2N1HyHk5TT22txTDDhoYWDghUBotMp+RAEwejH3Lbgao6yB5GBA0SlN3UuFxHoij9iMtiDiNQcSsqBIADhgS7wc9X/8LK67uoXiPJqGZb/fgEpR39A7skt6CXJE1+/ud8TGUNXot+kBQZvS3tWUS3D9dpezs0ZYtxsugS32+A1UbMybyUlJWHJkiUYN24cxo0bhyVLliAxMVHnekaPHg1BEOr9ffPNNwCAhQsXNvi6MjgEAHZ2dvjss8+Qn5+PsrIybN++vcn8Q2R6sksaDw41p5xRaXtASTnaaInOng7wcrJBtUyOOF4gmZyKahkOJNQOLzNQbixdWFta4PbaYW47Ylp3JklqnsyiCmQVV8LSQoLeHYw3hdjYHoq7sCeu5qE6PUa7N2n7g4+MqrSqRhymPErT3XYT70bfr5MbrCwkyCyqRFpBhVHb0lZcVfYgMnSAyEnLPIoF1xVTVAEIV/Yg4vUPqThQO7xsrIF7D6maO7gT3uiajA3W6+CDPLXXvIU89Dn+NM79/W2rtac9Ss4phSAAHo428HJqxXyfzQhutyU6B4h+++03hIeHIyoqCn369EGfPn0QHR2NiIgI/Pbbb4ZoI7VxPs7a3QnQtpzRlGYDZ77SruyP9wG/LATObQNKstRekkgkGNBZ2aWew8xMzeFL2aiQyhDoZm/UH/eq7uwbAAD4Oy4LldK22d21LVH2Hurh5wwHG8POHtSUrm4SPOx8At9ZrILNPyu0e5O2P/jIqE4k50EqE9DZ06HpIIAZdKN3sLESgwYcZtZycrmAa60VIOo8TNETDRp6fR14E9gwHIj9Bb38FDM6ZhRVIq+0yrDtI7NQXSPHkdohs2N7tt45SFZTgylp6wAAdfNUKx/7n1jF4WYGlJSlGF4W6uPUer1HK4uAK/9oV7aNXhPpfGX68ssvY9myZXjzzTfVnl+5ciVefvllzJo1S2+No/ZhULAH/F3tGh1mJgHg52ondjM3KYIApEcBp74ALv4OyKXavU9arih/8XfFY78IoOt4oOsEoOMgDAhyx96LWYi6ng8gpP77mzFDGunHnguKgN7UhoaXGWm79O/sjg7u9kgrqMC/Cdm4vbfxezZR486lGjH/kCAAN04D57ZAcvF3rJCWAhaAHICFpS0ga+wHmUTxQ6/zsNZsLTXT4UuKu+0ah5e1djf6Zh4jBwd7IOZGIU6n5GNmZIeWt6Mdu1lSiQqpDJYWEnT0cDDswiwsFcMUf36ggRdrz5/dpwApR4Dsi8D2h+HsHoRnXCdjY9EQXEgvwuhW7DFCpunstXyUVNXAy8kGvQNb78Zc4qm/0Qt5jcY3LSSAH/Jw8dTf6DX89lZrV3tyKbs2/5BfKwwvqyoFTm0Ejn8KVBZqKNy2r4l0DhBlZmbigQfqH+jnzZuH999/Xy+NovbF0kKCldPC8PjW6HqvKY/JK6eFwdJI00w2SFqpCO6c/gLIOHfr+cABQH4yUFGIhu/I1h5QZn0NJB9QRKgzzgFZFxR/x9YCti64238Yrlh2wLlr/SGXD1CfYtOEc0W0dZVSGf5NUHQnnVJ39jIjbheJRILpfQLw+aFk7IhJZ4DIxEXXBohaNf9QcSYQ+6Oi12LeZfHpCqdOWF8wBMccx2PHXXaQ/KzMp9DA8WvyGgaizYAgCDikbf4hbbvH//Yw0GEA4NUN8O4OeIUq/m+rw0V7C46RA4M88MWRqzjNHkQtllKbf6iThwOsLZuVaUI3YdOBbpOAS3vVn3cJUBxTwqYDFQXA6a+AUxuAgmt4Dhtxv+2PuHxyPtD5ZcDOuJNBkHEpZy8b092nVaecr8hP065cAYf3G8qlrFaYway6XDEC5L91QHntUEKv7kDoBODEZ7WFVK+JavfBNnxNpHOAaPTo0Th69Ci6du2q9vyxY8dw221tL0kTtY5Jvfzg6WSDvNJqtef9XO2wcloYJptArhcAQFEacPZ/QNS3QHmu4jlLWyB8FjDoESAw8lY+B0jQ6AGl81DF39hXgbLcW8GiK/8C5blwu7YXa6wBCF+hav062HafoDhQlecBvz6Eej/elLki7v2OQSIDOnIpB2XVMgS42qFfR7dbL4jb3Hjb5c6+gfj8UDIOJWWjqFwKVwdrgy6PmqeqRoa4jGIAiinuDaqmGrj0lyIodGU/INTOcmftAITdBfSbB0nAYPzv//5BRbEMCW63Ieze7+r/iLd1Bu78nMcWM3E1twxpBRWwsbTAkC4aJnbQtnt8aRaQuLv+8y6BKkEj5b/dAUcvQFLnxkYLjpEDghTflas5ZcgpqYK3s6127aZ6Wi3/kFJlEXDtPwCAbMxrOHc1F31vmwSrLiNv/biydwdGvQQMfQKI3oLSg2vhU5UFn5RPgXXfAQMfAYYsVuxX1O4cVOYf6tFKvcmkFcCFX9HrknaTHtm7Bxq4Qe3XpZulAAwUIJJWAlHfAMc+unWzxKMLMHqZ4nedhSXQcXAjNzbWtOlrIq0CRDt33kpQOH36dCxduhRRUVHiVPInT57EL7/8glWrVhmmldTmJd0sQV5pNWwsJdj0wAAUVUjh46wYVmb0nkOCAFz/TzGMLPFPQKjNxeDSARj4IBC5QP2iJWy64mJX2wOKoxfQ+17Fn1wOZMYAV/5B0rHf0bU6Abb5ScCJJODEetQPOomNhDYzpFHL/BWnGF42Odz/1vAyPcxcpw/d/ZzRw88ZiVkl+CsuE3MGdTLYsqj54jOKUV0jh7uDNYI8mzG8Qy6D5PoxBOafgOS6C6D6I0spKw44txW48POtu2EA0HEI0O9+oNcMseeHHYDhXT3xT0I2DiZlI2zMdMW+ev04cHG7IiDu0bVNXwi1NcrZywYGu8PRVsNlnjJHTKPDzCSKINKd64G8ZCA3Cci9DOQkAWXZiuFnxenA1YPqb7NzuxU08goF/vsYLTlGujnYiMe3s9fy6/fgJK2ltHaAKOpboLoE8O4B+dCnkV74F/p0HtHwtrZxBIY8jjiPO/Hrt2uxxGY3girTgaMfKO7k918ADF0CuHVsnbaT0V3NKcXV3DJYW0owItTAAcKC68DZr4Ho74CKAtij9qglqMe7leQCkC3xRI/BkwzbrnaqpFKK9ELFxATdfJ30V3FNNXBuC3D0Q8X5CwDcOgGjlgK95wCWKufNMMU1Uc3VI4g5+nf94HYbpVWA6K677qr33Oeff15vOvknn3wSjz/+uF4aRu3LP/GKyO1tod6mM968ugyI/Rk4/aVibLxS0G3AoEeB7lPVDyKqwlR+ZOmSa8HCQtELKTASuyrvxJaDMXghJAMPeF8GEvcAlU0lrW7bUy4aW1WNTNxPp0aozJSobQ6P7Y8AoZMAn56KH03WzUy63kQOjzv7BiJxbyL+iMlggMhEnUstBKDoPaRzwsXaITpWxRkYAADXN9waohM0Aoj7TXHRk3n+1nuc/IC+9wF971f8UG/AmB4++CchG/8m3MSTY7oq9qfg2wDPEEWAKDMGKM8HHEwwDxzVc/iSDtPbW1gCQ54E9r3awIu1++fU9xU9WEMnqL9cnq8IFuUmKQJGuZcU/xamKvI33Dil+NNIu3PXwCAPJGaV4DQDRC2iDBB18W6FAJFMqsjpASgCO1oe88I6eeFX2Sj8VnEbLsyWwunMJ4rh+Kc2KoaC9J4NDH8W8O5muLaTSVDOXjY42BPOdgboGS0IigD36S+BpL8gBrLdOgEDH8aFAmuEn1kGQVBPVF076R4yh66En5XxJptoyy5nK3oP+Tjbws1BDzOYyWqA8z8AR95TnKcARS/YkS8CfecBVo0sw8ISQucRSL9Y3Hhwu43Rao+Wy+WGbge1c/trf3iPD2uFbPCa7sDnXwXOfK34oVVZO82qtQPQZ46im7NvmHbLUf7Iaqb+Qe5YDyf8r7AvHnjkOSD2F2D7w5rf2EanXDS2Y5dzUVJVA18XW/XcMdp+3nG/Kf4AQGIBeIQogkU+Ybf+9ejSeNAR0JjDY1off7y7NxEnU/KQVVQJP1cTn/mvHbqVf8hNtzc2OkQnA/h5PmBhBchrZ1KxsFYkfu03HwgZ2/Q+hVvd9s/dKER+WTU8HGsvklwCAO+eQE4CkHJY0fOITFqlVIaTVxW9xkZ10/JmS+oJxb9WdkCNymQRmrrRO3gAnQYr/lRJK2oDR5cUf1f+BdLPam7H9keADgMB7x6K3kfePQDPrmIwfVCwB7acvI7TKVrkIdKmp107dTVH8aOrVXoQXfxdEfxz9FH0km6oE1kDXOwUPSyv5ZUj2vE2jHxkhuJH/NGPgGtHgZhtQMz3QM87gBHPK26sKXESjzblgKGGl1UWK4IFp79Uy8uHLmMUN4G7TQIsLNEbwDlrRwScWAVflanuJRIgKfxF9Ju0oH7dpBeXb+opQbVcBlz4FTi8RvEbD1AcG257QTEKpLk3bNswhjzJ6G4WV+J8miIQM66ngXsPNXYHftIaRdfm05uAy/sgXsW4BytOFH3nAvZuhm1bHZGd3CGRANfyymtzLvhpfhPQZqdcNDbl7GVTwv3VkyRq+3l3n6oION68qLi7nndZ8ZdwawgvLG0U+Tt8eqoHj9w6AQm7NObw6BA2HQOD3HHmWgF2nc/AIyO7tGidSf9UexBprclhjMoyNYBPLyByPhBxL+CoIfeMCn9Xe/T0d0FCZjEOX8rGjH4qs0SFjFEEiJIPMkBkBk6l5KOqRg5/VzvtuuRnxNTmFpIADx8AKvJb/sPa2h7w7634AxS9br+9Q/P7SjIVx0PVY6LEAnAPArx7YKxLCGZaSJGc1QHFxeFwcWnkO9RUT7t2PlSyukaOGwWKIRtdvPQ4ZKMhgqCYDQhQXEdZ2QJSLWd6BRAe6IpreeW4kF6Ekd28FcHukLFA2llFoCjpT8V5MWGX4kf9bc8rkl3vfYWTeLQRJZVSMSCst98HOUmKoND5H4BqRbAUNs6K6/yBDzfYK63fpAWQjbsfF0/9jaKcVMijtmCE5UV0AG/IGlJSlmL7hPo0ESBqKiAslwPxO4BDqxU3KwDAwRMY8Rww4CHAxsCzOJqxZgWIysrKcPjwYaSmpqK6Wj2p8NNPP62XhlH78U/trFB9O7rBx9mAUdym7sD/Umdmvq4TFBc0Xccrhn0Zgau9Nbr7KnIuRF3Px+QwZa6ITDQ5Q1obnXLRmKpr5NgfrwwQ1QnUaZPDwyUAmL1VcdISBMWJLDseyE5Q+TcRkJYBNy8o/lRZOwLyamiTw+POvoE4c60Af5xPZ4DIxGQXVyK9sAISCdBHNcm5JhqHMdaasgYIHtmsto3t4Y2EzGIcSMxRDxB1GQOc/FwRIBIaScRAJuOwyuxlWg1hPLRG8W/E3YBfL8M0qrMW5y4nX2DaJ0D+FSAnUfEjLidREVTPvwrkX4UjgI+UIwA+WgG4dlL8mFPtcZSXDOxYXH85nMgBAHCjoBwyuQB7a0v4uhg40fe1o0BWLGBlDwx8SOe3RwS6YndsJuLSi9Rf6DAAuO97xXnz2Drgwi+K3kV182ApcdubraOXc1EjF9DF2xGdPZvo8aap15isRjGL3ulNit6wSl7dFRPM9JmjcUZGSysrcSr7VakSjMhfCpuE7UD1u4obzKR3l8Up7hsJZjfaq34NILFUBIZuximet3MDhj8NDHoMsDVwcLwN0DlAdO7cOUydOhXl5eUoKyuDh4cHcnNz4eDgAB8fHwaISGfKvC4TDDm8TJs78JAAgx9TBIY8/5+98w5zqzzT930kTe+9eTzFfdx7xdgGgiG0lM2mEEgjGzaNZDdtNxuW/FKW9GySJT2BkIR0AgQMxgVw73U89niap/feJZ3fH9850hRJI82oz3dfF5eOpSPNO0hzdM77vc/zzPNdLR6wtiBFM+XsFEluux93kpCmEcaRi4HkcEUbPUNmMhKiWFc4wYfFYIRt/wYv/puDZzqIwlQUSMgW/83bZd/VaoXuGxOaRlfEhdJo/xQV2j087ly+kf9+7jKX6nu43tLH/Ez5RRgsnNGmhxZlJRA/lXnwWNyVMfa1eF6Uxq7FmfzoQAWvXW3BbLFi0uOvC7cKyVr3DXGhHiTHRoljXrsmPgNu+Q/VnRYpd4pBmHP6CoPRxXfXGJ+jRbcDY8xeVVV8psc0jK6XniGpv4IMpUd8JrtviATQKZFBDmCPuC9Kj/PcA81TjvxQ3K5697T8y5bnJQFwcWKDSCdzCbz1J7DzP4QJ+qlfOHkl+d6HKvuuiOPZLa7kZa6k9wVb4cyTwkuvu1Y8phjERPeGh6Do5mktemSvuIXq/VkU0gylfxefcYnXueoq4t6l7H7Mon9UImz+qEhBjE7yXbFhhsejEZ/61Ke4++676ezsJCYmhmPHjlFTU8PatWv51re+5YsaJWFM/7CZwxVC0+vTBlH5XjdW4FVYfFdQXQCt15oRJ2s0c2o9IS1xgkGnKUaujvmQly42ArB7abbjVD1d02ycsCKbmOv++2LQpBSL7hC66Lf9HB4+DP/ZCLf9P/cK7WsmNS5SjOMDz52rd+95Er9wVvMf8jje3l0Z4wzkpavyU0iJjaBnyGxrZAFiZXSuSCylYv+0X1/ie2o7Bqho7cdoUNgy3420n4NfE7cr3unUwNxrOPvucnWMVBRIyILim2Hjh+Gu73Bm529YP/xjPpD5DLz/Jbjre7DxYTHpFjOVrHKMGfYsxZZg5muD6tarUP4yoIiLs2mwVGsQ1XUO0tk/4nzHlAI35K/yvQ81rFaVg1d1/yEn3216k2Di+b3uzffthbDvMdEcikkV0qJPnod3/haKd0x7IvaWkmz+aNkBgOXUk9N6DYlrugZGaOkdBmDBxAaRu4v++vu94/OyOeQhHk8QnTt3jp/85CcYDAaMRiPDw8MUFxfzjW98gwcffJC3vvWtvqhTEqa8Ud7KiNlKQVosC7w96dBRCVf3iLHS6jfce06QGTyvLRAXkpfruxkcsRATaRyfkFb1unDjj4iFJXcHuNrwZNRi5RVtyu2O5Q58oIZ6RCQqiAudyDjvmmMaIyB3tXv7ag2Ce1flsr+shb+fb+BTty30/UqxxC3s/kPJnj2xYIvQzY+NrB/HzOWlRoPCzQszePZcA/vKmtlQNGbFv3iHOIZWHBCrrpKgRE8vWzM3maSYKdJ+bhwXkzeKEW7+jB+qY/rpnmNYr30uD9XDUO5Gosd+5i/+Gf7ihpQpyL7n/UmlnmDma4Pqoz8St4vunPaiW1JMBAVpsdS0D3CpoZubFriYinN7ynL2vvehxvm6Ltr7R0iINrGu0MGiirvefNkrYdNHYOlbvWZGPC8jnqMJt2Me/BOmumPQek0m6nmZa83CfygvOWbyxLVbsnsV5t0i01eniccTRBERERg0T5bMzExu3BAxcUlJSdTW1nq3OknYo19437oka+YXsVYL1ByFvV+CH26A/10NL39B6I1VN5P4gszgeU5KDFmJUZitKudqu+wP6Alp2z8jpocG24UsSeJ1jla00zUwSlpcJBuLHKxQn/sdjPRC2gJY8Cbxvix/u7j11ii77uGBi7+RxDxbg+C2kixiIozUtA+M/9xIAsaoxcqF+i6A8Sl47mAexvnXtQMZ4zTZtUQc/w6UTZCq6VLI6jdEbLUkKPEo3l6fHlr1bpGe6C/0765pHiML02LJSIhixGLl/MRjmx8m7UKdqjY/JJj1tcL5Z8T2lo/N6KWWTSUz05Hvfdihp5dtX5hBhNHB95+73ny3f0Uc57yYVKUoCitLFnPAqi3enX3Ka68tEVxt1uVlDoYHZEPY53jcIFq9ejUnT54E4Oabb+ZLX/oSv/3tb3nkkUdYtmyZ1wuUhC9mi9X2BWCTl1ktUPWGWAmsekP82xVDPSJG9a//At+cD7/aLbTobVfFymjhTXD71+CjJ6e4wFbGXWAHC4qi2DxvTtc4iPY1RULBZrFd9bofK5s9vHRJyMtuX+ZAXma1wPEfi+1ND/vO0Fz38ACcfoZ3f912sRUbaeJNS8Xf1N/PuXECJfE5ZY29DI1aSYw2eb56//o3YaBVjMgneCDR8ZCbF2RgNChca+6jrnPA/kDOSohJgeEeqD89458j8T4jZitHrrcBsGPRFGk/1Yeh8iAYTGKRIYRQFIUN2nfipLj7KRvpwfk97090iVlxhg+96U7+HCzDkLcW5m6e0UvpPkSTjKonIt/7sGNK/yE/ePO54pYlmfxBk5mp534PZhcySInH6BH3Cx1F3MuGsM/x+Grma1/7Gjk54gT1q1/9KikpKTz88MO0trby05/+1OsFSsKX0zWddA2MkhwbwbqCFKEl/t4yEYf7lw+K2+8tE/ePpbMajv0YnroXvlEMf3ofXHhGxPNGJ8Pyf4K3/QI+Wwnve0Ho3zMWurjA9t4KvC9Yp8nMTuk+RBMpulncVr7m+HHJtDFbrLx8WZyE3LksZ/IO1/ZAZ5X43K18p2+LcebhoTNhSu7eVbkAvHChEbPFzQk6ic84W2v3HzI48rFyRnMpHPlfsX3PD+BTlzHf/yynCh7GfP+z8MhFr3mPJcVGsFabbho3RWQw2o8zFU6SgiQB5XRNJ/0jFtLjIynJSXS+o6rCAW16aPV7hX9LiKHLH09UT2gQuWykB/f3vD/oHzbT3CM8PYpcJULNhNFBOPkzsb35YzNOPZzSqFpHvvdhRWP3IKWNPSiKi4Z3gJsEG4pSOWFaS7OajDLQJgz/JV5DN6he5MigWjaEfY7HDaJ169axc+dOQEjM9uzZQ09PD6dPn2blypVeL1ASvuzV5GW7FmViuvqCE6M5LZ709W/Dq/8NP9oE318pdMeVB8E6CmnzxYnI+/4Bn6kQ5r7L3w4xyeNfazommUHAetsEUSdWqwOtdbF24VZzWER5SrzG8aoOOvpHSImNYFOxAx3zsSfE7dr3+SfmtOQeeOQSPPiCaII++II9feiV/xIn5xo3LcggJTaCtr5hjlY6866R+IszNXqDKNn9J1mt8MKnhI/CojfDkrvAYEQt2EZ96mbUgm1ev+DZqa3W7ncmM3MWJS0JKLq8bPuCDNcNyKrXoeYQGCNh+7/7qTrvon8nnqnpnNz8dvY9H5MS1N/z/kCfHkqLiyQpdgqPquly/vfCKy1pLiyZ+f/rZbmiQVTbMUjXwBQTGs7e+/jMWf/ehxr698+auSmkxkU63qlgi3hvneLbJkGUyciWBdn82bJd3HFGysy8haqqXLNJzBw0iGwNYUf+U7Ih7A18pIeQSFyjqip7r2jx9kvSXRjNqeK//V+GQ9+F1itCOlawDd70FfjYafj4abj9q1C4DYxT+K5rF9i+WoH3BYuzE4iNNNI7ZOZaS+/kHbJXCHf+4R5oPOf3+sKZF7X0stuXZttjv3UaLwhPFsXoX+PeiR4eWx+BpHyR0nH4f227RRgNvHmFOFF+9qyUmQWas5pfikf+Q2efgtpjEBEHd37DN4VNYJfWIDpU3safT9VytKIdi1WFeWJhiLpTMDTFar7E7+hpPzcvcuE/NHZ6aO37IGmO7wvzAYuyE0iINtE/YqG0sWfyDmO+55sTNOuDZW8N6u95f6AbVPvMf8hqtZtTb3p46vMxN0iKjWBuaiwAl+odvNcTGbuIontrvekrs/69DzX0CdZdruLtAaKcJVP5p0mwa3GmLc2M6/ugu85nP2s20dY3QufAKIoC850FGJXcA6vun3x/kC/6hwpuHb1Xr17ttoHwmTNnZlSQZHZwvaWPmvYBIo0GdkRfd89ornA7rHkAFtwqVgOni74Cf7mHlT5Ygfc2JqOB1XOTOXy9nVPVnSzOniAfMGheS2UvCEPuOesCU2iYYbGqvHy5CYA7ljuQdeneQyX3BvZCKzIWbvsy/Pn9oom66t2QnA/AvavyePrYDV6+3MRXR5cRHRHcn/Vwpa1vmJp24emzMj/ZvSf1tQjDfYBdX/TbZ6yytQ+DAqNWlX//8wUAcpKiefTuEnanzYf268IfbsldfqlHMjXNPUOUNfWiKLDNVbx9xX7RcDRGwbZP+69AL2M0KKwvTGV/WQsnqjpYMSd58k7a9/yNtJvJ6r0Etcf9XmewUdXq4wZR+cvi+BCVBGve67WXXZ6XxI2OAS7Wd7NtgYvPt46+iFK8U6TZNl2EFe/wWj0S3zI0auGQ5qfmskF04mfQXg6maLFIOtaTKDFXNId83CTYsTiDz6rZHLGUsMVYCmd/Czs+59OfORvQp4cKUmNdn7cOatYbax6Eou3eSw+WuNcguu+++3xchmS2oU8PbZmfRszwDfeetPZBMTUxC1lXkKo1iDq4f5MDz4iim0WDqPI1uOnf/F9gGHKiqoO2vhGSYiLYMm9CellfC1z8k9je9K/+L24iS98ijEFrDsOrj8LbfwnA2rkp5CXHUN81yP6yFu501OiS+JxzWrz9gsz4qePHdV7+TzGpk70CNnzYd8WNYc+lRv71t2cmzXI2dQ/x8NNnOLh0AwXt14XMTDaIggZdXrYiL4m0+CjHO42dHlr/QedeZiHC2AbRh25ynsLWEbdAbDRfFn9P0c4mDsIfW4JZho8aREd+KG7XPghRDmQh02RZXhL/uNg4tVH1RHI024umC16rReJ7jla0MzRqJTcpmsWODIpBeJHue0xs3/5VWPt+kWrW1+zXJkFmQjQr5yTxTMNOrUH0GyHdlQ2KGeFSXqZjtUD1IbG95kGYs9YPlc0e3GoQPfroo7btBx98kA984APcfPPNPitKEv7sHRNvT/ywe0+axW706wqnMKrWfYhqj8PokFfjPGcrenrZm0qyJkesnvolWEYgbx3krw9AdRNQFLFa9tOb4dJfYP2HoGALBoPCPatyeeJgBc+erZcNogBx5oaH/kMV++HiH0ExwN3f94pUYyosVpXHni91KvRVgB/VzOUben2SoMGtePvyV6D+FJhiYNun/FSZ79CNqk/VdKKqqtMp96HIVNTkApSuGqg7CfNv9WeZQYUtwcwXE0QNZ4W3lcEEGz/i1Zd226h6IjkrxG3jedEgnaFhtsQ/7CvT/EmXZDr+u1ZVeO4TMDoABVth7QdEgmzRTX6uVLBrcRb/V7eefkMCcd21wh91/i0BqSVc0BtEi5w1CEE0foe7ISrR3gyWeA2PPYi6u7u57bbbWLBgAV/72tdoaJDeFhLPaOkd4pzmx3FbSZZ0o3eD1XNTMChQ1zlIU/fQ5B3SF0J8NpiHoO6E/wsMMyxWlZcuCXnZpKaKeVhM6wBsDoLpIZ2cFWIVBeClz4nVFexpZgevttI9MBqo6mY1Z7UJotXu+A+NDsILmvxnw4chb43vChvDiaoOGh0dWzRU4MW++aiKUcg2Omv8UpfENWaLlUPlQo5xs7O0H1WFA18V2xsemsLYNTRYnpdEdISBjv4RKlr7XO6r5m8UGzeO+aGy4ERV1TEeRD6IuNenh5a+FZLyvPrSy/KErP5Gx4Bn32GZJaJhNdgpvWFCBFVVOVAmGt63LHayKHzmKWGnYIoRyZ6GwNrp3rIkk2EiedayxV6fZEboCWYLXE0QVb0ubgu2+GURbbbh8V/Vs88+S319PQ8//DB/+MMfKCgo4I477uBPf/oTo6Py4kMyNfuvtKCqsHJOElmJ0RPiSSci3egB4qNMLNGii0/VdEzeQVGE/hZk3L0XOF3TSWvvMAnRJrZO9PS49BfobxVNSy+ktHiVXV8UEoqmC2LUGVicnciirARGLFb2XG4McIGzD7PFyvm6LsBNg+rXvwWdVZCQCzv/07fFjaGl13lzSKePWDpStJU6mWYWFJyv66Z7cJSkmAhWznEin7r6opiiiIiDrZ/0b4E+ItJkYJXm53W8ysF34hisc2SDqL1/hN4hM4oCBWmx3n3xrlq4/DexveVj3n1tIDk2kvzUGAAuNXgwRWSKgowlYrvxvNfrknifq8291HcNEh1hYPNEaT9Adz288kWxveuLkDbPvwU6YGluIlmJUTw9skPcUfYP6G8LaE2hjKqqlDeLpr/DiHudqjfErX7tI/Eq02q7ZmRk8OlPf5rz589z/Phx5s+fzwMPPEBubi6f+tSnKC8v93adkjBinLxMp+QekTQxEelGb2NdgSYzq55CZqZ31SXTRk8vu60ki0jTmMOkqsLR/xPbGx4Co4+igqdLXDrs+ILY3vdlGOwC4N7VYopIppn5n2vNfQyMWIiPMjlP49BpKYPD3xfbd34DohNd7+9FMhPck6UOzNHG+KXMLCjQ5WXbFqRPTloEkSx14Otie+O/iGNEmLChSFxAnpyiQaTmbxIbdafAPEVUepiiy8vykmO8H1Zw/MegWsSFmo+kHjOWmUkfopBg3xWRXrZ1Xvrkz6mqwgufEom9eetEUl4QoCgKuxZnckUtoD52MVhH4fwzgS4rZGnsHqJ32IzJoDg31LeMCs8pkA0iHzGjubzGxkb27t3L3r17MRqN3HnnnVy8eJGSkhK++93veqtGSRgxMGK2pRPctnTC+Khuapi9At72CxFTGuQR9P5kXaHuueDkZFg/SNafhiE34mAlDrFaVZv/0J3LJsjLqg9B80Ux2qzLuYKN9R+C9EUw0A6viWj0u1eIBtGxqnbHEkWJz9D9h1blJ2M0uPDAsFrhhUfEyeXCO2Cxf02gNxSlkpMU7UroS05SNLlr3yzuqHzNJmOUBI7X9Hh7Z/5DZc+LY1ZkAmz5uB8r8z0btO/EE1M0iEhfIJJPzYOztlFQqcnwvJ5gNtQNp58U25t99/laNu0Gkdawapyd73uosV+Pt1/iQAZ78U8iKc8YCff+MKhUBbs0OdzvRneIO87+RjS0JB6j+w8VpceNX6AdS8NZGO2HmFTIXOrH6mYPHjeIRkdH+ctf/sJdd91FQUEBf/rTn3jkkUdoaGjgySef5NVXX+WPf/wjX/7yl31RryTEeaO8jWGzlTkpMZNHBxvOitt5u0RaWdFNQfUFEGh0o+rShh76hs2Td0ieCylFYiXvxlE/Vxc+nK3tpLlnmPgoEzctnLDafuwJcbvqXRCb6v/i3MEYAbu1iYETP4HWq+SnxrKuIEUswF2QU0T+xO4/lOx6x3NPi7/biDi485t+N1Q1GhQevbsEcO4G9+jdJRjnrBUx1kNd0HDOX+VJHNDeN8wF7YLZYYPIarFPD23+1+A9Zk2TNQXJmAwKDd1D1HUOON9RMYA+RTRLvxsrfWVQfeYpGOkVixI+NADXJ4g8TjLLHmNULQlqOvpHOKstqEyKt+9rgZc+K7a3fxYyl/i5OtdsnZ9GpMnAk73rsJpioLVMmOJLPMatBLMqzUqjcFvAPajCFY//r+bk5PDQQw9RUFDAiRMnOHXqFB/5yEdITLSPwu/cuZPk5GRv1ikJE17V5GW3lWRNTifQG0S5q/1cVWiQkxRDXnIMVtUemz0J6UM0Y168KMypb12SSZRpTIOyo1J4eYDXU1q8zvxbYNGdYDXDni+AqnLvamEc+vdzskHkT/QTXpf+Q32t8Mp/ie2d/wHJ+X6obDK7l+XwxP1ryE4aLzdLjDbxxP1r2L0sR5hB6mkxlVJmFkgOXW9DVWFxdoLw85vI5b9B6xXR0NsURIb6XiI20sRSrXFwsnqKKaK5eoNodvoQVbXqBtVebBBZRuHYj8X25o/69EJtWa54n2vaB+ge9MDvNHsZoEBvgzjOSoKW1661YFVhSU4iOUkx4x988TPCbDx7OWx7JCD1uSI20sSWeWn0Ecu1dK1ReubJwBYVolxtEtOOrhtE0n/I13h8NP/ud79LQ0MDP/rRj1i1apXDfZKTk6mqqpppbZIww2JVbeOjty2ZIC8zD0PzZbEtG0ROscfdOzkZlj5EM8JqVXlJ8x+6Y2J62fGfAqpYJc1Y5P/iPOVNXxGj2BX74NrLvHl5DiaDwsX67ilTfyTeobN/xLZyrxvqOuSVL4qJnOzlAW8+7l6Ww6HP7eL3D23iPi0Bb83cZNEc0pm3U9xWHPR/gRIbr10VF7w7HKWXWS1w8H/E9paPQUyy/wrzIxu078QpZWZzN4vbG0dnpfRD9yAqyvBiglnp36GnDuIyYMU/e+91HZASF8mcFNE0uOzJFFFUgt3IuElOEQUzuv/QLROnh0qfg9JnQTHCvT8KPu9HDb3u345o5+GX/irtHqZBeYsece/kWGUehtrjYls2iHyGxw2i9773vURHu2dmKZGM5eyNTtr7R0iMNrG+aMKoe/Ml4b0RmyakUhKH6EbVp2ucGFUXagfL5osyRWEanK/roqF7iLhI43jJxlAPnH1abAeJMeKUpM2zTw28/AVSo1RuWiAkc3KKyD+cq+0ChKwjJS7S8U6VB+HCM4ACd30/KOJajQaFzfPS+NBNxQCcqunCbLHadyjWGkS1x2FYNhsDgdWq8nq5aBA5lJdd/DO0l0N0csCbjr5EN6qeskGUuwqMUcKbrf267wsLIixWlZp2IcHzmsRMVeHID8T2+ocgwvfXBdM3qpY+RMHOqMVqM9wf5z800AH/+Dexve0Rn5mge4OdeoOoMQdL6nwYHYDLfw1wVaGF1WpPMHMacV93EsxDEJ8F6Qv9WN3sQgr3JH5DTy/buTiTiIlpK/VnxG3uar97b4QSulH1mZrO8RdsOvEZdsM2OUXkMS9dEvKyXUuyxidonH3a7rMw75YAVTcNtv+7+BLtqITjP+Y+TWb23Ll61Fm4iu5vdHnZKmf+Q6NDIpUFRCrenLX+KcxNSnISSYqJoG/YPP6iLLUYkgtEU7/mcOAKnMWUNvbQ1jdCXKSRtQUT5IsWM7ymTQ9t/YRf0/D8jb5oUtHaT1vfsPMdTVGQp/19zTIfooauQUYsViJNBnKTY6Z+gjvUHIbGc2CKhvUf9M5rTsG0jaqlD1HQc6q6k94hM2lxkayck2x/4OX/gP4Wce61/bMBq88d5qTEsigrAauqcCXnPnHnmacCWlOoUdc5yOCohUiTgYLUWMc76dc2hTfJ60UfIhtEEr+xd4z/0CR0s9PcNf4rKARZmJVAQpSJ/hELZU29jnfSRy5lg8gjVFXlHxf09LJs+wNWi4jxBTE9FEpfSFEJcOt/i+3Xvslt+RATYaS6fYDzdR6eZEs85ozmFebUf+iNb4vmXUIO7Pqi/wpzE4NBYaM27XmscsyEhqKMkZkdCEBlEn21fcv89MlJLxeeEZ+r2DTY8C8BqM5/pMRF2gIvTkkfIofoMtfCtFjXSYqecOSH4nbluyAu3fW+XmLaRtX61MksTbALBQ5oaYw7FmXaP6Ple+H87wFFpJb5YUptpujTT78b2goGk0gV1u0zJFNyVTOonpcRj2niIIGOzX/oJj9VNTuRDSKJX6ho7aOyrZ8Io+J4HL5hzASRxClGg8JqbcXU6cmwzYdIGlV7wsX6buq7BomJMI739Lj6InTViJhkH/ss+IQV7xQr5yO9xL7xNVuD9u/n6gNcWHhjsao2iZnDBLPWa3Dou2L7jschOslvtXnC5nlCwnO0sn38A7rMrEIaVQeCg87i7S2j8NrjYnvrIxDlRc+ZIGV9ke5D5ER6rTPWh2gWUeXtiPu2crj2ktje/FHvvKYb6A2i6vYBeoY8MKrWG0QdlTAkF0aCkX1XxAKyLb1sqAee/6TY3vSvkL8hQJV5hu5D9I9KM9aFd4o7z/wmgBWFFnqC2aIsJ99bIwP2dDjpP+RTZINI4hf06aFNxWkkRE8wmBvpF5GQIBtEbrBebxA58yEq2CrM/DoqoavWj5WFNnp62a7FmcREjpGX6dH2a98PkU5GXoMZgwHu+IbYPvc09+cLb6rnzzdisUqZma+43tJH37CZ2EijbcLBhqrCC48IidaC22HJPQGp0R02FYsG0anqDkbHylqLtov48Lar0C2bjf6ke3DUNp02qUF07rfQdQPiMmH9h/xfXABYr0mvT1S3u94xfz2giO/G3mbfFxYk2Ayq073ULDz6I3G76E5IX+Cd13SDlLhI8jSJnEdTRLGpkKQlQzZd8kFlkplQ3dZPRWs/JoPCTQu1abS9X4KeekgpCsrpWmesnptCcmwE3YOjlM95q7jzwjNCTi6ZElvEfbYT/6HaY+K8KXGO+GxIfIZsEEn8gh5v/yZH8rKmi6BahcwiMWfy45JxrNWTzKo7HfvIRCfaG21SZuYWqqryoi29bIy8rOGc8FowmEL7YmvOOiEFANZdeZyUGBNtfcMcrZjigkoybXT/oRVzkiaPSp/7rfhcRcTCnd8MatnioqwEUmIjGBixcKGuy/5AbKr9OFN5MBClzVqOXG/DYlUpzogjf6xPg3kYXv+W2N72qdBsaE+DDZoMsrShh15XkyUxKZBZIrZrZ4/MTJeYecWgur9Nk/0Amz8289fzkGnLzKQPUdCipxtvKEolMTpCnLee/pV48J4fhNRxzGhQ2KlNoP+tZ4FoZAx2QtkLAa4sNLiqWWcszHTSINKvaYq2B/V5UzggG0QSn9PWN8xp7WLplonx9jDeoFoyJavykzEZFJp6hqjvGnS8k4y794jLDT3c6BggOsJg+3IH7N5DJfdBUl5AavMatzwKEXEY6k/yhfyLADwrZWY+44x2zJvkP9TfDq/8l9je8QVIKfBzZZ5hMCi2KaJxPkQgZWYBQvcf2rFwQhz02d9Ad61YbFn3/gBUFhhykmLIT43BqrpI+NSZhT5Ela16xL0XGkQnfy4ShHJXQ8GWmb+ehyyfoxtVexgfLn2Ighbdf2jX4kyhKHju4+KBdR8ISZ8ZXSa3r6wdVr9H3CnNqqfEbLHajlWLnE0QSf8hvyEbRBKfs7+sBVWFZXmJjhM0Gs6KW2lQ7RaxkSaW5opUGqcnwzaj6teEnEXikpcuiemhHQsziYvSYsZ7m0VUNNjj4kOZxByRagbc1/pTYhliz6UmhkYtAS4sPDmrSYBWT2wQvfJFGOyArOXC9DwEsPkQTZw4m7dL3FYeBKuDVEWJ11FV1dYgunnRGHnZ6BC8/m2xfdO/QYSX0qpChA2F4jN6ckqj6tnlQzQ0aqGhWywkzdiDaHQQTvxMbG/+WEBW8JdN26haThAFI33DZo5p/na7FmfC/q9CZ7WYvLn1scAWN022L8zAaFAob+mjvvBtgCLOxTuqAl1aUFPdPsCIxUpMhNEmJR3HUI/9erFQNoh8jWwQSXyO7j90q6PpIZAG1dNAv+j806k6jla0T/aSyd8IxijobYT26wGoMHQQ8jLhPzROXnbqF0LrPGdD0MWPT5vNH4WUIiIHm/lc3D/oGzZzQBvvlniP7sFRyluEMew4g+qq1+H87wAF7v4eGCMcPT3osPkQ1XQwbB7TUJyzHiLiYKANmqW3hz8ob+mjsXuIKJPBljAHwOlfQ28DJObBmgcCVl+g2GAzqnYzyazxAgz3+biqwFPTPoCqQkK0ibS4yJm92IU/iL/1pHwxVRsAdIlZVVv/9IyqW6+KRpckKDhU3sqoRaUoPY7ioVI49n/igbu/J+wSQpCkmAjWa1YQexsi7YmfZ58OYFXBT7nuP5QVj8FR2uKNo6BahPdQcr6fq5t9yAaRxKcMjVp4o1ysdjqMtx/qtjcwZIPILfZcarRJgw5db+NdPzvGtsf3s0ebggHE6rGe+iD9QVxS1tRLVVs/kSaDXQI5OgQnfyG2N4fB9JCOKQpu/yoA77E+T77SLGVmPuC8ll42NzWW9Pgocad5GF74lNhe/0HhCxUiLMiMJz0+kqFRK+drx6zcmyKhcJvYljIzv/DaVfF9uqk4jegIzUx/ZAAOfUdsb/938Xc+y9CNqs/XdrueikzOF9MJqgXqT/mpusBR1SaaYMXpcSgzmfixWu3m1Bs/AkaTF6rznNQxRtWXPZGZJeRAbLp435tLfVSdxFP2XRELVLctTIK/fwxQYeW7YcFtgS1shtyyWJxL7itrsTfsz/0WLOYAVhXcXLU1iNzwH5L4HNkgkviUQ+VtDI1ayUuOoSTHwWqAPu6bPBfi0vxbXAiy51IjDz99hq6B8StnTd1DPPz0mfFNIhl37xYvaebUNy/MIF6Xl136s1gpTZwDi+8OYHU+YNGdULwTkzrCf5p+x4GyVroHPViJlUyJLi9bM3Z66NB3RTM8Pgtu+VJA6pouiqKw0eZD5ExmdsDPVc1ODl5zEG9/6pfQ1yy+R1fdH6DKAktRehzp8VGMWKxcqJtCfjSLfIgqbQlmM5SXXd8LbdcgKjHgE2rL8sS5pEcyM0UZ40MkZWbBgNWqckBreD8w8geRiBmXaVvECmV2aj5Exys76Ct8E8SmiYn+in0Brix4uSYbREGFbBBJfIpdXpbpePVKGlS7jcWq8tjzpThyFNLve+z5UrvcrGiHuK16Q/qDOEFVVf6hNYju1OVlqmqPtt/44YCtlPoMRYHd/4OqGNltPMk69QIvX2oKdFVhhW5QbfMfaiuHNzR/mDseh+ikAFU2fXSZ2WQfIm18vuaolG74EItV5UBZC8cqhIRq2wItDnqkXzQfAbZ/Vkx1zUIURRkjM5sindHWIAp/H6IqzfS1OGOGEfdHfiBu1zwQcOmPLjO7KH2IQpqL9d209Q2zIeoGeZd/Ku686zsiITPEmZcRR0FaLCMWK4eqem0pstKs2jnXmsW0o8OI+4EOkXgN0n/IT8gGkcRnWK0q+8pEg+i2kmzHO0mDarc5UdVBY/eQ08dVoLF7yO7BkLsaIhNgqEsmdzihvKWPitZ+Io1j5GVVrws/lYjYgK+U+ozMxSgbHgLgUdNTPHf2RoALCh+sVpVzmsRszdwU0XB84VNgGYEFbwqYd8dM2aw1iE7f6Bwv4UlfKHxvLMNQcyRA1YU3ey41su3x/bz/1yexaKEDD/zyhJgYPfFTMe2YUgQr3xngSgPLBk1mdqJ6qiQzzai69mTYSz6qvDFB1Hgeqt8AxSjkZQFm+kbV2gRRozwfCgb2lbVgwsy3o36GolrEd+OS8JjYVhTFlma2v6wZVr9XPHD1JRGAIhnHsNliO1YtzHLQzK4+BKiQvggSnPjZSryKbBBJfMbZ2i7a+kZIiDKxocjJioA0qHabll7nzSGH+xlNULhVbMu4e4e8qE0P3bQgncRozTBYnx5a9W6ISXHyzDBgx+exRKewyFDHvJo/0Nzj3udL4prKtn66B0eJjjCwOCcBzv9eXFyZYuDObwUk+ccbzMuIIyMhihGz1SahA8Tvo8fdS5mZ19FlxRMXB5q7h/jM04cYeV2bHrr5cyFjeu4r1mvnGaerOzBbXEzNZi6BqCQY7Yfmi36qLjB4pUF05Ifidtlbg8IcVp8gqmzrp9cTo+psbYKo+TJYpKw60Owva+YjxufJH6mAmFS485uBLsmr6D5E+8tasaYvEoEnqkULqpCMpaqtH4tVJSHaRHZi9OQdqvV4eykv8xeyQSTxGa9eEV3yHYsziTQ5+Kj1t0OXNrmQu8p/hYUomQkODppT7Tc27l4yCb1BdMfyHHFHewVc2yO2g2Cl1KfEpGC85b8A+JTpz7xyShp3eoOzmrxsRV4yEUOd8PJ/igd2fB5SCgJY2cxQFMU2RTTZh0hrEFXIBpE3mUpW/KDxFSJHulHT5sPyf/J3eUHH4uxEEqJM9I9YuNLY63xHg9Ee4hDGPkTdA6O0948AM2gQddfB5b+K7c0f81JlMyMtPorcJHGeU9rggVF1SpHwULIMCz8lScBo7hliuKGUj5v+Ju644xsQnxnYorzMhqJU4iKNtPUNc6mh2z6RfuYpMVkssXG1ye4/5NCORPoP+R3ZIJL4jLH+Qw7R5WVp80PSk8PfbChKJScpGmfzBwqQkxQ9flqrSDOqrjkK5hFflxhSXG/p5VpzHxFGhdt0ednxnwCqkAKlLwhofX5h7fvoiF9IstJP6olvBbqasOCMNl2zem4y7P0SDHZA5lLY/NGA1uUNbD5EExtExTvEbfMl6Gvxb1FhjCtZcQIDPGR6AYDyJR8LP6+0aWA0KKzT4qVPVLsZdx/GPkSVWoJZVmIUcVHT/Hwc/zFYzcL3I4gW8pZNx4fIYIDs5WJb+hAFlINXGvlGxE+JUsywcDcsf3ugS/I6kSYD27UwgX1XWmDpWyAyHjoqoeZwgKsLLlwaVPe1QGsZoNhTUyU+RzaIJD6hqq2f6y19mAwKOxZN0SCS/kNuYTQoPHp3CYDTJtGjd5dgNIx5NLNEpCeM9kP9ad8XGUK8dFEYM2+dn05SbAQMdsHZp8WDm8Io2t4VBiPGN38DgN2DL1J35USACwp99AmiW6KvwbmnAQXu/l5YyH82zxMNonM3usb7EMWl2+UblQf9X1iY4kpW/AHjSyQpA1yz5nEl7RY/VhXc6DKzqY2qNR+iG8fCdjV/xvKyoR44/aTYDpLpIZ3l0/Uh0o9T0ocooCjHf8xqw3WGjXFw13dDVno9FXYfohaIiodlbxMPSLPqcegG1Ysc+Q/p00PZy8LCwDxUkA0iiU94VZse2licSlKMkwsjW4NI+g+5y+5lOTxx/xqyk8bLzeKijDxx/xp2L8sZ/wSDYYzMTPoQjcWWXqb/Pzv7tGikZSyxT0TMApKW7ORE7HaMior60ufD9mLJH/QNm7nW3Esko6y5+GVx57r32+UsIU5hWizZidGMWKycqZlgBGyTme33f2FhijNZcRJ9fND0IgDfM7+NzMQZRpiHERu1BtHJ6k5UV8eyvDVgiIC+Zuis8lN1/sXeIJpmgtnZ38BwD6QtEFO1QcSyOdNNMtONquUEUaAYbr7GPe2/AKBt65cgMTfAFfmOHYsyURTxOW3uGYI1D4oHSv8uFiUlwBQTRLr/UKGUl/kT2SCS+IS9mv+QTbrjCGlQPS12L8vh0Od28fuHNvHuDcIwsiQncXJzSEf6EE2isrWPsqZejAaF20qyRJLN8Z+IBzc9HLarWc7o3PpfDKkR5PecRi39e6DLCUksZjOv7fkrdylH+Gb0rzF1Xoe4TLjl0UCX5jUURbFNEU2Smc3bJW4rDsgmo5dwJiv+kOlFEpVByqz5nIvf7jwEYhayPC+ZKJOBjv4RKrSId4dExNjPPcLUh6hSaxAVT2eCyGKGYz8W25s/KhabgoixRtV9wx4k0elR900XwerCyFziG6xWBv/8r0Qro5xUVpC748OBrsinZCREsXJOMgAHylpEYzpzKZiH4OKfAltckDA4YuFGxwDgJOJe+g8FhOA64kvCgo7+EU5p+v9bS5w0iHoaobcRFIP9C1viNkaDuFB739YiAC439GCxOrko032Iak/AiIsT5lnES5eEvGzLvDRS4iLh6ovQfUMkaax4R4Cr8z/b1q3hF+o9AIy+9J8wOhjgikKLsy8/SdtXFvLmsw/xv5E/5F6EWXNV7p0QkxzY4rzMpmLRjDhaMaFBlL8JTNHQ16T5BUhmii4rVgEDVjYZSnmXYR8fMv4DgO+a386X7lk2XlY8y4k0GViVnwzAqZou1zuHuQ9RldYgK86YRoPoyt/Fd2JsOqx8p5crmznp8VHkJEWjqh4aVacvEsepkd6wnRwLOqwWqHoDLv4ZXvkPkltP0q9GcXDRF1GCrPHoC27RZGb7ylrE4uMaLfJeyswAuN7Sh6pCalwk6fFR4x/srhOeTYoBCjYHpsBZSvj/ZUr8zv6yFqwqLMlJZE5KrOOddHlZxmKIlOPx02VeRjyxkUYGRixcb+lzvFNqMSTlg3U0bFdKPeWlS5q8TE8vO/Z/4nbdB8TK8iwjLspExaKHaFBTieyrs8caS6bk7MtPsvLIJ8hQxzdMVBUKrv2asy8/GaDKfMPm4nQAztd1MTAyZuU+IhoKtohtKTPzGruX5fDtZTUcivoEz0R+ha9H/oIYZZRRjHxke6HzydFZjC4zOzVRBjmRsT5EYYaqqtP3IFJV+3fA+g8F7XfitIyqjSbhzQhSZuYPSp+D7y2DJ++Cv3wQjj0BwPOWzaxZuSqwtfmJXVpQz6HyNuHdt+KfwRgJTReg4VxgiwsC7PIyR/5Dmrwsd7UMM/IzskEk8Tq6/9BtzqaHQBpUewmjQbGdJJ2v63K8k6JImdkYbrQPcKm+B6NB4U0lWVB/RqwgG0ziZHiWcufqYr4++m4A1EPfge76AFcU/FjMZnKPPgbAxCEOXaWYc/QxLGYPJBBBTn5qDHnJMYxaVE5P8iEaIzOTeIfS53jr9S+QzfhULhMWVh97RFyAScaxfowPkUvyN4rbtmvQ3+bjqvxLc88wg6MWjAaF/FQnC3XOuHFUWAAYo4L6O3HaRtW6D1GTNKr2KaXPwR8fgJ6GcXerKrzDeJBto+E5uTeRkpxEshOjGRy1cKyyXRgtL7lbPCiniGwNokUu/Ydu8mNFEpANIomXGRq18Hp5K+Cu/9Aq3xcV5ujj9BecNYjALjOTRtW8qE0PbSpOJS0+SsT4Aix9KyTO3tX47QszeCNqOyesi1BGB2Dvl1BqDpHXcRSl5pAYE5eMo+z4y2TRPqk5pGNQIJt2yo6/7N/CfIiiKPa4+4kys2LNqLr6EJiH/VxZGGK1wJ7PAQ4akPrGns/Lv80JrJmbgkGBhu4hXmtUOF7V4ViCHZcmJEcAtcf9W6SPqWwVE8VzU2OJMHp4qq9PD616F8RneLky77F8OhNEYLc1kBNEvsN27Jr8d6cogAJRr/7HrDh2KYrCzrFpZgBrHhC3F/8EIwMBqiw4uKo1iBZMbBCpqvQfCiCyQSTxKkcr2hkYsZCdGM2yvETHO6mqnCDyIiu0NI8LdS5OkvSDa8M5GJxiVTVMsVhVjla087vjNQDcvjRbeGFd+qvYYdPDAawu8ESaDNy5IpfHRh/ECnDpz5ievo91NU9gevo+MSYupxXGMdjp3pSVu/uFCjYfoolG1VlLhTG3eTDsLrgDQs0R6GmYZFJtR4WeerGfxMYb5a02X6a/Vhu5/5en2Pb4fvZoiwPjCFMfosrpysvaK4QnH8Cmj3q5Ku+iT09XtPbR75FRtZ5kdkEa6vsK7djlDAPMqmOXzYfoSotIVyzcDskFIiVwlgeDlOsR9xMNqjurobtWpE3qx2mJ35ANIolXeUWTl91akoniLAmquxYG2sUfffYyP1YXnugJCVcaexg2O1mNScyB9IWACtWH/VZbsLDnUiPbHt/Pu352jBsdwoD5h/uvc/3F7wtvprmbRbrELOfelbnMUVpQHJ0z9zSKcXHZJLIRk5Ln1f1CBT3J7EJd9/gEIUUZE3cvZWYzpq/Zu/vNAvZcauThp88wahl/EGvqHuLhp89MbhKFqQ+Rx/5DupHwC58GVFhwO2Qs9F2BXiAjIYrsRM2outEDo+rMpaAYYaDNZRNDMgPksWscW+enE2UyUN81SHlLn0gF1M2qz/4msMUFkN6hUeq7xDn5wswJDSJ9emjOOulVGwBkg0jiNaxWlX16vH1JtvMd6zV5WVYJmKKc7ydxizkpMaTERjBqUbnS2Ot8x1nqQ6RfMDR2D427v6e3l5QrT4t/zPLpIZ31c5P4cuRvHAyFg21UXEpabCzeeDvNpOEsQNCqQhNpLN54u38L8zFzUmLJT43BYlVtiZU2dJmZNKqeOfEuZNrT2S/MsVhVHnu+1OHxS7/vsedLx8vN9JXphnNhJfXwqEE01ki46qC4r/50SCwG2IyqXU1QTyQiWgSkgPQh8hXy2DWOmEgjW7SFlX1XNJnZqveIdK6aw9B2PYDVBY5yLVwnKzGKpNiI8Q9K/6GAIhtEEq9xob6blt5h4qNMNgmCQ6S8zKsoisIKbYpI+hCNx9UFwz3Gw6QpvTSSgWXhnX6vLRgx1B516akjJS3jMZpMNGx+FJisVNCvQRs3P4rRZPJzZb5ns+5DNFFmpk8QNZ6HgQnNI4lnFGyhw5jutAEJCiTm2dPjZjknqjomLQSMRQUau4c4UTXmc5lSCPHZYpJU90YMA/QGUfFUDSInRsIMtIfExOj0jaqlD5FPKdgCibngVCA7+45duzRf1v1l2tRUYi7Mv01sn52dZtXXmvQEM+k/FGzIBpHEa+wtbQLg5oUZRJmMzne0GVSv9kNVs4OVmlH1+VoXJ0mF2wAFWsugt8kvdQUa5xcMKh8w7gHgF6Nv4kSNB+Pp4YwcC/eY1bc/yJ+SP8RERW2Lksb5Lf/L6tsfDExhPkY3qj420ag6IVuLkVah8qDf6wonrBj4ivV9Ti6xtHt3/w8YXHzfziJaep03h5zupyhh50M0arFyo0NMQxVnOIiO1nFhJBwqE6PL5wivS8+Nqsf4EEm8j8EIux8HJn+61Fl67Nql+RCdrumks39E3KmbVZ/7HVhGA1RZ4Lja7KRB1FYuzjONUTBnfQAqk8gGkcRrvFoqxiZdxttbrdCgrdjIBpHXWGkzqu5yvlNsqn3VbJZMETm7YNhquMRiQy39ahR/tOxw+8Ii7JFj4R6jqiqNmjKlKX4Zp9Z9k8u3/Y6ML14L2+YQ2H2ILtZ30zM04cRWysy8QmVbHyeG8h1LPhNz4R1PQck9/i4raMlMiJ7efmHmQ1TbMYDFqhITYSQr0YWMfwoj4VCYGB1rVD0w4oFRdbacIPI5JfdwedV/TmpwN5PK2c3fn3XHrrzkGBZnJ2BV4bVrIu2ZhbeLYIf+Vri2J7AFBgCbQfXEBpFuhTF3o5CESvyObBBJvMKN9gGuNvdiNCjsWOQiFrWjEoa7wRQNmUv8V2CYo0vMrrf2jTeNnYhNZjY7fIicXTDo00N/stxMD3FuX1iEPdpYuPNcl9k3Fj4VFa39FA6VApC6+m7W3fVhlm59c1jKysaSkxRDYVosVpXJPkTzdonbyoMyJWgGnK7p5J+NB4Tks3A7PPgCvO0X4vaRi7PuAmsqNhSlkpMU7UrUQk5SNBuKJkjg9Qmi2hNBPS3jLmP9h5yGhUBYTIxmJkSTlRiFVYXSBg8mgbOXi9ueOuhvd72vZFrsudTIT050AXDVmscnRj7GO0e+yNah7/PWA+mOUwXDnFuWaGlmety9MQJWvVtsn5l9MjN7xP2ESUeb/5CUlwUK2SCSeIW9mjn1hsJUkmMjne+o+w9lLxcHRolXyEiIIjdJpHm4NGvUG0SVr8+KC7exFwwGrGwylPJB4z+4xXgWqwpPWm53fMEwW7GNhSuTfE9U7b/ZNhY+FQevtrBaEQaTkYUbAlyNf9FlZkcnyswKtoAxUiRWtlcEoLLw4ExVK/9sPCj+sf6DUHQTLH+7uJV/g5MwGhQevbsEcO588ujdJRgnmqxlLYPIeBE53VLq2yL9gK1BlDGF/1CYTIzqPkQeycyiEyG1WGw3ySkib6P7Py43VAJw1LqU56xbOGYtwaJdek4yjJ8F7Fos/pZeu9rCqMUq7tRlZtdfhe76AFXmfzr7R2jtHQZgwdgJIqtVJCqC9B8KILJBJPEKuv/Qra7kZSANqn2IW0bVBZvBYILuG9BZ7Y+yAop+wfAmwwkORX2CZyK/wn9F/BaAESJYpNQ6vmCYxeyxrufhkU/SxPimWb8azcMjn2SPVerBx3Ly8jUKDNpq4Cw7rukys0lG1ZGxkL9RbEuZ2bSJrnyZTKWL4eh0WPzmQJcTEuxelsMT968hO2n8VGhSTARP3L+G3ctyJj/JaLL7XISBzKyi1U2D6jAxEl6aO40GEUgfIh+i+z8uN1QBcEktGve4Q8P4WcCq/GRS4yLpGTJzuqZT3Jk2Dwq2gWoVXkSzhGva9FBecgzxUWMmrltKYbADIuIgb3adUwUTskEkmTFdAyOcrBYHutuWTNUgkgbVvmJFvu5D5OIkKTLOfiI8S2Rmuw0n+XHk98hm/IlIlDLKE5HfZ7fhZIAqCz70Vb891g1sG/5f3jnyRX5lFhHtDWoqL1s3zMpVP2f0Do1irT0FwEjKAohJDmxBfkZPMrvc0EP3wAQfIpvM7ICfqwoP2vuGuaX/RQDUVffLiVsP2L0sh0Of28XTH1jH8hSxSr99Qbrj5pCOzYco9I2qq9qEr8eUEfe2iVFHx/PQMRKedpKZ9CHyGS29QyhYWapUA3DRWuR0v9mE0aCwY6Gw4divy8zAPkV09ikxQTML0BtEi7In+g9pHqkFm+X3XgCRDSLJjDlwtQWLVWVRVgJz02Kd72i12L+IZVfY66zSJojOu5oggtkVd29LaWFSdLui/RfsKS3+ZGzqmxUDx6wlfNf8NsyqgYWGBvKU1lm56ueMw9fbWKGUAxBZsDHA1fifzMRoijPiUFU4McmHSDOqrnpjVqazzJTSy+fYbryIFYXojR8IdDkhh9GgsLEolZtzRPPjaGUHqitZte5DVHM05OXXYz2IpqTkHlj1nsn3h5AJ+nItpON6i4dG1foEUZOcIPI2mQnRFCrNJCiDDKqRXFfznO4329il+xBdGePtVXIPRCVB141Zs3h7TTOonpRgJuPtgwLZIJLMGLfSywBar8LogND6p833Q2Wzi2XaSVJd5yDtfcPOd9QPulWzwIdIS2lxLiAL/pQWf+JoNa+HeE6rCwHYYTjndL/ZyP6yFlZrDSLmrAtsMQHCqQ9R9kqISYWRXqg7FYDKQhvTOWFYWp6wAVIKAlxN6FKUoBIdYaCtb9h2QeKQOetAMUJvg/DOClH6h80094jv/+J0FxH3YzFrx/PV94ekCXpWYjQZCcKo+kqjB0bVeoOo/ToM9/qmuFnKhqJUtseJv6NStQAL46fQnBrGzwJuWpCByaBQ0dpPTbto5hIRAyv+SWzPErNqe8T9mOOUxQw1h8V24U0BqEqiIxtEkhkxbLZw8KqbDSLdfyhnZdCPLIciidERFGumlBdcjVrPWQ8RsSJWs+WKn6oLEGGQ0uJPnK3mHbSsAmCX4azL/WYTqqryWlkTKzUTztnaINJlZpN8iAwGKN4htqXMzDPMw5Q0PQdA2yIH0x0StzEZYF1BCiAm/pwSGWdvGISwD5E+PZQWF0lSrJvyjDpNZr3s7SFrgm4zqnYlsZ9IXLrwWAJouuSDqmYvRoPC+4q6gMnyMn3Bbrb6PybFRLC+UDTGHMrMyl6AgfBO1lNVlXJbg2jMBFHTeREWEJVkPx5LAoJsEElmxNGKdvpHLGQmRNm+oJ0i/Yd8zkpdZlbb5XwnU6TdbyHcR1nDJKXFXziLid5vXQXAFsNlChOVWbnqN5HLDT0k9leToAyiRsRBxpJAlxQQ9AmiK409dPaPjH9Ql5lJo2qPGLn0d5LUHhrVVHI33BfockKezcXieOWyQQRh4UPkkbwMoK9FyFpQQlr6v8yWZObBBBF47ENksaocrWjn7+fqOVrRLv34XFA0ItI9JxpUZydFOzeMnyXocffjGkQ5K8V/lhEMrz9OXsdRlJpDYWmB0No3TOfAKAYF5meOmSDS08sKt4ZckzrckA0iyYx4VdPQ3lqShWGqlQBbgplsEPmKFXPcMKoGu8ysMswbRAVbaDOkT4pstxMaKS3+wllM9FU1nwY1lWhllG+t75mVq34T2V/WwmqDkJcpeWtEEtIsJCMhigXaCd7xid5UxVqDqP40DHb5t7AQZvjYzwF4znALhRmJAa4m9NmiNTGPV3VgtrgwgNV9iMJggsjtBpEu/8xYBNFTLPIFMdM2qvbAh2jPpUa2Pb6fd/3sGJ985hzv+tkxtj2+nz2XGj0tN/yxWlG1ptsFazGP3VPC99+5it8/tIlDn9s1q5tDALsWiwbRscp2+obH+GblrALAePqXrKt5AtPT98H3lkHpc/4v0odcaxJy34K0OKIjxjSCpP9Q0CAbRJJpo6qq3X9oqvQy84h9hDeEV6mCnZX5yYCIundpyFmsGVXXHBaa3zClb1TlS8PvBRzltIROSos/cRwTrfAG4u923Yj0kwFhzr9KESuks1VepqNPER2bKDNLzoe0BSK+t/qNAFQWgrReI6HpOBZVoSL/bSiKbMbOlCU5CSTFRNA3bOa8q8UTvUHUUgqDnf4pzsvYGkQZbjaI6rXjeV5oH8P0BlF5Sy+DIx5MXOS4N0G051IjDz99xhbioNPUPcTDT5+RTaKJdFSgjPQyqEbSGVPI/ZsKuXdVHpvnpckFJqA4I56i9DhGLSqHylvFnaXPOfYf6mmEPz4QVk2ia478h8wj9ua89B8KOLJBJJk2l+p7aOoZIjbSyOZ5aa53bikFy7BYoUpxHHcpmTklOYmYDAptfSM0dLswEs5eId6L4R5oPOe3+vzNiap2XrSs51uRH5lsVB1CKS3+ZmxM9K5cseJ+OkK7gCh/OfzNzaegvW+Yc7VdrDZoDaIQv7iaKfrxf5JRNUiZmaec/jUA+61rmD9/YWBrCROMBsXmlXXElcwsPhNS54nt2hN+qMz7VLaKlfliTyeI5qz1UUX+ISsxivR4YVRd6olRtS4xay2DUcfnTBarymPPlzpYZLIvPD32fKmUm42l4RwgDKpvXpIjm0IO0KeI9l1pGZO46+JTFkaJu9cc+Q81nIHRfohNg8ySAFUm0ZENIsm02VvaBMD2BRnjRwQdMVZeJldEfUZ0hJFF2eKAe8GVD5HBaO/Qh7EP0aFyccE6J0cbZ06dF5IpLYFAj4m+Y46VSJOB5/sWYjVECr+KtmuBLi+gvF7eSqw6yCJDnbhDThABIpVkUoLivF3itkIaVU/J6CDqud8C8FvLLtYWSK8vb7F1vviMHq4IXx8iVVWptEnM3Egws1qgXvOGnLPeh5X5HkVRWJ4n5JgeycyS5oi0RatZLGQ64ERVx6TJobGoQGP3ECcmSmxnMap2zn/RWsStUykMZim3aA2iA1dbsFYfhp4GF3uHV+KuwwaRzX9omwi5kAQU+Q5Ips3eK26ml8EYg2opL/M1KzSj6nN1Xa53LNJkZmHsQ3REuxjYFKklTc3bFbIpLYEi0gibilIYJJr6ZO3v99rLgS0qwOwva2WFoRIDKiTNhYTsQJcUUFLjIlmsNaYn+RAVbgODCTqroKMqANWFEKV/Rxnqok5N55hhNcvypP+Qt9g6Px2AMzVdriVIIexD1N4/Qu+QGUWBgrTYqZ/Qdg1GekWqaRiY7NuSzDxpECmKXWbmxIeopdfFNPY09psNDN04DcAVpZibFqQHuJrgZF1hKglRJtr6Rrhxw83vxjBI3BUJZmLScXyDSLsWkf5DQYFsEEmmRW3HAFcaezAosFPrgrtEGlT7jZW6UXXtFCdJug9R7XGno9WhTEvvEGVNYpUif0BbGQzxVdJAsV07wdtn1v5+y18JYDWBxWyx8trVFlbb/IdCW5rhLfQpokkys6gE+9+djLt3zalfAvB78y6Wz0khyiSb2N6iKD2OnKRoRixWTla7mPTQJ4jqT4fc96LuP5SbFDP1VDfY5WW54WGyv2ymRtWNjhtE8VHu/b/JTIieeqfZgNWKsfkiAKY5a4hz8//fbCPSZGD7wgwATrRGuPekMEjcbeweonfYjMmg2M30R4fsst5C2SAKBmSDSDIt9mnpZesKU0mNi3S98+ggtFwR29Kg2ufoRtWX6ruxutLEpy+E+GwwD0FdaPotuEK/UF2RHUNEs3biN8ulQNNlh3YS85t2zRPlxlEY8vAkPEw4W9tFz5CZ9REV4g7ZdATGNIgmGlWDlJm5Q3Mp1B7HgpE/Wm5mTUFKoCsKKxRFYcs80eh2KTNLmwex6WAZCTl/vqpW0SAqdteguu6kuA2TJvfyObpRdR9Dox54tTiJuh+1WHnqaDX//ifXBtYKkJMUzYYiKQkFoKOCSEs/g2okJcvlOZcr9AX23zTkCl/MyW6ZGuGTuHtVk5cVpccRadLaEHUnhE9tfDakLwhgdRId2SCSTIu9WoPoTe7Iy5ouCX13XIY4wEl8yoLMeKIjDPQOm21+BA5RFPsopx4tGUYcKhcXAW/NbRdfPLFpkFoc4KpCk4K0WArTYqmwZNEfXyj+nisPBrqsgLC/rAVQWWeUDaKxbCpORVHgekvfZKmFHndf9VrYmGx6ndO/AuCwaSOtpLBO+g95Hd2H6Mh1B01MHUUZIzMLLR8i/fvebYPqeiEDCheT/ezEaNLjI7FYVc+MqrVocZovg8WMqqrsL2tm9/de50t/v0znwChZiVGA88v3R+8ukUbMGr1VovFYqhawa2lugKsJbnYsykBR4GJjP53bv6LdO/FzFF6Ju9e0yf6F2Q78h4pukj61QYJsEEk8pntwlOOVYkTbLfM5m7xsjfzD9wMmo4FluZrMbCofouLw9CFSVZXDWlrNtuhqceec9fLzNwN2LBIrXWejN4g7rs1OmdmBshbmKK0kWDrBEGFffZ7lJMdGsjhbeObo3w82cldDVJKYOtO/DyR2Rvrh/DMA/HRAHJPXygkir6P7EF1q6KZrYMT5jjaj6tDyIapqE74eRe40iIb77KbMYdLkVhRlejKz1GKIjAfzIBVXznL/L47zgV+foqK1n9S4SP7fvUs59Lld/Pj+NWQnjZeRJUabeOL+NexeluPNXyWkabgi/m4aYheRkxQT4GqCm/T4KFZpU/8vWdaLZN3ECZ+l6KSwSty9pvkPLRrnP6QtUkv/oaAhoA2i119/nbvvvpvc3FwUReHZZ58d97iqqnzpS18iJyeHmJgYbr31VsrLy8ft09HRwXve8x4SExNJTk7mgx/8IH19fX78LWYfB6+2YLaqLMiMp9CdExGbQbX0H/IXulH1hbopTpL0g3H9aRjyYMUtyKluH6Che4hIo4HCIf0kODxWSQPFjkVCZvbHbs3M9PpesFoDWJH/aegapKyplzV6vH32coiQvhM6m53JzIwmKNaONVJmNplLf4XhHgbi8jlsXUpxRtzU0m2Jx2QlRjM/Mx5VdeCVNZaxDaIQOsbpHkRFGW4kmDWcBdUqpronXpCGMDaj6qnOfcZiMDCSsRSAH/7+rxy+3k6k0cC/3FzMwc/s4L2bC4kwGti9LIdDn9vF7x/axN0rxGTMpuJU2RyagKItAkTmh4d00dfoaWb7y1pEE+iRS5jvf5b6ZG0xrmBr2DSHYGyCmXacGumHes0PTU9XlgScgDaI+vv7WblyJT/60Y8cPv6Nb3yD//3f/+XHP/4xx48fJy4ujttvv52hIfv4+nve8x4uX77M3r17eeGFF3j99df58Ic/7K9fYVayt1TIy251R14G0qA6AKzMFydJ51xF3QMkz4WUIlAtITdO74pD2vTQmoJkTA3aF0+YrJIGik3FaUSZDOzpLcYSESfSNJpcezOEGwevtgLwpqRacYf8TI1j8zzRIDrm6OJbl5lV7PdjRSGCZk59PO1eVAysk9NDPmPrPDfi7nNWgCkGhrqg7ap/CpshFqtKdfsA4KbETL8gywuvi/hlHiaZDYyY+f6r5fyxTvzNLVWquGtFDvv+7Wa+cMcSEqPHmwcbDQqb56Xxvq2FAJyq6UJVXXg9zjKGRkbJHRQL+cUrtgW4mtBg12JxLXX4epvwzjIYUQu2cT3zDrFD9SGwmANYofewWlXKWyZE3N84KmwLkuZCSmHgipOMI6ANojvuuIOvfOUrvOUtb5n0mKqqfO973+OLX/wi9957LytWrOCpp56ioaHBNml05coV9uzZw89//nM2btzItm3b+MEPfsAzzzxDQ0ODn3+b8MdiVXnjWiuvag2iXe6klw33Qqt2giUbRH5DnyAqbexhxDzFCqg+RRRGMrPDmv/QbflA1w1ACbsTYX8THWFky7w0RojgRpK2slW+N7BF+RnhP4T0H3LChiLhQ1TZ1k9zzwQfIt2ouu6E+F6QCBrOiSlbQwS/HRQXVNJ/yHds0WRmLn2IjBH2idMQWThp6BpkxGwl0mggN9kNWU9deC6c6BNEUxlVW60qfzldx65vvcZ3X73GOXMBAP88p5MfvnsN+amxU/6c6AgDHf0jVLRK1YLOufOniVcGGSKS+SXynN8dluQkkJMUzeCoZdz0bVdsEWp0Mgx325UYIU5t5wBDo1YiTQYK0rRGtvQfCkqCNnuwqqqKpqYmbr31Vtt9SUlJbNy4kaNHj/LOd76To0ePkpyczLp1dunIrbfeisFg4Pjx4w4bTwDDw8MMDw/b/t3TI6Q1o6OjjI6Oev130V/TF6/tL16+3MxXXiyjqcf+/+3jvzvDF+9czO1LnU8SKXVnMKGiJuRijk6FIPl/EA7viSvyEiNIjDbRM2SmtL6TpbmJTvdVCrZhOvMkauVrmAP0/8Ob74fFqnK0UjSIboqpAkDNXILZEB00n79QYeL7ctP8NA5cbeWV0RX8CwewXt2DZcunAlmi3xgetXD4eiuRjJLZfw2A0exVAflMBevxK9YEJTkJXG7o5dC1Fu5ZOUZ6kTAHU3IhSlc15orXUBfcHrhCvcxM3g/DyV9gBMyL7uINkQzNyryEoHtvQxFH78u6/EQMWhPzRlsvOUmOJaKGORswVr+BtfoIlpXv9Uu9M6G8SUzMzE2NwWoxu/aCV1VMdSdRAHP2KlQ/ftZ8fexKjzWSGhdBR/8oF2s7bP4uYzle1cHX91zlcoNoVM9JjubOjbfBaz8hvrOU0ZGRKS9UFWDVnCSOVXVy5HorBSmhKTX29vtRfeEQm4Dm2AXkWgGrPI65w46F6fz+ZB2vXm5iW3GKeD8UA5aCbZiuvoDl2l6s2aHfcCut7wJgXnqc7ThlrHwNA2Ceu9WvxyJPCdbzLk9xt/6gbRA1NTUBkJU1vvmQlZVle6ypqYnMzPFTLCaTidTUVNs+jvj617/OY489Nun+V155hdhY16sGM2Hv3tBcbT/frvDLa/qwmf1Ls6lniI89c44PLLSyMs3xiO28lpdYBjQacjj54ou+L9ZDQvU9cYecKAM9QwZ+9/JhtmY5H4GOHB3mDkBpucSrf3+GkQjnzSRf443340YfdA+aiDaqqKUvAFBjyeR8EH7+QgX9fVGHAEz8umU+/xIFSsOZgH9m/EVZl8LgqJFtEdUYrCMMmxLYc+QyKKUBqykYj1+ZqoHLGPjz6+cx1Y83pF5hLKKIam4c+DUXy8MvzczT98NkGeT2S38E4NnuxQybrcSZVK6ceI0yuZDqNSa+L3NijdzoV/jJ3w6wIdPxd2NGj4EtwOC1A7waAt8drzcqgJEYcy8vTlFv9Eg7t/c1Y8XAngtNWC75//fz5bEr02SgAwPf/NsxNmSozEtUMSjQMgjP1Ri42CnOZ6ONKm/Ks7I9p4++bjMWxYRxuIeDzz7JQNTUE/LJowbAwN+PXCap9aJ3fwnVSlrfVaJHuxiKSKY9fhEovhN9eOP9UFUYqTkFBmg15nIuBP5ugoWEXvH3++K5G6w3VNn6k5cGMlgFdJ35G4f6lgWwQu/wSp34PePM3bz44ouYLAPc2XgOgH0VIwzVBv9nJhjPuzxhYGDArf2CtkHkS77whS/w6U9/2vbvnp4e8vPzedOb3kRiovcvdEZHR9m7dy+33XYbERERUz8hiLBYVb7+7deBYQePKijAS82xfPY92x1GfBr/9jeoh6zVu7lz652+LtdtQvk9cZeyiHKuvl6FmjKXO+9c6nJfteX/UFpKuW1hDOoS/79P3nw/fvJ6FVwsZ9uCTBZYOwGYs+kt5K0Kns9fqODofXn6xiGq2qE7aTFJ3WXcVmxAXR7+/29P/6MMuME/5bVBE0QUbubON785ILUE8/Er5morB54+S705jjvvHG84qZRZ4S8HKLJWk39n+Hxmpvt+GM78GuOFIdS0+bQteCtUlrNxXiZvfnPorxQHA87elysR5fz49Sr64udw553LHT95+CbUb3+buJE27ty2ChKDO6771AtXoLqWTUuLufP2hS73Va78HS6DkrWU2+9yPGnvK3x97Hr5cjO1py8BFk60GjjRCpkJUSzJSeDw9XbMVhWjQeGd6+bw8V3zSBtjBq80fR+azrNzcapb50Eple3s+dVp6kdiuOOO7ShekscoZS9gfOU/UHrtVhlqQi6WN30NdfFdXvkZOt58Py7V9zBy5msALLvpblauDp9jvK/ZNWrhqa8foHPEyvy1N1GcFs3evXtZdOfD8JNfkTpQyZ27tkF0aC/G7f3jBahtYsfqRdy5vQjl2h6UCypqajG77rs/0OW5JJjPuzxBV01NRdA2iLKzswFobm4mJ8c+pt7c3MyqVats+7S0tIx7ntlspqOjw/Z8R0RFRREVFTXp/oiICJ++6b5+fV9wqqJ9nKxsIirQ2D3M2bpem0HpODQTW+OctRiD8HcPxffEXVYVpAJVXKzvmfp3LN4BLaWYag7Birf7ozyHeOP9OFolIrZvmpeC4fVzAJgKN0OYvs/+YOz7smNxJlWHqzkduZ5dlGGq3Adr3hPgCn3P65qv1YaISgAMczdgCPBnKhiPX5vnZ2BQ4EbHIK395vF+KPN3gmJAaS8nYqAZkuYErlAf4NH7oapw9kkAlHUf4Nx1IXdZX5QWdO9pqDPxfblpYSY/fr2Ko5UdmEwmxxf2EakipbDxPBGNpyDtbX6s2HOqOwYBWJCVOPXnp1FM9in56wP2WfPFsWvPpUY+/sx5Js6EtfQO09IrzmN3Lc7kP+5czPzMhMkvkLsKms5jar0MK6Z+v9cXZWAyKDT1DNPcZ57St8gtSp+Dv7wfJvwWSm8jpr+832dx5954Pw5ea+VDSg0A0QXr5TmXB0RERLB1Xjr7ylp47XoHi7KFJ5YpvRhS56F0VBBRdxSWeLdB6G+ut4qkxSW5SeLzVis83pSim0Pmey8Yz7s8wd3aA2pS7YqioiKys7PZt2+f7b6enh6OHz/O5s0ignTz5s10dXVx+vRp2z779+/HarWyceNGv9ccjrT0Dk29k7P9BjuhQ1xMSYNq/7NSM6q+1tzLwMgUCQi6UXXV674tyscMjVo4WS2mhnamtsHoAEQlQdqCAFcWPuxYJEbvf9+px92/GjYJG86obO2jun2ACKNCVu8lcWfeOtdPmqUkREfYjGKPTYy7j0mG3DVie7bH3defgaaLYIxCXfkuTtWI49a6Qplg5mvWFqQQaTLQ0jvs2mB4bNx9kGOPuHcnwUw7Zw4jg2qLVeWx50snNYfGkhoXyc8eWOe4OQQivQ6g0b10zphII8vniGPdCW1hakZYLbDnc0xsDgm0+/Z8HtcGU4HjyuWzJCiDwu8x3fUUm2QyO8fG3Y9FD3ioDO3vzFGLlUqtQWRLMNOvOYpkvH2wEdAGUV9fH+fOnePcuXOAMKY+d+4cN27cQFEUHnnkEb7yla/w3HPPcfHiRR544AFyc3O57777AFiyZAm7d+/moYce4sSJExw+fJiPfexjvPOd7yQ3N7jHgUOFzAT3jPcc7tdwTtymFEKsTGXxN9lJ0WQlRmFV4XLDFCOFBVtBMUJHBXTX+adAH3C6ppMRs5WsxCjy+y+LO+esBUPQ9sJDjo1FqURHGNjXNxdzVDIMdUPdyUCX5VP0E7bb5howdNUgUvHWBLaoIGaTNk161FHcfZic7M4YLdqepW/hxmAUbX3DRBoNtuaaxHdERxhZVyAacYddpZnN3SRugzzJbGjUQn2XmCAqmiri3jIKDZo3WBg1uU9UddDY7XpBs6N/xHUjJ2eVuG08Lyb83GBDUart58+YmiPQ4yqBWYWeerFfkNHQNUi05sOkZi8HY9AKVIIWPRn6zI1OOvpH7A/o35kV+wNQlfeoae9nxGIlNtJIXnIMDHRAs+bdVSgbRMFGQK+aTp06xerVq1m9WkyXfPrTn2b16tV86UtfAuCzn/0sH//4x/nwhz/M+vXr6evrY8+ePURH25sRv/3tb1m8eDG33HILd955J9u2beOnP/1pQH6fcGRDUarTlA8QltU5SdG2L8lx6LGMufJCKlDocffna7tc7xidaJ/yCuG4+0PXhQxo6/x0lPrwjPENNCLuPh0rBiqTtAuo8pcDW5SPOXi1FYD7MrST94zFEC0v5J2xuVhrEE2cIAKYt1PcVh4Eq9V/RQUTg11w6S9ie937OaVNPS7LSyQ6whi4umYRW7W4+8Pad4ZD8rXjW/Nl0QgPUm50DKCqkBBtGuep45Dmy2Ae0iZr5/unQD8wo2l3ncwSYQTd3wq9zoNuxrKhUJz7nqz2QoOor9m7+/mRfVeaWWEQioGIOfKcfzrkJsewJCcRVbVL2gEo3KYt4FZCZ3XA6psp15rFtOaCrAQMBgWqtXj7jCUQP7UpvMS/BLRBtGPHDlRVnfTfr3/9awAUReHLX/4yTU1NDA0N8eqrr7Jw4fixxdTUVH73u9/R29tLd3c3v/zlL4mPjw/AbxOeGA0K//XmEoeP6ar9R+8ucWhQbVulkvKygLFSG3++UOfGyW3xzeI2hGVm+sn+1nnp9qkW2SDyOjsXZQCwZ0QbyS8P7VQHV/QNmzleJRod600V4s454bPy7gvWF6ZiNCjUdQ5S2zEhMWPOeoiIg4F2OPQdqHojaCUTPuPCH8A8KC5I8zeOkZfJSVt/oTeIjla2Y7E6mRZJzBET0Ko1qKckddlGcXrc1EbJtoWT8JqsndG0u05kLKQvEttNF9x6vXUFqSgKVLb1u92kckp81tT7eLKfH3n1SgvLDVXiH7mrAlpLKHOLNkV0QFuUAsQCbv4GsR3C0uyrTcJnb2Gmdo1uk5dtD1BFEleEz7eDxGdERYiPycTTjuykaJ64fw27l+VMfhLYJWZSihEw9AmiC3VdU+9s8yF6ze3x6mCie2CUi/WiEbYtzwDt18UDeWsDWFV4ovsQPdkyHxUFmi+FtDTRFYevtzFqUSlMiyWlU7tokA0il8RFmVgxx4kP0dWXQNUaQvv/Hzx5F3xvmTBnnQ2oKpz6ldhe+35QFE7XiOmDtQXSf8hfLM9LIiHaRO+Q2fa94ZAQ8CGqbBMr81PKywDqtAZRGMnLwD7t7qw95nLafSwe+hAlxUawOFskS52s6nSvWGcUbJkiLU+BxDyxXxDRN2zmWEUrJZpBtU2qJ/GYXUvsDaKTLQrHqzpEAzsMZGblLaJBtChb9x/SJoik/1BQIhtEkin5zTFx0H//1kJ+/9Amvv/OVfz+oU0c+twu582hvlborgUUyF7hv2Il49Av0qrbB+gaGHG9c/5GMEZBb6O9uRJCHK1sQ1VhfmY8WT2akXDaAul/5QPyU2Mpzoij3RpPV+pKcWeYThEd0PyHdi5ME8bCIKfS3MChzKz0OfjjA0LiMpaeRnH/bGgS1R6H1isQEQsr/5nugVHb6P2aubJB5C+MBoVN2mfUpczM5kMUvA2iKm2CqCjdjel5vUEUZk1uo0Hh0bvFtPvEJtGU0+5jydG+z9xsEIHw5QM4UeXCz8odDEa45VHX++z+H7FfEPHGtVbyrA0kKIOophhpUD0DmrqGMCgwOGrl6Qoj9//yFNse389Rlosdql4L2YlbfYJoQVaCkHC2XQUU4YEqCTpkg0jikpr2fl67JkYdH9hcyOZ5ady7Ko/N89Jcf9Hq8rL0BWI8UhIQkmMjKUgT0atTyswiYuxjrJUHfVuYD9D9h7bNl/Iyf7BTmyI6btIuNMpfCWA1vkFVVQ5cFQ2iN+f0wEgfRMYLDyKJSzZrRtXHKtpRVTXkE3q8hm5OveytEJ3EmRti6qAwLZaMhKgAFjb72Kp9Ro9UuGoQaRNEdafAPMUiS4BwO8FssBPay8V2mE0QAexelsMT968he4Jv5pTT7uN21ieI3JOYgZDUApyonuEEEUC7JmM2TDB5jk72WcT9TNl7pZnlivAfUnJWSIPqabLnUiMf/d0ZJipem7qHeO9Lo4xGJAovNP36KoQYNluobhdy80VZCVB9SDyQvVwu4gYpskEkccnTx2pQVbh5YQaF7owv60iD6qBhpScysxD2IdLTaLbMSxvTIAq/k+BgYYfmQ/R0h+bZUHkQzMOBK8gHlDb20NwzTEyEkZVcE3fmrQm6FdxgZG1BChFGhYbuIWo7BkM6ocdrDHTA5WfF9toPACJ5EWBtgTxJ9je6D9Gp6k6GRp00JtMXQkyq8Ixy05fG3+gNouKpztH0ePuUIohL83FVgWH3shwOfW6X+9PuE8nWJjW6b4i/VzdYXyQm/8qaeugeGJ1O2YK+Vjj6I7H91p/Dgy/AknvFv4t3BmVzyGJVOVA2xn9IysumhcWq8tjzpU6XTywYecOi+cGGoMyssrUfi1UlIdpEVmKUmIQC6T8UxMgGkcQpgyMW/nhK+Io8sLnAsydLg+qgQZeZnXfHqLpoh7itfiOkEobquwapauvHoMCm4hT7ibA+ESXxOhuKUomJMHKoL5fR2EwYHbCvCoUJurxs6/x0Ihq1pncYrrz7gthIk605fbSyLaQTerzGud+BZVhMKWjefKc0/6F1hVJe5m/mZ8aTmRDFsNnKmRon0x+KEtRx990Do7RrkdhTehDVad+LYT5ZazQo7k+7TyQmWRiTg9sNwcyEaIrT44S9WM0M0swOfRdG+0WTZel9wptl00fEYzWHgtIb8syNTjoHRlllqhZ3SIPqaXGiqoPGbucm5yqwd3ip+EcINoiuNWv+Q1kJwkjf5j8kG0TBimwQSZzy/PkGugdHmZMSYzOldQtVtXt1SIPqgLMyPxlwc4IodzVEJohR9OaLPq3Lm+geEivzk0nsrYThHpGUlLEkwJWFL1EmI1vnpwEK5YmaDCPMfIj0JJFdizPHeHeE98WVN9FlZkcr2kM6occrqCqc/rXYXifMqUctVs7Vdom7pEG131EUxTZFdChEfYiq2sX0UFZiFHFRU0h75GSte8xIZjbNBlF3HZz8udi+5b9EYxJEyIYpBvpbobVseq/tQ14tbUbByjJDtbhDThBNC3cS8N6watNtdSdhqMfHFXkXvUG0MDsBumqhswoUo13CKwk6ZINI4hBVVXnqWDUA928q8GwFpqcB+lvEH3/WMt8UKHGbpbmJGBRo7hmmycUKBSC044WaYVzla74vzkscduQ/lLdGauF9zM1a4/gfQ9qJS/nLAazGu3T2j3BW84fZWRhlPzmXF1duM9aoWp27WUvocZEzFIQJPV6j+pDwf4mMh+X/BEBpQw9Do1aSYiKYl+GGwbDE62zRmpiHK1wYDNuSzI4G3RRHlbsJZqpqn6yVU5CumYZR9QabUfU0G0SvfUNMFxZshXm32O83RcHcjWJbn7oIIvZeaaZYaSTaOiiM96VB9bTITIiecp86NZPBhAKwmkNuWvtqkzhOLcyMFwoFEAvS0qM2aJENIolDztZ2cam+h0iTgXesy/fsybq8LHMJRMZ6vziJR8RGmliYJWIlz3sUdx8aPkSqqo7xH0qXq6R+ZMdC4UP0m5ZiVEMEdFTaTTZDnNeutWJVYXF2Ajl9pYAKyXMh3oNpylnOmoIUIo0GmnuGqe4cht2Pa484aRIFYUKP19DNqZf/E0SJ4/Epm/9QCgZPFmEkXkOfILpY10X3oBP/mJyVYIqGgfagS/h0O8GsoxIGO0RSqe6zI3GM3iDywHNKbxBdrOtmYMTs2c9rr4CzT4vtW75knx7SKdRiwKuD65yssrWPytZ+VhmrxR3Zy+Wi3DTZUJRKTlK0q+UTcpKiiVp0m7gjxGRmesT9wuwE+7WFlJcFNbJBJHHIb46KaPu7V+SSGhfp2ZNtBtXSfyhY8Mioukgzqq45ErSpLWO51txHW98w0REG1hQkQ61MMPMX+amxzM+Mp8caTVvaWnHntfCYItLTy3YuzoR6KS+bDtERRlbNTQY0mVnJPSKJJ3GCYaxihHc8GZQmrF6hrxWuPC+2173fdvdpza9krZSXBYzc5BiK0uOwqnC80skUkSlKSH0g6HyIKtw1qNYlsjkrwOThOd1sQ28QtZXDcJ9bT5mTEkNuUjRmq8rZG12e/bwDXwXVAgtut8sZx6JfSFcfCipvyH1XxHfkrcmN4g4pL5s2RoPCo3cLE2pHTSIVePTuEgzzdoo7Kg/4rbaZMjBi5kaHSDBbmBk/xn/opgBWJZkK2SCSTKKtb5h/XBAHfI/NqUEaVAchK/KFUfWUUfcAmSUQmybMEvWR9CBG947YUJRGlLlvjBRIXsz7g51amtlRg3YBFQZx9xarymvXpP+QNxgrMwNEE+iRSyKh554fgiFCXByFszTh3NNgHRVNBu3iU1VVTlXbJ4gkgUN4qcERlzKz4PQhsk8QTZVgph3DpLxsauIzIT4bUKH5sltPURSF9dORmTVdhEt/Edu7vuh4n9zVwlNxsBNa3KvHH+y9IgIFVkeIBWVpUD0zdi/L4Yn715CdNFluZjIoLMpOFE0VxSgmGTtrAlCl51xv6UNVIS0ukvSReuipE9/7+Q6aoZKgQTaIJJP4w8laRixWVs5Jshkcu42q2htE0qA6aLBPEHWjTuWhYDCElMzM7j+Uppmjq5BcIKVAfkI3sH+qXYu7rzns9qprsHKutpOugVGSYiJYPSdpjGxRNog8ZaxRte3YYzCKE90174V5u8R9+oRNuGG12s2p19qnh+o6B2npHcZkUGzHZ0lg2DrPHaPqMT5EQYKqqraI+6IMNyeIpPTaPWYgM/OoQbT/K+J26VvFdJcjjBFQoH3+gsSHqLN/hFPVHShYyezTFuXkovCM2b0sh0Of28XTH1jHAwss/Ob9a9m+IB2zVeXLz1+G6CT7eUiITBFda9b8h7IS7P5D+RukBUmQIxtEknFYrCq/O34DgPduLvT8BTqrxSqHMVJMokiCgkXZCUSaDHQPjlLTPjD1E3SZWVVwG1WPWqw2WYDwH5KTHv5mXWEKsZFGTvWlMZIwFywjQf+5mYr9Wrz99oUZmHpuCO8RY6T07pgGq/KTiTQZaOsbpkKbdhjHkrvEbbg2iCoPiO/FqCRY9lbb3ac1/6GleUnERIap71KIsHleGooiVrqbe5wEOcxZDyjCy6e32a/1OaO5Z5jBUQtGg8LcVBcXW6NDYlIFZIPIXfRmTeM5t5+yUWsQnbnRyYjZDSnYjeNwbY+YCNn5n673tfkQBUeD6OC1Fqwq3JLRg2G0XxpUexGjQWFjUSpr01U2Fafx3/csJcKocOBqK/uuNIMuM6sI/gaRxapyUJPrJ0absFZqi86FUl4W7MgGkWQc+640U981SEpsBHetyJn6CRPRp4eylgrdviQoiDAaKMkRaQEeGVXXnoARNxpKAeJ8bRf9IxZSYiPE7ycnPfyOiLtPBxSuJGirnCHuQ7S/TJeXZdibjtkr5DFtGkRHGFk7V0iojjryeFl0JygGsVIfIiPzHnH6V+J25T9DpH3K45TmPyTj7QNPcmwkS3PF9+ORCidTRDHJ4rwGoDY4ZGaVWoLZ3NRYIowuTuebLgiJY1yGmK6VTI0tycz9CaJ5GfGkxkUybLZysX4KOb+qwr4vi+1V74b0+a731/1aqg+D1eJ2Tb7i1VJx0f+WbO3vJXt5+AYMBJjijHg+uK0YgMeeL2W4QFvArTwYFJ8FZ+y51Mi2x/fzgmZZ8nJpE52X94kHpUF10CMbRJJx/OaYOEF/x/p8oiOmcbC3GVRLeVmwsXKO8CE6X+uGD1FqMSTli5PKIBqpn4guCdgyPx2Dgr1BlC8bRP5kh+ZD9NzAMnFH+d6gi4N2l6buIa409qAosH1Bhmw6egFdZnbMkcdLXDrM1aLty/7hx6r8QE8jlL0otsfIywCb/5BsEAUHusxMT8R0SJD5ENnkZe4aVOetm5yQJXFMtjZB1HLF7bAORVFYXyj+nqeUmVUegJpDYjJ1x+fdqGclRCXCcLdHsjdfMGK22jz6NkRqTX1pUO1TPr5rPlmJUdzoGODnlcliInWoCxrOBbgyx+y51MjDT5+hsds+kTlfqSeNLobUCF7unhPA6iTuIBtEEhuVrX28Ud6GosD9G6e5yqQfrKQWOejQ/aTcSjJTlJDwIbL7D6WPj/HNklIgf6L7EP22OR/VFAO9DdB8KcBVTQ89vWxVfjJp8VFjGkRSmjFdbA2iynbHHmi6zKzsBT9W5QfOPi0MuPM3QZZdct0zNMrVZhH7u7ZQNoiCgS1a3P2R623OffqCzIfIbYNq2zFsrY8rCiOS50J0slgka73i9tM2FIlj3YkqF43GsdND6z8ESW5cLBtNUKA10gPsQ3S8qp2+YTOZCVGk92r/b+Q5v0+JizLxH3cuAeAHB6sZnLNVPBCEcfcWq8pjz5cy8Si62VAKwCnrIv77xetYrKG5iDhbkA0iiQ19emjXokzyXenZnWG12htE0qA66FihGaFeaujGbHFDHx/kPkT9w2ZbnOzWeen2k+DcVTLG18/kJcewMCueITWS5vSN4s4QTTM7oPkP7VqUKb07vMSKOUlERxho7x+hvMWBgfniN4vbG0dFJHw4YLXAmSfF9rrx00Nnb3ShqkIalJkwObFG4n/WF6YQaTTQ0D1km8yZhD5B1HghKIz43Z4gqpfefB6jKGN8iM67/TTdh+hUdafzC+Arzws7hog42PZp92uyxd0HtkH0aqnw4Lp1cTqKPs0kE8x8zj0rc9lQlMrQqJVne7RQkCA0qj5R1TFuckhni0Ek8B2xltDYPeSZmbvE78gGkQSAgREzfz5dB8B7pxNtDyJ2caQXTDGQvsiL1Um8QXF6HAlRJoZGrY4v0iain4w0nBMpPFVvBJXe+URVB2arSn5qDHPTYqUUKMDoU0RvoDWHr4Veg2jYbLHJFncuzpTeHV4iymRkXYG4cDrqSGaWPFd4fqhWuPqin6vzEddfhe5aiEmBknvHPXS6WvoPBRuxkSZWz00G4LCzuPukOUJ6rVrsTZcAojeIil01iPpaoOsGoEjpv6dMw4doSU4i8VEmeofNoCMalwAAYh9JREFUlDX1TN7BarEnl23+V4jPcL8e3di35ghYRt1/nhdRVZVXr4hFlLvnDMBInzSo9hOKovDYPUsxKPB/dXPFnbXHYbg3sIVNoKV3cnNIwcomg5g2O2pd6nQ/SfAgG0QSAJ4920DvkJnCtFjhuzEddP+hnJViHFYSVBgMCsvyhA+RWzKzupNgMAEqPP9JePIu+N4yKH3Op3W6y6Gx8jIQhtogG0QBQvch+nXrAnFH3QkYCK0VopNVnQyMWMhMiBKmtWObjtK7Y0aMjbt3yOK7xW24yMxO6ebU74aImPEPaQlma2SDKKjYOkZm5pQg8SEatVi50SECJFxG3Ov+QxmLIDrRD5WFEdl6g8j9CSKjQWFtgQsfogt/hLarQr62+WOe1ZO1TDScR/oC5j1zpbGX+q5BoiMMrI0UicfSoNp/LMlJ5L2bCqhVs2hQssFqhupDgS5rHI6mYpcoN0hR+uhTo7moFjndTxI8yAaRBFVVeepoNQD3byrAYJjmhZCeYCa1yEHLinzRIDo3lVF16XPwxwfEl89YehrF/UHQJNL9h7bOT4eRfmgW46uyQRQY1hWkEhdp5HJ/EoMpi8Q0SBDq412hx9vvWJSBoij2BlGe9O6YKZuKNR+iqnasjqQXS7QGUeVBGHKw8h5KdNdBuZbkN0FeZrZYOVfbJR6S/kNBxdb5WhOz0slnFMY0iALrQ1TbMYDZqhITYSTL1YWWTV4mJbIeo08QNV/yaHp6gyYzm9QgMo/Awa+J7W2PiGQ8TzAYoEDznqkOjDfkvitCXrZtfgaRzdpklTSo9iufvm0RqXGR7B/VUhWDLO5+cU4CxgnXkbr/0AnrYiyYyEmKtv2dSIIT2SCScKqmk7KmXqIjDPzT2vzpv5BsEAU9qzQfIpcTRFYL7PkcTLKYw37fns8HVG7W2jtMWZMYq91cnCZW01QLJORCUl7A6prNRJoMthX4y3Gh6UOkG1TvWizkctSdFrey6ThjVsxJIjbSSNeA3aB5HBmLIG0+WEbg+l7/F+hNzjwlGqSFN0H6gnEPlTX1MjBiISHaxMLMhAAVKHHEijnJxGmf0dJGJ01K3ai69iRYzI738QO6vKwwPc71op6tyS0bRB6TNk/Ip0YHhIWCm4xtEI0zPD/zpJD7xWfBhn+ZXk228JDA+BC9qjWIbivJlOf8ASIpNoLP3r6IN6zCI8tcvi/AFdmxWFX+7Y/nJ/lvbdb8h45ZRVjDo3eXTGoiSYIL2SCS8NRRYU5978o8kmIjpvciFrNdpy0NqoOWFVqS2dWmXoZGnTR4ao5AT4OLV1Ghp17sFyCOVIjpoZKcRJk0FUTs1Borz/ZpK1vle4PKt8oVVW39VLX1E2FURKOrtwm6Ne8OeUybMRFGA+sKXfgQKQos1tLMroSwzMxiFg0imDQ9BHBK8x9aMzdl+tO6Ep8QYTTYJt0OO5OZZSwREdOj/dB80Y/VjcfmP+RKXma1QL12ES+/Gz3HYBTyKfDIh2jFnCQiTcKUv1I3PB8ZgNe/Kba3fwYipxEEA3YfotrjYiLJjzT3DHG+rhtFgV2LMoRHH0iD6gDwjnX59GZvwqIqmDqvQ1dtoEsC4JsvX2V/WQtRJgOfv2MxOUnRGLGw0VAGwNXY1Txx/xp2L8sJcKWSqZANollOS+8Qey41AjMwpwZoLQPzIEQlQuo8L1Un8Ta5SdGkx0ditqrOV0j7mt17MXf38wG2ePsFmv+QNKgOCnQfoj8052CNSoTBDqg/E+Cq3ENPL1tfmEpCdITduyOzBKLkpIc32Fxsl/A4RJeZlb8iEuRCkWt7oLcRYtPtvkpj0P2HpEF1cKLH3R9y1iAyGGCuNiEZQB+iSncMqtuuieCQiDjR2JJ4TraeZHbO7adEmYys1hbjbDKzEz8R50zJc2HNg9OvJ3OJOLaMDkD96em/zjTYp5lTr8pPJmO4VhpUBxCDQeEz923knDofgOqT/whwRfC3s3X8+LUKAL7x9hV85OZ5HPrMzezdXk2CMojFFMsvP/OgbA6FCLJBNMt55kQtoxaVNXOTbQbG02KsQbVBfqyCFUVRbHH3FzQfjEnEZ7n3Yu7u52VUVeVQ+Rj/IVW1N4jyNwSkJokgJymGxdkJjKomGtO3iDt1L5YgZ7K8TG86Sv8hb7GpWEwQHa9sdxwBnbsGEnLEhUfVa36uzkuc1sypV78HTJGTH9YaRGul/1BQovsQnazuYNjsZPoxCHyIqlrdiLjXj2G5q2VwyHTRfYia3J8gAnvc/YmqDhjsgkPfEw/s+A+HxwW3URQo3Ca2/Rx3r/sP3boky94wkwbVAWP13BQ6soQn1Y2TLzj+TvUT52q7+NxfxETlv+6Yx72r8qD0OYz/u5ziE/8FgNE8gPGHq4LCw1QyNfJKfhZjtlj53XGRQvDA5sKZvZjUIocMK+aIRuD5OidG1QVbIDEXcCZ/UCAxT+wXAKrbB2joHiLCqLC+MEVESfc1i8Q1/WROEjBu1qaIXrNqx4IQ8CHqHzZzvFKs9O5YpDWI6qX/kLdZnpdEfJSJniEzVxxNMBoMY2Rmz/u3OG/QVQPXNT+Ite+b9HB91yCN3UMYDQqrtAkDSXCxKCuB9PhIhkatnL3R5Xgn3YfoxjGxQBEAdImZ6waRNKieMTn6BNF5j97r9WMbREd/CENdkL4IVrxj5jUVaTKzKv8ZVQ+MmG1TdbcuyZLn/EHC+lvfBsDy4bP88UR1QGpo7hniw0+dYsRs5dYlmfz7mxbZg24m2lUEUdCNxDWyQTSL2VvaTFPPEGlxkdyxPHtmL6bLSOSXRdCzUpsgOu/MqNpghN2Pa/9w0iTa/T8BWzXS5WVr5qYQG2myr5JmL58UJy3xPzsWigbLL5vF6DON54WfTxBz+HobIxYrc1NjmZcRJ3xk9GOabBB5DZPRIJq6wDGnMjOtQXT1pZDxr9IxnP0NoELxTkgtnvS47j+0NDdRHLskQYeiKGyeN0Xcfe4aMEaKhYnOKj9WJ+gfNtPUIySYLhtEtia3bBBNm4wlYIiAoW5hMO0ma+amYDQoDHU1YT36I3Hnri9657ypUDOqrj3hNynuofI2hs1W8lNjWJgVL4JBQCaYBZjk+ZsZMcWTovTxwst76Brwry/V0KiFDz91ipbeYRZmxfPdf16FAWvQB91IpkY2iGYxujn1OzfkE2WawZeWedgeMS7NXIMefYKosrWfnqFRxzuV3APveAoSJ2iFjVHi/pJ7fFylc2z+Q/N1/yF9lVReyAcD6wpTiI8ycX0ghoF0baKrPLhTqQ5cbQWEvExRFGi9IkxooxLFqq/Ea2yep/kQOTKqBhHjHJ0MA20B9XjxFMVqxnD+d+If6z7gcB9dXrZmrpSXBTPbNJnZYWef0Yho+2JYAD6j+vRQalwkybFO5ErDfdAioqVlgtkMMEUK3x8Qix1uEhdlYlleEv9qeg7D6ID4vCyZ7Ek2LdIXCIm/Zdi+QOZjXh0jL1NUVRpUBwvGCEzzbgZg5cgZvv3KNb/9aFVV+fxfLnC+rpvk2Ah+/sB64d8YAkE3kqmRDaJZSnlzL0cr2zEo8O6NMzCnBtEcso5CTCokz/C1JD4nLT6KOSli0uaSM5kZiCbQI5fgwRfExBCICGp9vD4AWKwqR7ST9q26QXXtCXE7R/oPBQMRRoOteXc+Vo+7D14fIlVVOaj5D+km27amY+5q6anmZfSUqBNVHZgt1sk7GCNg0R1iuywE0sysFpSaQyytfwalvwXiMu31T+BUtWZQLf2Hgpot2gTRudouep0togTQh8gteVnDWVCtkDhn8kKPxDOm6UN0a84w9xu1xZFbviT8g7yBotjTzPzgQ2S1quzXQhxuXZIF7delQXUQYZi3E4Dtxov89ngNlxtcnNd7kZ+8Xsmz5xowGhT+7z1rmJumJfOFQNCNZGrkme8s5TfHxPTQrUuyyEueoSynYYy8zFtfgBKfYpeZTfFFYjAKvfumh8VYPSpcCZx2uLShh+7BURKiTKzISxLTa/pJmxyjDxp2LhaNlr/0lIg7Kg76PZLXXcqaemnsHiI6wh5xLafSfMfS3CTio4z0Dpt54rUKjlY4MKweG3cfII8Xtyh9Dr63DNPT9zGvTfPaGh0Q8rgJ9A2bKWsSvkvrClL9WaXEQ/JTY5mbGovFqtpTqCYy1ofIz7jnPyRN9r2G3iDyYIII4K29vyVKMXPOuEzITr1JkSYzq/J9g+hcXRdtfSMkRJvYUJRq9x/KXiENqoOBebsAWGcoJ1od4tG/X0b18ffmvivNPL5HRNf/990ltqY6EPRBNxL3kA2iWUjfsJm/nqkHvGBODdKsLgTRZWYXnPkQOWLpfeL28t+8Xo+76CaJG4vTMBkN0HhBTDXFpkNKYcDqkoznZs2H6K/N6Vhj00XUcgATf1yhr4xunZdOdIR2smu7uJINIm+zt7SJUYs4ef32K9d418+Ose3x/ey51Gjfad4uMMVA9w2PL8r8hjMTzpF+hyac5250YVUhLzmG7KRoPxYqmQ56mtnh605kZvnadGTbNTj1S3Gh7idPDb1BVJzhhv+QlJfNHFvUvQcTRG3l5FaLc6UvD7ydtn4vL5DoRtV1J2FkwLuvPYFXS8Wkx45FmUQYDfYEMykvCw5SiyG5ABNmtkdc5VRNJ8+eq/fZjytv7uWTz5xDVeHdG+dy/6YJyhFb0I0zAht0I3EP2SCahfztTB19w2aKM+JsJ0Ezol5rEEn/oZBBj7o/7yzq3hEl94nbmsPQ1+LtktzC7j+kT3qMuZCX02tBQ3ZSNIuzE7CqBurStEjeIE0zO6A1iHbq8faDXdB2VWzLqTSvsudSIw8/fYZh83hpWVP3EA8/fcbeJIqMhQW3iu1glJlZLR6bcJ6qEZMoUl4WGugr4kcqnBhVVx8SyZkAL3wKnrwLvrfML+k8lXqDyNkEkarKJrc3yV4GKNDXBL1uymIOfA1FtXDMtJ4z6kJOOptEmy4pRUI+aB2FWt9Osdn9h7TvSGlQHVwoCmgys4fnCnXI114scy6PnQFdAyN86KlT9A2b2ViUyn/fvVT4No7FYIQl9zorVtwEMOhG4h6yQTTLUFXVZk793k0Fk/+wPWVkQBi6gpwgCiGWz0lCUaChe4jW3mH3npRSAHlrha9BAGRmQ6MWTmopQNt0/6E63X9IXsgHG3rDZZ9llbgjCBtEXQMjnLkhfGFsDSJdMptSCHHpjp8o8RiLVeWx50tdtVR47PlSu9xssWboeiUIG0TTMOHUDarXFcgGUSiwRTNTL2vqnfwdqU+PWc3j7/dDhLOqqlS19gFQlB7veKfuOuHvoRjt8ijJ9ImME8bQ4J4PUeMFuPxXAE7N+xgAJ6q93CBSlDFx976Tmd1oH+Bacx9GgyISSq1WaVAdjGgys+XDZyhMi6W1d5gf7L/u1R8xarHy0d+doaZ9gDkpMfzfe9YQaXLQRrCY7ed7UQnjH0vMDXjQjcQ9ZINolnGssoPylj5iI428be2cmb9g0wXRMIjPggRphBgqxEeZmJ8hTi49kpnpU0SXn/V2SVNypqaTYbOVzIQo5mm127xi8qVBdbCxY6HwIfplYyGqYhRSjA7/R0K74rVrrVhVWJSVYPdik/5DPuFEVQeN3c4jmVWgsXvI7vmy8E1iQqP1CrRX+KdId/HQhNNiVTl7owuAtdJ/KCRIi49iSU4iMGGKaBrTY96ko3+EniEzigIFuinsROq1Y1j2MjGNJ5k5Nh+ic1Pvu/8r4nbZ2ykoEecmTr2sZoIfjKr16aENhakkxUZIg+pgpWg7KAYMbdf42i7xHfPLQ1Vcb+nz2o/46j+ucPh6O7GRRn72wDrS4qMc73jhGeiogNg0eOSyCLp52y/E7SMXZXMoRJANolnGb45VA3Df6jwSoyNm/oI2/6E1UuITYqxw16h6LLoPUfUh90etvcShMfH2iqKI1druWlAMcnotCFlTkEJCtInawUj6srQJryCLuz+oxdvbpodASjN8REuv8+aQw/1iUuwXQFee91FV08RDE86yph76hs3ER5lYlJ0wxZMkwcJWbYroyFgfogBHOOv+Q7lJMXbPtInoTW7pP+Q93PUhunFMpHYqRtj5H8LUGSht7KHH25IffYKo/gwM93r3tTVs8rIS7ZgnDaqDk5gULUgGtigXuHVJJmarymPPe8ew+vcnbvDrI9UAfPefV9ma55Mwj8DBx8X2tk9BTJL4nC5/u7iVn5mQQTaIZhFN3UO8fFkc7B/Y7KU4emlQHbKszJ+GUXXyXO2k0/9pZrr/0FYtQt22SppZMnmMVRJwIowGbtKkgGejtAmvIJKZWaz2ePudery9qsqLKx+RmeCeMfO4/ZZoaWbB5kPkoQmnLi9bPTcZo0EupIQK+nfN4bETRAGOcK50x6DaNgUpj2Few52oe1WFfV8W26vvh7R5ZCVGU5AWi6rC6epO79aUPBeSC0C1+CRNr3tw1Db5ZPMfkgbVwYsmM6PyAP91VwmRJgNvlLfZrvumy4mqDr7090sA/NttC7l9abbznc/+RoRLxGfBug/O6OdKAotsEM0ifnfiBharyobCVBZnO+n+ekq95tchDapDjrFG1R6tMCx9i7j1o8yse2CUC/Vi0snWIKqV/kPBzg4tzeyP3UvEHdVv+DxxxV3O1XbROTBKQrSJtbovTEclDHaAMQqylwe2wDBjQ1EqOUnROGuPKEBOUrRtxR3Q4u4VMdXlcmrDzxiMwmTTIZNNOE9pF4Zrpf9QSLGhKBWTQaGuc5Ab7dpxK8ARzpWtU0TcW0btF/FyCtJ76N8HndUiyMARFftEiIcxCm7+rO3uDYXimOZ1HyIY40P0utdf+rVrrZitKgsy4ylI0z5v0qA6eNEbRBUHKEiJ4V+2FwPw/14oZXBkepLXus4BPvL0aUYtKnetyOFju+Y733l0CF7/lti+6d+lvDXEkQ2iWcKI2crvT9wA4L3emh4a6oH2crEtvyxCjiU5CUQYFToHRqnrHHT/iSVaOkHNYeht8k1xEzha2Y6qwryMOHtEtG2VVPoPBSs3a5M5/2hOwpIwB8xDPvVL8AR9emj7wgxMRu2rUP9M5awEU2SAKgtPjAaFR+8uAXDYJFKBR+8uGT9hk5Btv8gt+4fPa/SIuAzH9zsw4bQbVEv/oVAiLsrE6rnJwJgpItv0mItWpw8jnKvadINqJw2i5sviOBudBKnzfFLDrCQ2VUzsADRdnPz42Omh9R+CJLvHp9709o0P0XZx64PvVT3e3iYvs1qkQXUwM2cdRCaIRa6m8/zrjvnkJkVT3zXIj1/z3Mevf9jMh548RUf/CEtzE/nm21e6DjY6/SvobRDpemsfnMEvIgkGZINolvDy5SZae4fJSIhyPR7oCfoqVVI+xDs5WZYELVEmo01HfN4jmVm+dtGm+iXSF8bG22vTQ5ZRu7xRrpIGLVmJ0ZTkJKKqCjVpW8Wd114ObFEa+7V4+12LpP+Qv9i9LIcn7l9jb/KOYUNhCruXOQg6CFaZ2eHvi9s1D2K+/1lOFTyM+f5nJ5lwNnUPUd81iEGBVVqzQRI66HH3ugeemB7TPDacNYl8GOFcZZOYOUkw049heWvBIE/xvYrNh+j85MeuPCfuj4yHmz497qGNRcLL6kJd17QnOZyiTxA1nochD/wkp2DUYuWAtohy6xKtQSQNqoMbY4T981BxgJhII1+8SyzKPPFaBbUd7k9vW60q//bH85Q19ZIeH8XPHlhHTKSLY9pIP7zxHbF982fA5MTAWhIyyG+PWcJvtGj7d22Y6ziWcDpI/6GQZ8Uc3YfIwxMLXWZW+qx3C3LCJP+h5stgHhSrpGkuRl4lAWeHNkW0d1TzcCjfK1ZbA0hzzxCXG3pQFPuUEzCmQbQ2MIXNAnYvy+HQ53bx+4c28f13ruJrb1kGwKmaTq63ODBaXaw1iKoPwaCXPTymS0sZXNsDKLD1k6gF26hP3YxasG1SY+BUjZgaWJKTSHyUKQDFSmaC/p1ztKIdq1U7bpXcI6bEEh00NNd9wGcpPRarSrUmdSt2NkFUf1rcyia399En5Sf6EFkt9uSyzR+FuPRxD+enxpCVGMWoReVsrZePYYm5YlJMtXrVGP1kdQe9Q2bS4iJZlZ8s7tTlZdKgOnixycz2A3DHsmy2zEtjxGzl/71Q6vbLfH9fOXsuNxFpNPCT964lV095dcaJn0F/C6QUwqr3TLN4STAhG0SzgLKmHk5Ud2A0KLx7w1zvvbDuPyQbRCHLWB8ij7DJzI6INDEf0tA1SGVbPwYFNmmpMvZV0nVylTTI0RPCftUwF9UYJQwMW8sCWpMuL1sxJ5l0Pap1dBCahRGjvLjyLUaDwuZ5ady7Ko93byzgTSVZWFX4/r7rk3dOmyeM6K3moJk+48gPxO2Su0R9LtD9h9ZJ/6GQZFV+MjERRjr6RyhrGtPALLkHHrlkj3Be/yFxf8V+sJh9UktD1yAjZiuRRoPzC7ax340S75LjZILowh+g7ZpIktr80UlPUxSFDdoU0ckqHzS5bT5E3pOZvVqqTdguzrTLfqVBdfCjN4huHIORfhRF4bF7lmIyKLxS2sxr11qnfIkXLzby/X3CPuQrb1k2tXfeUI99ovbmz4lJJknII6+sZgFPadNDty/NcjjaP230CSJpUB2yrNQaRJfqu7FYPZjqSJqjef/4Ps1Mnx5aMSeZxGjti0c/Cc6X/kPBzur8ZBKjTTQNGujJ3iTuDHCa2YEycZI0Tl7WeF40IeKzhGxW4jceuVXIFV640MDVJhdTRMEQd9/TIC4IAbZ8csrddf+htYXSfygUiTQZbB4yR8ammYGYotAjnG99DGJSobPKZ5O1urysIC3WcRreYKeQAYGQmEm8i55k1nbNHrZgHoYDXxfb2z4lppodYPMhqm73fl1Fug+Rd4yqVVVl7xXhL2nzHwJpUB0KpBZD0lywjtomyhZkJfDglkIAHnvuMiNmq9OnX27o5t/+KBqgH9xWxDvWuXEudPzHwvcobQEsf8eMfwVJcCAbRGFOz9Aoz56tB+C9mwq998IDHdAlGk/yyyJ0mZ8ZT2ykkf4RCxWtfZ492ZZm9jfvFzaGSf5DMEYKJFdJgx2T0cBNC4WM63SkNplzLXANohGz1eYnsnOxA3lZ3jpwZcQo8ToluYncuTwbVYXv77s2eYcld4vb6/sCn4J3/Mfi5HvuFsh3PWnWP2ymtLEHkBNEoczW+WL6Q/8uckhUPGx6WGy/8R2fyGj1BpFTg2pdXpZaDHFpXv/5s56EbIjLFHKu5svivtNParHe2bD+IadP1ZPMztR0MWpxfoE+LQq1CaKmS+LcfIaUt/RR2zFIpMnATQu08y6rxT45JVUDwYuiwLydYluTmQF88tYFpMdHUdnWzy8PVzl8amvvMA89eYrBUQvbF2bwhTsWT/3zBjvhyA/F9o7Pg1HKqMMF2SAKc/5yuo6BEQsLs+LZVOzFFcwGTV6WOg9ikr33uhK/YjQoLMsTK17TlpndOOazCGpVVTl0Xay42fyH+ttFHDnIVdIQYYfWIPpdp3bCceOoVw01PeFkdQd9w2bS46NYljtmtdeWiiebjoHgk7csRFHgxYtNlDb0jH8we7lIEDIPiijpQDHUA6d+Jba3fmLK3c/XdmGxquQkRU/t4SAJWvTvnuNVHS5X39nwkDApbrnsEzmkrUGU4aRBpB/DpLzMd+hTRE3nhTHv698U/775My5jvRdkxpMcG8HgqIVL9V7+7ovPhIzFgCq82mbIq1dEetnWeWnERmoX/O3XYbQfIuIgfcGMf4bEh0zwIQJIjI7g81rD5wf7ymnqHhr3lBGzlYefPk1D9xDF6XH84F2r7emurjjyQxjuFjLwpW/12q8gCTyyQRTGqKrKb46JKZ/3bipwHU/oKdKgOmxYOV2j6qQ8yN+EL9PMrjX30dY3THSEgTUFyeLOeu0kOH2h0PxLgh7dCPrVpljMqQtAtYw7efEnB7T0sp2LMjCMlWnYGkTSfygQLMpO4K4VuQB879UJU0SKAou1KaIrAUwzO/1rGO6B9EWw4Papd9flZXJ6KKRZkp1IalwkAyMW14mfMSmw/oNi+41veX2KqFJPMHM2QSSPYb4nW5jqc/lZ2PMFuzHv6gdcPs1gUFhf6Mu4e22KyAtx95Pi7WGMQfVyaVAd7BRtBxTh9Thm8fatq/NYMzeZ/hELX3uxlKMV7fz9XD1HK9r4z79d5FRNJwnRJn724DqSYtzwEepvFxO1ADv/Q/qBhhny3QxjDl9vp7K1n/goE29ZM8e7L14vG0Thgm5UfcGTqHudpfeJWx/JzPSR/vWFqUSZtJOS2hPiVp4EhwyZCdEsy0sEoDJ5i7izfG9AatmvGVTr5tmAOInqqQPFII9pAeSTt8xHUeCV0ubJq+x63P21l8Ay6v/izCNw7AmxveXjbp0Mn6qRBtXhgMGgsLnYDZkZwKaPgjFKSFa9MM0xlkpNBl6U7iDiXlXtiycyhdE3lD4nJGUgGjFntO0Fu8EUOeXTNxb5sEHkJaPq1t5hzmrT5LcsHtMgkgbVoUNsqt0btuKA7W6DQeHL94oG53PnG3nXz47xyWfO8a6fHedPp+tQgB++ew3zMhwcXxxx+Hsw0iem6nSfQEnYIBtEYcxTR6sBeOuaPO/H60qD6rBBN6q+0tjLsNni2ZN1mVntMeiu925hTOU/JBtEocSOhaIhs2dES4IpfwWsXvZimIKa9n4qW/sxGRS2LRj7mdIurDJLhJeIJCDMz0zg3pVOpojyN0JsupAmevnC2y0u/Rl6G4TXyIqpjTitVpUzN7QGkTSoDnm2aD5ER65PYTKckAVr3iu23/i2137+0KiF+q5BAIodScw6KoUfiDEKspZ77edKNEqfgz8+IMx4J3LiJ25NUesTRCerO7B6EgriDgXbxG3rFeibOqnKGQfKWlBVWJ6XND7URqoGQgsHMjOAuk7nHn4qMDjiZgJjb7OItgfY+UXp2xiGyAZRmFLfNWjTEd+/qcC7L97bJE6UFQNkr/Dua0v8Tn5qDCmxEYxYrJQ1OkgQckViLszdLLa9nGY2arFyrHKC/5DVAvWa/5VsEIUUuiH0k3U5qJEJ0N9qX5X0E7q8bF1hij0RD8asvEvvjkDziVsWYFDg1Sst433RDEZY/Gax7e80M1WFw/8rtjd9BExRUz7lWksvvUNmYiONLM5O8HGBEl+zdZ74Djpb28nAVBdRWz4BihEqD9iNo2fIjY4BVBUSok2kxTmYVtGb3Dkr3ZpmkXiA1QJ7Poe4hHbCns+L/VywNDeR2EgjPUNmrjZ7eK41FXFpkKXJ32YgM9OvG25dMmZ6yGqBxgtiW4bShAbFmlF15UHbQpzFqvLY86VOn6IAjz1f6l6i8aHvCE/AOethwW0zr1cSdMgGUZjyu+M1WFXYVJzKwiwvn5zqKwnpi+RqexigKArLZyQz802a2fnaLvpHLKTERlCSI+RJtF6FkV5hlJi5xKs/T+JbVuWnkBQTQfsQdOVsFXf6Oe5+/1Ut3n6svAykd0cQUZwRz1tWC0n0dydOEelpZmX/8O/0WflesTIfmQBr3+/WU05Vi+mh1XOT3TP7lAQ1BWmx5CXHMGpRp5YIpRTYp8ze+I5Xfn5lq91/yKGfpGxy+46aI1MEcajQU2+LFXeGyWiw+ZEFow/R0KiFN8rF1PatJWO+I6VBdegxZ70wzB9og+aLgPjMNU4wpx6LCjR2D0392eyug1O/FNu75PRQuCLPWsKQYbOFZ07UAvDA5kLv/wA5ahp2rNKMqs97alQNsOQeQIHa4+KLw0sc1kb5t8xPt5sJ12n+Q3lrpFFiiGE0KLbI3OMm7SLGTw0ii1Xl4NUWm2Tx5oVj4u0tZjmVFmR84pb5GA0KB6+22oyeAWG+GZkAfU1em8xwi8PfF7fr3ud2aqfdoFrKy8IBRVHci7vX2fYpQIGyF6ClbMY/f8qIe116LZM9vU9fs9f22+BLo+oZ+hAdqWhjcNRCblK0fVEOpEF1KGKKtDcMNZlZS6/z5tBYptzv9W+BZUTIGotunkmVkiBGNojCkD2XmmjvHyErMYrbxqYQeAv9Yko2iMKGGRlVJ+bYZWalf/daTfpJuD7aD9hPgvM3eO3nSPzHzkViVfLp9oXijvozM/JLcIc9lxrZ9vh+3verk7bR6Qd/dZI9lxrFDi2Xxah0VBKkydXRYKAgLY63a8EK47yITFGw8E1iu8xPMrO601BzCAwm2Piw2087VSMuAKVBdfigS50PT+VDBJCxyG6sfui7M/q5FqvK8UrxfWgyGCZLQEaHoOmS2JZNbu8T7+Z5tBv7bdCNqqs7UL2cckfBFkCB9nLoafT46XtLhQT7liVZ46fUbIvCq2Zeo8R/zNNkZppRdWZCtIud7bjcr6MKzv5GbO/6Tzk9FMbIBlEY8tRREW3/7g0FRHh7tF1VpUF1GLIiX0wQXW/po3/YTZO6sXhZZtY/bLYZvI43qJZSoFBmuza5c6jJxGjmCkCF66/67OftudTIw0+fmTRW3dw9xMNPnxFNIv0zlbdGxrQGER/bNR+TQeGN8rbxq+16WsqVF7weI+6QI9r00PJ3QFKeW09p6RmitmMQRYFVc5N9V5vEr2yeJyaISht76OgfmfoJ2z4tbi/+CTqrp/Uz9Qb3wWuiQfTnM3Vse3y/vcEN0HQBrKMQlwHJc6f1cyQuKNgi/BZxdjGsQGKe1qBxzcr8ZCKNBlp7h6lud24YPC1iUiBH8wX10MjfalXZX+Yg3h7GJJjJReGQQjeqvnEURgbYUJRKTlK0q08xOUnRtiamQ17/JljN4rXd+LxLQhd5NhxmXG7o5nRNJyaDwrs25Hv/B3TXCk2rwWQ3xJOEPJkJ0eQkRWNVmRwv7Q4lmsys7iR01c64nhNVHZitKvmpMcxNixV3DnZBqzaqnyd9FkKRjIQoVmhyxvIkbeqs/GWf/CzdkNFRC0G/77HnS7HKVLygJD81ln9aJ77Dvrt3zBTRgttEUlNHhf144CvaK+zpRFs+7vbTdHnZoqyE8WbokpAmMyGahVnCd/FohRtTRHlrxIWUarGbnHuAswZ309gGN4yRl62TK/q+wGCE3Y9r/5j4/1f79+7/cUt+FR1hZFV+MgAnqtz4DHmKzYfodY+edrmxh+aeYeIijWwqHtMgkAbVoUvafEjKF3KwmiMYDQqP3l0COP0U8+jdJRgNTo4hbdfh/O/F9s4v+qRkSfAgG0Rhxm+06aHdy7LJTHRvnNBtrBY49zuxnTQXjPLEN5xYYfMh6vL8yQnZ9tUEL8jMHMbbN2jSxpRCiM+Y/CRJSLBDmyL6x6C20nntFTj/B+GbMEUKjCe4a8g4XHVc3CHNXYOOj+2aT4RR4Whlu/2CPCrBPjrv6zSzoz8CVFjwJsgqcftpp2r0eHspLws3bDKzCjd8iABu+jdxe/ZpkQDrJu42uC1WdcxkrTyG+YySe+AdTwlJ/VgSc8X9Jfe4/VLri8Rx4bhPfIg0TxgPfYj2lQmp982LMogyjWl0SYPq0EVRoHiH2NZ8iHYvy+GJ+9eQnTT++jA7KZon7l/D7mUTPt9jOfh1UK2w8A6YI73Owh3ZIAoDLFaVoxXtPHPiBn85I0yCvW5OXfocfG+ZOEAAdFaKf+urq5KQZ6W2qjUto2rwqszskNYg2jLWf6hWTnqEAzdrPkSNdRWoikGcfP7tw/DkXV47pvQNm/n9iRtT7pdIHzE9leIfciot6MhLjuGd64Vk5ruvXrN7dthkZj5sEPW1wrnfiu0tn/DoqbYGkTSoDjt0Tzy3jKoBCrZC/kawDGsNR/fwKHFINoj8Q8k98MglePAFeNsvxO0jFz1qDgFsKBJSxZPVPmgQFWwGxQidVR5Nc+/XGkS3LJ4gL5MG1aGNLjOrPGC7a/eyHA59bhe/f2gT33/nKn7/0CYOfW6X6+ZQcylc+ovY3vkfPixYEizIBlGIo+vT3/WzY3z+rxcZtaiYDArtfcPe+yGlz8EfH5gc89nTKO6XTaKwYOVMjKrBnmZWfwq6pr44d0Zr7zBlTb0AbNE8HwD7GP0caVAdyqzKT+atMWf4lvodsRo1lhkeU7oGRvju3mts/Z/9PHfeVSyxVouhQmykFkNcmuudJQHhX3fOI9Jk4ERVB0f0KaJFd4BiEN4rnTW++cEnfwbmIchdA4Xb3H7a4IiFy5pMd600qA47NhanYjQo1LQPUNfphoeMotiniE79Egbcawq4K/Xubq2D7huAIj6rEt9iMIq0sOVvF7fTaJqsLUjBoEBtxyANXYPerS8qwe4V5GbcfccwXGnqxaDAzsWZ4x+UqcWhTfEOQIGW0nHG5UaDwuZ5ady7Ko/N89Kcy8p0Dn4NUKHkXrvPlSSskQ2iEMaZPt1sVfnX354Zb2I4XawW2PM5cDXovOfzXpWGSALDsjwhMavtGHTPgHMiCVn2C6kZyMyOaKP7JTmJpMVHiTtVdUyDSK6ShjJGrPyX8UnAkeXn9I4pLb1DfP3FK2z9n/18f1853YOjFKXFkhQT4dKQ8aaYavEPOZUWtOQkxfDuDWKK6Dt7tSmiuHSYq0lay/7h/R860g8nfiq2t37CI1+X83VdmK0qWYlRzEmJ8X5tkoCSEB1hk2MfcSfNDDSJ4jIY6YMTP3O56+maTj781Cm++uIVt166cEjz4cpYDNGJrneWBAXxUSaW5orPkE+miDyMu7/cKY5v6wpSSY2LHP+gzaB6lXdqk/iX2FT7ezdmisgjGs5p07oK7PiClwqTBDuyQRSiuNKn69j06TOh5sjkyaFxqNBTL/aThDRJMREUp8cBM5giKrlX3M5AZqafdG9bMEZe1l4BQ11gipbm6KFOzRFSzK04X7By/5hS1znAfz17iW2PH+Anr1fSP2JhSU4iP3z3al79tx08/rblgHNDxvvStWOblJcFNf+6Yx5RJgOnazp5vVyT9ugR4mUveP8Hnv0tDHYKv7MlnslHTo+RlynSMDgsscnM3PUhUhS4SUs0O/4EDPeNe9hqVdlb2szbnzjC2544wiulIk0qyuT8FF1PHFo4qjWIpCdISKEnRfnEh8hmVP2Gy6RHi1XleFUHR5rEcWrXkgnejtKgOjzQZWYV02wQHfiauF3+T5C5xDs1SYIe2SAKUTzSp08HqxWqD8Nrj0+9L0Bf8/R+jiSosBlV107Th2jJPUL6UX96WtIPVVXH+A+NlZedELc5q8AUOfmJktDB3WOFi/0qWvv49z+dZ8c3D/KbYzWMmK2snpvMLx5cx4uf2MZdK3IxGhTXhozvWU1G90Vxh5xKC2oyE6N576YCYMwUke5DdOOo8AvyFhYzHP2B2N78MY8lJKe0iYA1Ul4WttiMqq+3232xpqLkPiFlHeyE078GYNhs4Q8nb3Dbd1/joadOcaqmkwijwjvWzWHvp7bz/XeuQsF14pChQfcfklOQoYTeIDrpiwbR3E1giBCpw53VDnfR7Snu/+UpGgbFpeAv3qgarzyQBtXhwVgfIqvV9b4TqT0pkmYVI+z4vPdrkwQtpkAXIJkeLb3Om0PT2c9G6zW48Axc+JOma3eT+Kyp95EEPSvmJPPsuYbpTxAlZAlTzuo3oPRZ2PpJj55+o2OQ+q5BIoyK7QQKsMvL8uVJcMjj7rHCwX6XG7r5vwMVvHip0bYwunV+Gh/dOZ/NxWkOJzZ2L8vhtpJsTlR10NI7RGZCNBuKUjF2yKm0UOJfbp7H08drOF/bxYGrLexanC8axo3n4OqLsPZB7/ygK38XHmqxabDqPR491WpVx0wQyQZRuLKmIJnoCANtfcNca+5jUXbC1E8yGGHbp+C5j2M9/L/8ZHAXvzzWQGuv8ItMiDbxno0FvH9rIVlaAu2CrASeuH8Njz1fOm5BMDspmkfvLmF3SSY8p3nEyCnIkGJ9oTi/KW/po71v2C6n9waRcZC3FmqPiXOx1KJxD+v2FBNbm219Izz89Bl7mpXuP5SzQhpUhzJzNogmX38rNF/yzEPowFfF7ap3Qdo839QnCUpkgyhEyUxwL8Lerf36WoU7/YVn7F8IAJEJUHI3XHtZM1Z0tFKmiJhPPeJcEtKMTTJTVXV6EomlbxEnJZef9bhBdFgzoV0zN4XYyDGHpzqZYBY2FGyBxFzUngan/kDEpIw7ppyu6eRHB66zv6zFdt+tSzL56M75rJ479YW4bsg4Dv0zJafSQoKMhCge3FzIT16v5Lt7y9m5KBNlyV2iQVT2gncaRKoKh78vtjd8GCJjPXp6RVs/PUNmYiKMlORKP5hwJcpkZH1hKm+Ut3H4ept7DSKgvuBe4iP+H0n9zdzY/3NaLbeQkxTNB7YW8c4N+SRER0x6jtMGt0ERyUIjveLiT0o/QorUuEgWZMZT3tLHyepOdi/L9u4PKLpJNIiq3oA1D9judmVPoSKm0x57vpTbSrIx6glmUl4W2pgihT9o+ctiisjdBlH1YbG/IQK2f9a3NUqCDikxC1E2FKWSkxTt0oA1Jyl6/BTGWEYH4eKf4bfvgG8vEkbUDWfFGOGC2+Htv4TPlMN9T8Bd3xvzqhN/CrD7f+TqQpiwNDcRk0GhrW/YpYTRJbrMrOGM0/FmZxytFOPW2+aP8R8a6Yfmy2JbNohCH4MRdj8OKEy0SLP9c7AT9dSvOFTexjt/epS3PXGE/WUtGBS4e2UuL33yJn7+4Hq3mkNOkabnIceHtxcTG2nkYn03r15pgcV3iwcqD8JQz8x/QNXr0HgeTDGw/iGPn37mRhcAK/OTiDDK06twZovmQ3TEDR+iK409fOoP57j524f53sDtAHw86h98521Lee0zO3loe7HD5pCO08Shek1elrdGnoOFIDaZmS+Mqp34EHlkTyENqsMHmw/Rfvf2V1XY/xWxveYBSCnwTV2SoEWewYQoRoPCo3eXiG2sbDKUco/hCJsMpRgRGtNH7y4ZH11otYoT4Gc/Ct9cAH/54P9v787joiz3/oF/7lkYdhBBFhcEEfcNFMQtM1OsI2qLZVmaZeWxxeOpY/WrY/Wc85QdO0916rHtpMc9ecpSKz3uK+7gEcUFRFRWRQUE2Wau3x/XDItsM4AwM3zerxevgXvuuedivsPNNd/7ur6XzCgLvVwedcJHwB/PAk+uA/o+DGiNK7D0jgGmLgfc/as3wj1Abu9tWRFPsl6OWjVCfeXV0EZPM3P1qVzN7NRPZj/MIICDxgTRsKoJovTjcjl0947yPUe2r3cMjkd+iixUT2Bnoz2yfe8BACi//hE7l/0ZBy9ch0Yl63Js/+No/GPaIPTyb4bRGaYPV0wQ2Yz2rjrMHNYVgKxFZGgfCrQPAfSlQPLWpj+BafTQoOmAS/v6963FMWOCaHBgHRdmyG4MD5Hvj/3J17D++BXEpeRWWxRECIEDydfw9HeHMeHTvVgfn45yg0Bq50dQqmuHAEMWHtIdgUM9hagbdMWUIGKBaltkShA1ulZofTpHAGoHoCBTLvJhZHZ5ivxCFqi2J93ulbdpcXKAQEMu7AQuHQDUOmDUa3e3bWSVOMXMhkX39ceP915DQNx78EXlcqvZaI+MqIUY1NeY0MlJAk6sBU7GytWBTDy6AP2nAv0fA3xC63+y3jFAzwflykK3smV9kMBhvGplhwZ09sDpzHycuJIn56E3Rp8pMhl5aj0wYp5ZD0kvBG7eLoOrToMBxmLZADi9zA5tTszEnN3eUPAZIlRn0AE3kQNPHDb0hCFNwesaD8zVbMA72lUY3sUZPR77L3T0bMYlw0uLgKxE+T3fVzZl9shgLI9LQ1JmPv6dlI3onr8D9n8CJG2SFzYaK+skkLJdjn6MmtuoQxxPuwkACO/K+kP27sr121AA3C4z4A/rTgCQo7bffrAXDAL4ak8KEtPlqDaVAkzo548XRgWjfydPYM9ceXV+39/le1bVyCTRFRaotmWmBNGpjDwUFJfVO4rMYlonWXsmbR9wcQ/gHQLA/PIUgYYMFqi2J96h8iKraYXYkPvq3lcIYIex9tCQZ3lhto1igsiWnd6AQXGvQtwxm7gDrsM37lXg9kH5ISjrP5V36jyAPpOBAY8DnYda1jFRqeW8ZrJr/Tt5Ys3hyzhx+WbjD9IrBvjlj3KI8vXUGkUSa3M2T452GxrcHpqq0zPYCbYrVWsgCKhw0NC7xj6Lyx9H30A/3JP+NcZkfgMcdQPu+7NcLro5ZCbIkZOufrLTRDajnYsDZg3vis92JON/tp7HuEcmQrX/E+D8v4GyYkBr3gegGg4YVy7rPdms89Wd8kuBtOtFUBRZQ43s1+bETMxdXbPIb2ZeMeaurqzj6KhVYergznhuRDC6tK9Sz2rIbGDfp0DOaeDcZqDnA5Y3oqQAuJokv+coSJvk7+GEzl5OuHz9No6l3cDoHh2a9wmCRsoEUepeYPAsADIp5eGkRd7tslofokAWQe+vTjU2kgWq7YKiyFFE8SvlNLP6EkTntsgR1lpnWVif2iROMbNVBr2sGwRRS2Ug+dELCatlckilBXo8KKeDvXYOiPnMOPqH4aeaBnTyBACcvJIHw51FYszl4l05B/70T/XuqjcIHEq9jqNX5Tt5WLcq0zOE4AgiO9NQDQRA1kFwGPMGMM44B37f34HNb1arpdAkVesPNVfSiVrMsyOC4eaowdnsAvx63Q9wCwBKbwGpuxt3wJuXZU0+ABj+SqMOkVog30ehHdzg4dSMIwHIqtRX5NdEUYBX7gvBgTfuw/uT+lZPDgGAkycQ8Zz8fu/ixp3XMuKNU687AW7NXOCYWkxEVzlV8a7UIQoaJW8v7qt4j51Mz0NhSXmtu5v+Ey6c2BuqTDkqjtPL7EiwcZpZys669xGicuWyiNmAazMnLclmMENgq9IOAPkZDe8X+aJMCk1bDfSe1Pirq9RmhPq6wlGrQkFJOVJzCxt/oD5T5O2p9XXusjkxEyMW7cD0744i87Y8Hf3vrhRsTsyUO9y8BBTmyCSnJUtzktUyuwZCQTEw7GXggcVyw6ElwKZ5spZaU3FUmk3zcNbiuRHBAIBPtqfA0PNBeUfSxsYd8OASOaIsaBQQMKhRhzAliMK4vL1dMyvBLYCoYG94udSzOuLQ3wMaRyD9mJyObakrrKFmDyLvZh2ijuGy4H5hDnD1LK7dKsGclcdQbhDo38kDfu7VPw/4eThWLnHPAtX2J/heAAqQcwooyKp9n6SNcmCBgxswfF5Lto6sDBNEtupWtnn7dRoCOLNgJplPo1ahT4CsAdToQtUA0GuiXBUv8wRw/UKNuzcnZmLOyuM1Otu5t0oxZ+VxmSQyjfTw61dZNJ1smrk1ECr2i5gNTPpfWRvm2DLgpzmAvvYroGZjgsjmPTOiK9wdNUjOuYU4h6Fy49nf5OhaS9y+Id9XADDs1Ua3x5QgGswEkV2zKMFdH9cOlcuP7/3Y8oakH5O3TBDZtCHGBNGJy3koLrPw3NUQjQ7oEgkA0F/YhbmrZH8r2McFq56LxP43xmDlrMF4urseK2cNxr4FY2RyyKCX/TaAI4jsiUt7wH+A/P7Crpr3G/TAzv+W3w+dw8+ObRwTRLbK1bd59yOqor+xSPSJy3mNP4iLd+UQ5ztWM6tvmL5p23sbT8Nw+bD8oXNE49tBViUiyAv+Ho41psaaKJDFXk0FPAEAg54EHvpGJhz/sxb4YRZQXtq4BuSlAwUZ8li8Omqz3B21eH6UHEX0boInhKMnUHQNuHTQsgMd/U4WY+3Qp/66DHXQGwT2nr+GtFvy50FdPC0+BtkOixPc9Rn2MqDSyKmRpqS1OapOve7IBJEt69reGT5uOpTqDU2r+1jnE8ip/ucO/oZDqdfhqtPg66cGw81RC7VKQWSQF8K9BSKDvCpXPb52HigrYoFqe1Tfcven1su6Zo4ejV6ogewHE0S2KnCYsbJ8PR+z3DvK/YgsZKpD1KQRRIAsiA7UmGbW0DB9AVnwszDF+GGPIz3shlqlYOFEWZi6Zv00aeHE3pWdVZN+j8g6amoH4PTPwLqnZVFiS5k+WPn2BhxcLH88WY2Zw4Pg6azF+dwSXPK5R248s8n8A5QVAwe/lN8Pf8XielSmKbKzlh+vqAb4xLeHKqfIkt1pVIK7Lp5d5CqyALD37+Y3Iu+KHEWu0lSOCCCbpCjK3V3u3niRzu/GUSgw4OOpAxDSwbX+x5iml7FAtf3pVqUOUdXaZ/ryytFDw16WddKoTWOCyFap1ED0IuMPdXzMiv6QJ3dqFNMIolMZ+SjTN6HmS0/jNLOs/wC5KRWbs/Ib/mCvQylcrp+SP3AYvV2J7uuPJdPD4OdRTw2E2vT6HfD4Glm749xvwJrH5ZL1lkjn9DJ74arT4IVR3QAAX+UYV8NL2mR+0d//fC/rc7h3ksuNW6CuKbLZecWVU2TJ7jQ6wV2X4fPkI8/+AmSfNu8xpnOYbx/Awbn+fcnqRXQ1JojuQqHqU0owCoUO7ZRbeDcCGN/HjILmGQnyltPL7E/nSLk6WWEOkH2qcvt/vgeupwDO7WXtWmrzmCCyZb1j5BV19zs+TLkHyO29Y1qnXWTzurZ3gbujBiXlBpzNKmj8gVzaA8HGK/un1qOotBz/OnARH/ya1OBD+ygXoRLlgIsP4BnY+DaQVYru6499C8Zgzeyh+PTxgVgze2hlDYT6dB8LPBkrh79f2AmsfFgu+Wwu1h+yK09HBaK9iwN+yOuBcrUTkHepsn5GfQyGyqXth84B1OavPGbuFFl9Y1eBJKvW6AR3bXxCK/tq+/7HvMeYzmGcXmYXTCOIjqXdQHlTLsjd4UZhKV5Y9R8cMfQEADzld8m8B7JAtf3S6ICuI+T3pmlm5aXA7g/l98PnATq3VmkaWRcmiGxd7xhgXiIwYxPw8D/l7byTTA5Rk6hUCvpXTDNrQh0ioGI1s5yDazHswx1YuOEUcgpK6p3NoQC4x/mi/KFTBJcit1NqlYKobu0xaWBHRHVrb/5V96BRwFPrAZ07cOkAsHyyLDbcEH2ZXB4a4IcrO+Gi0+DFe7qhBA7YJ4zTbcyZZnbuNyD3PKDzAMJnWPSc5k6RvStTRsgqNDrBXZsR8+Vt4v8B11Mb3p9JbrvSw9cN7o4aFJXqcSojv1mOqTcIvLI2Hldu3EaSozwvqtL2NfzAqgWqG7miI1k503L3F4zL3SeslCsGu/oCQ55rvXaRVWGCyB6o1EDQSFmjI2gkp5VRszBNM2tKHaKUq7fwX8nBKBcqdCg6j3a3L6GLlzPen9QHnzw2EArqHqY/1d+4DCenl1FtukQCMzYATu3klIt/TQQKr9X/mOxTQHmxLMLYPqRl2kl33fShgfB21eGn4jC5IcmMBNH+T+XtkFkWXzHNNmOKLGD+ildkmxqd4L5TwEAgZCwgDJXvy7royypHePB/o11QqZq/DtHftpzF3vPX4KRVY8LER+XGi/sbXuWxaoFq/o+0T6ZC1al7gWP/Arb/Rf488o+cskoVmCAiolqZRhCdsHAEkRACRy5ex3P/Oor7Pt6Nfx7Pw35DXwDAV2GXsfO10Xg6qismDexY7zB9//xEuYFXSakuAYOAmb/IaYhZJ4FlDwIFWXXvX3XlHxX//dkLJwc15ozuhh2GgSiHWq7EUqXmWQ2XDgKXD8mC5xbUWyjTG/Dj8Sv4aMsZs/Y3d8UrIoz8o7xNWAXk11O/KjvRmOT2BLy6tUjT6O4bYqxDdKgZEkS/nszEl7vl+W/RI/3Rtc8wOdq2JE/Wg6wPC1Tbv2tnAUUFGMqAja8At3Plz87erd0ysiJW30MuKCjAvHnzEBgYCCcnJwwbNgxHjhypuF8IgT//+c/w9/eHk5MTxo4di/Pnz7dii4nsw4DOcgTRuewC3C5t4KoT5JDm305m4qElB/Dol3HYlpQNABjbyxddRj0JAAi9tq3aVVbTMP2Vswbj6e56rJw1WA7T7yKA/CvynxaHOVN9fPsAz/wGuAUAV88ASycANy/Xvm/F1Axeebc3T0Z2gaNbexzQm4pVb6x75/2fydv+jwFuDRdtLS7TY8XBNNy7eBfmrzuBjJvFda5iBVi4khURIFec7RIF6EuBuM/r3q+i/lA4k9x2xHSuOJp2HYYm1C47l12A12LlFLHnRwUjZkAAoNZUrmicurf+A7BAtX07vQFYN0OOVqxKGIAfnpX3E8EGEkTPPfcctm7dihUrVuDkyZMYN24cxo4di/T0dADARx99hM8++wxffvklDh06BBcXF4wfPx7FxRzaTdQUfu6O6OCmg94gcCqj7lFEt0v1WBF3EWM+3oU5q44j/tJNOGhUmBbRGdvm34NvZwxG0PCpckne7EQ5hLkKtUpBZJAXwr0FIoO8ZALJNNKjQx9A18CSrETe3YFnfpXLRl+/ACx9oPZaHqb3FUel2R1HrRpz7w3BFoOMraGuBNHVc3LFKAAY9kq9x7xVUo6vdqdgxKKdeOenRFy5cRverg5YEN0Tf586oN4pshatZEUEVI4iOroUKKpjJEn6MXnLJLdd6dvRA05aNW4WleF8zq1GHSPvdhleWHEMRaV6DA9pjz+N71F5Z9eR8vZiQwkiY40+XpizPwY9sHkBUOvyCkab32h4GiK1CVadILp9+zZ++OEHfPTRRxg1ahRCQkLw7rvvIiQkBEuWLIEQAp988gnefvttTJo0Cf3798fy5cuRkZGBn376qbWbT2TTFEVBv45yFNGqQ5cQl5JbbVWea7dK8Pet5zDsw+145+dTSMstgqezFi+PCcH+BWPwwUP9EdLBmNxx9gKCR8vvT/3U8JObPsh35gd5MpNXkBxJ5NVNrmS1dIJMBpgUXZfLuALy6jvZnceGdMYJl2EwCAWq9KNAfkbNneKMK5f1eECuIFWLG4Wl8tz2wXZ88NsZXLtVgo6eTnh/Uh/sWzAGc0Z3w5SwTs23khURIOsQ+fUDygqBQ1/Vvg+T3HZJq1YhLNATAHA4NdfixxsMAvO/T0DqtUJ09HTCP6aFQaOu8hEvaJS8TYsD9OV1HERfOQWNK5jZn7QDtf9PrCCA/HS5H7V5mtZuQH3Ky8uh1+vh6Fi9A+bk5IR9+/YhNTUVWVlZGDt2bMV9Hh4eiIyMRFxcHB5//PFaj1tSUoKSkpKKn/Pz5aoBZWVlKCsra/bfw3TMu3FsahzGpGFbTmXjkLGjsj4+Hevj0+HnrsPzI4NwLucW1sdnoKRcDlPt1M4Js4YF4uGwADg7yNPKna+t0jMGmuRtEKd+RPmwedXuuzMe6stHoAJQ7h8GwRi1Gpv7O3H2BZ7aAM3qh6FcPQOxdALKn/gB8OkJ1ZHvoAYg3AJQrnYGbOV3qoXNxaWFqAE8ek844reEIFw5j+KTP0MdUWVVloIsaE6shQKgPHJujXNLdn4xvtufhrVHr6DIOK022NsZL4wKwsT+/tCqVQAMKCuT5737enhjdPeROJhyFTvijmFMVDiGdvOBWqUwNlbAFv9OlGHzoPnxWYhDX6J8yAvVC6jfvgFtbjIAoKxDf5s8h9liTFpKeBdP7E/OxcGUXDw+uKNFj/1sRzK2n8mBTqPCF9MGwM3hjnNQ+x7QOLWDcvsGyi8fhTBeJKkWj6sXoC0rgtC6oNw90CbfX/bibvydKHnpZn3oL89LZ7+7FvZy7jK3/YoQovGTXVvAsGHD4ODggNWrV8PX1xdr1qzBjBkzEBISgqVLl2L48OHIyMiAv3/l1bqpU6dCURR8//33tR7z3XffxXvvvVdj++rVq+HszAruRCdyFXx3znT1qeo0CVFtWxcXgTEdDRjgJdDQbApteSGiE1+CSuixvdcHuOVYewdIEeV48MQLUIsybO+1CLcceSWeLONQXoCo5I/geTsNZSod9CodHMsrlw++rfXCyU5PItOTV+HtTbkBuBT/G/6gWoOzDn1wps+Civt6ZcQiNHsjcl26Y1/oOxXbrxUD29NVOHRVgV7IE1knF4H7OxrQ34xzG1GzEQbcl/QGXEuykBgwDSm+Eyru6pD/H0SlLMYtnS+29/5bKzaS7obzeQo+P62Gh1bgvXA9FDPPOyevK/j2rCwo/WSIHhE+tX+sG3LhUwTkHcNp/0dx3m9ijfs7Xd+P8LSvkOsSin2hbzf69yDr1L4gCSOSP2hwv30hbyLXrVcLtIhaQ1FREZ544gnk5eXB3d29zv2segQRAKxYsQKzZs1Cx44doVarERYWhmnTpuHYsWONPuabb76J+fPnV/ycn5+Pzp07Y9y4cfW+WI1VVlaGrVu34v7774dWq23245PlGJO66Q0CH3y8B0BJLffKHotOo8K3Tw1CZJAXFHN7MQBQ9AOQsg2jfW7CMHJ2xeaq8XC4mgh1QhmEoydGTXlGFqqmVmHTfyfF0TAsHQft9RRoDNXfy45lNzAk9XPoH14K0fN3rdTAxrPpuLSAX908gX1rEFx6BpldQnHd4AJ/xzJ0P/0SAMBjwjt4oMcDOJddgC/3pOKXk1kwzZ4dHOiJOfcEY2RIe7PPbYyHdbLVuCid8oFNr6BP/g70eOpvgEaOolftSQRSAOfuo/DAAw+0cisbx1Zj0hKKy/T46q87kFcG9I0ajUCvhi9YX7haiP/31SEA5XgqsjP+/Lu6P9irjqQD/z6Gno5X0d34/qkaD93OfUAa4NnnXjxwv22+v+zFXfk7MYyH+PxfQEEmlFrqEAkogHsAIh+dxxXsamEv5y7TrKmGWH2CqFu3bti9ezcKCwuRn58Pf39/PPbYYwgODoafn1x9JDs7u9oIouzsbAwcOLDOY+p0Ouh0uhrbtVrtXQ363T4+WY4xqeloSi6y8mtLDlUqKTdAo9HCwcHBsoP3fQhI2Qb1mY1Qj3mrxt1arRaarAQAgNJpCLQONf9OqeXZ5N+Juh1QVgSgtkLCAoACzdb/B/SJsdnOkE3GpQU8cO9InN/XBd1xCT/H/gvrDSPxrPpXRGnzUejaFefcRuB/15zA1tPZFY8Z3cMHvx8d0qSVxxgP62RzcRk4DdjzEZT8K9CeigUGz5LbM2UBYVXnCKhs6fephc3FpAVotVr07+SJY2k3cPxyPkJ8Perd/1ZJOeauPYFbJeUY0rUd/hzT1zgNtg7dRgMAVFcOQ6UIQFPZf9NqtVAb6w+pO4ZDzdhYheb9O9ECExYB656G7BVVTRIpsp8U/SG0OsdaH02SrZ+7zG27zVyad3Fxgb+/P27cuIEtW7Zg0qRJCAoKgp+fH7Zv316xX35+Pg4dOoSoqKhWbC2R7copMG8FQHP3q6bnA4BKC+ScBnLO1L4Pi3BSc0g7ABRk1rMDCzLaq+1J2fi1XK7yNE29HZNVe/GiRi7f+183xmLKkoPYejobigI82M8fm14egWXPRHBZerIOGgdguHGFvX2fyKLCQgDpxiXuuYKZ3TKdgw6n1rGKnZEQAq/HnkByzi34uuvwxZNh9SeHAKBDL8DZW144Sb9jFgYLVLcNvWOAqcsB9ztKN7gHyO29Y1qnXWR1rH4E0ZYtWyCEQI8ePZCcnIzXX38dPXv2xDPPPANFUTBv3jz85S9/Qffu3REUFIR33nkHAQEBmDx5cms3ncgmdXAz7+qBuftV49QO6DYGOL8FOP0T0OGNmvtUJIjYCaYmuJXd8D6W7Ec2QW8QeG/jaTwo5PkpQn0OEWq5mp1eKLhl3P5IWEfMuTcE3XxcW62tRHUa9BSw+yPgZhpw6ke58uLtG4BaB/j2be3W0V0SEeSFJbtScORi/QmiJbtT8FtiFrRqBUumh5vXH1MUoOsI2fe6uBcIrHIhPTdZJo60LkD7kKb9EmTdescAPR+UF8duZQOuvkDgMJsdSU13h9WPIMrLy8PcuXPRs2dPPP300xgxYgS2bNlSMUTqT3/6E15++WU8//zzGDJkCG7duoXNmzfXWPmMiMwTEeQFfw/HGtNyTBQA/h6Ojb/a3meKvD21vuZ9hVeBG6nyey5FTk3h6tu8+5FNOJx6Hf0L9uAtzWrcuQSHCgKfaT/HeNVhPBzemckhsl4OzkDU7+X3ez4Gjq+Q37cL4gc5OxYe2A6KAqTlFiErr/ZR2rvPXcXftpwFALwX0xdhXdqZ/wRBI+Vt6p5qm5XMBPmNf3++v9oClVq+F/o9Im8Zc7qD1SeIpk6dipSUFJSUlCAzMxOff/45PDwq5+UqioL3338fWVlZKC4uxrZt2xAaGtqKLSaybWqVgoUTewOorXaLtHBib6gbu7RPjwlymtnVM0BOUvXjm4Y9+/QEnDwbd3wiQF4Rcw9AzXexiQK4d5T7kd3IyS/EQu1yAKixCpDp54XaFcjJL2zhlhFZaMhzgMYJuHYG2P8/ctu1M8AnfYHTG1q3bXRXuDtq0dtfLpZzuJZRRJdyi/DKmngIAUyL6IwnIrtY9gRdR8nby4eBssoElJJ1Qn4TMKhR7SYi+2L1CSIiannRff2xZHoY/Dyqj8Tz83DEkulhiO7bhKXnnTyBkPvk96d+qnZXRYKI08uoqVRqIHqR8Yc6Up3RH/LKmZ0JKTqJAOV6nUvTqxQgQMlFSNHJlm0YkaUu7AbKb9fcnp8pC80ySWSXTKOzj9xRh6iotBzPrziKvNtlGNjZE+/G9LH84N7d5ahZfUnldH4ASqYxQeQ/sLHNJiI7wgQREdUquq8/9i0YgzWzh+LTxwdizeyh2LdgTNOSQyZVp5lVmQeiZJiKcLJANTUDFmRsc3q5FTXrfkStwqAHNi+o407j/8zNb8j9yK5E1lKoWgiBN344iTNZBfB2dcCS6WHQaRpxcUNRgK7GaWYX9xoPboCSbUyYs0A1EcEGilQTUetRqxREdWvf/AfuMQFQOwDXzsppZl7dZScl47i8nwkiai4syNimqNz8mnU/olaRdgDIz6hnhyqrMJrqypBdGNxVJojOZhdg9aE0BHm74mT6TWw4kQGNSsEXT4TB38Op8U8QNBJI/D8gdS8w4nW4FWdAYYFqIqqCCSIianmOHkC3+4Bzv8kVNUa8DvfiK1BKCwEHN1mDiKi5mAoykv0z1p4S+ZlQIGrcLaBAcQ9g7SmyblyFsc06evE61CoFeoPAW+sTq9339oO9EBncxIt2QcY6RFeOAGVF8Cy6KH/2H8ALJ0QEgFPMiKi13DHNrF1hivy5Yxg7KUTUOMbaUwpkMqgqAUVuYe0psnZchbFN2pyYiTkrj0NvqJncBgBf92ZYobldEODeCTCUQblyBB63L8rtnF5GREZMEBFR6+gxAVDrgGvngKtJaFeYLLdzehkRNYWx9pRyR+0phbWnyFZwFcY2R28QeG/j6VrGPUoKgPc3na4zeWQ2RakYUatc3AvPolS5nQWqiciIU8yIqHU4usvVzM7+CtXpn+HFBBERNRfWniJbZlqFcd3TgHE8XCWuwmiPDqdeR2ZecZ33CwCZecU4nHq96bUhu44ETqyBkrobHrfT5DaOICIiI44gIqLWY5xmpor/F9xKMuW2gEGt2CAishum2lP9HpG3/DBNtoSrMLYpOQV1J4cas1+9jCOIVJnx0BhKITSOcuoZERE4goiIWpNxiXul6Frltm9Gyyun7PwSEVFbxpFwbUYHN/PqC5m7X70yEgBFDQg9AEApLwY+G8C+FxEB4AgiImotpzcA61+ouT0/Uw6rP72h5dtERERkTTgSrk2ICPKCv4djfVWn4O/hiIggr6Y90ekNso9lTA5VYN+LiIyYICKilmfQA5sXALWWYzRu2/yG3I+IiIjIjqlVChZO7A2gZmly088LJ/aGWlVXCskM7HsRkRmYICKilpd2AMjPqGcHAeSny/2IiIiI7Fx0X38smR4GP4/q08j8PByxZHoYovv61/FIM7HvRURmYA0iImp5t7Kbdz8iIiIiGxfd1x/39/bD4dTryCkoRgc3Oa2sSSOHTNj3IiIzMEFERC3P1bd59yMiIiKyA2qV0vSl7GvDvhcRmYFTzIio5QUOk0v11leO0b2j3I+IiIiImoZ9LyIyAxNERNTyVGq5nCqAOssxRn/I1VqIiIiImgP7XkRkBiaIiKh19I4Bpi4H3O8ouugeILf3jmmddhERERHZI/a9iKgBrEFERK2ndwzQ80GUX9iDhL1bMHDkeGiCR/HqFREREdHdwL4XEdWDI4iIqHWp1BCBI5DuFQUROIIdFCIiIqK7iX0vIqoDE0RERERERERERG0cE0RERERERERERG0cE0RERERERERERG0cE0RERERERERERG0cE0RERERERERERG0cE0RERERERERERG0cE0RERERERERERG0cE0RERERERERERG0cE0RERERERERERG2cprUbYA2EEACA/Pz8u3L8srIyFBUVIT8/H1qt9q48B1mGMbEujId1YlysE+NiXRgP68S4WB/GxLowHtaJcbE+9hITU67DlPuoCxNEAAoKCgAAnTt3buWWEBERERERERE1v4KCAnh4eNR5vyIaSiG1AQaDARkZGXBzc4OiKM1+/Pz8fHTu3BmXL1+Gu7t7sx+fLMeYWBfGwzoxLtaJcbEujId1YlysD2NiXRgP68S4WB97iYkQAgUFBQgICIBKVXelIY4gAqBSqdCpU6e7/jzu7u42/aayR4yJdWE8rBPjYp0YF+vCeFgnxsX6MCbWhfGwToyL9bGHmNQ3csiERaqJiIiIiIiIiNo4JoiIiIiIiIiIiNo4JohagE6nw8KFC6HT6Vq7KWTEmFgXxsM6MS7WiXGxLoyHdWJcrA9jYl0YD+vEuFifthYTFqkmIiIiIiIiImrjOIKIiIiIiIiIiKiNY4KIiIiIiIiIiKiNY4KIiIiIiIiIiKiNY4KIiIiIiIiIiKiNa7MJog8++ABDhgyBm5sbOnTogMmTJ+Ps2bPV9ikuLsbcuXPRvn17uLq64uGHH0Z2dna1fV555RWEh4dDp9Nh4MCB9T5ncnIy3Nzc4OnpaVYbv/jiC3Tt2hWOjo6IjIzE4cOHq92fkpKCKVOmwMfHB+7u7pg6dWqN9tmalorLxYsXoShKja+DBw822MaG4vL1119j9OjRcHd3h6IouHnzpsWvgzWwh1iMHj26xnFffPFFy18MK2IPceG5q2n/U4QQWLx4MUJDQ6HT6dCxY0f89a9/bbCNsbGx6NmzJxwdHdGvXz/8+uuv1e7/8ccfMW7cOLRv3x6KoiAhIcGi18Ca2EM8Zs6cWePvLzo62rIXwsrYQ1yys7Mxc+ZMBAQEwNnZGdHR0Th//rxlL4SVaam4vPvuu7X+X3FxcWmwjex7VbL2WLDvZZ1xYd+raf9TtmzZgqFDh8LNzQ0+Pj54+OGHcfHixQbbaIt9rzabINq9ezfmzp2LgwcPYuvWrSgrK8O4ceNQWFhYsc8f/vAHbNy4EbGxsdi9ezcyMjLw0EMP1TjWrFmz8Nhjj9X7fGVlZZg2bRpGjhxpVvu+//57zJ8/HwsXLsTx48cxYMAAjB8/Hjk5OQCAwsJCjBs3DoqiYMeOHdi/fz9KS0sxceJEGAwGC14J69LScdm2bRsyMzMrvsLDw+vdv6G4AEBRURGio6Px1ltvWfjbWxd7iAUAzJ49u9pxP/roIwteBetj63HhuavpcXn11Vfx7bffYvHixThz5gw2bNiAiIiIett34MABTJs2Dc8++yzi4+MxefJkTJ48GYmJiRX7FBYWYsSIEVi0aFEjXgHrYg/xAIDo6Ohqf39r1qyx8JWwLrYeFyEEJk+ejAsXLuDnn39GfHw8AgMDMXbs2Gq/g61pqbi89tpr1d7PmZmZ6N27Nx599NF628e+l23FAmDfy9riwr5X0+KSmpqKSZMmYcyYMUhISMCWLVtw7dq1Wo9Tlc32vQQJIYTIyckRAMTu3buFEELcvHlTaLVaERsbW7FPUlKSACDi4uJqPH7hwoViwIABdR7/T3/6k5g+fbpYunSp8PDwaLA9ERERYu7cuRU/6/V6ERAQID744AMhhBBbtmwRKpVK5OXlVexz8+ZNoSiK2Lp1a4PHtxV3Ky6pqakCgIiPj7eoPQ3FpaqdO3cKAOLGjRsWPYe1ssVY3HPPPeLVV1+16Li2xtbiwnNX0+Jy+vRpodFoxJkzZyxqz9SpU8WDDz5YbVtkZKR44YUXauzb2NhbM1uMx4wZM8SkSZMsOq6tsbW4nD17VgAQiYmJFffr9Xrh4+MjvvnmG4uey5rd7T6xSUJCggAg9uzZU+9+7HvZVizY95KsKS7sezUtLrGxsUKj0Qi9Xl+xbcOGDUJRFFFaWlpne2y179VmRxDdKS8vDwDg5eUFADh27BjKysowduzYin169uyJLl26IC4uzqJj79ixA7Gxsfjiiy/M2r+0tBTHjh2r9twqlQpjx46teO6SkhIoigKdTlexj6OjI1QqFfbt22dR+6zZ3YwLAMTExKBDhw4YMWIENmzYUO++5sTFntlqLFatWgVvb2/07dsXb775JoqKiixumzWztbjw3NW0uGzcuBHBwcHYtGkTgoKC0LVrVzz33HO4fv16vY+Li4ur9twAMH78+DZx7gJsNx67du1Chw4d0KNHD8yZMwe5ublmt80W2FpcSkpKAMhzlolKpYJOp+P5qxG+/fZbhIaG1ju6nn0v24wF+17WFRf2vZoWl/DwcKhUKixduhR6vR55eXlYsWIFxo4dC61WW+fjbLXvxQQRAIPBgHnz5mH48OHo27cvACArKwsODg416gX5+voiKyvL7GPn5uZi5syZWLZsGdzd3c16zLVr16DX6+Hr61vncw8dOhQuLi5YsGABioqKUFhYiNdeew16vR6ZmZlmt8+a3c24uLq64uOPP0ZsbCx++eUXjBgxApMnT673A7A5cbFXthqLJ554AitXrsTOnTvx5ptvYsWKFZg+fbrZbbN2thgXnrs8q+1raVwuXLiAtLQ0xMbGYvny5Vi2bBmOHTuGRx55pN7HZWVltclzF2C78YiOjsby5cuxfft2LFq0CLt378aECROg1+vNbp81s8W4mD5YvPnmm7hx4wZKS0uxaNEiXLlyhecvCxUXF2PVqlV49tln692PfS/biwX7XpWsJS7se3lW29fSuAQFBeHf//433nrrLeh0Onh6euLKlStYt25dvY+z1b4XE0QA5s6di8TERKxdu7bZjz179mw88cQTGDVqVK337927F66urhVfq1atMuu4Pj4+iI2NxcaNG+Hq6goPDw/cvHkTYWFhUKnsI6x3My7e3t6YP38+IiMjMWTIEHz44YeYPn06/va3vwFofFzsla3G4vnnn8f48ePRr18/PPnkk1i+fDnWr1+PlJSUZv89WoMtxoXnrqYxGAwoKSnB8uXLMXLkSIwePRr//Oc/sXPnTpw9exaXLl2qFpf//u//bvY22Bpbjcfjjz+OmJgY9OvXD5MnT8amTZtw5MgR7Nq1q9l/j9Zgi3HRarX48ccfce7cOXh5ecHZ2Rk7d+7EhAkTeP6y0Pr161FQUIAZM2ZUbGPfqzpbjQX7Xs2jOePCvlfTZGVlYfbs2ZgxYwaOHDmC3bt3w8HBAY888giEEHbX99K0dgNa20svvYRNmzZhz5496NSpU8V2Pz8/lJaW4ubNm9WyjtnZ2fDz8zP7+Dt27MCGDRuwePFiALLAocFggEajwddff41p06ZVq1bu6+sLnU4HtVpdo8L6nc89btw4pKSk4Nq1a9BoNPD09ISfnx+Cg4MtfBWsz92OS20iIyOxdetWAMDgwYMbHRd7Y0+xiIyMBCBXFOzWrVuT2tjabDkuPHd5Vmy3NC7+/v7QaDQIDQ2t2NarVy8AwKVLl3DvvfdWi4tpmLWfn1+bO3cB9hWP4OBgeHt7Izk5Gffdd5/ZbbRGthyX8PBwJCQkIC8vD6WlpfDx8UFkZCQGDx5sdvusVUv+X/n222/xu9/9rtrVdfa9KtlTLNj3so64sO/lWbHd0rh88cUX8PDwqFZsfeXKlejcuTMOHTpUIy623veyj5RhIwgh8NJLL2H9+vXYsWMHgoKCqt0fHh4OrVaL7du3V2wzXXWKiooy+3ni4uKQkJBQ8fX+++/Dzc0NCQkJmDJlCpycnBASElLx5ebmBgcHB4SHh1d7boPBgO3bt9f63N7e3vD09MSOHTuQk5ODmJiYRrwi1qGl4lKbhIQE+Pv7A0CzxMXW2WMsTCdv07FtkT3Fhecuy+MyfPhwlJeXV7sSe+7cOQBAYGAgNBpNtbiYOilRUVHVnhsAtm7dapfnLsA+43HlyhXk5uby/GWGloiLh4cHfHx8cP78eRw9ehSTJk0yu33WpqX/r6SmpmLnzp01ps6w72WfsWDfy7riwr6X5XEpKiqqMdJKrVYDQMXAD7vqe7VKaWwrMGfOHOHh4SF27dolMjMzK76Kiooq9nnxxRdFly5dxI4dO8TRo0dFVFSUiIqKqnac8+fPi/j4ePHCCy+I0NBQER8fL+Lj40VJSUmtz2vuKmZr164VOp1OLFu2TJw+fVo8//zzwtPTU2RlZVXs891334m4uDiRnJwsVqxYIby8vMT8+fMb94JYiZaKy7Jly8Tq1atFUlKSSEpKEn/961+FSqUS3333Xb3tMycumZmZIj4+XnzzzTcVKw/Ex8eL3NzcZnyl7j5bj0VycrJ4//33xdGjR0Vqaqr4+eefRXBwsBg1alQzv1Ity9bjIgTPXU2Ji16vF2FhYWLUqFHi+PHj4ujRoyIyMlLcf//99bZv//79QqPRiMWLF4ukpCSxcOFCodVqxcmTJyv2yc3NFfHx8eKXX34RAMTatWtFfHy8yMzMbMZXqmXYejwKCgrEa6+9JuLi4kRqaqrYtm2bCAsLE927dxfFxcXN/Gq1HFuPixBCrFu3TuzcuVOkpKSIn376SQQGBoqHHnqoGV+lltfSfeK3335bBAQEiPLycrPax76X7cSCfS/rjIsQ7Hs1JS7bt28XiqKI9957T5w7d04cO3ZMjB8/XgQGBlZ7rjvZat+rzSaIANT6tXTp0op9bt++LX7/+9+Ldu3aCWdnZzFlypQawbrnnntqPU5qamqtz2tugkgIIf7xj3+ILl26CAcHBxERESEOHjxY7f4FCxYIX19fodVqRffu3cXHH38sDAaDJS+D1WmpuCxbtkz06tVLODs7C3d3dxEREVFtCcT6NBSXhQsXNvg72AJbj8WlS5fEqFGjhJeXl9DpdCIkJES8/vrr1Zb4tEW2HhcheO5q6v+U9PR08dBDDwlXV1fh6+srZs6cadaHoHXr1onQ0FDh4OAg+vTpI3755Zdq9y9durTW5164cGFTXppWYevxKCoqEuPGjRM+Pj5Cq9WKwMBAMXv27GqdfVtk63ERQohPP/1UdOrUSWi1WtGlSxfx9ttv13lR0Fa0ZFz0er3o1KmTeOuttyxqI/teSyv2seZYsO9lnXERgn2vpsZlzZo1YtCgQcLFxUX4+PiImJgYkZSU1GAbbbHvpQghBIiIiIiIiIiIqM1qszWIiIiIiIiIiIhIYoKIiIiIiIiIiKiNY4KIiIiIiIiIiKiNY4KIiIiIiIiIiKiNY4KIiIiIiIiIiKiNY4KIiIiIiIiIiKiNY4KIiIiIiIiIiKiNY4KIiIiIiIiIiKiNY4KIiIiIiIiIiKiNY4KIiIiIiIiIiKiNY4KIiIiIiIiIiKiNY4KIiIiIiIiIiKiN+/8KTKl0GfBX8wAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 1400x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(df[-len(y_val):].index, y_val.cpu(), label=\"actual\", marker=\"o\")\n",
        "plt.plot(df[-len(y_val):].index, val_predict_GRU.detach().cpu(), label=\"predicted\", marker=\"o\")\n",
        "plt.title(\"Electric production IP prediction by GRU RNN model\", fontsize=25)\n",
        "plt.ylabel(\"ylabel\")\n",
        "plt.legend(title_fontsize=14, fontsize=13, fancybox=True, shadow=True, frameon=True)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDMYeUTH5Ki7"
      },
      "source": [
        "---\n",
        "---\n",
        "## 6 LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1qXDXQ9VfIW"
      },
      "source": [
        "---\n",
        "### 6.1 Define single RNN cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "bVP9ju7m39TL"
      },
      "outputs": [],
      "source": [
        "class LSTMCell(torch.nn.Module):\n",
        "    def __init__(self, input_length=10, hidden_size=20, bias=True):\n",
        "        super(LSTMCell, self).__init__()\n",
        "        self.input_length = input_length\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bias = bias\n",
        "\n",
        "        self.w_f = nn.Parameter(torch.Tensor(hidden_size, input_length))\n",
        "        self.u_f = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.w_i = nn.Parameter(torch.Tensor(hidden_size, input_length))\n",
        "        self.u_i = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.w_o = nn.Parameter(torch.Tensor(hidden_size, input_length))\n",
        "        self.u_o = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.w_c = nn.Parameter(torch.Tensor(hidden_size, input_length))\n",
        "        self.u_c = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "\n",
        "        if self.bias:\n",
        "            self.b_f = nn.Parameter(torch.Tensor(hidden_size))\n",
        "            self.b_i = nn.Parameter(torch.Tensor(hidden_size))\n",
        "            self.b_o = nn.Parameter(torch.Tensor(hidden_size))\n",
        "            self.b_c = nn.Parameter(torch.Tensor(hidden_size))\n",
        "        else:\n",
        "            self.b_f = 0\n",
        "            self.b_i = 0\n",
        "            self.b_o = 0\n",
        "            self.b_c = 0\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        std = 1.0 / np.sqrt(self.hidden_size)\n",
        "        for w in self.parameters():\n",
        "            w.data.uniform_(-std, std)\n",
        "\n",
        "\n",
        "    def forget(self, x, h):\n",
        "        fg = torch.sigmoid(x @ self.w_f.T + h @ self.u_f.T + self.b_f)\n",
        "        return fg\n",
        "\n",
        "\n",
        "    def input_gate(self, x, h):\n",
        "        ig = torch.sigmoid(x @ self.w_i.T + h @ self.u_i.T + self.b_i)\n",
        "        return ig\n",
        "\n",
        "\n",
        "    def cell_memory_gate(self, i, f, x, h, c_prev):\n",
        "        gates = torch.tanh(x @ self.w_c.T + h @ self.u_c.T + self.b_c)\n",
        "        c_next = i * gates + f * c_prev\n",
        "        return c_next\n",
        "\n",
        "\n",
        "    def output_gate(self, x, h):\n",
        "        og = torch.sigmoid(x @ self.w_o.T + h @ self.u_o.T + self.b_o)\n",
        "        return og\n",
        "\n",
        "\n",
        "    def forward(self, x, hx=None):\n",
        "        if hx is None:\n",
        "            hx = (torch.zeros(x.size(0), self.hidden_size, dtype=x.dtype, device=x.device),\n",
        "                  torch.zeros(x.size(0), self.hidden_size, dtype=x.dtype, device=x.device))\n",
        "\n",
        "        h_prev, c_prev = hx\n",
        "\n",
        "        fg = self.forget(x, h_prev)\n",
        "        ig = self.input_gate(x, h_prev)\n",
        "        og = self.output_gate(x, h_prev)\n",
        "\n",
        "        c_next = self.cell_memory_gate(ig, fg, x, h_prev, c_prev)\n",
        "\n",
        "        h_next = og * torch.tanh(c_next)\n",
        "        return h_next, c_next"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQpH0TuoVpVf"
      },
      "source": [
        "---\n",
        "### 6.2 LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "PshO9vv35REY"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, bias, output_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bias = bias\n",
        "        self.num_layers = num_layers\n",
        "        self.output_size = output_size\n",
        "\n",
        "        cells = []\n",
        "        for i in range(num_layers):\n",
        "            if i == 0:\n",
        "                cells.append(LSTMCell(input_size, hidden_size, bias))\n",
        "            else:\n",
        "                cells.append(LSTMCell(hidden_size, hidden_size, bias))\n",
        "        self.rnn_cell_list = nn.ModuleList(cells)\n",
        "        self.fc = nn.Linear(hidden_size, output_size, bias)\n",
        "\n",
        "    def forward(self, input, hx=None):\n",
        "        batch_size, sequence, _ = input.size()\n",
        "        if hx is None:\n",
        "            hx = [(torch.zeros(batch_size, self.hidden_size, dtype=input.dtype, device=input.device),\n",
        "                   torch.zeros(batch_size, self.hidden_size, dtype=input.dtype, device=input.device))\n",
        "                  for _ in range(self.num_layers)]\n",
        "\n",
        "        h_t, c_t = list(zip(*hx))\n",
        "        h_t = list(h_t)\n",
        "        c_t = list(c_t)\n",
        "\n",
        "        out = []\n",
        "        for s in range(sequence):\n",
        "            next_input = input[:,s]  \n",
        "            for layer in range(self.num_layers):\n",
        "                h_t[layer], c_t[layer] = self.rnn_cell_list[layer](next_input, (h_t[layer], c_t[layer]))\n",
        "                next_input = h_t[layer]\n",
        "            out.append(h_t[-1])\n",
        "\n",
        "        out = torch.stack(out)\n",
        "        out = self.fc(out[-1])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7G5_9S7VxJ0"
      },
      "source": [
        "---\n",
        "### 6.3 Train LSTM model and plot losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQTMa2Xj6maS",
        "outputId": "6039457e-cc99-4bb3-f80b-cf61f17db3fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (rnn_cell_list): ModuleList(\n",
              "    (0): LSTMCell()\n",
              "  )\n",
              "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LSTM_model = LSTM(input_size=1, hidden_size=50, num_layers=1, bias=True, output_size=1)\n",
        "LSTM_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "XuzlCGTrV_xH"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.008\n",
        "n_epochs = 2000\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(LSTM_model.parameters(), lr = learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwpdUK6Z6skV",
        "outputId": "ca3ca71a-1aff-4313-9c79-ece01a35f71f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Validation loss decreased (inf --> 10639.678711).\n",
            "\t Train_Loss: 8027.1577 Val_Loss: 10639.6787  BEST VAL Loss: 10639.6787\n",
            "\n",
            "Epoch 1: Validation loss decreased (10639.678711 --> 10552.540039).\n",
            "\t Train_Loss: 7938.7686 Val_Loss: 10552.5400  BEST VAL Loss: 10552.5400\n",
            "\n",
            "Epoch 2: Validation loss decreased (10552.540039 --> 10495.843750).\n",
            "\t Train_Loss: 7864.7549 Val_Loss: 10495.8438  BEST VAL Loss: 10495.8438\n",
            "\n",
            "Epoch 3: Validation loss decreased (10495.843750 --> 10434.509766).\n",
            "\t Train_Loss: 7811.8325 Val_Loss: 10434.5098  BEST VAL Loss: 10434.5098\n",
            "\n",
            "Epoch 4: Validation loss decreased (10434.509766 --> 10360.820312).\n",
            "\t Train_Loss: 7757.7983 Val_Loss: 10360.8203  BEST VAL Loss: 10360.8203\n",
            "\n",
            "Epoch 5: Validation loss decreased (10360.820312 --> 10300.435547).\n",
            "\t Train_Loss: 7693.7212 Val_Loss: 10300.4355  BEST VAL Loss: 10300.4355\n",
            "\n",
            "Epoch 6: Validation loss decreased (10300.435547 --> 10232.059570).\n",
            "\t Train_Loss: 7638.3613 Val_Loss: 10232.0596  BEST VAL Loss: 10232.0596\n",
            "\n",
            "Epoch 7: Validation loss decreased (10232.059570 --> 10155.877930).\n",
            "\t Train_Loss: 7577.5293 Val_Loss: 10155.8779  BEST VAL Loss: 10155.8779\n",
            "\n",
            "Epoch 8: Validation loss decreased (10155.877930 --> 10094.108398).\n",
            "\t Train_Loss: 7517.8081 Val_Loss: 10094.1084  BEST VAL Loss: 10094.1084\n",
            "\n",
            "Epoch 9: Validation loss decreased (10094.108398 --> 10031.968750).\n",
            "\t Train_Loss: 7466.5620 Val_Loss: 10031.9688  BEST VAL Loss: 10031.9688\n",
            "\n",
            "Epoch 10: Validation loss decreased (10031.968750 --> 9966.922852).\n",
            "\t Train_Loss: 7413.0566 Val_Loss: 9966.9229  BEST VAL Loss: 9966.9229\n",
            "\n",
            "Epoch 11: Validation loss decreased (9966.922852 --> 9902.231445).\n",
            "\t Train_Loss: 7357.6294 Val_Loss: 9902.2314  BEST VAL Loss: 9902.2314\n",
            "\n",
            "Epoch 12: Validation loss decreased (9902.231445 --> 9837.631836).\n",
            "\t Train_Loss: 7301.8022 Val_Loss: 9837.6318  BEST VAL Loss: 9837.6318\n",
            "\n",
            "Epoch 13: Validation loss decreased (9837.631836 --> 9769.752930).\n",
            "\t Train_Loss: 7244.8521 Val_Loss: 9769.7529  BEST VAL Loss: 9769.7529\n",
            "\n",
            "Epoch 14: Validation loss decreased (9769.752930 --> 9695.105469).\n",
            "\t Train_Loss: 7184.7466 Val_Loss: 9695.1055  BEST VAL Loss: 9695.1055\n",
            "\n",
            "Epoch 15: Validation loss decreased (9695.105469 --> 9617.168945).\n",
            "\t Train_Loss: 7120.8823 Val_Loss: 9617.1689  BEST VAL Loss: 9617.1689\n",
            "\n",
            "Epoch 16: Validation loss decreased (9617.168945 --> 9541.885742).\n",
            "\t Train_Loss: 7056.4263 Val_Loss: 9541.8857  BEST VAL Loss: 9541.8857\n",
            "\n",
            "Epoch 17: Validation loss decreased (9541.885742 --> 9469.502930).\n",
            "\t Train_Loss: 6993.9429 Val_Loss: 9469.5029  BEST VAL Loss: 9469.5029\n",
            "\n",
            "Epoch 18: Validation loss decreased (9469.502930 --> 9395.096680).\n",
            "\t Train_Loss: 6932.2231 Val_Loss: 9395.0967  BEST VAL Loss: 9395.0967\n",
            "\n",
            "Epoch 19: Validation loss decreased (9395.096680 --> 9319.906250).\n",
            "\t Train_Loss: 6869.1294 Val_Loss: 9319.9062  BEST VAL Loss: 9319.9062\n",
            "\n",
            "Epoch 20: Validation loss decreased (9319.906250 --> 9247.680664).\n",
            "\t Train_Loss: 6806.6294 Val_Loss: 9247.6807  BEST VAL Loss: 9247.6807\n",
            "\n",
            "Epoch 21: Validation loss decreased (9247.680664 --> 9178.198242).\n",
            "\t Train_Loss: 6746.7534 Val_Loss: 9178.1982  BEST VAL Loss: 9178.1982\n",
            "\n",
            "Epoch 22: Validation loss decreased (9178.198242 --> 9114.179688).\n",
            "\t Train_Loss: 6689.6187 Val_Loss: 9114.1797  BEST VAL Loss: 9114.1797\n",
            "\n",
            "Epoch 23: Validation loss decreased (9114.179688 --> 9053.572266).\n",
            "\t Train_Loss: 6636.1177 Val_Loss: 9053.5723  BEST VAL Loss: 9053.5723\n",
            "\n",
            "Epoch 24: Validation loss decreased (9053.572266 --> 8994.126953).\n",
            "\t Train_Loss: 6584.7495 Val_Loss: 8994.1270  BEST VAL Loss: 8994.1270\n",
            "\n",
            "Epoch 25: Validation loss decreased (8994.126953 --> 8934.346680).\n",
            "\t Train_Loss: 6534.0737 Val_Loss: 8934.3467  BEST VAL Loss: 8934.3467\n",
            "\n",
            "Epoch 26: Validation loss decreased (8934.346680 --> 8874.179688).\n",
            "\t Train_Loss: 6483.2783 Val_Loss: 8874.1797  BEST VAL Loss: 8874.1797\n",
            "\n",
            "Epoch 27: Validation loss decreased (8874.179688 --> 8815.538086).\n",
            "\t Train_Loss: 6432.7759 Val_Loss: 8815.5381  BEST VAL Loss: 8815.5381\n",
            "\n",
            "Epoch 28: Validation loss decreased (8815.538086 --> 8757.476562).\n",
            "\t Train_Loss: 6383.3193 Val_Loss: 8757.4766  BEST VAL Loss: 8757.4766\n",
            "\n",
            "Epoch 29: Validation loss decreased (8757.476562 --> 8699.319336).\n",
            "\t Train_Loss: 6334.0459 Val_Loss: 8699.3193  BEST VAL Loss: 8699.3193\n",
            "\n",
            "Epoch 30: Validation loss decreased (8699.319336 --> 8640.795898).\n",
            "\t Train_Loss: 6284.6001 Val_Loss: 8640.7959  BEST VAL Loss: 8640.7959\n",
            "\n",
            "Epoch 31: Validation loss decreased (8640.795898 --> 8581.364258).\n",
            "\t Train_Loss: 6234.7212 Val_Loss: 8581.3643  BEST VAL Loss: 8581.3643\n",
            "\n",
            "Epoch 32: Validation loss decreased (8581.364258 --> 8520.334961).\n",
            "\t Train_Loss: 6184.0410 Val_Loss: 8520.3350  BEST VAL Loss: 8520.3350\n",
            "\n",
            "Epoch 33: Validation loss decreased (8520.334961 --> 8458.194336).\n",
            "\t Train_Loss: 6132.3916 Val_Loss: 8458.1943  BEST VAL Loss: 8458.1943\n",
            "\n",
            "Epoch 34: Validation loss decreased (8458.194336 --> 8397.250977).\n",
            "\t Train_Loss: 6080.5146 Val_Loss: 8397.2510  BEST VAL Loss: 8397.2510\n",
            "\n",
            "Epoch 35: Validation loss decreased (8397.250977 --> 8338.291992).\n",
            "\t Train_Loss: 6029.7402 Val_Loss: 8338.2920  BEST VAL Loss: 8338.2920\n",
            "\n",
            "Epoch 36: Validation loss decreased (8338.291992 --> 8280.432617).\n",
            "\t Train_Loss: 5980.2969 Val_Loss: 8280.4326  BEST VAL Loss: 8280.4326\n",
            "\n",
            "Epoch 37: Validation loss decreased (8280.432617 --> 8223.111328).\n",
            "\t Train_Loss: 5931.6289 Val_Loss: 8223.1113  BEST VAL Loss: 8223.1113\n",
            "\n",
            "Epoch 38: Validation loss decreased (8223.111328 --> 8166.148438).\n",
            "\t Train_Loss: 5883.3994 Val_Loss: 8166.1484  BEST VAL Loss: 8166.1484\n",
            "\n",
            "Epoch 39: Validation loss decreased (8166.148438 --> 8109.504883).\n",
            "\t Train_Loss: 5835.4941 Val_Loss: 8109.5049  BEST VAL Loss: 8109.5049\n",
            "\n",
            "Epoch 40: Validation loss decreased (8109.504883 --> 8053.171875).\n",
            "\t Train_Loss: 5787.8799 Val_Loss: 8053.1719  BEST VAL Loss: 8053.1719\n",
            "\n",
            "Epoch 41: Validation loss decreased (8053.171875 --> 7997.148438).\n",
            "\t Train_Loss: 5740.5410 Val_Loss: 7997.1484  BEST VAL Loss: 7997.1484\n",
            "\n",
            "Epoch 42: Validation loss decreased (7997.148438 --> 7941.394043).\n",
            "\t Train_Loss: 5693.4395 Val_Loss: 7941.3940  BEST VAL Loss: 7941.3940\n",
            "\n",
            "Epoch 43: Validation loss decreased (7941.394043 --> 7885.675293).\n",
            "\t Train_Loss: 5646.4546 Val_Loss: 7885.6753  BEST VAL Loss: 7885.6753\n",
            "\n",
            "Epoch 44: Validation loss decreased (7885.675293 --> 7829.395508).\n",
            "\t Train_Loss: 5599.3086 Val_Loss: 7829.3955  BEST VAL Loss: 7829.3955\n",
            "\n",
            "Epoch 45: Validation loss decreased (7829.395508 --> 7771.606445).\n",
            "\t Train_Loss: 5551.4565 Val_Loss: 7771.6064  BEST VAL Loss: 7771.6064\n",
            "\n",
            "Epoch 46: Validation loss decreased (7771.606445 --> 7710.287598).\n",
            "\t Train_Loss: 5501.9316 Val_Loss: 7710.2876  BEST VAL Loss: 7710.2876\n",
            "\n",
            "Epoch 47: Validation loss decreased (7710.287598 --> 7646.106445).\n",
            "\t Train_Loss: 5450.1094 Val_Loss: 7646.1064  BEST VAL Loss: 7646.1064\n",
            "\n",
            "Epoch 48: Validation loss decreased (7646.106445 --> 7586.799316).\n",
            "\t Train_Loss: 5398.3037 Val_Loss: 7586.7993  BEST VAL Loss: 7586.7993\n",
            "\n",
            "Epoch 49: Validation loss decreased (7586.799316 --> 7530.715820).\n",
            "\t Train_Loss: 5350.0688 Val_Loss: 7530.7158  BEST VAL Loss: 7530.7158\n",
            "\n",
            "Epoch 50: Validation loss decreased (7530.715820 --> 7475.402344).\n",
            "\t Train_Loss: 5303.4121 Val_Loss: 7475.4023  BEST VAL Loss: 7475.4023\n",
            "\n",
            "Epoch 51: Validation loss decreased (7475.402344 --> 7420.508789).\n",
            "\t Train_Loss: 5257.3203 Val_Loss: 7420.5088  BEST VAL Loss: 7420.5088\n",
            "\n",
            "Epoch 52: Validation loss decreased (7420.508789 --> 7365.950684).\n",
            "\t Train_Loss: 5211.5850 Val_Loss: 7365.9507  BEST VAL Loss: 7365.9507\n",
            "\n",
            "Epoch 53: Validation loss decreased (7365.950684 --> 7311.709473).\n",
            "\t Train_Loss: 5166.1431 Val_Loss: 7311.7095  BEST VAL Loss: 7311.7095\n",
            "\n",
            "Epoch 54: Validation loss decreased (7311.709473 --> 7257.751465).\n",
            "\t Train_Loss: 5120.9531 Val_Loss: 7257.7515  BEST VAL Loss: 7257.7515\n",
            "\n",
            "Epoch 55: Validation loss decreased (7257.751465 --> 7203.964355).\n",
            "\t Train_Loss: 5075.9219 Val_Loss: 7203.9644  BEST VAL Loss: 7203.9644\n",
            "\n",
            "Epoch 56: Validation loss decreased (7203.964355 --> 7149.987793).\n",
            "\t Train_Loss: 5030.8330 Val_Loss: 7149.9878  BEST VAL Loss: 7149.9878\n",
            "\n",
            "Epoch 57: Validation loss decreased (7149.987793 --> 7094.938965).\n",
            "\t Train_Loss: 4985.2803 Val_Loss: 7094.9390  BEST VAL Loss: 7094.9390\n",
            "\n",
            "Epoch 58: Validation loss decreased (7094.938965 --> 7037.780762).\n",
            "\t Train_Loss: 4938.7832 Val_Loss: 7037.7808  BEST VAL Loss: 7037.7808\n",
            "\n",
            "Epoch 59: Validation loss decreased (7037.780762 --> 6979.850098).\n",
            "\t Train_Loss: 4891.4414 Val_Loss: 6979.8501  BEST VAL Loss: 6979.8501\n",
            "\n",
            "Epoch 60: Validation loss decreased (6979.850098 --> 6924.429199).\n",
            "\t Train_Loss: 4844.6733 Val_Loss: 6924.4292  BEST VAL Loss: 6924.4292\n",
            "\n",
            "Epoch 61: Validation loss decreased (6924.429199 --> 6870.940918).\n",
            "\t Train_Loss: 4799.6001 Val_Loss: 6870.9409  BEST VAL Loss: 6870.9409\n",
            "\n",
            "Epoch 62: Validation loss decreased (6870.940918 --> 6817.867188).\n",
            "\t Train_Loss: 4755.4526 Val_Loss: 6817.8672  BEST VAL Loss: 6817.8672\n",
            "\n",
            "Epoch 63: Validation loss decreased (6817.867188 --> 6764.112793).\n",
            "\t Train_Loss: 4711.3242 Val_Loss: 6764.1128  BEST VAL Loss: 6764.1128\n",
            "\n",
            "Epoch 64: Validation loss decreased (6764.112793 --> 6708.657715).\n",
            "\t Train_Loss: 4666.4858 Val_Loss: 6708.6577  BEST VAL Loss: 6708.6577\n",
            "\n",
            "Epoch 65: Validation loss decreased (6708.657715 --> 6654.676758).\n",
            "\t Train_Loss: 4621.0288 Val_Loss: 6654.6768  BEST VAL Loss: 6654.6768\n",
            "\n",
            "Epoch 66: Validation loss decreased (6654.676758 --> 6601.856934).\n",
            "\t Train_Loss: 4577.1401 Val_Loss: 6601.8569  BEST VAL Loss: 6601.8569\n",
            "\n",
            "Epoch 67: Validation loss decreased (6601.856934 --> 6547.936035).\n",
            "\t Train_Loss: 4533.5132 Val_Loss: 6547.9360  BEST VAL Loss: 6547.9360\n",
            "\n",
            "Epoch 68: Validation loss decreased (6547.936035 --> 6494.233887).\n",
            "\t Train_Loss: 4489.2739 Val_Loss: 6494.2339  BEST VAL Loss: 6494.2339\n",
            "\n",
            "Epoch 69: Validation loss decreased (6494.233887 --> 6442.362793).\n",
            "\t Train_Loss: 4445.4810 Val_Loss: 6442.3628  BEST VAL Loss: 6442.3628\n",
            "\n",
            "Epoch 70: Validation loss decreased (6442.362793 --> 6391.575684).\n",
            "\t Train_Loss: 4403.0176 Val_Loss: 6391.5757  BEST VAL Loss: 6391.5757\n",
            "\n",
            "Epoch 71: Validation loss decreased (6391.575684 --> 6341.245605).\n",
            "\t Train_Loss: 4361.3784 Val_Loss: 6341.2456  BEST VAL Loss: 6341.2456\n",
            "\n",
            "Epoch 72: Validation loss decreased (6341.245605 --> 6291.233887).\n",
            "\t Train_Loss: 4320.1426 Val_Loss: 6291.2339  BEST VAL Loss: 6291.2339\n",
            "\n",
            "Epoch 73: Validation loss decreased (6291.233887 --> 6241.524414).\n",
            "\t Train_Loss: 4279.2080 Val_Loss: 6241.5244  BEST VAL Loss: 6241.5244\n",
            "\n",
            "Epoch 74: Validation loss decreased (6241.524414 --> 6192.124512).\n",
            "\t Train_Loss: 4238.5596 Val_Loss: 6192.1245  BEST VAL Loss: 6192.1245\n",
            "\n",
            "Epoch 75: Validation loss decreased (6192.124512 --> 6143.045410).\n",
            "\t Train_Loss: 4198.2012 Val_Loss: 6143.0454  BEST VAL Loss: 6143.0454\n",
            "\n",
            "Epoch 76: Validation loss decreased (6143.045410 --> 6094.294922).\n",
            "\t Train_Loss: 4158.1406 Val_Loss: 6094.2949  BEST VAL Loss: 6094.2949\n",
            "\n",
            "Epoch 77: Validation loss decreased (6094.294922 --> 6045.880371).\n",
            "\t Train_Loss: 4118.3848 Val_Loss: 6045.8804  BEST VAL Loss: 6045.8804\n",
            "\n",
            "Epoch 78: Validation loss decreased (6045.880371 --> 5997.806152).\n",
            "\t Train_Loss: 4078.9380 Val_Loss: 5997.8062  BEST VAL Loss: 5997.8062\n",
            "\n",
            "Epoch 79: Validation loss decreased (5997.806152 --> 5950.079102).\n",
            "\t Train_Loss: 4039.8047 Val_Loss: 5950.0791  BEST VAL Loss: 5950.0791\n",
            "\n",
            "Epoch 80: Validation loss decreased (5950.079102 --> 5902.700684).\n",
            "\t Train_Loss: 4000.9871 Val_Loss: 5902.7007  BEST VAL Loss: 5902.7007\n",
            "\n",
            "Epoch 81: Validation loss decreased (5902.700684 --> 5855.671387).\n",
            "\t Train_Loss: 3962.4878 Val_Loss: 5855.6714  BEST VAL Loss: 5855.6714\n",
            "\n",
            "Epoch 82: Validation loss decreased (5855.671387 --> 5808.994141).\n",
            "\t Train_Loss: 3924.3052 Val_Loss: 5808.9941  BEST VAL Loss: 5808.9941\n",
            "\n",
            "Epoch 83: Validation loss decreased (5808.994141 --> 5762.666016).\n",
            "\t Train_Loss: 3886.4390 Val_Loss: 5762.6660  BEST VAL Loss: 5762.6660\n",
            "\n",
            "Epoch 84: Validation loss decreased (5762.666016 --> 5716.684082).\n",
            "\t Train_Loss: 3848.8845 Val_Loss: 5716.6841  BEST VAL Loss: 5716.6841\n",
            "\n",
            "Epoch 85: Validation loss decreased (5716.684082 --> 5671.041504).\n",
            "\t Train_Loss: 3811.6353 Val_Loss: 5671.0415  BEST VAL Loss: 5671.0415\n",
            "\n",
            "Epoch 86: Validation loss decreased (5671.041504 --> 5625.723633).\n",
            "\t Train_Loss: 3774.6797 Val_Loss: 5625.7236  BEST VAL Loss: 5625.7236\n",
            "\n",
            "Epoch 87: Validation loss decreased (5625.723633 --> 5580.698730).\n",
            "\t Train_Loss: 3737.9954 Val_Loss: 5580.6987  BEST VAL Loss: 5580.6987\n",
            "\n",
            "Epoch 88: Validation loss decreased (5580.698730 --> 5535.903320).\n",
            "\t Train_Loss: 3701.5449 Val_Loss: 5535.9033  BEST VAL Loss: 5535.9033\n",
            "\n",
            "Epoch 89: Validation loss decreased (5535.903320 --> 5491.197266).\n",
            "\t Train_Loss: 3665.2629 Val_Loss: 5491.1973  BEST VAL Loss: 5491.1973\n",
            "\n",
            "Epoch 90: Validation loss decreased (5491.197266 --> 5446.318359).\n",
            "\t Train_Loss: 3629.0413 Val_Loss: 5446.3184  BEST VAL Loss: 5446.3184\n",
            "\n",
            "Epoch 91: Validation loss decreased (5446.318359 --> 5400.979980).\n",
            "\t Train_Loss: 3592.7441 Val_Loss: 5400.9800  BEST VAL Loss: 5400.9800\n",
            "\n",
            "Epoch 92: Validation loss decreased (5400.979980 --> 5355.358887).\n",
            "\t Train_Loss: 3556.3220 Val_Loss: 5355.3589  BEST VAL Loss: 5355.3589\n",
            "\n",
            "Epoch 93: Validation loss decreased (5355.358887 --> 5310.196777).\n",
            "\t Train_Loss: 3520.0024 Val_Loss: 5310.1968  BEST VAL Loss: 5310.1968\n",
            "\n",
            "Epoch 94: Validation loss decreased (5310.196777 --> 5265.774414).\n",
            "\t Train_Loss: 3484.1106 Val_Loss: 5265.7744  BEST VAL Loss: 5265.7744\n",
            "\n",
            "Epoch 95: Validation loss decreased (5265.774414 --> 5221.903809).\n",
            "\t Train_Loss: 3448.6887 Val_Loss: 5221.9038  BEST VAL Loss: 5221.9038\n",
            "\n",
            "Epoch 96: Validation loss decreased (5221.903809 --> 5178.433594).\n",
            "\t Train_Loss: 3413.6411 Val_Loss: 5178.4336  BEST VAL Loss: 5178.4336\n",
            "\n",
            "Epoch 97: Validation loss decreased (5178.433594 --> 5135.308105).\n",
            "\t Train_Loss: 3378.9097 Val_Loss: 5135.3081  BEST VAL Loss: 5135.3081\n",
            "\n",
            "Epoch 98: Validation loss decreased (5135.308105 --> 5092.509766).\n",
            "\t Train_Loss: 3344.4756 Val_Loss: 5092.5098  BEST VAL Loss: 5092.5098\n",
            "\n",
            "Epoch 99: Validation loss decreased (5092.509766 --> 5050.037598).\n",
            "\t Train_Loss: 3310.3347 Val_Loss: 5050.0376  BEST VAL Loss: 5050.0376\n",
            "\n",
            "Epoch 100: Validation loss decreased (5050.037598 --> 5007.894043).\n",
            "\t Train_Loss: 3276.4888 Val_Loss: 5007.8940  BEST VAL Loss: 5007.8940\n",
            "\n",
            "Epoch 101: Validation loss decreased (5007.894043 --> 4966.079590).\n",
            "\t Train_Loss: 3242.9392 Val_Loss: 4966.0796  BEST VAL Loss: 4966.0796\n",
            "\n",
            "Epoch 102: Validation loss decreased (4966.079590 --> 4924.597656).\n",
            "\t Train_Loss: 3209.6882 Val_Loss: 4924.5977  BEST VAL Loss: 4924.5977\n",
            "\n",
            "Epoch 103: Validation loss decreased (4924.597656 --> 4883.449707).\n",
            "\t Train_Loss: 3176.7366 Val_Loss: 4883.4497  BEST VAL Loss: 4883.4497\n",
            "\n",
            "Epoch 104: Validation loss decreased (4883.449707 --> 4842.634277).\n",
            "\t Train_Loss: 3144.0852 Val_Loss: 4842.6343  BEST VAL Loss: 4842.6343\n",
            "\n",
            "Epoch 105: Validation loss decreased (4842.634277 --> 4802.152832).\n",
            "\t Train_Loss: 3111.7329 Val_Loss: 4802.1528  BEST VAL Loss: 4802.1528\n",
            "\n",
            "Epoch 106: Validation loss decreased (4802.152832 --> 4762.002441).\n",
            "\t Train_Loss: 3079.6775 Val_Loss: 4762.0024  BEST VAL Loss: 4762.0024\n",
            "\n",
            "Epoch 107: Validation loss decreased (4762.002441 --> 4722.180664).\n",
            "\t Train_Loss: 3047.9165 Val_Loss: 4722.1807  BEST VAL Loss: 4722.1807\n",
            "\n",
            "Epoch 108: Validation loss decreased (4722.180664 --> 4682.680664).\n",
            "\t Train_Loss: 3016.4424 Val_Loss: 4682.6807  BEST VAL Loss: 4682.6807\n",
            "\n",
            "Epoch 109: Validation loss decreased (4682.680664 --> 4643.487793).\n",
            "\t Train_Loss: 2985.2427 Val_Loss: 4643.4878  BEST VAL Loss: 4643.4878\n",
            "\n",
            "Epoch 110: Validation loss decreased (4643.487793 --> 4604.558594).\n",
            "\t Train_Loss: 2954.2842 Val_Loss: 4604.5586  BEST VAL Loss: 4604.5586\n",
            "\n",
            "Epoch 111: Validation loss decreased (4604.558594 --> 4565.723145).\n",
            "\t Train_Loss: 2923.4734 Val_Loss: 4565.7231  BEST VAL Loss: 4565.7231\n",
            "\n",
            "Epoch 112: Validation loss decreased (4565.723145 --> 4526.153320).\n",
            "\t Train_Loss: 2892.4946 Val_Loss: 4526.1533  BEST VAL Loss: 4526.1533\n",
            "\n",
            "Epoch 113: Validation loss decreased (4526.153320 --> 4482.522949).\n",
            "\t Train_Loss: 2860.1550 Val_Loss: 4482.5229  BEST VAL Loss: 4482.5229\n",
            "\n",
            "Epoch 114: Validation loss decreased (4482.522949 --> 4432.880859).\n",
            "\t Train_Loss: 2823.8306 Val_Loss: 4432.8809  BEST VAL Loss: 4432.8809\n",
            "\n",
            "Epoch 115: Validation loss decreased (4432.880859 --> 4387.040527).\n",
            "\t Train_Loss: 2786.0447 Val_Loss: 4387.0405  BEST VAL Loss: 4387.0405\n",
            "\n",
            "Epoch 116: Validation loss decreased (4387.040527 --> 4347.923340).\n",
            "\t Train_Loss: 2752.4363 Val_Loss: 4347.9233  BEST VAL Loss: 4347.9233\n",
            "\n",
            "Epoch 117: Validation loss decreased (4347.923340 --> 4310.562012).\n",
            "\t Train_Loss: 2722.7126 Val_Loss: 4310.5620  BEST VAL Loss: 4310.5620\n",
            "\n",
            "Epoch 118: Validation loss decreased (4310.562012 --> 4273.489746).\n",
            "\t Train_Loss: 2693.5637 Val_Loss: 4273.4897  BEST VAL Loss: 4273.4897\n",
            "\n",
            "Epoch 119: Validation loss decreased (4273.489746 --> 4236.707520).\n",
            "\t Train_Loss: 2664.6660 Val_Loss: 4236.7075  BEST VAL Loss: 4236.7075\n",
            "\n",
            "Epoch 120: Validation loss decreased (4236.707520 --> 4200.205566).\n",
            "\t Train_Loss: 2636.0051 Val_Loss: 4200.2056  BEST VAL Loss: 4200.2056\n",
            "\n",
            "Epoch 121: Validation loss decreased (4200.205566 --> 4163.935059).\n",
            "\t Train_Loss: 2607.5378 Val_Loss: 4163.9351  BEST VAL Loss: 4163.9351\n",
            "\n",
            "Epoch 122: Validation loss decreased (4163.935059 --> 4127.615234).\n",
            "\t Train_Loss: 2579.1038 Val_Loss: 4127.6152  BEST VAL Loss: 4127.6152\n",
            "\n",
            "Epoch 123: Validation loss decreased (4127.615234 --> 4090.426514).\n",
            "\t Train_Loss: 2550.2009 Val_Loss: 4090.4265  BEST VAL Loss: 4090.4265\n",
            "\n",
            "Epoch 124: Validation loss decreased (4090.426514 --> 4052.402344).\n",
            "\t Train_Loss: 2521.0400 Val_Loss: 4052.4023  BEST VAL Loss: 4052.4023\n",
            "\n",
            "Epoch 125: Validation loss decreased (4052.402344 --> 4015.437256).\n",
            "\t Train_Loss: 2492.6042 Val_Loss: 4015.4373  BEST VAL Loss: 4015.4373\n",
            "\n",
            "Epoch 126: Validation loss decreased (4015.437256 --> 3979.450928).\n",
            "\t Train_Loss: 2464.7361 Val_Loss: 3979.4509  BEST VAL Loss: 3979.4509\n",
            "\n",
            "Epoch 127: Validation loss decreased (3979.450928 --> 3943.662842).\n",
            "\t Train_Loss: 2437.0583 Val_Loss: 3943.6628  BEST VAL Loss: 3943.6628\n",
            "\n",
            "Epoch 128: Validation loss decreased (3943.662842 --> 3908.044922).\n",
            "\t Train_Loss: 2409.5178 Val_Loss: 3908.0449  BEST VAL Loss: 3908.0449\n",
            "\n",
            "Epoch 129: Validation loss decreased (3908.044922 --> 3872.626221).\n",
            "\t Train_Loss: 2382.1460 Val_Loss: 3872.6262  BEST VAL Loss: 3872.6262\n",
            "\n",
            "Epoch 130: Validation loss decreased (3872.626221 --> 3837.432373).\n",
            "\t Train_Loss: 2354.9658 Val_Loss: 3837.4324  BEST VAL Loss: 3837.4324\n",
            "\n",
            "Epoch 131: Validation loss decreased (3837.432373 --> 3802.482910).\n",
            "\t Train_Loss: 2327.9971 Val_Loss: 3802.4829  BEST VAL Loss: 3802.4829\n",
            "\n",
            "Epoch 132: Validation loss decreased (3802.482910 --> 3767.791504).\n",
            "\t Train_Loss: 2301.2532 Val_Loss: 3767.7915  BEST VAL Loss: 3767.7915\n",
            "\n",
            "Epoch 133: Validation loss decreased (3767.791504 --> 3733.370361).\n",
            "\t Train_Loss: 2274.7451 Val_Loss: 3733.3704  BEST VAL Loss: 3733.3704\n",
            "\n",
            "Epoch 134: Validation loss decreased (3733.370361 --> 3699.227295).\n",
            "\t Train_Loss: 2248.4824 Val_Loss: 3699.2273  BEST VAL Loss: 3699.2273\n",
            "\n",
            "Epoch 135: Validation loss decreased (3699.227295 --> 3665.370361).\n",
            "\t Train_Loss: 2222.4705 Val_Loss: 3665.3704  BEST VAL Loss: 3665.3704\n",
            "\n",
            "Epoch 136: Validation loss decreased (3665.370361 --> 3631.804688).\n",
            "\t Train_Loss: 2196.7144 Val_Loss: 3631.8047  BEST VAL Loss: 3631.8047\n",
            "\n",
            "Epoch 137: Validation loss decreased (3631.804688 --> 3598.533203).\n",
            "\t Train_Loss: 2171.2175 Val_Loss: 3598.5332  BEST VAL Loss: 3598.5332\n",
            "\n",
            "Epoch 138: Validation loss decreased (3598.533203 --> 3565.559326).\n",
            "\t Train_Loss: 2145.9827 Val_Loss: 3565.5593  BEST VAL Loss: 3565.5593\n",
            "\n",
            "Epoch 139: Validation loss decreased (3565.559326 --> 3532.884033).\n",
            "\t Train_Loss: 2121.0112 Val_Loss: 3532.8840  BEST VAL Loss: 3532.8840\n",
            "\n",
            "Epoch 140: Validation loss decreased (3532.884033 --> 3500.509766).\n",
            "\t Train_Loss: 2096.3035 Val_Loss: 3500.5098  BEST VAL Loss: 3500.5098\n",
            "\n",
            "Epoch 141: Validation loss decreased (3500.509766 --> 3468.435303).\n",
            "\t Train_Loss: 2071.8611 Val_Loss: 3468.4353  BEST VAL Loss: 3468.4353\n",
            "\n",
            "Epoch 142: Validation loss decreased (3468.435303 --> 3436.661865).\n",
            "\t Train_Loss: 2047.6821 Val_Loss: 3436.6619  BEST VAL Loss: 3436.6619\n",
            "\n",
            "Epoch 143: Validation loss decreased (3436.661865 --> 3405.186279).\n",
            "\t Train_Loss: 2023.7672 Val_Loss: 3405.1863  BEST VAL Loss: 3405.1863\n",
            "\n",
            "Epoch 144: Validation loss decreased (3405.186279 --> 3374.010498).\n",
            "\t Train_Loss: 2000.1141 Val_Loss: 3374.0105  BEST VAL Loss: 3374.0105\n",
            "\n",
            "Epoch 145: Validation loss decreased (3374.010498 --> 3343.131592).\n",
            "\t Train_Loss: 1976.7228 Val_Loss: 3343.1316  BEST VAL Loss: 3343.1316\n",
            "\n",
            "Epoch 146: Validation loss decreased (3343.131592 --> 3312.548584).\n",
            "\t Train_Loss: 1953.5906 Val_Loss: 3312.5486  BEST VAL Loss: 3312.5486\n",
            "\n",
            "Epoch 147: Validation loss decreased (3312.548584 --> 3282.258545).\n",
            "\t Train_Loss: 1930.7167 Val_Loss: 3282.2585  BEST VAL Loss: 3282.2585\n",
            "\n",
            "Epoch 148: Validation loss decreased (3282.258545 --> 3252.260498).\n",
            "\t Train_Loss: 1908.0983 Val_Loss: 3252.2605  BEST VAL Loss: 3252.2605\n",
            "\n",
            "Epoch 149: Validation loss decreased (3252.260498 --> 3222.551758).\n",
            "\t Train_Loss: 1885.7341 Val_Loss: 3222.5518  BEST VAL Loss: 3222.5518\n",
            "\n",
            "Epoch 150: Validation loss decreased (3222.551758 --> 3193.130615).\n",
            "\t Train_Loss: 1863.6215 Val_Loss: 3193.1306  BEST VAL Loss: 3193.1306\n",
            "\n",
            "Epoch 151: Validation loss decreased (3193.130615 --> 3163.994629).\n",
            "\t Train_Loss: 1841.7588 Val_Loss: 3163.9946  BEST VAL Loss: 3163.9946\n",
            "\n",
            "Epoch 152: Validation loss decreased (3163.994629 --> 3135.140381).\n",
            "\t Train_Loss: 1820.1431 Val_Loss: 3135.1404  BEST VAL Loss: 3135.1404\n",
            "\n",
            "Epoch 153: Validation loss decreased (3135.140381 --> 3106.566162).\n",
            "\t Train_Loss: 1798.7722 Val_Loss: 3106.5662  BEST VAL Loss: 3106.5662\n",
            "\n",
            "Epoch 154: Validation loss decreased (3106.566162 --> 3078.269531).\n",
            "\t Train_Loss: 1777.6438 Val_Loss: 3078.2695  BEST VAL Loss: 3078.2695\n",
            "\n",
            "Epoch 155: Validation loss decreased (3078.269531 --> 3050.248047).\n",
            "\t Train_Loss: 1756.7557 Val_Loss: 3050.2480  BEST VAL Loss: 3050.2480\n",
            "\n",
            "Epoch 156: Validation loss decreased (3050.248047 --> 3022.498535).\n",
            "\t Train_Loss: 1736.1051 Val_Loss: 3022.4985  BEST VAL Loss: 3022.4985\n",
            "\n",
            "Epoch 157: Validation loss decreased (3022.498535 --> 2995.018311).\n",
            "\t Train_Loss: 1715.6901 Val_Loss: 2995.0183  BEST VAL Loss: 2995.0183\n",
            "\n",
            "Epoch 158: Validation loss decreased (2995.018311 --> 2967.805908).\n",
            "\t Train_Loss: 1695.5076 Val_Loss: 2967.8059  BEST VAL Loss: 2967.8059\n",
            "\n",
            "Epoch 159: Validation loss decreased (2967.805908 --> 2940.857178).\n",
            "\t Train_Loss: 1675.5554 Val_Loss: 2940.8572  BEST VAL Loss: 2940.8572\n",
            "\n",
            "Epoch 160: Validation loss decreased (2940.857178 --> 2914.169922).\n",
            "\t Train_Loss: 1655.8312 Val_Loss: 2914.1699  BEST VAL Loss: 2914.1699\n",
            "\n",
            "Epoch 161: Validation loss decreased (2914.169922 --> 2887.741943).\n",
            "\t Train_Loss: 1636.3319 Val_Loss: 2887.7419  BEST VAL Loss: 2887.7419\n",
            "\n",
            "Epoch 162: Validation loss decreased (2887.741943 --> 2861.570557).\n",
            "\t Train_Loss: 1617.0553 Val_Loss: 2861.5706  BEST VAL Loss: 2861.5706\n",
            "\n",
            "Epoch 163: Validation loss decreased (2861.570557 --> 2835.648193).\n",
            "\t Train_Loss: 1597.9924 Val_Loss: 2835.6482  BEST VAL Loss: 2835.6482\n",
            "\n",
            "Epoch 164: Validation loss decreased (2835.648193 --> 2809.912842).\n",
            "\t Train_Loss: 1579.1157 Val_Loss: 2809.9128  BEST VAL Loss: 2809.9128\n",
            "\n",
            "Epoch 165: Validation loss decreased (2809.912842 --> 2784.147705).\n",
            "\t Train_Loss: 1560.3209 Val_Loss: 2784.1477  BEST VAL Loss: 2784.1477\n",
            "\n",
            "Epoch 166: Validation loss decreased (2784.147705 --> 2757.223389).\n",
            "\t Train_Loss: 1541.2241 Val_Loss: 2757.2234  BEST VAL Loss: 2757.2234\n",
            "\n",
            "Epoch 167: Validation loss decreased (2757.223389 --> 2726.763672).\n",
            "\t Train_Loss: 1520.8787 Val_Loss: 2726.7637  BEST VAL Loss: 2726.7637\n",
            "\n",
            "Epoch 168: Validation loss decreased (2726.763672 --> 2696.062500).\n",
            "\t Train_Loss: 1498.9071 Val_Loss: 2696.0625  BEST VAL Loss: 2696.0625\n",
            "\n",
            "Epoch 169: Validation loss decreased (2696.062500 --> 2669.160645).\n",
            "\t Train_Loss: 1478.0023 Val_Loss: 2669.1606  BEST VAL Loss: 2669.1606\n",
            "\n",
            "Epoch 170: Validation loss decreased (2669.160645 --> 2643.180664).\n",
            "\t Train_Loss: 1458.9532 Val_Loss: 2643.1807  BEST VAL Loss: 2643.1807\n",
            "\n",
            "Epoch 171: Validation loss decreased (2643.180664 --> 2617.426758).\n",
            "\t Train_Loss: 1440.3580 Val_Loss: 2617.4268  BEST VAL Loss: 2617.4268\n",
            "\n",
            "Epoch 172: Validation loss decreased (2617.426758 --> 2591.850098).\n",
            "\t Train_Loss: 1421.9396 Val_Loss: 2591.8501  BEST VAL Loss: 2591.8501\n",
            "\n",
            "Epoch 173: Validation loss decreased (2591.850098 --> 2566.463623).\n",
            "\t Train_Loss: 1403.6832 Val_Loss: 2566.4636  BEST VAL Loss: 2566.4636\n",
            "\n",
            "Epoch 174: Validation loss decreased (2566.463623 --> 2541.282959).\n",
            "\t Train_Loss: 1385.5996 Val_Loss: 2541.2830  BEST VAL Loss: 2541.2830\n",
            "\n",
            "Epoch 175: Validation loss decreased (2541.282959 --> 2516.321045).\n",
            "\t Train_Loss: 1367.7002 Val_Loss: 2516.3210  BEST VAL Loss: 2516.3210\n",
            "\n",
            "Epoch 176: Validation loss decreased (2516.321045 --> 2491.588379).\n",
            "\t Train_Loss: 1349.9937 Val_Loss: 2491.5884  BEST VAL Loss: 2491.5884\n",
            "\n",
            "Epoch 177: Validation loss decreased (2491.588379 --> 2467.091064).\n",
            "\t Train_Loss: 1332.4863 Val_Loss: 2467.0911  BEST VAL Loss: 2467.0911\n",
            "\n",
            "Epoch 178: Validation loss decreased (2467.091064 --> 2442.835205).\n",
            "\t Train_Loss: 1315.1829 Val_Loss: 2442.8352  BEST VAL Loss: 2442.8352\n",
            "\n",
            "Epoch 179: Validation loss decreased (2442.835205 --> 2418.824707).\n",
            "\t Train_Loss: 1298.0864 Val_Loss: 2418.8247  BEST VAL Loss: 2418.8247\n",
            "\n",
            "Epoch 180: Validation loss decreased (2418.824707 --> 2395.062012).\n",
            "\t Train_Loss: 1281.2000 Val_Loss: 2395.0620  BEST VAL Loss: 2395.0620\n",
            "\n",
            "Epoch 181: Validation loss decreased (2395.062012 --> 2371.549072).\n",
            "\t Train_Loss: 1264.5239 Val_Loss: 2371.5491  BEST VAL Loss: 2371.5491\n",
            "\n",
            "Epoch 182: Validation loss decreased (2371.549072 --> 2348.286133).\n",
            "\t Train_Loss: 1248.0598 Val_Loss: 2348.2861  BEST VAL Loss: 2348.2861\n",
            "\n",
            "Epoch 183: Validation loss decreased (2348.286133 --> 2325.275146).\n",
            "\t Train_Loss: 1231.8069 Val_Loss: 2325.2751  BEST VAL Loss: 2325.2751\n",
            "\n",
            "Epoch 184: Validation loss decreased (2325.275146 --> 2302.514160).\n",
            "\t Train_Loss: 1215.7650 Val_Loss: 2302.5142  BEST VAL Loss: 2302.5142\n",
            "\n",
            "Epoch 185: Validation loss decreased (2302.514160 --> 2280.002441).\n",
            "\t Train_Loss: 1199.9335 Val_Loss: 2280.0024  BEST VAL Loss: 2280.0024\n",
            "\n",
            "Epoch 186: Validation loss decreased (2280.002441 --> 2257.739258).\n",
            "\t Train_Loss: 1184.3110 Val_Loss: 2257.7393  BEST VAL Loss: 2257.7393\n",
            "\n",
            "Epoch 187: Validation loss decreased (2257.739258 --> 2235.722900).\n",
            "\t Train_Loss: 1168.8958 Val_Loss: 2235.7229  BEST VAL Loss: 2235.7229\n",
            "\n",
            "Epoch 188: Validation loss decreased (2235.722900 --> 2213.952637).\n",
            "\t Train_Loss: 1153.6866 Val_Loss: 2213.9526  BEST VAL Loss: 2213.9526\n",
            "\n",
            "Epoch 189: Validation loss decreased (2213.952637 --> 2192.425781).\n",
            "\t Train_Loss: 1138.6816 Val_Loss: 2192.4258  BEST VAL Loss: 2192.4258\n",
            "\n",
            "Epoch 190: Validation loss decreased (2192.425781 --> 2171.139404).\n",
            "\t Train_Loss: 1123.8792 Val_Loss: 2171.1394  BEST VAL Loss: 2171.1394\n",
            "\n",
            "Epoch 191: Validation loss decreased (2171.139404 --> 2150.093018).\n",
            "\t Train_Loss: 1109.2765 Val_Loss: 2150.0930  BEST VAL Loss: 2150.0930\n",
            "\n",
            "Epoch 192: Validation loss decreased (2150.093018 --> 2129.283203).\n",
            "\t Train_Loss: 1094.8718 Val_Loss: 2129.2832  BEST VAL Loss: 2129.2832\n",
            "\n",
            "Epoch 193: Validation loss decreased (2129.283203 --> 2108.707764).\n",
            "\t Train_Loss: 1080.6628 Val_Loss: 2108.7078  BEST VAL Loss: 2108.7078\n",
            "\n",
            "Epoch 194: Validation loss decreased (2108.707764 --> 2088.364502).\n",
            "\t Train_Loss: 1066.6475 Val_Loss: 2088.3645  BEST VAL Loss: 2088.3645\n",
            "\n",
            "Epoch 195: Validation loss decreased (2088.364502 --> 2068.250732).\n",
            "\t Train_Loss: 1052.8232 Val_Loss: 2068.2507  BEST VAL Loss: 2068.2507\n",
            "\n",
            "Epoch 196: Validation loss decreased (2068.250732 --> 2048.364014).\n",
            "\t Train_Loss: 1039.1880 Val_Loss: 2048.3640  BEST VAL Loss: 2048.3640\n",
            "\n",
            "Epoch 197: Validation loss decreased (2048.364014 --> 2028.701050).\n",
            "\t Train_Loss: 1025.7394 Val_Loss: 2028.7010  BEST VAL Loss: 2028.7010\n",
            "\n",
            "Epoch 198: Validation loss decreased (2028.701050 --> 2009.260132).\n",
            "\t Train_Loss: 1012.4745 Val_Loss: 2009.2601  BEST VAL Loss: 2009.2601\n",
            "\n",
            "Epoch 199: Validation loss decreased (2009.260132 --> 1990.038696).\n",
            "\t Train_Loss: 999.3915 Val_Loss: 1990.0387  BEST VAL Loss: 1990.0387\n",
            "\n",
            "Epoch 200: Validation loss decreased (1990.038696 --> 1971.033569).\n",
            "\t Train_Loss: 986.4883 Val_Loss: 1971.0336  BEST VAL Loss: 1971.0336\n",
            "\n",
            "Epoch 201: Validation loss decreased (1971.033569 --> 1952.242554).\n",
            "\t Train_Loss: 973.7621 Val_Loss: 1952.2426  BEST VAL Loss: 1952.2426\n",
            "\n",
            "Epoch 202: Validation loss decreased (1952.242554 --> 1933.663330).\n",
            "\t Train_Loss: 961.2106 Val_Loss: 1933.6633  BEST VAL Loss: 1933.6633\n",
            "\n",
            "Epoch 203: Validation loss decreased (1933.663330 --> 1915.293335).\n",
            "\t Train_Loss: 948.8319 Val_Loss: 1915.2933  BEST VAL Loss: 1915.2933\n",
            "\n",
            "Epoch 204: Validation loss decreased (1915.293335 --> 1897.129150).\n",
            "\t Train_Loss: 936.6237 Val_Loss: 1897.1292  BEST VAL Loss: 1897.1292\n",
            "\n",
            "Epoch 205: Validation loss decreased (1897.129150 --> 1879.169556).\n",
            "\t Train_Loss: 924.5829 Val_Loss: 1879.1696  BEST VAL Loss: 1879.1696\n",
            "\n",
            "Epoch 206: Validation loss decreased (1879.169556 --> 1861.410767).\n",
            "\t Train_Loss: 912.7083 Val_Loss: 1861.4108  BEST VAL Loss: 1861.4108\n",
            "\n",
            "Epoch 207: Validation loss decreased (1861.410767 --> 1843.851807).\n",
            "\t Train_Loss: 900.9969 Val_Loss: 1843.8518  BEST VAL Loss: 1843.8518\n",
            "\n",
            "Epoch 208: Validation loss decreased (1843.851807 --> 1826.489136).\n",
            "\t Train_Loss: 889.4471 Val_Loss: 1826.4891  BEST VAL Loss: 1826.4891\n",
            "\n",
            "Epoch 209: Validation loss decreased (1826.489136 --> 1809.320923).\n",
            "\t Train_Loss: 878.0567 Val_Loss: 1809.3209  BEST VAL Loss: 1809.3209\n",
            "\n",
            "Epoch 210: Validation loss decreased (1809.320923 --> 1792.344971).\n",
            "\t Train_Loss: 866.8234 Val_Loss: 1792.3450  BEST VAL Loss: 1792.3450\n",
            "\n",
            "Epoch 211: Validation loss decreased (1792.344971 --> 1775.557861).\n",
            "\t Train_Loss: 855.7451 Val_Loss: 1775.5579  BEST VAL Loss: 1775.5579\n",
            "\n",
            "Epoch 212: Validation loss decreased (1775.557861 --> 1758.958374).\n",
            "\t Train_Loss: 844.8196 Val_Loss: 1758.9584  BEST VAL Loss: 1758.9584\n",
            "\n",
            "Epoch 213: Validation loss decreased (1758.958374 --> 1742.544312).\n",
            "\t Train_Loss: 834.0450 Val_Loss: 1742.5443  BEST VAL Loss: 1742.5443\n",
            "\n",
            "Epoch 214: Validation loss decreased (1742.544312 --> 1726.313110).\n",
            "\t Train_Loss: 823.4197 Val_Loss: 1726.3131  BEST VAL Loss: 1726.3131\n",
            "\n",
            "Epoch 215: Validation loss decreased (1726.313110 --> 1710.263062).\n",
            "\t Train_Loss: 812.9412 Val_Loss: 1710.2631  BEST VAL Loss: 1710.2631\n",
            "\n",
            "Epoch 216: Validation loss decreased (1710.263062 --> 1694.391479).\n",
            "\t Train_Loss: 802.6080 Val_Loss: 1694.3915  BEST VAL Loss: 1694.3915\n",
            "\n",
            "Epoch 217: Validation loss decreased (1694.391479 --> 1678.695923).\n",
            "\t Train_Loss: 792.4180 Val_Loss: 1678.6959  BEST VAL Loss: 1678.6959\n",
            "\n",
            "Epoch 218: Validation loss decreased (1678.695923 --> 1663.175049).\n",
            "\t Train_Loss: 782.3688 Val_Loss: 1663.1750  BEST VAL Loss: 1663.1750\n",
            "\n",
            "Epoch 219: Validation loss decreased (1663.175049 --> 1647.826538).\n",
            "\t Train_Loss: 772.4592 Val_Loss: 1647.8265  BEST VAL Loss: 1647.8265\n",
            "\n",
            "Epoch 220: Validation loss decreased (1647.826538 --> 1632.649048).\n",
            "\t Train_Loss: 762.6873 Val_Loss: 1632.6490  BEST VAL Loss: 1632.6490\n",
            "\n",
            "Epoch 221: Validation loss decreased (1632.649048 --> 1617.638794).\n",
            "\t Train_Loss: 753.0513 Val_Loss: 1617.6388  BEST VAL Loss: 1617.6388\n",
            "\n",
            "Epoch 222: Validation loss decreased (1617.638794 --> 1602.796143).\n",
            "\t Train_Loss: 743.5489 Val_Loss: 1602.7961  BEST VAL Loss: 1602.7961\n",
            "\n",
            "Epoch 223: Validation loss decreased (1602.796143 --> 1588.117554).\n",
            "\t Train_Loss: 734.1792 Val_Loss: 1588.1176  BEST VAL Loss: 1588.1176\n",
            "\n",
            "Epoch 224: Validation loss decreased (1588.117554 --> 1573.601074).\n",
            "\t Train_Loss: 724.9401 Val_Loss: 1573.6011  BEST VAL Loss: 1573.6011\n",
            "\n",
            "Epoch 225: Validation loss decreased (1573.601074 --> 1559.245972).\n",
            "\t Train_Loss: 715.8295 Val_Loss: 1559.2460  BEST VAL Loss: 1559.2460\n",
            "\n",
            "Epoch 226: Validation loss decreased (1559.245972 --> 1545.049194).\n",
            "\t Train_Loss: 706.8464 Val_Loss: 1545.0492  BEST VAL Loss: 1545.0492\n",
            "\n",
            "Epoch 227: Validation loss decreased (1545.049194 --> 1531.009766).\n",
            "\t Train_Loss: 697.9883 Val_Loss: 1531.0098  BEST VAL Loss: 1531.0098\n",
            "\n",
            "Epoch 228: Validation loss decreased (1531.009766 --> 1517.126099).\n",
            "\t Train_Loss: 689.2545 Val_Loss: 1517.1261  BEST VAL Loss: 1517.1261\n",
            "\n",
            "Epoch 229: Validation loss decreased (1517.126099 --> 1503.395386).\n",
            "\t Train_Loss: 680.6434 Val_Loss: 1503.3954  BEST VAL Loss: 1503.3954\n",
            "\n",
            "Epoch 230: Validation loss decreased (1503.395386 --> 1489.816772).\n",
            "\t Train_Loss: 672.1524 Val_Loss: 1489.8168  BEST VAL Loss: 1489.8168\n",
            "\n",
            "Epoch 231: Validation loss decreased (1489.816772 --> 1476.387573).\n",
            "\t Train_Loss: 663.7810 Val_Loss: 1476.3876  BEST VAL Loss: 1476.3876\n",
            "\n",
            "Epoch 232: Validation loss decreased (1476.387573 --> 1463.107422).\n",
            "\t Train_Loss: 655.5269 Val_Loss: 1463.1074  BEST VAL Loss: 1463.1074\n",
            "\n",
            "Epoch 233: Validation loss decreased (1463.107422 --> 1449.973755).\n",
            "\t Train_Loss: 647.3893 Val_Loss: 1449.9738  BEST VAL Loss: 1449.9738\n",
            "\n",
            "Epoch 234: Validation loss decreased (1449.973755 --> 1436.985596).\n",
            "\t Train_Loss: 639.3661 Val_Loss: 1436.9856  BEST VAL Loss: 1436.9856\n",
            "\n",
            "Epoch 235: Validation loss decreased (1436.985596 --> 1424.140381).\n",
            "\t Train_Loss: 631.4565 Val_Loss: 1424.1404  BEST VAL Loss: 1424.1404\n",
            "\n",
            "Epoch 236: Validation loss decreased (1424.140381 --> 1411.437134).\n",
            "\t Train_Loss: 623.6583 Val_Loss: 1411.4371  BEST VAL Loss: 1411.4371\n",
            "\n",
            "Epoch 237: Validation loss decreased (1411.437134 --> 1398.873901).\n",
            "\t Train_Loss: 615.9705 Val_Loss: 1398.8739  BEST VAL Loss: 1398.8739\n",
            "\n",
            "Epoch 238: Validation loss decreased (1398.873901 --> 1386.450073).\n",
            "\t Train_Loss: 608.3915 Val_Loss: 1386.4501  BEST VAL Loss: 1386.4501\n",
            "\n",
            "Epoch 239: Validation loss decreased (1386.450073 --> 1374.162720).\n",
            "\t Train_Loss: 600.9202 Val_Loss: 1374.1627  BEST VAL Loss: 1374.1627\n",
            "\n",
            "Epoch 240: Validation loss decreased (1374.162720 --> 1362.011963).\n",
            "\t Train_Loss: 593.5548 Val_Loss: 1362.0120  BEST VAL Loss: 1362.0120\n",
            "\n",
            "Epoch 241: Validation loss decreased (1362.011963 --> 1349.994873).\n",
            "\t Train_Loss: 586.2946 Val_Loss: 1349.9949  BEST VAL Loss: 1349.9949\n",
            "\n",
            "Epoch 242: Validation loss decreased (1349.994873 --> 1338.110107).\n",
            "\t Train_Loss: 579.1378 Val_Loss: 1338.1101  BEST VAL Loss: 1338.1101\n",
            "\n",
            "Epoch 243: Validation loss decreased (1338.110107 --> 1326.357300).\n",
            "\t Train_Loss: 572.0826 Val_Loss: 1326.3573  BEST VAL Loss: 1326.3573\n",
            "\n",
            "Epoch 244: Validation loss decreased (1326.357300 --> 1314.733643).\n",
            "\t Train_Loss: 565.1288 Val_Loss: 1314.7336  BEST VAL Loss: 1314.7336\n",
            "\n",
            "Epoch 245: Validation loss decreased (1314.733643 --> 1303.239258).\n",
            "\t Train_Loss: 558.2742 Val_Loss: 1303.2393  BEST VAL Loss: 1303.2393\n",
            "\n",
            "Epoch 246: Validation loss decreased (1303.239258 --> 1291.870972).\n",
            "\t Train_Loss: 551.5183 Val_Loss: 1291.8710  BEST VAL Loss: 1291.8710\n",
            "\n",
            "Epoch 247: Validation loss decreased (1291.870972 --> 1280.629517).\n",
            "\t Train_Loss: 544.8589 Val_Loss: 1280.6295  BEST VAL Loss: 1280.6295\n",
            "\n",
            "Epoch 248: Validation loss decreased (1280.629517 --> 1269.511108).\n",
            "\t Train_Loss: 538.2959 Val_Loss: 1269.5111  BEST VAL Loss: 1269.5111\n",
            "\n",
            "Epoch 249: Validation loss decreased (1269.511108 --> 1258.516602).\n",
            "\t Train_Loss: 531.8270 Val_Loss: 1258.5166  BEST VAL Loss: 1258.5166\n",
            "\n",
            "Epoch 250: Validation loss decreased (1258.516602 --> 1247.642822).\n",
            "\t Train_Loss: 525.4518 Val_Loss: 1247.6428  BEST VAL Loss: 1247.6428\n",
            "\n",
            "Epoch 251: Validation loss decreased (1247.642822 --> 1236.889893).\n",
            "\t Train_Loss: 519.1684 Val_Loss: 1236.8899  BEST VAL Loss: 1236.8899\n",
            "\n",
            "Epoch 252: Validation loss decreased (1236.889893 --> 1226.255493).\n",
            "\t Train_Loss: 512.9763 Val_Loss: 1226.2555  BEST VAL Loss: 1226.2555\n",
            "\n",
            "Epoch 253: Validation loss decreased (1226.255493 --> 1215.739624).\n",
            "\t Train_Loss: 506.8739 Val_Loss: 1215.7396  BEST VAL Loss: 1215.7396\n",
            "\n",
            "Epoch 254: Validation loss decreased (1215.739624 --> 1205.339111).\n",
            "\t Train_Loss: 500.8605 Val_Loss: 1205.3391  BEST VAL Loss: 1205.3391\n",
            "\n",
            "Epoch 255: Validation loss decreased (1205.339111 --> 1195.054321).\n",
            "\t Train_Loss: 494.9341 Val_Loss: 1195.0543  BEST VAL Loss: 1195.0543\n",
            "\n",
            "Epoch 256: Validation loss decreased (1195.054321 --> 1184.883667).\n",
            "\t Train_Loss: 489.0945 Val_Loss: 1184.8837  BEST VAL Loss: 1184.8837\n",
            "\n",
            "Epoch 257: Validation loss decreased (1184.883667 --> 1174.824829).\n",
            "\t Train_Loss: 483.3403 Val_Loss: 1174.8248  BEST VAL Loss: 1174.8248\n",
            "\n",
            "Epoch 258: Validation loss decreased (1174.824829 --> 1164.878296).\n",
            "\t Train_Loss: 477.6700 Val_Loss: 1164.8783  BEST VAL Loss: 1164.8783\n",
            "\n",
            "Epoch 259: Validation loss decreased (1164.878296 --> 1155.041992).\n",
            "\t Train_Loss: 472.0830 Val_Loss: 1155.0420  BEST VAL Loss: 1155.0420\n",
            "\n",
            "Epoch 260: Validation loss decreased (1155.041992 --> 1145.313965).\n",
            "\t Train_Loss: 466.5782 Val_Loss: 1145.3140  BEST VAL Loss: 1145.3140\n",
            "\n",
            "Epoch 261: Validation loss decreased (1145.313965 --> 1135.694702).\n",
            "\t Train_Loss: 461.1540 Val_Loss: 1135.6947  BEST VAL Loss: 1135.6947\n",
            "\n",
            "Epoch 262: Validation loss decreased (1135.694702 --> 1126.182251).\n",
            "\t Train_Loss: 455.8102 Val_Loss: 1126.1823  BEST VAL Loss: 1126.1823\n",
            "\n",
            "Epoch 263: Validation loss decreased (1126.182251 --> 1116.774658).\n",
            "\t Train_Loss: 450.5454 Val_Loss: 1116.7747  BEST VAL Loss: 1116.7747\n",
            "\n",
            "Epoch 264: Validation loss decreased (1116.774658 --> 1107.472046).\n",
            "\t Train_Loss: 445.3580 Val_Loss: 1107.4720  BEST VAL Loss: 1107.4720\n",
            "\n",
            "Epoch 265: Validation loss decreased (1107.472046 --> 1098.272827).\n",
            "\t Train_Loss: 440.2480 Val_Loss: 1098.2728  BEST VAL Loss: 1098.2728\n",
            "\n",
            "Epoch 266: Validation loss decreased (1098.272827 --> 1089.176147).\n",
            "\t Train_Loss: 435.2137 Val_Loss: 1089.1761  BEST VAL Loss: 1089.1761\n",
            "\n",
            "Epoch 267: Validation loss decreased (1089.176147 --> 1080.180298).\n",
            "\t Train_Loss: 430.2547 Val_Loss: 1080.1803  BEST VAL Loss: 1080.1803\n",
            "\n",
            "Epoch 268: Validation loss decreased (1080.180298 --> 1071.284180).\n",
            "\t Train_Loss: 425.3694 Val_Loss: 1071.2842  BEST VAL Loss: 1071.2842\n",
            "\n",
            "Epoch 269: Validation loss decreased (1071.284180 --> 1062.487549).\n",
            "\t Train_Loss: 420.5569 Val_Loss: 1062.4875  BEST VAL Loss: 1062.4875\n",
            "\n",
            "Epoch 270: Validation loss decreased (1062.487549 --> 1053.789062).\n",
            "\t Train_Loss: 415.8168 Val_Loss: 1053.7891  BEST VAL Loss: 1053.7891\n",
            "\n",
            "Epoch 271: Validation loss decreased (1053.789062 --> 1045.186401).\n",
            "\t Train_Loss: 411.1478 Val_Loss: 1045.1864  BEST VAL Loss: 1045.1864\n",
            "\n",
            "Epoch 272: Validation loss decreased (1045.186401 --> 1036.680542).\n",
            "\t Train_Loss: 406.5485 Val_Loss: 1036.6805  BEST VAL Loss: 1036.6805\n",
            "\n",
            "Epoch 273: Validation loss decreased (1036.680542 --> 1028.269531).\n",
            "\t Train_Loss: 402.0189 Val_Loss: 1028.2695  BEST VAL Loss: 1028.2695\n",
            "\n",
            "Epoch 274: Validation loss decreased (1028.269531 --> 1019.952148).\n",
            "\t Train_Loss: 397.5577 Val_Loss: 1019.9521  BEST VAL Loss: 1019.9521\n",
            "\n",
            "Epoch 275: Validation loss decreased (1019.952148 --> 1011.726990).\n",
            "\t Train_Loss: 393.1639 Val_Loss: 1011.7270  BEST VAL Loss: 1011.7270\n",
            "\n",
            "Epoch 276: Validation loss decreased (1011.726990 --> 1003.594421).\n",
            "\t Train_Loss: 388.8363 Val_Loss: 1003.5944  BEST VAL Loss: 1003.5944\n",
            "\n",
            "Epoch 277: Validation loss decreased (1003.594421 --> 995.552551).\n",
            "\t Train_Loss: 384.5748 Val_Loss: 995.5526  BEST VAL Loss: 995.5526\n",
            "\n",
            "Epoch 278: Validation loss decreased (995.552551 --> 987.600403).\n",
            "\t Train_Loss: 380.3781 Val_Loss: 987.6004  BEST VAL Loss: 987.6004\n",
            "\n",
            "Epoch 279: Validation loss decreased (987.600403 --> 979.737122).\n",
            "\t Train_Loss: 376.2453 Val_Loss: 979.7371  BEST VAL Loss: 979.7371\n",
            "\n",
            "Epoch 280: Validation loss decreased (979.737122 --> 971.962036).\n",
            "\t Train_Loss: 372.1757 Val_Loss: 971.9620  BEST VAL Loss: 971.9620\n",
            "\n",
            "Epoch 281: Validation loss decreased (971.962036 --> 964.273071).\n",
            "\t Train_Loss: 368.1683 Val_Loss: 964.2731  BEST VAL Loss: 964.2731\n",
            "\n",
            "Epoch 282: Validation loss decreased (964.273071 --> 956.671326).\n",
            "\t Train_Loss: 364.2220 Val_Loss: 956.6713  BEST VAL Loss: 956.6713\n",
            "\n",
            "Epoch 283: Validation loss decreased (956.671326 --> 949.153503).\n",
            "\t Train_Loss: 360.3370 Val_Loss: 949.1535  BEST VAL Loss: 949.1535\n",
            "\n",
            "Epoch 284: Validation loss decreased (949.153503 --> 941.720886).\n",
            "\t Train_Loss: 356.5112 Val_Loss: 941.7209  BEST VAL Loss: 941.7209\n",
            "\n",
            "Epoch 285: Validation loss decreased (941.720886 --> 934.371033).\n",
            "\t Train_Loss: 352.7449 Val_Loss: 934.3710  BEST VAL Loss: 934.3710\n",
            "\n",
            "Epoch 286: Validation loss decreased (934.371033 --> 927.103699).\n",
            "\t Train_Loss: 349.0366 Val_Loss: 927.1037  BEST VAL Loss: 927.1037\n",
            "\n",
            "Epoch 287: Validation loss decreased (927.103699 --> 919.918457).\n",
            "\t Train_Loss: 345.3858 Val_Loss: 919.9185  BEST VAL Loss: 919.9185\n",
            "\n",
            "Epoch 288: Validation loss decreased (919.918457 --> 912.812988).\n",
            "\t Train_Loss: 341.7918 Val_Loss: 912.8130  BEST VAL Loss: 912.8130\n",
            "\n",
            "Epoch 289: Validation loss decreased (912.812988 --> 905.787720).\n",
            "\t Train_Loss: 338.2534 Val_Loss: 905.7877  BEST VAL Loss: 905.7877\n",
            "\n",
            "Epoch 290: Validation loss decreased (905.787720 --> 898.841736).\n",
            "\t Train_Loss: 334.7703 Val_Loss: 898.8417  BEST VAL Loss: 898.8417\n",
            "\n",
            "Epoch 291: Validation loss decreased (898.841736 --> 891.973083).\n",
            "\t Train_Loss: 331.3419 Val_Loss: 891.9731  BEST VAL Loss: 891.9731\n",
            "\n",
            "Epoch 292: Validation loss decreased (891.973083 --> 885.182068).\n",
            "\t Train_Loss: 327.9667 Val_Loss: 885.1821  BEST VAL Loss: 885.1821\n",
            "\n",
            "Epoch 293: Validation loss decreased (885.182068 --> 878.467773).\n",
            "\t Train_Loss: 324.6446 Val_Loss: 878.4678  BEST VAL Loss: 878.4678\n",
            "\n",
            "Epoch 294: Validation loss decreased (878.467773 --> 871.828308).\n",
            "\t Train_Loss: 321.3751 Val_Loss: 871.8283  BEST VAL Loss: 871.8283\n",
            "\n",
            "Epoch 295: Validation loss decreased (871.828308 --> 865.263672).\n",
            "\t Train_Loss: 318.1566 Val_Loss: 865.2637  BEST VAL Loss: 865.2637\n",
            "\n",
            "Epoch 296: Validation loss decreased (865.263672 --> 858.773865).\n",
            "\t Train_Loss: 314.9889 Val_Loss: 858.7739  BEST VAL Loss: 858.7739\n",
            "\n",
            "Epoch 297: Validation loss decreased (858.773865 --> 852.356384).\n",
            "\t Train_Loss: 311.8718 Val_Loss: 852.3564  BEST VAL Loss: 852.3564\n",
            "\n",
            "Epoch 298: Validation loss decreased (852.356384 --> 846.011658).\n",
            "\t Train_Loss: 308.8036 Val_Loss: 846.0117  BEST VAL Loss: 846.0117\n",
            "\n",
            "Epoch 299: Validation loss decreased (846.011658 --> 839.738098).\n",
            "\t Train_Loss: 305.7844 Val_Loss: 839.7381  BEST VAL Loss: 839.7381\n",
            "\n",
            "Epoch 300: Validation loss decreased (839.738098 --> 833.535828).\n",
            "\t Train_Loss: 302.8131 Val_Loss: 833.5358  BEST VAL Loss: 833.5358\n",
            "\n",
            "Epoch 301: Validation loss decreased (833.535828 --> 827.403503).\n",
            "\t Train_Loss: 299.8893 Val_Loss: 827.4035  BEST VAL Loss: 827.4035\n",
            "\n",
            "Epoch 302: Validation loss decreased (827.403503 --> 821.340271).\n",
            "\t Train_Loss: 297.0123 Val_Loss: 821.3403  BEST VAL Loss: 821.3403\n",
            "\n",
            "Epoch 303: Validation loss decreased (821.340271 --> 815.345581).\n",
            "\t Train_Loss: 294.1813 Val_Loss: 815.3456  BEST VAL Loss: 815.3456\n",
            "\n",
            "Epoch 304: Validation loss decreased (815.345581 --> 809.418457).\n",
            "\t Train_Loss: 291.3957 Val_Loss: 809.4185  BEST VAL Loss: 809.4185\n",
            "\n",
            "Epoch 305: Validation loss decreased (809.418457 --> 803.558716).\n",
            "\t Train_Loss: 288.6548 Val_Loss: 803.5587  BEST VAL Loss: 803.5587\n",
            "\n",
            "Epoch 306: Validation loss decreased (803.558716 --> 797.765076).\n",
            "\t Train_Loss: 285.9583 Val_Loss: 797.7651  BEST VAL Loss: 797.7651\n",
            "\n",
            "Epoch 307: Validation loss decreased (797.765076 --> 792.036804).\n",
            "\t Train_Loss: 283.3052 Val_Loss: 792.0368  BEST VAL Loss: 792.0368\n",
            "\n",
            "Epoch 308: Validation loss decreased (792.036804 --> 786.374023).\n",
            "\t Train_Loss: 280.6949 Val_Loss: 786.3740  BEST VAL Loss: 786.3740\n",
            "\n",
            "Epoch 309: Validation loss decreased (786.374023 --> 780.774841).\n",
            "\t Train_Loss: 278.1273 Val_Loss: 780.7748  BEST VAL Loss: 780.7748\n",
            "\n",
            "Epoch 310: Validation loss decreased (780.774841 --> 775.239685).\n",
            "\t Train_Loss: 275.6012 Val_Loss: 775.2397  BEST VAL Loss: 775.2397\n",
            "\n",
            "Epoch 311: Validation loss decreased (775.239685 --> 769.766479).\n",
            "\t Train_Loss: 273.1164 Val_Loss: 769.7665  BEST VAL Loss: 769.7665\n",
            "\n",
            "Epoch 312: Validation loss decreased (769.766479 --> 764.355896).\n",
            "\t Train_Loss: 270.6718 Val_Loss: 764.3559  BEST VAL Loss: 764.3559\n",
            "\n",
            "Epoch 313: Validation loss decreased (764.355896 --> 759.006531).\n",
            "\t Train_Loss: 268.2675 Val_Loss: 759.0065  BEST VAL Loss: 759.0065\n",
            "\n",
            "Epoch 314: Validation loss decreased (759.006531 --> 753.717773).\n",
            "\t Train_Loss: 265.9025 Val_Loss: 753.7178  BEST VAL Loss: 753.7178\n",
            "\n",
            "Epoch 315: Validation loss decreased (753.717773 --> 748.489563).\n",
            "\t Train_Loss: 263.5762 Val_Loss: 748.4896  BEST VAL Loss: 748.4896\n",
            "\n",
            "Epoch 316: Validation loss decreased (748.489563 --> 743.320618).\n",
            "\t Train_Loss: 261.2885 Val_Loss: 743.3206  BEST VAL Loss: 743.3206\n",
            "\n",
            "Epoch 317: Validation loss decreased (743.320618 --> 738.210205).\n",
            "\t Train_Loss: 259.0384 Val_Loss: 738.2102  BEST VAL Loss: 738.2102\n",
            "\n",
            "Epoch 318: Validation loss decreased (738.210205 --> 733.158508).\n",
            "\t Train_Loss: 256.8254 Val_Loss: 733.1585  BEST VAL Loss: 733.1585\n",
            "\n",
            "Epoch 319: Validation loss decreased (733.158508 --> 728.163269).\n",
            "\t Train_Loss: 254.6494 Val_Loss: 728.1633  BEST VAL Loss: 728.1633\n",
            "\n",
            "Epoch 320: Validation loss decreased (728.163269 --> 723.225586).\n",
            "\t Train_Loss: 252.5090 Val_Loss: 723.2256  BEST VAL Loss: 723.2256\n",
            "\n",
            "Epoch 321: Validation loss decreased (723.225586 --> 718.344177).\n",
            "\t Train_Loss: 250.4045 Val_Loss: 718.3442  BEST VAL Loss: 718.3442\n",
            "\n",
            "Epoch 322: Validation loss decreased (718.344177 --> 713.518127).\n",
            "\t Train_Loss: 248.3351 Val_Loss: 713.5181  BEST VAL Loss: 713.5181\n",
            "\n",
            "Epoch 323: Validation loss decreased (713.518127 --> 708.747009).\n",
            "\t Train_Loss: 246.3002 Val_Loss: 708.7470  BEST VAL Loss: 708.7470\n",
            "\n",
            "Epoch 324: Validation loss decreased (708.747009 --> 704.030212).\n",
            "\t Train_Loss: 244.2994 Val_Loss: 704.0302  BEST VAL Loss: 704.0302\n",
            "\n",
            "Epoch 325: Validation loss decreased (704.030212 --> 699.367798).\n",
            "\t Train_Loss: 242.3320 Val_Loss: 699.3678  BEST VAL Loss: 699.3678\n",
            "\n",
            "Epoch 326: Validation loss decreased (699.367798 --> 694.757996).\n",
            "\t Train_Loss: 240.3979 Val_Loss: 694.7580  BEST VAL Loss: 694.7580\n",
            "\n",
            "Epoch 327: Validation loss decreased (694.757996 --> 690.200439).\n",
            "\t Train_Loss: 238.4962 Val_Loss: 690.2004  BEST VAL Loss: 690.2004\n",
            "\n",
            "Epoch 328: Validation loss decreased (690.200439 --> 685.695923).\n",
            "\t Train_Loss: 236.6265 Val_Loss: 685.6959  BEST VAL Loss: 685.6959\n",
            "\n",
            "Epoch 329: Validation loss decreased (685.695923 --> 681.242493).\n",
            "\t Train_Loss: 234.7888 Val_Loss: 681.2425  BEST VAL Loss: 681.2425\n",
            "\n",
            "Epoch 330: Validation loss decreased (681.242493 --> 676.839783).\n",
            "\t Train_Loss: 232.9820 Val_Loss: 676.8398  BEST VAL Loss: 676.8398\n",
            "\n",
            "Epoch 331: Validation loss decreased (676.839783 --> 672.487427).\n",
            "\t Train_Loss: 231.2059 Val_Loss: 672.4874  BEST VAL Loss: 672.4874\n",
            "\n",
            "Epoch 332: Validation loss decreased (672.487427 --> 668.184570).\n",
            "\t Train_Loss: 229.4601 Val_Loss: 668.1846  BEST VAL Loss: 668.1846\n",
            "\n",
            "Epoch 333: Validation loss decreased (668.184570 --> 663.930908).\n",
            "\t Train_Loss: 227.7439 Val_Loss: 663.9309  BEST VAL Loss: 663.9309\n",
            "\n",
            "Epoch 334: Validation loss decreased (663.930908 --> 659.726135).\n",
            "\t Train_Loss: 226.0571 Val_Loss: 659.7261  BEST VAL Loss: 659.7261\n",
            "\n",
            "Epoch 335: Validation loss decreased (659.726135 --> 655.569519).\n",
            "\t Train_Loss: 224.3992 Val_Loss: 655.5695  BEST VAL Loss: 655.5695\n",
            "\n",
            "Epoch 336: Validation loss decreased (655.569519 --> 651.460938).\n",
            "\t Train_Loss: 222.7697 Val_Loss: 651.4609  BEST VAL Loss: 651.4609\n",
            "\n",
            "Epoch 337: Validation loss decreased (651.460938 --> 647.398987).\n",
            "\t Train_Loss: 221.1685 Val_Loss: 647.3990  BEST VAL Loss: 647.3990\n",
            "\n",
            "Epoch 338: Validation loss decreased (647.398987 --> 643.383179).\n",
            "\t Train_Loss: 219.5947 Val_Loss: 643.3832  BEST VAL Loss: 643.3832\n",
            "\n",
            "Epoch 339: Validation loss decreased (643.383179 --> 639.413696).\n",
            "\t Train_Loss: 218.0480 Val_Loss: 639.4137  BEST VAL Loss: 639.4137\n",
            "\n",
            "Epoch 340: Validation loss decreased (639.413696 --> 635.489990).\n",
            "\t Train_Loss: 216.5281 Val_Loss: 635.4900  BEST VAL Loss: 635.4900\n",
            "\n",
            "Epoch 341: Validation loss decreased (635.489990 --> 631.610901).\n",
            "\t Train_Loss: 215.0347 Val_Loss: 631.6109  BEST VAL Loss: 631.6109\n",
            "\n",
            "Epoch 342: Validation loss decreased (631.610901 --> 627.776367).\n",
            "\t Train_Loss: 213.5670 Val_Loss: 627.7764  BEST VAL Loss: 627.7764\n",
            "\n",
            "Epoch 343: Validation loss decreased (627.776367 --> 623.986145).\n",
            "\t Train_Loss: 212.1250 Val_Loss: 623.9861  BEST VAL Loss: 623.9861\n",
            "\n",
            "Epoch 344: Validation loss decreased (623.986145 --> 620.239136).\n",
            "\t Train_Loss: 210.7082 Val_Loss: 620.2391  BEST VAL Loss: 620.2391\n",
            "\n",
            "Epoch 345: Validation loss decreased (620.239136 --> 616.535583).\n",
            "\t Train_Loss: 209.3161 Val_Loss: 616.5356  BEST VAL Loss: 616.5356\n",
            "\n",
            "Epoch 346: Validation loss decreased (616.535583 --> 612.873962).\n",
            "\t Train_Loss: 207.9486 Val_Loss: 612.8740  BEST VAL Loss: 612.8740\n",
            "\n",
            "Epoch 347: Validation loss decreased (612.873962 --> 609.254883).\n",
            "\t Train_Loss: 206.6048 Val_Loss: 609.2549  BEST VAL Loss: 609.2549\n",
            "\n",
            "Epoch 348: Validation loss decreased (609.254883 --> 605.677246).\n",
            "\t Train_Loss: 205.2848 Val_Loss: 605.6772  BEST VAL Loss: 605.6772\n",
            "\n",
            "Epoch 349: Validation loss decreased (605.677246 --> 602.140320).\n",
            "\t Train_Loss: 203.9881 Val_Loss: 602.1403  BEST VAL Loss: 602.1403\n",
            "\n",
            "Epoch 350: Validation loss decreased (602.140320 --> 598.644714).\n",
            "\t Train_Loss: 202.7140 Val_Loss: 598.6447  BEST VAL Loss: 598.6447\n",
            "\n",
            "Epoch 351: Validation loss decreased (598.644714 --> 595.189148).\n",
            "\t Train_Loss: 201.4628 Val_Loss: 595.1891  BEST VAL Loss: 595.1891\n",
            "\n",
            "Epoch 352: Validation loss decreased (595.189148 --> 591.772949).\n",
            "\t Train_Loss: 200.2337 Val_Loss: 591.7729  BEST VAL Loss: 591.7729\n",
            "\n",
            "Epoch 353: Validation loss decreased (591.772949 --> 588.396240).\n",
            "\t Train_Loss: 199.0264 Val_Loss: 588.3962  BEST VAL Loss: 588.3962\n",
            "\n",
            "Epoch 354: Validation loss decreased (588.396240 --> 585.058716).\n",
            "\t Train_Loss: 197.8405 Val_Loss: 585.0587  BEST VAL Loss: 585.0587\n",
            "\n",
            "Epoch 355: Validation loss decreased (585.058716 --> 581.759766).\n",
            "\t Train_Loss: 196.6760 Val_Loss: 581.7598  BEST VAL Loss: 581.7598\n",
            "\n",
            "Epoch 356: Validation loss decreased (581.759766 --> 578.498047).\n",
            "\t Train_Loss: 195.5323 Val_Loss: 578.4980  BEST VAL Loss: 578.4980\n",
            "\n",
            "Epoch 357: Validation loss decreased (578.498047 --> 575.274719).\n",
            "\t Train_Loss: 194.4088 Val_Loss: 575.2747  BEST VAL Loss: 575.2747\n",
            "\n",
            "Epoch 358: Validation loss decreased (575.274719 --> 572.088074).\n",
            "\t Train_Loss: 193.3058 Val_Loss: 572.0881  BEST VAL Loss: 572.0881\n",
            "\n",
            "Epoch 359: Validation loss decreased (572.088074 --> 568.938293).\n",
            "\t Train_Loss: 192.2225 Val_Loss: 568.9383  BEST VAL Loss: 568.9383\n",
            "\n",
            "Epoch 360: Validation loss decreased (568.938293 --> 565.825012).\n",
            "\t Train_Loss: 191.1588 Val_Loss: 565.8250  BEST VAL Loss: 565.8250\n",
            "\n",
            "Epoch 361: Validation loss decreased (565.825012 --> 562.747681).\n",
            "\t Train_Loss: 190.1143 Val_Loss: 562.7477  BEST VAL Loss: 562.7477\n",
            "\n",
            "Epoch 362: Validation loss decreased (562.747681 --> 559.705383).\n",
            "\t Train_Loss: 189.0887 Val_Loss: 559.7054  BEST VAL Loss: 559.7054\n",
            "\n",
            "Epoch 363: Validation loss decreased (559.705383 --> 556.698242).\n",
            "\t Train_Loss: 188.0816 Val_Loss: 556.6982  BEST VAL Loss: 556.6982\n",
            "\n",
            "Epoch 364: Validation loss decreased (556.698242 --> 553.726257).\n",
            "\t Train_Loss: 187.0928 Val_Loss: 553.7263  BEST VAL Loss: 553.7263\n",
            "\n",
            "Epoch 365: Validation loss decreased (553.726257 --> 550.788696).\n",
            "\t Train_Loss: 186.1222 Val_Loss: 550.7887  BEST VAL Loss: 550.7887\n",
            "\n",
            "Epoch 366: Validation loss decreased (550.788696 --> 547.884460).\n",
            "\t Train_Loss: 185.1692 Val_Loss: 547.8845  BEST VAL Loss: 547.8845\n",
            "\n",
            "Epoch 367: Validation loss decreased (547.884460 --> 545.014099).\n",
            "\t Train_Loss: 184.2335 Val_Loss: 545.0141  BEST VAL Loss: 545.0141\n",
            "\n",
            "Epoch 368: Validation loss decreased (545.014099 --> 542.177185).\n",
            "\t Train_Loss: 183.3150 Val_Loss: 542.1772  BEST VAL Loss: 542.1772\n",
            "\n",
            "Epoch 369: Validation loss decreased (542.177185 --> 539.372742).\n",
            "\t Train_Loss: 182.4136 Val_Loss: 539.3727  BEST VAL Loss: 539.3727\n",
            "\n",
            "Epoch 370: Validation loss decreased (539.372742 --> 536.600525).\n",
            "\t Train_Loss: 181.5286 Val_Loss: 536.6005  BEST VAL Loss: 536.6005\n",
            "\n",
            "Epoch 371: Validation loss decreased (536.600525 --> 533.860535).\n",
            "\t Train_Loss: 180.6598 Val_Loss: 533.8605  BEST VAL Loss: 533.8605\n",
            "\n",
            "Epoch 372: Validation loss decreased (533.860535 --> 531.152588).\n",
            "\t Train_Loss: 179.8071 Val_Loss: 531.1526  BEST VAL Loss: 531.1526\n",
            "\n",
            "Epoch 373: Validation loss decreased (531.152588 --> 528.475891).\n",
            "\t Train_Loss: 178.9703 Val_Loss: 528.4759  BEST VAL Loss: 528.4759\n",
            "\n",
            "Epoch 374: Validation loss decreased (528.475891 --> 525.830200).\n",
            "\t Train_Loss: 178.1490 Val_Loss: 525.8302  BEST VAL Loss: 525.8302\n",
            "\n",
            "Epoch 375: Validation loss decreased (525.830200 --> 523.214844).\n",
            "\t Train_Loss: 177.3429 Val_Loss: 523.2148  BEST VAL Loss: 523.2148\n",
            "\n",
            "Epoch 376: Validation loss decreased (523.214844 --> 520.630127).\n",
            "\t Train_Loss: 176.5518 Val_Loss: 520.6301  BEST VAL Loss: 520.6301\n",
            "\n",
            "Epoch 377: Validation loss decreased (520.630127 --> 518.075134).\n",
            "\t Train_Loss: 175.7755 Val_Loss: 518.0751  BEST VAL Loss: 518.0751\n",
            "\n",
            "Epoch 378: Validation loss decreased (518.075134 --> 515.549683).\n",
            "\t Train_Loss: 175.0137 Val_Loss: 515.5497  BEST VAL Loss: 515.5497\n",
            "\n",
            "Epoch 379: Validation loss decreased (515.549683 --> 513.053528).\n",
            "\t Train_Loss: 174.2661 Val_Loss: 513.0535  BEST VAL Loss: 513.0535\n",
            "\n",
            "Epoch 380: Validation loss decreased (513.053528 --> 510.586517).\n",
            "\t Train_Loss: 173.5325 Val_Loss: 510.5865  BEST VAL Loss: 510.5865\n",
            "\n",
            "Epoch 381: Validation loss decreased (510.586517 --> 508.148163).\n",
            "\t Train_Loss: 172.8128 Val_Loss: 508.1482  BEST VAL Loss: 508.1482\n",
            "\n",
            "Epoch 382: Validation loss decreased (508.148163 --> 505.737885).\n",
            "\t Train_Loss: 172.1067 Val_Loss: 505.7379  BEST VAL Loss: 505.7379\n",
            "\n",
            "Epoch 383: Validation loss decreased (505.737885 --> 503.355621).\n",
            "\t Train_Loss: 171.4138 Val_Loss: 503.3556  BEST VAL Loss: 503.3556\n",
            "\n",
            "Epoch 384: Validation loss decreased (503.355621 --> 501.000793).\n",
            "\t Train_Loss: 170.7340 Val_Loss: 501.0008  BEST VAL Loss: 501.0008\n",
            "\n",
            "Epoch 385: Validation loss decreased (501.000793 --> 498.673248).\n",
            "\t Train_Loss: 170.0671 Val_Loss: 498.6732  BEST VAL Loss: 498.6732\n",
            "\n",
            "Epoch 386: Validation loss decreased (498.673248 --> 496.373383).\n",
            "\t Train_Loss: 169.4128 Val_Loss: 496.3734  BEST VAL Loss: 496.3734\n",
            "\n",
            "Epoch 387: Validation loss decreased (496.373383 --> 494.099609).\n",
            "\t Train_Loss: 168.7711 Val_Loss: 494.0996  BEST VAL Loss: 494.0996\n",
            "\n",
            "Epoch 388: Validation loss decreased (494.099609 --> 491.852539).\n",
            "\t Train_Loss: 168.1414 Val_Loss: 491.8525  BEST VAL Loss: 491.8525\n",
            "\n",
            "Epoch 389: Validation loss decreased (491.852539 --> 489.631348).\n",
            "\t Train_Loss: 167.5239 Val_Loss: 489.6313  BEST VAL Loss: 489.6313\n",
            "\n",
            "Epoch 390: Validation loss decreased (489.631348 --> 487.436371).\n",
            "\t Train_Loss: 166.9182 Val_Loss: 487.4364  BEST VAL Loss: 487.4364\n",
            "\n",
            "Epoch 391: Validation loss decreased (487.436371 --> 485.266693).\n",
            "\t Train_Loss: 166.3242 Val_Loss: 485.2667  BEST VAL Loss: 485.2667\n",
            "\n",
            "Epoch 392: Validation loss decreased (485.266693 --> 483.122223).\n",
            "\t Train_Loss: 165.7415 Val_Loss: 483.1222  BEST VAL Loss: 483.1222\n",
            "\n",
            "Epoch 393: Validation loss decreased (483.122223 --> 481.002686).\n",
            "\t Train_Loss: 165.1700 Val_Loss: 481.0027  BEST VAL Loss: 481.0027\n",
            "\n",
            "Epoch 394: Validation loss decreased (481.002686 --> 478.907806).\n",
            "\t Train_Loss: 164.6095 Val_Loss: 478.9078  BEST VAL Loss: 478.9078\n",
            "\n",
            "Epoch 395: Validation loss decreased (478.907806 --> 476.837463).\n",
            "\t Train_Loss: 164.0599 Val_Loss: 476.8375  BEST VAL Loss: 476.8375\n",
            "\n",
            "Epoch 396: Validation loss decreased (476.837463 --> 474.791016).\n",
            "\t Train_Loss: 163.5209 Val_Loss: 474.7910  BEST VAL Loss: 474.7910\n",
            "\n",
            "Epoch 397: Validation loss decreased (474.791016 --> 472.768555).\n",
            "\t Train_Loss: 162.9923 Val_Loss: 472.7686  BEST VAL Loss: 472.7686\n",
            "\n",
            "Epoch 398: Validation loss decreased (472.768555 --> 470.769135).\n",
            "\t Train_Loss: 162.4741 Val_Loss: 470.7691  BEST VAL Loss: 470.7691\n",
            "\n",
            "Epoch 399: Validation loss decreased (470.769135 --> 468.793610).\n",
            "\t Train_Loss: 161.9658 Val_Loss: 468.7936  BEST VAL Loss: 468.7936\n",
            "\n",
            "Epoch 400: Validation loss decreased (468.793610 --> 466.841034).\n",
            "\t Train_Loss: 161.4675 Val_Loss: 466.8410  BEST VAL Loss: 466.8410\n",
            "\n",
            "Epoch 401: Validation loss decreased (466.841034 --> 464.911194).\n",
            "\t Train_Loss: 160.9790 Val_Loss: 464.9112  BEST VAL Loss: 464.9112\n",
            "\n",
            "Epoch 402: Validation loss decreased (464.911194 --> 463.003754).\n",
            "\t Train_Loss: 160.5000 Val_Loss: 463.0038  BEST VAL Loss: 463.0038\n",
            "\n",
            "Epoch 403: Validation loss decreased (463.003754 --> 461.118469).\n",
            "\t Train_Loss: 160.0305 Val_Loss: 461.1185  BEST VAL Loss: 461.1185\n",
            "\n",
            "Epoch 404: Validation loss decreased (461.118469 --> 459.255371).\n",
            "\t Train_Loss: 159.5700 Val_Loss: 459.2554  BEST VAL Loss: 459.2554\n",
            "\n",
            "Epoch 405: Validation loss decreased (459.255371 --> 457.414124).\n",
            "\t Train_Loss: 159.1188 Val_Loss: 457.4141  BEST VAL Loss: 457.4141\n",
            "\n",
            "Epoch 406: Validation loss decreased (457.414124 --> 455.593597).\n",
            "\t Train_Loss: 158.6765 Val_Loss: 455.5936  BEST VAL Loss: 455.5936\n",
            "\n",
            "Epoch 407: Validation loss decreased (455.593597 --> 453.794983).\n",
            "\t Train_Loss: 158.2426 Val_Loss: 453.7950  BEST VAL Loss: 453.7950\n",
            "\n",
            "Epoch 408: Validation loss decreased (453.794983 --> 452.017487).\n",
            "\t Train_Loss: 157.8176 Val_Loss: 452.0175  BEST VAL Loss: 452.0175\n",
            "\n",
            "Epoch 409: Validation loss decreased (452.017487 --> 450.260406).\n",
            "\t Train_Loss: 157.4010 Val_Loss: 450.2604  BEST VAL Loss: 450.2604\n",
            "\n",
            "Epoch 410: Validation loss decreased (450.260406 --> 448.523926).\n",
            "\t Train_Loss: 156.9926 Val_Loss: 448.5239  BEST VAL Loss: 448.5239\n",
            "\n",
            "Epoch 411: Validation loss decreased (448.523926 --> 446.807922).\n",
            "\t Train_Loss: 156.5924 Val_Loss: 446.8079  BEST VAL Loss: 446.8079\n",
            "\n",
            "Epoch 412: Validation loss decreased (446.807922 --> 445.111725).\n",
            "\t Train_Loss: 156.2002 Val_Loss: 445.1117  BEST VAL Loss: 445.1117\n",
            "\n",
            "Epoch 413: Validation loss decreased (445.111725 --> 443.435455).\n",
            "\t Train_Loss: 155.8158 Val_Loss: 443.4355  BEST VAL Loss: 443.4355\n",
            "\n",
            "Epoch 414: Validation loss decreased (443.435455 --> 441.779022).\n",
            "\t Train_Loss: 155.4390 Val_Loss: 441.7790  BEST VAL Loss: 441.7790\n",
            "\n",
            "Epoch 415: Validation loss decreased (441.779022 --> 440.141815).\n",
            "\t Train_Loss: 155.0699 Val_Loss: 440.1418  BEST VAL Loss: 440.1418\n",
            "\n",
            "Epoch 416: Validation loss decreased (440.141815 --> 438.523285).\n",
            "\t Train_Loss: 154.7082 Val_Loss: 438.5233  BEST VAL Loss: 438.5233\n",
            "\n",
            "Epoch 417: Validation loss decreased (438.523285 --> 436.924469).\n",
            "\t Train_Loss: 154.3536 Val_Loss: 436.9245  BEST VAL Loss: 436.9245\n",
            "\n",
            "Epoch 418: Validation loss decreased (436.924469 --> 435.343903).\n",
            "\t Train_Loss: 154.0064 Val_Loss: 435.3439  BEST VAL Loss: 435.3439\n",
            "\n",
            "Epoch 419: Validation loss decreased (435.343903 --> 433.782227).\n",
            "\t Train_Loss: 153.6661 Val_Loss: 433.7822  BEST VAL Loss: 433.7822\n",
            "\n",
            "Epoch 420: Validation loss decreased (433.782227 --> 432.238586).\n",
            "\t Train_Loss: 153.3328 Val_Loss: 432.2386  BEST VAL Loss: 432.2386\n",
            "\n",
            "Epoch 421: Validation loss decreased (432.238586 --> 430.712952).\n",
            "\t Train_Loss: 153.0061 Val_Loss: 430.7130  BEST VAL Loss: 430.7130\n",
            "\n",
            "Epoch 422: Validation loss decreased (430.712952 --> 429.205170).\n",
            "\t Train_Loss: 152.6861 Val_Loss: 429.2052  BEST VAL Loss: 429.2052\n",
            "\n",
            "Epoch 423: Validation loss decreased (429.205170 --> 427.715179).\n",
            "\t Train_Loss: 152.3726 Val_Loss: 427.7152  BEST VAL Loss: 427.7152\n",
            "\n",
            "Epoch 424: Validation loss decreased (427.715179 --> 426.242584).\n",
            "\t Train_Loss: 152.0655 Val_Loss: 426.2426  BEST VAL Loss: 426.2426\n",
            "\n",
            "Epoch 425: Validation loss decreased (426.242584 --> 424.787567).\n",
            "\t Train_Loss: 151.7647 Val_Loss: 424.7876  BEST VAL Loss: 424.7876\n",
            "\n",
            "Epoch 426: Validation loss decreased (424.787567 --> 423.349426).\n",
            "\t Train_Loss: 151.4700 Val_Loss: 423.3494  BEST VAL Loss: 423.3494\n",
            "\n",
            "Epoch 427: Validation loss decreased (423.349426 --> 421.927887).\n",
            "\t Train_Loss: 151.1815 Val_Loss: 421.9279  BEST VAL Loss: 421.9279\n",
            "\n",
            "Epoch 428: Validation loss decreased (421.927887 --> 420.523651).\n",
            "\t Train_Loss: 150.8987 Val_Loss: 420.5237  BEST VAL Loss: 420.5237\n",
            "\n",
            "Epoch 429: Validation loss decreased (420.523651 --> 419.135559).\n",
            "\t Train_Loss: 150.6219 Val_Loss: 419.1356  BEST VAL Loss: 419.1356\n",
            "\n",
            "Epoch 430: Validation loss decreased (419.135559 --> 417.763733).\n",
            "\t Train_Loss: 150.3508 Val_Loss: 417.7637  BEST VAL Loss: 417.7637\n",
            "\n",
            "Epoch 431: Validation loss decreased (417.763733 --> 416.408417).\n",
            "\t Train_Loss: 150.0852 Val_Loss: 416.4084  BEST VAL Loss: 416.4084\n",
            "\n",
            "Epoch 432: Validation loss decreased (416.408417 --> 415.068359).\n",
            "\t Train_Loss: 149.8253 Val_Loss: 415.0684  BEST VAL Loss: 415.0684\n",
            "\n",
            "Epoch 433: Validation loss decreased (415.068359 --> 413.744385).\n",
            "\t Train_Loss: 149.5705 Val_Loss: 413.7444  BEST VAL Loss: 413.7444\n",
            "\n",
            "Epoch 434: Validation loss decreased (413.744385 --> 412.436127).\n",
            "\t Train_Loss: 149.3211 Val_Loss: 412.4361  BEST VAL Loss: 412.4361\n",
            "\n",
            "Epoch 435: Validation loss decreased (412.436127 --> 411.143158).\n",
            "\t Train_Loss: 149.0769 Val_Loss: 411.1432  BEST VAL Loss: 411.1432\n",
            "\n",
            "Epoch 436: Validation loss decreased (411.143158 --> 409.865540).\n",
            "\t Train_Loss: 148.8378 Val_Loss: 409.8655  BEST VAL Loss: 409.8655\n",
            "\n",
            "Epoch 437: Validation loss decreased (409.865540 --> 408.602753).\n",
            "\t Train_Loss: 148.6037 Val_Loss: 408.6028  BEST VAL Loss: 408.6028\n",
            "\n",
            "Epoch 438: Validation loss decreased (408.602753 --> 407.355164).\n",
            "\t Train_Loss: 148.3745 Val_Loss: 407.3552  BEST VAL Loss: 407.3552\n",
            "\n",
            "Epoch 439: Validation loss decreased (407.355164 --> 406.122009).\n",
            "\t Train_Loss: 148.1501 Val_Loss: 406.1220  BEST VAL Loss: 406.1220\n",
            "\n",
            "Epoch 440: Validation loss decreased (406.122009 --> 404.903564).\n",
            "\t Train_Loss: 147.9304 Val_Loss: 404.9036  BEST VAL Loss: 404.9036\n",
            "\n",
            "Epoch 441: Validation loss decreased (404.903564 --> 403.699219).\n",
            "\t Train_Loss: 147.7154 Val_Loss: 403.6992  BEST VAL Loss: 403.6992\n",
            "\n",
            "Epoch 442: Validation loss decreased (403.699219 --> 402.509155).\n",
            "\t Train_Loss: 147.5048 Val_Loss: 402.5092  BEST VAL Loss: 402.5092\n",
            "\n",
            "Epoch 443: Validation loss decreased (402.509155 --> 401.333313).\n",
            "\t Train_Loss: 147.2986 Val_Loss: 401.3333  BEST VAL Loss: 401.3333\n",
            "\n",
            "Epoch 444: Validation loss decreased (401.333313 --> 400.171356).\n",
            "\t Train_Loss: 147.0969 Val_Loss: 400.1714  BEST VAL Loss: 400.1714\n",
            "\n",
            "Epoch 445: Validation loss decreased (400.171356 --> 399.023163).\n",
            "\t Train_Loss: 146.8995 Val_Loss: 399.0232  BEST VAL Loss: 399.0232\n",
            "\n",
            "Epoch 446: Validation loss decreased (399.023163 --> 397.888275).\n",
            "\t Train_Loss: 146.7062 Val_Loss: 397.8883  BEST VAL Loss: 397.8883\n",
            "\n",
            "Epoch 447: Validation loss decreased (397.888275 --> 396.766815).\n",
            "\t Train_Loss: 146.5170 Val_Loss: 396.7668  BEST VAL Loss: 396.7668\n",
            "\n",
            "Epoch 448: Validation loss decreased (396.766815 --> 395.659149).\n",
            "\t Train_Loss: 146.3318 Val_Loss: 395.6591  BEST VAL Loss: 395.6591\n",
            "\n",
            "Epoch 449: Validation loss decreased (395.659149 --> 394.564270).\n",
            "\t Train_Loss: 146.1507 Val_Loss: 394.5643  BEST VAL Loss: 394.5643\n",
            "\n",
            "Epoch 450: Validation loss decreased (394.564270 --> 393.481964).\n",
            "\t Train_Loss: 145.9734 Val_Loss: 393.4820  BEST VAL Loss: 393.4820\n",
            "\n",
            "Epoch 451: Validation loss decreased (393.481964 --> 392.412903).\n",
            "\t Train_Loss: 145.7998 Val_Loss: 392.4129  BEST VAL Loss: 392.4129\n",
            "\n",
            "Epoch 452: Validation loss decreased (392.412903 --> 391.356476).\n",
            "\t Train_Loss: 145.6300 Val_Loss: 391.3565  BEST VAL Loss: 391.3565\n",
            "\n",
            "Epoch 453: Validation loss decreased (391.356476 --> 390.312378).\n",
            "\t Train_Loss: 145.4638 Val_Loss: 390.3124  BEST VAL Loss: 390.3124\n",
            "\n",
            "Epoch 454: Validation loss decreased (390.312378 --> 389.280731).\n",
            "\t Train_Loss: 145.3013 Val_Loss: 389.2807  BEST VAL Loss: 389.2807\n",
            "\n",
            "Epoch 455: Validation loss decreased (389.280731 --> 388.261292).\n",
            "\t Train_Loss: 145.1421 Val_Loss: 388.2613  BEST VAL Loss: 388.2613\n",
            "\n",
            "Epoch 456: Validation loss decreased (388.261292 --> 387.254364).\n",
            "\t Train_Loss: 144.9865 Val_Loss: 387.2544  BEST VAL Loss: 387.2544\n",
            "\n",
            "Epoch 457: Validation loss decreased (387.254364 --> 386.259033).\n",
            "\t Train_Loss: 144.8342 Val_Loss: 386.2590  BEST VAL Loss: 386.2590\n",
            "\n",
            "Epoch 458: Validation loss decreased (386.259033 --> 385.275726).\n",
            "\t Train_Loss: 144.6853 Val_Loss: 385.2757  BEST VAL Loss: 385.2757\n",
            "\n",
            "Epoch 459: Validation loss decreased (385.275726 --> 384.303802).\n",
            "\t Train_Loss: 144.5396 Val_Loss: 384.3038  BEST VAL Loss: 384.3038\n",
            "\n",
            "Epoch 460: Validation loss decreased (384.303802 --> 383.343323).\n",
            "\t Train_Loss: 144.3969 Val_Loss: 383.3433  BEST VAL Loss: 383.3433\n",
            "\n",
            "Epoch 461: Validation loss decreased (383.343323 --> 382.394592).\n",
            "\t Train_Loss: 144.2574 Val_Loss: 382.3946  BEST VAL Loss: 382.3946\n",
            "\n",
            "Epoch 462: Validation loss decreased (382.394592 --> 381.457001).\n",
            "\t Train_Loss: 144.1210 Val_Loss: 381.4570  BEST VAL Loss: 381.4570\n",
            "\n",
            "Epoch 463: Validation loss decreased (381.457001 --> 380.531006).\n",
            "\t Train_Loss: 143.9875 Val_Loss: 380.5310  BEST VAL Loss: 380.5310\n",
            "\n",
            "Epoch 464: Validation loss decreased (380.531006 --> 379.615265).\n",
            "\t Train_Loss: 143.8570 Val_Loss: 379.6153  BEST VAL Loss: 379.6153\n",
            "\n",
            "Epoch 465: Validation loss decreased (379.615265 --> 378.711151).\n",
            "\t Train_Loss: 143.7293 Val_Loss: 378.7112  BEST VAL Loss: 378.7112\n",
            "\n",
            "Epoch 466: Validation loss decreased (378.711151 --> 377.817352).\n",
            "\t Train_Loss: 143.6045 Val_Loss: 377.8174  BEST VAL Loss: 377.8174\n",
            "\n",
            "Epoch 467: Validation loss decreased (377.817352 --> 376.934631).\n",
            "\t Train_Loss: 143.4823 Val_Loss: 376.9346  BEST VAL Loss: 376.9346\n",
            "\n",
            "Epoch 468: Validation loss decreased (376.934631 --> 376.061890).\n",
            "\t Train_Loss: 143.3629 Val_Loss: 376.0619  BEST VAL Loss: 376.0619\n",
            "\n",
            "Epoch 469: Validation loss decreased (376.061890 --> 375.200104).\n",
            "\t Train_Loss: 143.2461 Val_Loss: 375.2001  BEST VAL Loss: 375.2001\n",
            "\n",
            "Epoch 470: Validation loss decreased (375.200104 --> 374.348206).\n",
            "\t Train_Loss: 143.1319 Val_Loss: 374.3482  BEST VAL Loss: 374.3482\n",
            "\n",
            "Epoch 471: Validation loss decreased (374.348206 --> 373.506683).\n",
            "\t Train_Loss: 143.0202 Val_Loss: 373.5067  BEST VAL Loss: 373.5067\n",
            "\n",
            "Epoch 472: Validation loss decreased (373.506683 --> 372.675201).\n",
            "\t Train_Loss: 142.9110 Val_Loss: 372.6752  BEST VAL Loss: 372.6752\n",
            "\n",
            "Epoch 473: Validation loss decreased (372.675201 --> 371.853577).\n",
            "\t Train_Loss: 142.8042 Val_Loss: 371.8536  BEST VAL Loss: 371.8536\n",
            "\n",
            "Epoch 474: Validation loss decreased (371.853577 --> 371.042084).\n",
            "\t Train_Loss: 142.6998 Val_Loss: 371.0421  BEST VAL Loss: 371.0421\n",
            "\n",
            "Epoch 475: Validation loss decreased (371.042084 --> 370.239838).\n",
            "\t Train_Loss: 142.5977 Val_Loss: 370.2398  BEST VAL Loss: 370.2398\n",
            "\n",
            "Epoch 476: Validation loss decreased (370.239838 --> 369.447601).\n",
            "\t Train_Loss: 142.4978 Val_Loss: 369.4476  BEST VAL Loss: 369.4476\n",
            "\n",
            "Epoch 477: Validation loss decreased (369.447601 --> 368.664764).\n",
            "\t Train_Loss: 142.4003 Val_Loss: 368.6648  BEST VAL Loss: 368.6648\n",
            "\n",
            "Epoch 478: Validation loss decreased (368.664764 --> 367.891388).\n",
            "\t Train_Loss: 142.3048 Val_Loss: 367.8914  BEST VAL Loss: 367.8914\n",
            "\n",
            "Epoch 479: Validation loss decreased (367.891388 --> 367.127167).\n",
            "\t Train_Loss: 142.2115 Val_Loss: 367.1272  BEST VAL Loss: 367.1272\n",
            "\n",
            "Epoch 480: Validation loss decreased (367.127167 --> 366.372314).\n",
            "\t Train_Loss: 142.1203 Val_Loss: 366.3723  BEST VAL Loss: 366.3723\n",
            "\n",
            "Epoch 481: Validation loss decreased (366.372314 --> 365.626160).\n",
            "\t Train_Loss: 142.0311 Val_Loss: 365.6262  BEST VAL Loss: 365.6262\n",
            "\n",
            "Epoch 482: Validation loss decreased (365.626160 --> 364.889404).\n",
            "\t Train_Loss: 141.9438 Val_Loss: 364.8894  BEST VAL Loss: 364.8894\n",
            "\n",
            "Epoch 483: Validation loss decreased (364.889404 --> 364.161346).\n",
            "\t Train_Loss: 141.8584 Val_Loss: 364.1613  BEST VAL Loss: 364.1613\n",
            "\n",
            "Epoch 484: Validation loss decreased (364.161346 --> 363.442139).\n",
            "\t Train_Loss: 141.7746 Val_Loss: 363.4421  BEST VAL Loss: 363.4421\n",
            "\n",
            "Epoch 485: Validation loss decreased (363.442139 --> 362.731598).\n",
            "\t Train_Loss: 141.6926 Val_Loss: 362.7316  BEST VAL Loss: 362.7316\n",
            "\n",
            "Epoch 486: Validation loss decreased (362.731598 --> 362.029877).\n",
            "\t Train_Loss: 141.6119 Val_Loss: 362.0299  BEST VAL Loss: 362.0299\n",
            "\n",
            "Epoch 487: Validation loss decreased (362.029877 --> 361.336823).\n",
            "\t Train_Loss: 141.5324 Val_Loss: 361.3368  BEST VAL Loss: 361.3368\n",
            "\n",
            "Epoch 488: Validation loss decreased (361.336823 --> 360.652313).\n",
            "\t Train_Loss: 141.4532 Val_Loss: 360.6523  BEST VAL Loss: 360.6523\n",
            "\n",
            "Epoch 489: Validation loss decreased (360.652313 --> 359.977203).\n",
            "\t Train_Loss: 141.3734 Val_Loss: 359.9772  BEST VAL Loss: 359.9772\n",
            "\n",
            "Epoch 490: Validation loss decreased (359.977203 --> 359.312561).\n",
            "\t Train_Loss: 141.2901 Val_Loss: 359.3126  BEST VAL Loss: 359.3126\n",
            "\n",
            "Epoch 491: Validation loss decreased (359.312561 --> 358.662354).\n",
            "\t Train_Loss: 141.1969 Val_Loss: 358.6624  BEST VAL Loss: 358.6624\n",
            "\n",
            "Epoch 492: Validation loss decreased (358.662354 --> 358.047821).\n",
            "\t Train_Loss: 141.0736 Val_Loss: 358.0478  BEST VAL Loss: 358.0478\n",
            "\n",
            "Epoch 493: Validation loss decreased (358.047821 --> 357.661224).\n",
            "\t Train_Loss: 140.8381 Val_Loss: 357.6612  BEST VAL Loss: 357.6612\n",
            "\n",
            "Epoch 494: Validation loss did not decrease\n",
            "\t Train_Loss: 139.9935 Val_Loss: 360.6384  BEST VAL Loss: 357.6612\n",
            "\n",
            "Epoch 495: Validation loss did not decrease\n",
            "\t Train_Loss: 136.8905 Val_Loss: 393.9738  BEST VAL Loss: 357.6612\n",
            "\n",
            "Epoch 496: Validation loss did not decrease\n",
            "\t Train_Loss: 143.9043 Val_Loss: 374.3489  BEST VAL Loss: 357.6612\n",
            "\n",
            "Epoch 497: Validation loss decreased (357.661224 --> 357.544037).\n",
            "\t Train_Loss: 138.5604 Val_Loss: 357.5440  BEST VAL Loss: 357.5440\n",
            "\n",
            "Epoch 498: Validation loss decreased (357.544037 --> 354.640442).\n",
            "\t Train_Loss: 136.7865 Val_Loss: 354.6404  BEST VAL Loss: 354.6404\n",
            "\n",
            "Epoch 499: Validation loss decreased (354.640442 --> 353.644867).\n",
            "\t Train_Loss: 138.6083 Val_Loss: 353.6449  BEST VAL Loss: 353.6449\n",
            "\n",
            "Epoch 500: Validation loss did not decrease\n",
            "\t Train_Loss: 139.0454 Val_Loss: 354.1560  BEST VAL Loss: 353.6449\n",
            "\n",
            "Epoch 501: Validation loss did not decrease\n",
            "\t Train_Loss: 137.4620 Val_Loss: 367.7867  BEST VAL Loss: 353.6449\n",
            "\n",
            "Epoch 502: Validation loss did not decrease\n",
            "\t Train_Loss: 137.0639 Val_Loss: 359.2672  BEST VAL Loss: 353.6449\n",
            "\n",
            "Epoch 503: Validation loss did not decrease\n",
            "\t Train_Loss: 135.6401 Val_Loss: 353.9251  BEST VAL Loss: 353.6449\n",
            "\n",
            "Epoch 504: Validation loss decreased (353.644867 --> 353.326202).\n",
            "\t Train_Loss: 135.5719 Val_Loss: 353.3262  BEST VAL Loss: 353.3262\n",
            "\n",
            "Epoch 505: Validation loss did not decrease\n",
            "\t Train_Loss: 134.6248 Val_Loss: 356.1832  BEST VAL Loss: 353.3262\n",
            "\n",
            "Epoch 506: Validation loss did not decrease\n",
            "\t Train_Loss: 132.4559 Val_Loss: 363.2949  BEST VAL Loss: 353.3262\n",
            "\n",
            "Epoch 507: Validation loss did not decrease\n",
            "\t Train_Loss: 131.1983 Val_Loss: 367.2430  BEST VAL Loss: 353.3262\n",
            "\n",
            "Epoch 508: Validation loss did not decrease\n",
            "\t Train_Loss: 131.3945 Val_Loss: 365.0990  BEST VAL Loss: 353.3262\n",
            "\n",
            "Epoch 509: Validation loss did not decrease\n",
            "\t Train_Loss: 131.0692 Val_Loss: 359.9670  BEST VAL Loss: 353.3262\n",
            "\n",
            "Epoch 510: Validation loss did not decrease\n",
            "\t Train_Loss: 127.6748 Val_Loss: 378.3441  BEST VAL Loss: 353.3262\n",
            "\n",
            "Epoch 511: Validation loss did not decrease\n",
            "\t Train_Loss: 131.4376 Val_Loss: 358.1859  BEST VAL Loss: 353.3262\n",
            "\n",
            "Epoch 512: Validation loss decreased (353.326202 --> 351.034760).\n",
            "\t Train_Loss: 126.6113 Val_Loss: 351.0348  BEST VAL Loss: 351.0348\n",
            "\n",
            "Epoch 513: Validation loss did not decrease\n",
            "\t Train_Loss: 127.2644 Val_Loss: 351.3383  BEST VAL Loss: 351.0348\n",
            "\n",
            "Epoch 514: Validation loss decreased (351.034760 --> 349.938568).\n",
            "\t Train_Loss: 128.3699 Val_Loss: 349.9386  BEST VAL Loss: 349.9386\n",
            "\n",
            "Epoch 515: Validation loss decreased (349.938568 --> 347.661682).\n",
            "\t Train_Loss: 127.8875 Val_Loss: 347.6617  BEST VAL Loss: 347.6617\n",
            "\n",
            "Epoch 516: Validation loss did not decrease\n",
            "\t Train_Loss: 126.5197 Val_Loss: 348.0484  BEST VAL Loss: 347.6617\n",
            "\n",
            "Epoch 517: Validation loss did not decrease\n",
            "\t Train_Loss: 125.5581 Val_Loss: 351.3368  BEST VAL Loss: 347.6617\n",
            "\n",
            "Epoch 518: Validation loss did not decrease\n",
            "\t Train_Loss: 125.5399 Val_Loss: 353.2250  BEST VAL Loss: 347.6617\n",
            "\n",
            "Epoch 519: Validation loss did not decrease\n",
            "\t Train_Loss: 124.9660 Val_Loss: 351.5465  BEST VAL Loss: 347.6617\n",
            "\n",
            "Epoch 520: Validation loss did not decrease\n",
            "\t Train_Loss: 122.7613 Val_Loss: 350.2167  BEST VAL Loss: 347.6617\n",
            "\n",
            "Epoch 521: Validation loss did not decrease\n",
            "\t Train_Loss: 120.6194 Val_Loss: 352.5027  BEST VAL Loss: 347.6617\n",
            "\n",
            "Epoch 522: Validation loss did not decrease\n",
            "\t Train_Loss: 120.4549 Val_Loss: 353.0597  BEST VAL Loss: 347.6617\n",
            "\n",
            "Epoch 523: Validation loss did not decrease\n",
            "\t Train_Loss: 119.3647 Val_Loss: 355.4754  BEST VAL Loss: 347.6617\n",
            "\n",
            "Epoch 524: Validation loss decreased (347.661682 --> 346.497772).\n",
            "\t Train_Loss: 119.2326 Val_Loss: 346.4978  BEST VAL Loss: 346.4978\n",
            "\n",
            "Epoch 525: Validation loss did not decrease\n",
            "\t Train_Loss: 116.0182 Val_Loss: 353.3597  BEST VAL Loss: 346.4978\n",
            "\n",
            "Epoch 526: Validation loss did not decrease\n",
            "\t Train_Loss: 116.6459 Val_Loss: 352.7937  BEST VAL Loss: 346.4978\n",
            "\n",
            "Epoch 527: Validation loss decreased (346.497772 --> 339.791565).\n",
            "\t Train_Loss: 116.6046 Val_Loss: 339.7916  BEST VAL Loss: 339.7916\n",
            "\n",
            "Epoch 528: Validation loss decreased (339.791565 --> 338.425995).\n",
            "\t Train_Loss: 115.1045 Val_Loss: 338.4260  BEST VAL Loss: 338.4260\n",
            "\n",
            "Epoch 529: Validation loss did not decrease\n",
            "\t Train_Loss: 115.3541 Val_Loss: 339.5791  BEST VAL Loss: 338.4260\n",
            "\n",
            "Epoch 530: Validation loss did not decrease\n",
            "\t Train_Loss: 114.5401 Val_Loss: 341.8606  BEST VAL Loss: 338.4260\n",
            "\n",
            "Epoch 531: Validation loss did not decrease\n",
            "\t Train_Loss: 113.4018 Val_Loss: 345.2011  BEST VAL Loss: 338.4260\n",
            "\n",
            "Epoch 532: Validation loss did not decrease\n",
            "\t Train_Loss: 113.2987 Val_Loss: 342.9049  BEST VAL Loss: 338.4260\n",
            "\n",
            "Epoch 533: Validation loss decreased (338.425995 --> 336.205170).\n",
            "\t Train_Loss: 112.1346 Val_Loss: 336.2052  BEST VAL Loss: 336.2052\n",
            "\n",
            "Epoch 534: Validation loss decreased (336.205170 --> 334.184052).\n",
            "\t Train_Loss: 109.7088 Val_Loss: 334.1841  BEST VAL Loss: 334.1841\n",
            "\n",
            "Epoch 535: Validation loss did not decrease\n",
            "\t Train_Loss: 108.3185 Val_Loss: 339.2343  BEST VAL Loss: 334.1841\n",
            "\n",
            "Epoch 536: Validation loss did not decrease\n",
            "\t Train_Loss: 109.0409 Val_Loss: 338.1974  BEST VAL Loss: 334.1841\n",
            "\n",
            "Epoch 537: Validation loss decreased (334.184052 --> 331.561920).\n",
            "\t Train_Loss: 108.6725 Val_Loss: 331.5619  BEST VAL Loss: 331.5619\n",
            "\n",
            "Epoch 538: Validation loss decreased (331.561920 --> 329.446228).\n",
            "\t Train_Loss: 106.7324 Val_Loss: 329.4462  BEST VAL Loss: 329.4462\n",
            "\n",
            "Epoch 539: Validation loss did not decrease\n",
            "\t Train_Loss: 105.6255 Val_Loss: 332.5381  BEST VAL Loss: 329.4462\n",
            "\n",
            "Epoch 540: Validation loss did not decrease\n",
            "\t Train_Loss: 105.4170 Val_Loss: 330.4851  BEST VAL Loss: 329.4462\n",
            "\n",
            "Epoch 541: Validation loss decreased (329.446228 --> 328.330292).\n",
            "\t Train_Loss: 104.5305 Val_Loss: 328.3303  BEST VAL Loss: 328.3303\n",
            "\n",
            "Epoch 542: Validation loss decreased (328.330292 --> 326.786713).\n",
            "\t Train_Loss: 103.9540 Val_Loss: 326.7867  BEST VAL Loss: 326.7867\n",
            "\n",
            "Epoch 543: Validation loss decreased (326.786713 --> 325.517639).\n",
            "\t Train_Loss: 103.3091 Val_Loss: 325.5176  BEST VAL Loss: 325.5176\n",
            "\n",
            "Epoch 544: Validation loss decreased (325.517639 --> 324.672607).\n",
            "\t Train_Loss: 102.5190 Val_Loss: 324.6726  BEST VAL Loss: 324.6726\n",
            "\n",
            "Epoch 545: Validation loss decreased (324.672607 --> 323.388397).\n",
            "\t Train_Loss: 101.9366 Val_Loss: 323.3884  BEST VAL Loss: 323.3884\n",
            "\n",
            "Epoch 546: Validation loss decreased (323.388397 --> 321.159058).\n",
            "\t Train_Loss: 101.3930 Val_Loss: 321.1591  BEST VAL Loss: 321.1591\n",
            "\n",
            "Epoch 547: Validation loss decreased (321.159058 --> 319.209808).\n",
            "\t Train_Loss: 100.6037 Val_Loss: 319.2098  BEST VAL Loss: 319.2098\n",
            "\n",
            "Epoch 548: Validation loss decreased (319.209808 --> 318.736420).\n",
            "\t Train_Loss: 99.7589 Val_Loss: 318.7364  BEST VAL Loss: 318.7364\n",
            "\n",
            "Epoch 549: Validation loss did not decrease\n",
            "\t Train_Loss: 99.1039 Val_Loss: 319.2472  BEST VAL Loss: 318.7364\n",
            "\n",
            "Epoch 550: Validation loss decreased (318.736420 --> 318.337067).\n",
            "\t Train_Loss: 98.7516 Val_Loss: 318.3371  BEST VAL Loss: 318.3371\n",
            "\n",
            "Epoch 551: Validation loss decreased (318.337067 --> 316.269501).\n",
            "\t Train_Loss: 98.2032 Val_Loss: 316.2695  BEST VAL Loss: 316.2695\n",
            "\n",
            "Epoch 552: Validation loss decreased (316.269501 --> 314.352753).\n",
            "\t Train_Loss: 97.5244 Val_Loss: 314.3528  BEST VAL Loss: 314.3528\n",
            "\n",
            "Epoch 553: Validation loss decreased (314.352753 --> 312.772217).\n",
            "\t Train_Loss: 96.9547 Val_Loss: 312.7722  BEST VAL Loss: 312.7722\n",
            "\n",
            "Epoch 554: Validation loss decreased (312.772217 --> 311.596222).\n",
            "\t Train_Loss: 96.3606 Val_Loss: 311.5962  BEST VAL Loss: 311.5962\n",
            "\n",
            "Epoch 555: Validation loss decreased (311.596222 --> 310.666107).\n",
            "\t Train_Loss: 95.7739 Val_Loss: 310.6661  BEST VAL Loss: 310.6661\n",
            "\n",
            "Epoch 556: Validation loss decreased (310.666107 --> 309.540985).\n",
            "\t Train_Loss: 95.2589 Val_Loss: 309.5410  BEST VAL Loss: 309.5410\n",
            "\n",
            "Epoch 557: Validation loss decreased (309.540985 --> 308.037262).\n",
            "\t Train_Loss: 94.7310 Val_Loss: 308.0373  BEST VAL Loss: 308.0373\n",
            "\n",
            "Epoch 558: Validation loss decreased (308.037262 --> 306.736694).\n",
            "\t Train_Loss: 94.0921 Val_Loss: 306.7367  BEST VAL Loss: 306.7367\n",
            "\n",
            "Epoch 559: Validation loss did not decrease\n",
            "\t Train_Loss: 93.2227 Val_Loss: 310.4561  BEST VAL Loss: 306.7367\n",
            "\n",
            "Epoch 560: Validation loss decreased (306.736694 --> 303.300598).\n",
            "\t Train_Loss: 93.4292 Val_Loss: 303.3006  BEST VAL Loss: 303.3006\n",
            "\n",
            "Epoch 561: Validation loss decreased (303.300598 --> 301.260376).\n",
            "\t Train_Loss: 92.1302 Val_Loss: 301.2604  BEST VAL Loss: 301.2604\n",
            "\n",
            "Epoch 562: Validation loss decreased (301.260376 --> 300.992126).\n",
            "\t Train_Loss: 91.9584 Val_Loss: 300.9921  BEST VAL Loss: 300.9921\n",
            "\n",
            "Epoch 563: Validation loss did not decrease\n",
            "\t Train_Loss: 91.3868 Val_Loss: 301.5961  BEST VAL Loss: 300.9921\n",
            "\n",
            "Epoch 564: Validation loss did not decrease\n",
            "\t Train_Loss: 90.8283 Val_Loss: 301.8946  BEST VAL Loss: 300.9921\n",
            "\n",
            "Epoch 565: Validation loss decreased (300.992126 --> 300.784515).\n",
            "\t Train_Loss: 90.4214 Val_Loss: 300.7845  BEST VAL Loss: 300.7845\n",
            "\n",
            "Epoch 566: Validation loss decreased (300.784515 --> 298.609528).\n",
            "\t Train_Loss: 89.8220 Val_Loss: 298.6095  BEST VAL Loss: 298.6095\n",
            "\n",
            "Epoch 567: Validation loss decreased (298.609528 --> 296.580292).\n",
            "\t Train_Loss: 89.1113 Val_Loss: 296.5803  BEST VAL Loss: 296.5803\n",
            "\n",
            "Epoch 568: Validation loss decreased (296.580292 --> 295.349762).\n",
            "\t Train_Loss: 88.5995 Val_Loss: 295.3498  BEST VAL Loss: 295.3498\n",
            "\n",
            "Epoch 569: Validation loss decreased (295.349762 --> 294.897552).\n",
            "\t Train_Loss: 88.1710 Val_Loss: 294.8976  BEST VAL Loss: 294.8976\n",
            "\n",
            "Epoch 570: Validation loss did not decrease\n",
            "\t Train_Loss: 87.6242 Val_Loss: 294.9177  BEST VAL Loss: 294.8976\n",
            "\n",
            "Epoch 571: Validation loss decreased (294.897552 --> 294.690063).\n",
            "\t Train_Loss: 87.0670 Val_Loss: 294.6901  BEST VAL Loss: 294.6901\n",
            "\n",
            "Epoch 572: Validation loss decreased (294.690063 --> 293.355377).\n",
            "\t Train_Loss: 86.5981 Val_Loss: 293.3554  BEST VAL Loss: 293.3554\n",
            "\n",
            "Epoch 573: Validation loss decreased (293.355377 --> 291.126678).\n",
            "\t Train_Loss: 86.0406 Val_Loss: 291.1267  BEST VAL Loss: 291.1267\n",
            "\n",
            "Epoch 574: Validation loss decreased (291.126678 --> 288.865204).\n",
            "\t Train_Loss: 85.4297 Val_Loss: 288.8652  BEST VAL Loss: 288.8652\n",
            "\n",
            "Epoch 575: Validation loss decreased (288.865204 --> 287.233490).\n",
            "\t Train_Loss: 84.9357 Val_Loss: 287.2335  BEST VAL Loss: 287.2335\n",
            "\n",
            "Epoch 576: Validation loss decreased (287.233490 --> 286.366943).\n",
            "\t Train_Loss: 84.5058 Val_Loss: 286.3669  BEST VAL Loss: 286.3669\n",
            "\n",
            "Epoch 577: Validation loss decreased (286.366943 --> 285.973511).\n",
            "\t Train_Loss: 84.0246 Val_Loss: 285.9735  BEST VAL Loss: 285.9735\n",
            "\n",
            "Epoch 578: Validation loss decreased (285.973511 --> 285.481537).\n",
            "\t Train_Loss: 83.5171 Val_Loss: 285.4815  BEST VAL Loss: 285.4815\n",
            "\n",
            "Epoch 579: Validation loss decreased (285.481537 --> 284.565521).\n",
            "\t Train_Loss: 83.0245 Val_Loss: 284.5655  BEST VAL Loss: 284.5655\n",
            "\n",
            "Epoch 580: Validation loss decreased (284.565521 --> 283.280426).\n",
            "\t Train_Loss: 82.5277 Val_Loss: 283.2804  BEST VAL Loss: 283.2804\n",
            "\n",
            "Epoch 581: Validation loss decreased (283.280426 --> 281.895996).\n",
            "\t Train_Loss: 82.0233 Val_Loss: 281.8960  BEST VAL Loss: 281.8960\n",
            "\n",
            "Epoch 582: Validation loss decreased (281.895996 --> 280.689789).\n",
            "\t Train_Loss: 81.5370 Val_Loss: 280.6898  BEST VAL Loss: 280.6898\n",
            "\n",
            "Epoch 583: Validation loss decreased (280.689789 --> 279.795013).\n",
            "\t Train_Loss: 81.0751 Val_Loss: 279.7950  BEST VAL Loss: 279.7950\n",
            "\n",
            "Epoch 584: Validation loss decreased (279.795013 --> 279.153534).\n",
            "\t Train_Loss: 80.6183 Val_Loss: 279.1535  BEST VAL Loss: 279.1535\n",
            "\n",
            "Epoch 585: Validation loss decreased (279.153534 --> 278.542694).\n",
            "\t Train_Loss: 80.1614 Val_Loss: 278.5427  BEST VAL Loss: 278.5427\n",
            "\n",
            "Epoch 586: Validation loss decreased (278.542694 --> 277.668579).\n",
            "\t Train_Loss: 79.7133 Val_Loss: 277.6686  BEST VAL Loss: 277.6686\n",
            "\n",
            "Epoch 587: Validation loss decreased (277.668579 --> 276.370422).\n",
            "\t Train_Loss: 79.2656 Val_Loss: 276.3704  BEST VAL Loss: 276.3704\n",
            "\n",
            "Epoch 588: Validation loss decreased (276.370422 --> 274.809784).\n",
            "\t Train_Loss: 78.8056 Val_Loss: 274.8098  BEST VAL Loss: 274.8098\n",
            "\n",
            "Epoch 589: Validation loss decreased (274.809784 --> 273.311676).\n",
            "\t Train_Loss: 78.3529 Val_Loss: 273.3117  BEST VAL Loss: 273.3117\n",
            "\n",
            "Epoch 590: Validation loss decreased (273.311676 --> 272.096741).\n",
            "\t Train_Loss: 77.9243 Val_Loss: 272.0967  BEST VAL Loss: 272.0967\n",
            "\n",
            "Epoch 591: Validation loss decreased (272.096741 --> 271.192047).\n",
            "\t Train_Loss: 77.5063 Val_Loss: 271.1920  BEST VAL Loss: 271.1920\n",
            "\n",
            "Epoch 592: Validation loss decreased (271.192047 --> 270.466095).\n",
            "\t Train_Loss: 77.0876 Val_Loss: 270.4661  BEST VAL Loss: 270.4661\n",
            "\n",
            "Epoch 593: Validation loss decreased (270.466095 --> 269.705902).\n",
            "\t Train_Loss: 76.6740 Val_Loss: 269.7059  BEST VAL Loss: 269.7059\n",
            "\n",
            "Epoch 594: Validation loss decreased (269.705902 --> 268.747162).\n",
            "\t Train_Loss: 76.2669 Val_Loss: 268.7472  BEST VAL Loss: 268.7472\n",
            "\n",
            "Epoch 595: Validation loss decreased (268.747162 --> 267.600800).\n",
            "\t Train_Loss: 75.8596 Val_Loss: 267.6008  BEST VAL Loss: 267.6008\n",
            "\n",
            "Epoch 596: Validation loss decreased (267.600800 --> 266.421356).\n",
            "\t Train_Loss: 75.4552 Val_Loss: 266.4214  BEST VAL Loss: 266.4214\n",
            "\n",
            "Epoch 597: Validation loss decreased (266.421356 --> 265.362305).\n",
            "\t Train_Loss: 75.0616 Val_Loss: 265.3623  BEST VAL Loss: 265.3623\n",
            "\n",
            "Epoch 598: Validation loss decreased (265.362305 --> 264.475494).\n",
            "\t Train_Loss: 74.6759 Val_Loss: 264.4755  BEST VAL Loss: 264.4755\n",
            "\n",
            "Epoch 599: Validation loss decreased (264.475494 --> 263.697540).\n",
            "\t Train_Loss: 74.2928 Val_Loss: 263.6975  BEST VAL Loss: 263.6975\n",
            "\n",
            "Epoch 600: Validation loss decreased (263.697540 --> 262.894714).\n",
            "\t Train_Loss: 73.9148 Val_Loss: 262.8947  BEST VAL Loss: 262.8947\n",
            "\n",
            "Epoch 601: Validation loss decreased (262.894714 --> 261.943939).\n",
            "\t Train_Loss: 73.5445 Val_Loss: 261.9439  BEST VAL Loss: 261.9439\n",
            "\n",
            "Epoch 602: Validation loss decreased (261.943939 --> 260.822968).\n",
            "\t Train_Loss: 73.1773 Val_Loss: 260.8230  BEST VAL Loss: 260.8230\n",
            "\n",
            "Epoch 603: Validation loss decreased (260.822968 --> 259.623779).\n",
            "\t Train_Loss: 72.8106 Val_Loss: 259.6238  BEST VAL Loss: 259.6238\n",
            "\n",
            "Epoch 604: Validation loss decreased (259.623779 --> 258.477448).\n",
            "\t Train_Loss: 72.4483 Val_Loss: 258.4774  BEST VAL Loss: 258.4774\n",
            "\n",
            "Epoch 605: Validation loss decreased (258.477448 --> 257.466858).\n",
            "\t Train_Loss: 72.0924 Val_Loss: 257.4669  BEST VAL Loss: 257.4669\n",
            "\n",
            "Epoch 606: Validation loss decreased (257.466858 --> 256.587158).\n",
            "\t Train_Loss: 71.7410 Val_Loss: 256.5872  BEST VAL Loss: 256.5872\n",
            "\n",
            "Epoch 607: Validation loss decreased (256.587158 --> 255.759567).\n",
            "\t Train_Loss: 71.3938 Val_Loss: 255.7596  BEST VAL Loss: 255.7596\n",
            "\n",
            "Epoch 608: Validation loss decreased (255.759567 --> 254.887283).\n",
            "\t Train_Loss: 71.0521 Val_Loss: 254.8873  BEST VAL Loss: 254.8873\n",
            "\n",
            "Epoch 609: Validation loss decreased (254.887283 --> 253.929153).\n",
            "\t Train_Loss: 70.7140 Val_Loss: 253.9292  BEST VAL Loss: 253.9292\n",
            "\n",
            "Epoch 610: Validation loss decreased (253.929153 --> 252.922531).\n",
            "\t Train_Loss: 70.3778 Val_Loss: 252.9225  BEST VAL Loss: 252.9225\n",
            "\n",
            "Epoch 611: Validation loss decreased (252.922531 --> 251.938889).\n",
            "\t Train_Loss: 70.0447 Val_Loss: 251.9389  BEST VAL Loss: 251.9389\n",
            "\n",
            "Epoch 612: Validation loss decreased (251.938889 --> 251.026855).\n",
            "\t Train_Loss: 69.7155 Val_Loss: 251.0269  BEST VAL Loss: 251.0269\n",
            "\n",
            "Epoch 613: Validation loss decreased (251.026855 --> 250.187012).\n",
            "\t Train_Loss: 69.3896 Val_Loss: 250.1870  BEST VAL Loss: 250.1870\n",
            "\n",
            "Epoch 614: Validation loss decreased (250.187012 --> 249.377151).\n",
            "\t Train_Loss: 69.0669 Val_Loss: 249.3772  BEST VAL Loss: 249.3772\n",
            "\n",
            "Epoch 615: Validation loss decreased (249.377151 --> 248.542221).\n",
            "\t Train_Loss: 68.7484 Val_Loss: 248.5422  BEST VAL Loss: 248.5422\n",
            "\n",
            "Epoch 616: Validation loss decreased (248.542221 --> 247.650879).\n",
            "\t Train_Loss: 68.4342 Val_Loss: 247.6509  BEST VAL Loss: 247.6509\n",
            "\n",
            "Epoch 617: Validation loss decreased (247.650879 --> 246.719437).\n",
            "\t Train_Loss: 68.1229 Val_Loss: 246.7194  BEST VAL Loss: 246.7194\n",
            "\n",
            "Epoch 618: Validation loss decreased (246.719437 --> 245.803497).\n",
            "\t Train_Loss: 67.8133 Val_Loss: 245.8035  BEST VAL Loss: 245.8035\n",
            "\n",
            "Epoch 619: Validation loss decreased (245.803497 --> 245.023300).\n",
            "\t Train_Loss: 67.5015 Val_Loss: 245.0233  BEST VAL Loss: 245.0233\n",
            "\n",
            "Epoch 620: Validation loss decreased (245.023300 --> 244.814072).\n",
            "\t Train_Loss: 67.1810 Val_Loss: 244.8141  BEST VAL Loss: 244.8141\n",
            "\n",
            "Epoch 621: Validation loss decreased (244.814072 --> 242.960449).\n",
            "\t Train_Loss: 66.9350 Val_Loss: 242.9604  BEST VAL Loss: 242.9604\n",
            "\n",
            "Epoch 622: Validation loss did not decrease\n",
            "\t Train_Loss: 66.5321 Val_Loss: 243.6642  BEST VAL Loss: 242.9604\n",
            "\n",
            "Epoch 623: Validation loss decreased (242.960449 --> 241.081909).\n",
            "\t Train_Loss: 66.3926 Val_Loss: 241.0819  BEST VAL Loss: 241.0819\n",
            "\n",
            "Epoch 624: Validation loss decreased (241.081909 --> 239.256348).\n",
            "\t Train_Loss: 65.8852 Val_Loss: 239.2563  BEST VAL Loss: 239.2563\n",
            "\n",
            "Epoch 625: Validation loss did not decrease\n",
            "\t Train_Loss: 65.6363 Val_Loss: 239.7746  BEST VAL Loss: 239.2563\n",
            "\n",
            "Epoch 626: Validation loss decreased (239.256348 --> 237.433884).\n",
            "\t Train_Loss: 65.4112 Val_Loss: 237.4339  BEST VAL Loss: 237.4339\n",
            "\n",
            "Epoch 627: Validation loss decreased (237.433884 --> 237.085495).\n",
            "\t Train_Loss: 65.1073 Val_Loss: 237.0855  BEST VAL Loss: 237.0855\n",
            "\n",
            "Epoch 628: Validation loss did not decrease\n",
            "\t Train_Loss: 64.8442 Val_Loss: 237.2733  BEST VAL Loss: 237.0855\n",
            "\n",
            "Epoch 629: Validation loss did not decrease\n",
            "\t Train_Loss: 64.5618 Val_Loss: 237.2278  BEST VAL Loss: 237.0855\n",
            "\n",
            "Epoch 630: Validation loss decreased (237.085495 --> 236.367676).\n",
            "\t Train_Loss: 64.3123 Val_Loss: 236.3677  BEST VAL Loss: 236.3677\n",
            "\n",
            "Epoch 631: Validation loss decreased (236.367676 --> 234.977783).\n",
            "\t Train_Loss: 63.9907 Val_Loss: 234.9778  BEST VAL Loss: 234.9778\n",
            "\n",
            "Epoch 632: Validation loss decreased (234.977783 --> 233.584915).\n",
            "\t Train_Loss: 63.6314 Val_Loss: 233.5849  BEST VAL Loss: 233.5849\n",
            "\n",
            "Epoch 633: Validation loss decreased (233.584915 --> 232.038162).\n",
            "\t Train_Loss: 63.3457 Val_Loss: 232.0382  BEST VAL Loss: 232.0382\n",
            "\n",
            "Epoch 634: Validation loss decreased (232.038162 --> 230.959518).\n",
            "\t Train_Loss: 63.1118 Val_Loss: 230.9595  BEST VAL Loss: 230.9595\n",
            "\n",
            "Epoch 635: Validation loss decreased (230.959518 --> 230.679916).\n",
            "\t Train_Loss: 62.8612 Val_Loss: 230.6799  BEST VAL Loss: 230.6799\n",
            "\n",
            "Epoch 636: Validation loss decreased (230.679916 --> 230.391724).\n",
            "\t Train_Loss: 62.5475 Val_Loss: 230.3917  BEST VAL Loss: 230.3917\n",
            "\n",
            "Epoch 637: Validation loss decreased (230.391724 --> 229.983200).\n",
            "\t Train_Loss: 62.2292 Val_Loss: 229.9832  BEST VAL Loss: 229.9832\n",
            "\n",
            "Epoch 638: Validation loss decreased (229.983200 --> 229.566605).\n",
            "\t Train_Loss: 61.9519 Val_Loss: 229.5666  BEST VAL Loss: 229.5666\n",
            "\n",
            "Epoch 639: Validation loss decreased (229.566605 --> 228.878342).\n",
            "\t Train_Loss: 61.7029 Val_Loss: 228.8783  BEST VAL Loss: 228.8783\n",
            "\n",
            "Epoch 640: Validation loss decreased (228.878342 --> 227.803177).\n",
            "\t Train_Loss: 61.4329 Val_Loss: 227.8032  BEST VAL Loss: 227.8032\n",
            "\n",
            "Epoch 641: Validation loss decreased (227.803177 --> 226.479538).\n",
            "\t Train_Loss: 61.1331 Val_Loss: 226.4795  BEST VAL Loss: 226.4795\n",
            "\n",
            "Epoch 642: Validation loss decreased (226.479538 --> 225.206985).\n",
            "\t Train_Loss: 60.8385 Val_Loss: 225.2070  BEST VAL Loss: 225.2070\n",
            "\n",
            "Epoch 643: Validation loss decreased (225.206985 --> 224.203369).\n",
            "\t Train_Loss: 60.5750 Val_Loss: 224.2034  BEST VAL Loss: 224.2034\n",
            "\n",
            "Epoch 644: Validation loss decreased (224.203369 --> 223.433640).\n",
            "\t Train_Loss: 60.3239 Val_Loss: 223.4336  BEST VAL Loss: 223.4336\n",
            "\n",
            "Epoch 645: Validation loss decreased (223.433640 --> 222.900391).\n",
            "\t Train_Loss: 60.0550 Val_Loss: 222.9004  BEST VAL Loss: 222.9004\n",
            "\n",
            "Epoch 646: Validation loss decreased (222.900391 --> 222.567749).\n",
            "\t Train_Loss: 59.7697 Val_Loss: 222.5677  BEST VAL Loss: 222.5677\n",
            "\n",
            "Epoch 647: Validation loss decreased (222.567749 --> 222.215332).\n",
            "\t Train_Loss: 59.4928 Val_Loss: 222.2153  BEST VAL Loss: 222.2153\n",
            "\n",
            "Epoch 648: Validation loss decreased (222.215332 --> 221.894150).\n",
            "\t Train_Loss: 59.2350 Val_Loss: 221.8941  BEST VAL Loss: 221.8941\n",
            "\n",
            "Epoch 649: Validation loss decreased (221.894150 --> 220.524170).\n",
            "\t Train_Loss: 59.0141 Val_Loss: 220.5242  BEST VAL Loss: 220.5242\n",
            "\n",
            "Epoch 650: Validation loss decreased (220.524170 --> 219.897415).\n",
            "\t Train_Loss: 58.7242 Val_Loss: 219.8974  BEST VAL Loss: 219.8974\n",
            "\n",
            "Epoch 651: Validation loss decreased (219.897415 --> 219.246078).\n",
            "\t Train_Loss: 58.4531 Val_Loss: 219.2461  BEST VAL Loss: 219.2461\n",
            "\n",
            "Epoch 652: Validation loss decreased (219.246078 --> 217.784088).\n",
            "\t Train_Loss: 58.2153 Val_Loss: 217.7841  BEST VAL Loss: 217.7841\n",
            "\n",
            "Epoch 653: Validation loss decreased (217.784088 --> 217.206543).\n",
            "\t Train_Loss: 57.9680 Val_Loss: 217.2065  BEST VAL Loss: 217.2065\n",
            "\n",
            "Epoch 654: Validation loss decreased (217.206543 --> 217.163071).\n",
            "\t Train_Loss: 57.7132 Val_Loss: 217.1631  BEST VAL Loss: 217.1631\n",
            "\n",
            "Epoch 655: Validation loss decreased (217.163071 --> 216.240280).\n",
            "\t Train_Loss: 57.4705 Val_Loss: 216.2403  BEST VAL Loss: 216.2403\n",
            "\n",
            "Epoch 656: Validation loss decreased (216.240280 --> 215.426178).\n",
            "\t Train_Loss: 57.2179 Val_Loss: 215.4262  BEST VAL Loss: 215.4262\n",
            "\n",
            "Epoch 657: Validation loss decreased (215.426178 --> 215.039841).\n",
            "\t Train_Loss: 56.9890 Val_Loss: 215.0398  BEST VAL Loss: 215.0398\n",
            "\n",
            "Epoch 658: Validation loss decreased (215.039841 --> 214.433456).\n",
            "\t Train_Loss: 56.7443 Val_Loss: 214.4335  BEST VAL Loss: 214.4335\n",
            "\n",
            "Epoch 659: Validation loss decreased (214.433456 --> 213.229523).\n",
            "\t Train_Loss: 56.5106 Val_Loss: 213.2295  BEST VAL Loss: 213.2295\n",
            "\n",
            "Epoch 660: Validation loss decreased (213.229523 --> 212.395004).\n",
            "\t Train_Loss: 56.2743 Val_Loss: 212.3950  BEST VAL Loss: 212.3950\n",
            "\n",
            "Epoch 661: Validation loss decreased (212.395004 --> 212.045456).\n",
            "\t Train_Loss: 56.0492 Val_Loss: 212.0455  BEST VAL Loss: 212.0455\n",
            "\n",
            "Epoch 662: Validation loss decreased (212.045456 --> 211.375977).\n",
            "\t Train_Loss: 55.8214 Val_Loss: 211.3760  BEST VAL Loss: 211.3760\n",
            "\n",
            "Epoch 663: Validation loss decreased (211.375977 --> 210.581497).\n",
            "\t Train_Loss: 55.5901 Val_Loss: 210.5815  BEST VAL Loss: 210.5815\n",
            "\n",
            "Epoch 664: Validation loss decreased (210.581497 --> 210.133789).\n",
            "\t Train_Loss: 55.3659 Val_Loss: 210.1338  BEST VAL Loss: 210.1338\n",
            "\n",
            "Epoch 665: Validation loss decreased (210.133789 --> 209.657349).\n",
            "\t Train_Loss: 55.1446 Val_Loss: 209.6573  BEST VAL Loss: 209.6573\n",
            "\n",
            "Epoch 666: Validation loss decreased (209.657349 --> 208.698410).\n",
            "\t Train_Loss: 54.9268 Val_Loss: 208.6984  BEST VAL Loss: 208.6984\n",
            "\n",
            "Epoch 667: Validation loss decreased (208.698410 --> 207.811630).\n",
            "\t Train_Loss: 54.7019 Val_Loss: 207.8116  BEST VAL Loss: 207.8116\n",
            "\n",
            "Epoch 668: Validation loss decreased (207.811630 --> 207.311386).\n",
            "\t Train_Loss: 54.4900 Val_Loss: 207.3114  BEST VAL Loss: 207.3114\n",
            "\n",
            "Epoch 669: Validation loss decreased (207.311386 --> 206.797470).\n",
            "\t Train_Loss: 54.2761 Val_Loss: 206.7975  BEST VAL Loss: 206.7975\n",
            "\n",
            "Epoch 670: Validation loss decreased (206.797470 --> 206.114410).\n",
            "\t Train_Loss: 54.0607 Val_Loss: 206.1144  BEST VAL Loss: 206.1144\n",
            "\n",
            "Epoch 671: Validation loss decreased (206.114410 --> 205.476791).\n",
            "\t Train_Loss: 53.8508 Val_Loss: 205.4768  BEST VAL Loss: 205.4768\n",
            "\n",
            "Epoch 672: Validation loss decreased (205.476791 --> 204.910919).\n",
            "\t Train_Loss: 53.6422 Val_Loss: 204.9109  BEST VAL Loss: 204.9109\n",
            "\n",
            "Epoch 673: Validation loss decreased (204.910919 --> 204.212921).\n",
            "\t Train_Loss: 53.4332 Val_Loss: 204.2129  BEST VAL Loss: 204.2129\n",
            "\n",
            "Epoch 674: Validation loss decreased (204.212921 --> 203.418762).\n",
            "\t Train_Loss: 53.2307 Val_Loss: 203.4188  BEST VAL Loss: 203.4188\n",
            "\n",
            "Epoch 675: Validation loss decreased (203.418762 --> 202.842880).\n",
            "\t Train_Loss: 53.0272 Val_Loss: 202.8429  BEST VAL Loss: 202.8429\n",
            "\n",
            "Epoch 676: Validation loss decreased (202.842880 --> 202.349289).\n",
            "\t Train_Loss: 52.8246 Val_Loss: 202.3493  BEST VAL Loss: 202.3493\n",
            "\n",
            "Epoch 677: Validation loss decreased (202.349289 --> 201.543686).\n",
            "\t Train_Loss: 52.6283 Val_Loss: 201.5437  BEST VAL Loss: 201.5437\n",
            "\n",
            "Epoch 678: Validation loss decreased (201.543686 --> 200.750931).\n",
            "\t Train_Loss: 52.4292 Val_Loss: 200.7509  BEST VAL Loss: 200.7509\n",
            "\n",
            "Epoch 679: Validation loss decreased (200.750931 --> 200.234573).\n",
            "\t Train_Loss: 52.2369 Val_Loss: 200.2346  BEST VAL Loss: 200.2346\n",
            "\n",
            "Epoch 680: Validation loss decreased (200.234573 --> 199.754868).\n",
            "\t Train_Loss: 52.0434 Val_Loss: 199.7549  BEST VAL Loss: 199.7549\n",
            "\n",
            "Epoch 681: Validation loss decreased (199.754868 --> 199.087708).\n",
            "\t Train_Loss: 51.8528 Val_Loss: 199.0877  BEST VAL Loss: 199.0877\n",
            "\n",
            "Epoch 682: Validation loss decreased (199.087708 --> 198.430710).\n",
            "\t Train_Loss: 51.6646 Val_Loss: 198.4307  BEST VAL Loss: 198.4307\n",
            "\n",
            "Epoch 683: Validation loss decreased (198.430710 --> 197.896286).\n",
            "\t Train_Loss: 51.4773 Val_Loss: 197.8963  BEST VAL Loss: 197.8963\n",
            "\n",
            "Epoch 684: Validation loss decreased (197.896286 --> 197.296600).\n",
            "\t Train_Loss: 51.2935 Val_Loss: 197.2966  BEST VAL Loss: 197.2966\n",
            "\n",
            "Epoch 685: Validation loss decreased (197.296600 --> 196.665237).\n",
            "\t Train_Loss: 51.1093 Val_Loss: 196.6652  BEST VAL Loss: 196.6652\n",
            "\n",
            "Epoch 686: Validation loss decreased (196.665237 --> 196.077637).\n",
            "\t Train_Loss: 50.9285 Val_Loss: 196.0776  BEST VAL Loss: 196.0776\n",
            "\n",
            "Epoch 687: Validation loss decreased (196.077637 --> 195.460724).\n",
            "\t Train_Loss: 50.7478 Val_Loss: 195.4607  BEST VAL Loss: 195.4607\n",
            "\n",
            "Epoch 688: Validation loss decreased (195.460724 --> 194.798492).\n",
            "\t Train_Loss: 50.5699 Val_Loss: 194.7985  BEST VAL Loss: 194.7985\n",
            "\n",
            "Epoch 689: Validation loss decreased (194.798492 --> 194.246063).\n",
            "\t Train_Loss: 50.3926 Val_Loss: 194.2461  BEST VAL Loss: 194.2461\n",
            "\n",
            "Epoch 690: Validation loss decreased (194.246063 --> 193.755981).\n",
            "\t Train_Loss: 50.2168 Val_Loss: 193.7560  BEST VAL Loss: 193.7560\n",
            "\n",
            "Epoch 691: Validation loss decreased (193.755981 --> 193.137741).\n",
            "\t Train_Loss: 50.0425 Val_Loss: 193.1377  BEST VAL Loss: 193.1377\n",
            "\n",
            "Epoch 692: Validation loss decreased (193.137741 --> 192.495163).\n",
            "\t Train_Loss: 49.8692 Val_Loss: 192.4952  BEST VAL Loss: 192.4952\n",
            "\n",
            "Epoch 693: Validation loss decreased (192.495163 --> 191.993729).\n",
            "\t Train_Loss: 49.6978 Val_Loss: 191.9937  BEST VAL Loss: 191.9937\n",
            "\n",
            "Epoch 694: Validation loss decreased (191.993729 --> 191.513794).\n",
            "\t Train_Loss: 49.5270 Val_Loss: 191.5138  BEST VAL Loss: 191.5138\n",
            "\n",
            "Epoch 695: Validation loss decreased (191.513794 --> 190.884567).\n",
            "\t Train_Loss: 49.3584 Val_Loss: 190.8846  BEST VAL Loss: 190.8846\n",
            "\n",
            "Epoch 696: Validation loss decreased (190.884567 --> 190.268814).\n",
            "\t Train_Loss: 49.1903 Val_Loss: 190.2688  BEST VAL Loss: 190.2688\n",
            "\n",
            "Epoch 697: Validation loss decreased (190.268814 --> 189.783020).\n",
            "\t Train_Loss: 49.0244 Val_Loss: 189.7830  BEST VAL Loss: 189.7830\n",
            "\n",
            "Epoch 698: Validation loss decreased (189.783020 --> 189.287476).\n",
            "\t Train_Loss: 48.8591 Val_Loss: 189.2875  BEST VAL Loss: 189.2875\n",
            "\n",
            "Epoch 699: Validation loss decreased (189.287476 --> 188.698517).\n",
            "\t Train_Loss: 48.6958 Val_Loss: 188.6985  BEST VAL Loss: 188.6985\n",
            "\n",
            "Epoch 700: Validation loss decreased (188.698517 --> 188.137848).\n",
            "\t Train_Loss: 48.5336 Val_Loss: 188.1378  BEST VAL Loss: 188.1378\n",
            "\n",
            "Epoch 701: Validation loss decreased (188.137848 --> 187.641800).\n",
            "\t Train_Loss: 48.3730 Val_Loss: 187.6418  BEST VAL Loss: 187.6418\n",
            "\n",
            "Epoch 702: Validation loss decreased (187.641800 --> 187.140244).\n",
            "\t Train_Loss: 48.2137 Val_Loss: 187.1402  BEST VAL Loss: 187.1402\n",
            "\n",
            "Epoch 703: Validation loss decreased (187.140244 --> 186.609924).\n",
            "\t Train_Loss: 48.0559 Val_Loss: 186.6099  BEST VAL Loss: 186.6099\n",
            "\n",
            "Epoch 704: Validation loss decreased (186.609924 --> 186.076767).\n",
            "\t Train_Loss: 47.8993 Val_Loss: 186.0768  BEST VAL Loss: 186.0768\n",
            "\n",
            "Epoch 705: Validation loss decreased (186.076767 --> 185.564468).\n",
            "\t Train_Loss: 47.7444 Val_Loss: 185.5645  BEST VAL Loss: 185.5645\n",
            "\n",
            "Epoch 706: Validation loss decreased (185.564468 --> 185.088486).\n",
            "\t Train_Loss: 47.5905 Val_Loss: 185.0885  BEST VAL Loss: 185.0885\n",
            "\n",
            "Epoch 707: Validation loss decreased (185.088486 --> 184.599487).\n",
            "\t Train_Loss: 47.4382 Val_Loss: 184.5995  BEST VAL Loss: 184.5995\n",
            "\n",
            "Epoch 708: Validation loss decreased (184.599487 --> 184.061050).\n",
            "\t Train_Loss: 47.2871 Val_Loss: 184.0611  BEST VAL Loss: 184.0611\n",
            "\n",
            "Epoch 709: Validation loss decreased (184.061050 --> 183.544922).\n",
            "\t Train_Loss: 47.1374 Val_Loss: 183.5449  BEST VAL Loss: 183.5449\n",
            "\n",
            "Epoch 710: Validation loss decreased (183.544922 --> 183.089462).\n",
            "\t Train_Loss: 46.9889 Val_Loss: 183.0895  BEST VAL Loss: 183.0895\n",
            "\n",
            "Epoch 711: Validation loss decreased (183.089462 --> 182.604782).\n",
            "\t Train_Loss: 46.8416 Val_Loss: 182.6048  BEST VAL Loss: 182.6048\n",
            "\n",
            "Epoch 712: Validation loss decreased (182.604782 --> 182.072693).\n",
            "\t Train_Loss: 46.6956 Val_Loss: 182.0727  BEST VAL Loss: 182.0727\n",
            "\n",
            "Epoch 713: Validation loss decreased (182.072693 --> 181.589386).\n",
            "\t Train_Loss: 46.5507 Val_Loss: 181.5894  BEST VAL Loss: 181.5894\n",
            "\n",
            "Epoch 714: Validation loss decreased (181.589386 --> 181.150146).\n",
            "\t Train_Loss: 46.4070 Val_Loss: 181.1501  BEST VAL Loss: 181.1501\n",
            "\n",
            "Epoch 715: Validation loss decreased (181.150146 --> 180.674744).\n",
            "\t Train_Loss: 46.2644 Val_Loss: 180.6747  BEST VAL Loss: 180.6747\n",
            "\n",
            "Epoch 716: Validation loss decreased (180.674744 --> 180.177811).\n",
            "\t Train_Loss: 46.1228 Val_Loss: 180.1778  BEST VAL Loss: 180.1778\n",
            "\n",
            "Epoch 717: Validation loss decreased (180.177811 --> 179.722000).\n",
            "\t Train_Loss: 45.9815 Val_Loss: 179.7220  BEST VAL Loss: 179.7220\n",
            "\n",
            "Epoch 718: Validation loss decreased (179.722000 --> 179.468048).\n",
            "\t Train_Loss: 45.8358 Val_Loss: 179.4680  BEST VAL Loss: 179.4680\n",
            "\n",
            "Epoch 719: Validation loss decreased (179.468048 --> 178.784897).\n",
            "\t Train_Loss: 45.6995 Val_Loss: 178.7849  BEST VAL Loss: 178.7849\n",
            "\n",
            "Epoch 720: Validation loss decreased (178.784897 --> 178.337082).\n",
            "\t Train_Loss: 45.5531 Val_Loss: 178.3371  BEST VAL Loss: 178.3371\n",
            "\n",
            "Epoch 721: Validation loss decreased (178.337082 --> 177.863571).\n",
            "\t Train_Loss: 45.4234 Val_Loss: 177.8636  BEST VAL Loss: 177.8636\n",
            "\n",
            "Epoch 722: Validation loss decreased (177.863571 --> 177.390137).\n",
            "\t Train_Loss: 45.2902 Val_Loss: 177.3901  BEST VAL Loss: 177.3901\n",
            "\n",
            "Epoch 723: Validation loss decreased (177.390137 --> 176.979355).\n",
            "\t Train_Loss: 45.1565 Val_Loss: 176.9794  BEST VAL Loss: 176.9794\n",
            "\n",
            "Epoch 724: Validation loss decreased (176.979355 --> 176.545029).\n",
            "\t Train_Loss: 45.0222 Val_Loss: 176.5450  BEST VAL Loss: 176.5450\n",
            "\n",
            "Epoch 725: Validation loss decreased (176.545029 --> 176.097672).\n",
            "\t Train_Loss: 44.8879 Val_Loss: 176.0977  BEST VAL Loss: 176.0977\n",
            "\n",
            "Epoch 726: Validation loss decreased (176.097672 --> 175.697586).\n",
            "\t Train_Loss: 44.7527 Val_Loss: 175.6976  BEST VAL Loss: 175.6976\n",
            "\n",
            "Epoch 727: Validation loss decreased (175.697586 --> 175.284332).\n",
            "\t Train_Loss: 44.6124 Val_Loss: 175.2843  BEST VAL Loss: 175.2843\n",
            "\n",
            "Epoch 728: Validation loss decreased (175.284332 --> 174.979614).\n",
            "\t Train_Loss: 44.4503 Val_Loss: 174.9796  BEST VAL Loss: 174.9796\n",
            "\n",
            "Epoch 729: Validation loss decreased (174.979614 --> 174.535583).\n",
            "\t Train_Loss: 44.2623 Val_Loss: 174.5356  BEST VAL Loss: 174.5356\n",
            "\n",
            "Epoch 730: Validation loss decreased (174.535583 --> 173.712967).\n",
            "\t Train_Loss: 44.1463 Val_Loss: 173.7130  BEST VAL Loss: 173.7130\n",
            "\n",
            "Epoch 731: Validation loss decreased (173.712967 --> 173.596176).\n",
            "\t Train_Loss: 44.0029 Val_Loss: 173.5962  BEST VAL Loss: 173.5962\n",
            "\n",
            "Epoch 732: Validation loss decreased (173.596176 --> 173.416367).\n",
            "\t Train_Loss: 43.8959 Val_Loss: 173.4164  BEST VAL Loss: 173.4164\n",
            "\n",
            "Epoch 733: Validation loss decreased (173.416367 --> 172.761246).\n",
            "\t Train_Loss: 43.7537 Val_Loss: 172.7612  BEST VAL Loss: 172.7612\n",
            "\n",
            "Epoch 734: Validation loss decreased (172.761246 --> 172.276978).\n",
            "\t Train_Loss: 43.5474 Val_Loss: 172.2770  BEST VAL Loss: 172.2770\n",
            "\n",
            "Epoch 735: Validation loss decreased (172.276978 --> 171.583267).\n",
            "\t Train_Loss: 43.4051 Val_Loss: 171.5833  BEST VAL Loss: 171.5833\n",
            "\n",
            "Epoch 736: Validation loss decreased (171.583267 --> 171.445602).\n",
            "\t Train_Loss: 43.2266 Val_Loss: 171.4456  BEST VAL Loss: 171.4456\n",
            "\n",
            "Epoch 737: Validation loss did not decrease\n",
            "\t Train_Loss: 43.0764 Val_Loss: 171.7211  BEST VAL Loss: 171.4456\n",
            "\n",
            "Epoch 738: Validation loss decreased (171.445602 --> 171.140305).\n",
            "\t Train_Loss: 42.9421 Val_Loss: 171.1403  BEST VAL Loss: 171.1403\n",
            "\n",
            "Epoch 739: Validation loss decreased (171.140305 --> 170.206696).\n",
            "\t Train_Loss: 42.7584 Val_Loss: 170.2067  BEST VAL Loss: 170.2067\n",
            "\n",
            "Epoch 740: Validation loss decreased (170.206696 --> 169.679626).\n",
            "\t Train_Loss: 42.6340 Val_Loss: 169.6796  BEST VAL Loss: 169.6796\n",
            "\n",
            "Epoch 741: Validation loss did not decrease\n",
            "\t Train_Loss: 42.4785 Val_Loss: 169.9262  BEST VAL Loss: 169.6796\n",
            "\n",
            "Epoch 742: Validation loss did not decrease\n",
            "\t Train_Loss: 42.3181 Val_Loss: 170.1365  BEST VAL Loss: 169.6796\n",
            "\n",
            "Epoch 743: Validation loss decreased (169.679626 --> 169.032776).\n",
            "\t Train_Loss: 42.1944 Val_Loss: 169.0328  BEST VAL Loss: 169.0328\n",
            "\n",
            "Epoch 744: Validation loss decreased (169.032776 --> 168.010239).\n",
            "\t Train_Loss: 42.0228 Val_Loss: 168.0102  BEST VAL Loss: 168.0102\n",
            "\n",
            "Epoch 745: Validation loss decreased (168.010239 --> 167.935287).\n",
            "\t Train_Loss: 41.9053 Val_Loss: 167.9353  BEST VAL Loss: 167.9353\n",
            "\n",
            "Epoch 746: Validation loss did not decrease\n",
            "\t Train_Loss: 41.7483 Val_Loss: 168.3797  BEST VAL Loss: 167.9353\n",
            "\n",
            "Epoch 747: Validation loss did not decrease\n",
            "\t Train_Loss: 41.6165 Val_Loss: 168.0181  BEST VAL Loss: 167.9353\n",
            "\n",
            "Epoch 748: Validation loss decreased (167.935287 --> 167.155426).\n",
            "\t Train_Loss: 41.4781 Val_Loss: 167.1554  BEST VAL Loss: 167.1554\n",
            "\n",
            "Epoch 749: Validation loss decreased (167.155426 --> 166.703568).\n",
            "\t Train_Loss: 41.3366 Val_Loss: 166.7036  BEST VAL Loss: 166.7036\n",
            "\n",
            "Epoch 750: Validation loss decreased (166.703568 --> 166.652863).\n",
            "\t Train_Loss: 41.2123 Val_Loss: 166.6529  BEST VAL Loss: 166.6529\n",
            "\n",
            "Epoch 751: Validation loss did not decrease\n",
            "\t Train_Loss: 41.0654 Val_Loss: 166.6839  BEST VAL Loss: 166.6529\n",
            "\n",
            "Epoch 752: Validation loss decreased (166.652863 --> 166.185760).\n",
            "\t Train_Loss: 40.9429 Val_Loss: 166.1858  BEST VAL Loss: 166.1858\n",
            "\n",
            "Epoch 753: Validation loss decreased (166.185760 --> 165.404236).\n",
            "\t Train_Loss: 40.8024 Val_Loss: 165.4042  BEST VAL Loss: 165.4042\n",
            "\n",
            "Epoch 754: Validation loss decreased (165.404236 --> 165.015182).\n",
            "\t Train_Loss: 40.6755 Val_Loss: 165.0152  BEST VAL Loss: 165.0152\n",
            "\n",
            "Epoch 755: Validation loss did not decrease\n",
            "\t Train_Loss: 40.5423 Val_Loss: 165.0942  BEST VAL Loss: 165.0152\n",
            "\n",
            "Epoch 756: Validation loss decreased (165.015182 --> 164.918137).\n",
            "\t Train_Loss: 40.4100 Val_Loss: 164.9181  BEST VAL Loss: 164.9181\n",
            "\n",
            "Epoch 757: Validation loss decreased (164.918137 --> 164.212631).\n",
            "\t Train_Loss: 40.2827 Val_Loss: 164.2126  BEST VAL Loss: 164.2126\n",
            "\n",
            "Epoch 758: Validation loss decreased (164.212631 --> 163.703613).\n",
            "\t Train_Loss: 40.1497 Val_Loss: 163.7036  BEST VAL Loss: 163.7036\n",
            "\n",
            "Epoch 759: Validation loss decreased (163.703613 --> 163.584015).\n",
            "\t Train_Loss: 40.0242 Val_Loss: 163.5840  BEST VAL Loss: 163.5840\n",
            "\n",
            "Epoch 760: Validation loss decreased (163.584015 --> 163.414230).\n",
            "\t Train_Loss: 39.8911 Val_Loss: 163.4142  BEST VAL Loss: 163.4142\n",
            "\n",
            "Epoch 761: Validation loss decreased (163.414230 --> 162.824478).\n",
            "\t Train_Loss: 39.7681 Val_Loss: 162.8245  BEST VAL Loss: 162.8245\n",
            "\n",
            "Epoch 762: Validation loss decreased (162.824478 --> 162.302444).\n",
            "\t Train_Loss: 39.6361 Val_Loss: 162.3024  BEST VAL Loss: 162.3024\n",
            "\n",
            "Epoch 763: Validation loss decreased (162.302444 --> 162.120148).\n",
            "\t Train_Loss: 39.5132 Val_Loss: 162.1201  BEST VAL Loss: 162.1201\n",
            "\n",
            "Epoch 764: Validation loss decreased (162.120148 --> 161.959198).\n",
            "\t Train_Loss: 39.3832 Val_Loss: 161.9592  BEST VAL Loss: 161.9592\n",
            "\n",
            "Epoch 765: Validation loss decreased (161.959198 --> 161.418900).\n",
            "\t Train_Loss: 39.2615 Val_Loss: 161.4189  BEST VAL Loss: 161.4189\n",
            "\n",
            "Epoch 766: Validation loss decreased (161.418900 --> 160.892944).\n",
            "\t Train_Loss: 39.1335 Val_Loss: 160.8929  BEST VAL Loss: 160.8929\n",
            "\n",
            "Epoch 767: Validation loss decreased (160.892944 --> 160.649155).\n",
            "\t Train_Loss: 39.0124 Val_Loss: 160.6492  BEST VAL Loss: 160.6492\n",
            "\n",
            "Epoch 768: Validation loss decreased (160.649155 --> 160.388885).\n",
            "\t Train_Loss: 38.8866 Val_Loss: 160.3889  BEST VAL Loss: 160.3889\n",
            "\n",
            "Epoch 769: Validation loss decreased (160.388885 --> 159.876343).\n",
            "\t Train_Loss: 38.7670 Val_Loss: 159.8763  BEST VAL Loss: 159.8763\n",
            "\n",
            "Epoch 770: Validation loss decreased (159.876343 --> 159.455902).\n",
            "\t Train_Loss: 38.6434 Val_Loss: 159.4559  BEST VAL Loss: 159.4559\n",
            "\n",
            "Epoch 771: Validation loss decreased (159.455902 --> 159.218857).\n",
            "\t Train_Loss: 38.5247 Val_Loss: 159.2189  BEST VAL Loss: 159.2189\n",
            "\n",
            "Epoch 772: Validation loss decreased (159.218857 --> 158.919586).\n",
            "\t Train_Loss: 38.4033 Val_Loss: 158.9196  BEST VAL Loss: 158.9196\n",
            "\n",
            "Epoch 773: Validation loss decreased (158.919586 --> 158.479691).\n",
            "\t Train_Loss: 38.2862 Val_Loss: 158.4797  BEST VAL Loss: 158.4797\n",
            "\n",
            "Epoch 774: Validation loss decreased (158.479691 --> 158.077545).\n",
            "\t Train_Loss: 38.1666 Val_Loss: 158.0775  BEST VAL Loss: 158.0775\n",
            "\n",
            "Epoch 775: Validation loss decreased (158.077545 --> 157.750076).\n",
            "\t Train_Loss: 38.0507 Val_Loss: 157.7501  BEST VAL Loss: 157.7501\n",
            "\n",
            "Epoch 776: Validation loss decreased (157.750076 --> 157.412338).\n",
            "\t Train_Loss: 37.9326 Val_Loss: 157.4123  BEST VAL Loss: 157.4123\n",
            "\n",
            "Epoch 777: Validation loss decreased (157.412338 --> 157.006744).\n",
            "\t Train_Loss: 37.8177 Val_Loss: 157.0067  BEST VAL Loss: 157.0067\n",
            "\n",
            "Epoch 778: Validation loss decreased (157.006744 --> 156.618759).\n",
            "\t Train_Loss: 37.7010 Val_Loss: 156.6188  BEST VAL Loss: 156.6188\n",
            "\n",
            "Epoch 779: Validation loss decreased (156.618759 --> 156.309006).\n",
            "\t Train_Loss: 37.5867 Val_Loss: 156.3090  BEST VAL Loss: 156.3090\n",
            "\n",
            "Epoch 780: Validation loss decreased (156.309006 --> 156.000626).\n",
            "\t Train_Loss: 37.4713 Val_Loss: 156.0006  BEST VAL Loss: 156.0006\n",
            "\n",
            "Epoch 781: Validation loss decreased (156.000626 --> 155.609131).\n",
            "\t Train_Loss: 37.3576 Val_Loss: 155.6091  BEST VAL Loss: 155.6091\n",
            "\n",
            "Epoch 782: Validation loss decreased (155.609131 --> 155.224289).\n",
            "\t Train_Loss: 37.2432 Val_Loss: 155.2243  BEST VAL Loss: 155.2243\n",
            "\n",
            "Epoch 783: Validation loss decreased (155.224289 --> 154.911728).\n",
            "\t Train_Loss: 37.1302 Val_Loss: 154.9117  BEST VAL Loss: 154.9117\n",
            "\n",
            "Epoch 784: Validation loss decreased (154.911728 --> 154.585846).\n",
            "\t Train_Loss: 37.0167 Val_Loss: 154.5858  BEST VAL Loss: 154.5858\n",
            "\n",
            "Epoch 785: Validation loss decreased (154.585846 --> 154.198227).\n",
            "\t Train_Loss: 36.9045 Val_Loss: 154.1982  BEST VAL Loss: 154.1982\n",
            "\n",
            "Epoch 786: Validation loss decreased (154.198227 --> 153.854965).\n",
            "\t Train_Loss: 36.7920 Val_Loss: 153.8550  BEST VAL Loss: 153.8550\n",
            "\n",
            "Epoch 787: Validation loss decreased (153.854965 --> 153.563675).\n",
            "\t Train_Loss: 36.6805 Val_Loss: 153.5637  BEST VAL Loss: 153.5637\n",
            "\n",
            "Epoch 788: Validation loss decreased (153.563675 --> 153.226212).\n",
            "\t Train_Loss: 36.5690 Val_Loss: 153.2262  BEST VAL Loss: 153.2262\n",
            "\n",
            "Epoch 789: Validation loss decreased (153.226212 --> 152.866745).\n",
            "\t Train_Loss: 36.4583 Val_Loss: 152.8667  BEST VAL Loss: 152.8667\n",
            "\n",
            "Epoch 790: Validation loss decreased (152.866745 --> 152.544388).\n",
            "\t Train_Loss: 36.3478 Val_Loss: 152.5444  BEST VAL Loss: 152.5444\n",
            "\n",
            "Epoch 791: Validation loss decreased (152.544388 --> 152.225143).\n",
            "\t Train_Loss: 36.2379 Val_Loss: 152.2251  BEST VAL Loss: 152.2251\n",
            "\n",
            "Epoch 792: Validation loss decreased (152.225143 --> 151.890427).\n",
            "\t Train_Loss: 36.1282 Val_Loss: 151.8904  BEST VAL Loss: 151.8904\n",
            "\n",
            "Epoch 793: Validation loss decreased (151.890427 --> 151.551682).\n",
            "\t Train_Loss: 36.0190 Val_Loss: 151.5517  BEST VAL Loss: 151.5517\n",
            "\n",
            "Epoch 794: Validation loss decreased (151.551682 --> 151.220871).\n",
            "\t Train_Loss: 35.9100 Val_Loss: 151.2209  BEST VAL Loss: 151.2209\n",
            "\n",
            "Epoch 795: Validation loss decreased (151.220871 --> 150.899048).\n",
            "\t Train_Loss: 35.8015 Val_Loss: 150.8990  BEST VAL Loss: 150.8990\n",
            "\n",
            "Epoch 796: Validation loss decreased (150.899048 --> 150.576035).\n",
            "\t Train_Loss: 35.6932 Val_Loss: 150.5760  BEST VAL Loss: 150.5760\n",
            "\n",
            "Epoch 797: Validation loss decreased (150.576035 --> 150.242569).\n",
            "\t Train_Loss: 35.5852 Val_Loss: 150.2426  BEST VAL Loss: 150.2426\n",
            "\n",
            "Epoch 798: Validation loss decreased (150.242569 --> 149.916214).\n",
            "\t Train_Loss: 35.4775 Val_Loss: 149.9162  BEST VAL Loss: 149.9162\n",
            "\n",
            "Epoch 799: Validation loss decreased (149.916214 --> 149.612839).\n",
            "\t Train_Loss: 35.3700 Val_Loss: 149.6128  BEST VAL Loss: 149.6128\n",
            "\n",
            "Epoch 800: Validation loss decreased (149.612839 --> 149.292725).\n",
            "\t Train_Loss: 35.2628 Val_Loss: 149.2927  BEST VAL Loss: 149.2927\n",
            "\n",
            "Epoch 801: Validation loss decreased (149.292725 --> 148.962112).\n",
            "\t Train_Loss: 35.1559 Val_Loss: 148.9621  BEST VAL Loss: 148.9621\n",
            "\n",
            "Epoch 802: Validation loss decreased (148.962112 --> 148.649200).\n",
            "\t Train_Loss: 35.0493 Val_Loss: 148.6492  BEST VAL Loss: 148.6492\n",
            "\n",
            "Epoch 803: Validation loss decreased (148.649200 --> 148.344238).\n",
            "\t Train_Loss: 34.9429 Val_Loss: 148.3442  BEST VAL Loss: 148.3442\n",
            "\n",
            "Epoch 804: Validation loss decreased (148.344238 --> 148.025620).\n",
            "\t Train_Loss: 34.8369 Val_Loss: 148.0256  BEST VAL Loss: 148.0256\n",
            "\n",
            "Epoch 805: Validation loss decreased (148.025620 --> 147.710861).\n",
            "\t Train_Loss: 34.7310 Val_Loss: 147.7109  BEST VAL Loss: 147.7109\n",
            "\n",
            "Epoch 806: Validation loss decreased (147.710861 --> 147.403839).\n",
            "\t Train_Loss: 34.6254 Val_Loss: 147.4038  BEST VAL Loss: 147.4038\n",
            "\n",
            "Epoch 807: Validation loss decreased (147.403839 --> 147.091904).\n",
            "\t Train_Loss: 34.5200 Val_Loss: 147.0919  BEST VAL Loss: 147.0919\n",
            "\n",
            "Epoch 808: Validation loss decreased (147.091904 --> 146.776947).\n",
            "\t Train_Loss: 34.4148 Val_Loss: 146.7769  BEST VAL Loss: 146.7769\n",
            "\n",
            "Epoch 809: Validation loss decreased (146.776947 --> 146.460648).\n",
            "\t Train_Loss: 34.3098 Val_Loss: 146.4606  BEST VAL Loss: 146.4606\n",
            "\n",
            "Epoch 810: Validation loss decreased (146.460648 --> 146.149490).\n",
            "\t Train_Loss: 34.2050 Val_Loss: 146.1495  BEST VAL Loss: 146.1495\n",
            "\n",
            "Epoch 811: Validation loss decreased (146.149490 --> 145.842270).\n",
            "\t Train_Loss: 34.1003 Val_Loss: 145.8423  BEST VAL Loss: 145.8423\n",
            "\n",
            "Epoch 812: Validation loss decreased (145.842270 --> 145.534790).\n",
            "\t Train_Loss: 33.9958 Val_Loss: 145.5348  BEST VAL Loss: 145.5348\n",
            "\n",
            "Epoch 813: Validation loss decreased (145.534790 --> 145.216309).\n",
            "\t Train_Loss: 33.8914 Val_Loss: 145.2163  BEST VAL Loss: 145.2163\n",
            "\n",
            "Epoch 814: Validation loss decreased (145.216309 --> 144.913239).\n",
            "\t Train_Loss: 33.7871 Val_Loss: 144.9132  BEST VAL Loss: 144.9132\n",
            "\n",
            "Epoch 815: Validation loss decreased (144.913239 --> 144.601883).\n",
            "\t Train_Loss: 33.6830 Val_Loss: 144.6019  BEST VAL Loss: 144.6019\n",
            "\n",
            "Epoch 816: Validation loss decreased (144.601883 --> 144.302765).\n",
            "\t Train_Loss: 33.5791 Val_Loss: 144.3028  BEST VAL Loss: 144.3028\n",
            "\n",
            "Epoch 817: Validation loss decreased (144.302765 --> 143.974960).\n",
            "\t Train_Loss: 33.4752 Val_Loss: 143.9750  BEST VAL Loss: 143.9750\n",
            "\n",
            "Epoch 818: Validation loss did not decrease\n",
            "\t Train_Loss: 33.3715 Val_Loss: 145.5986  BEST VAL Loss: 143.9750\n",
            "\n",
            "Epoch 819: Validation loss decreased (143.974960 --> 143.277267).\n",
            "\t Train_Loss: 33.7157 Val_Loss: 143.2773  BEST VAL Loss: 143.2773\n",
            "\n",
            "Epoch 820: Validation loss decreased (143.277267 --> 142.953690).\n",
            "\t Train_Loss: 33.2567 Val_Loss: 142.9537  BEST VAL Loss: 142.9537\n",
            "\n",
            "Epoch 821: Validation loss decreased (142.953690 --> 142.673615).\n",
            "\t Train_Loss: 33.1077 Val_Loss: 142.6736  BEST VAL Loss: 142.6736\n",
            "\n",
            "Epoch 822: Validation loss decreased (142.673615 --> 142.552658).\n",
            "\t Train_Loss: 32.9835 Val_Loss: 142.5527  BEST VAL Loss: 142.5527\n",
            "\n",
            "Epoch 823: Validation loss decreased (142.552658 --> 142.358505).\n",
            "\t Train_Loss: 32.9171 Val_Loss: 142.3585  BEST VAL Loss: 142.3585\n",
            "\n",
            "Epoch 824: Validation loss decreased (142.358505 --> 141.687805).\n",
            "\t Train_Loss: 32.7887 Val_Loss: 141.6878  BEST VAL Loss: 141.6878\n",
            "\n",
            "Epoch 825: Validation loss did not decrease\n",
            "\t Train_Loss: 32.6926 Val_Loss: 143.4472  BEST VAL Loss: 141.6878\n",
            "\n",
            "Epoch 826: Validation loss decreased (141.687805 --> 141.404419).\n",
            "\t Train_Loss: 32.9628 Val_Loss: 141.4044  BEST VAL Loss: 141.4044\n",
            "\n",
            "Epoch 827: Validation loss decreased (141.404419 --> 141.084793).\n",
            "\t Train_Loss: 32.5261 Val_Loss: 141.0848  BEST VAL Loss: 141.0848\n",
            "\n",
            "Epoch 828: Validation loss decreased (141.084793 --> 140.948441).\n",
            "\t Train_Loss: 32.4121 Val_Loss: 140.9484  BEST VAL Loss: 140.9484\n",
            "\n",
            "Epoch 829: Validation loss decreased (140.948441 --> 140.442413).\n",
            "\t Train_Loss: 32.3119 Val_Loss: 140.4424  BEST VAL Loss: 140.4424\n",
            "\n",
            "Epoch 830: Validation loss did not decrease\n",
            "\t Train_Loss: 32.1951 Val_Loss: 140.9902  BEST VAL Loss: 140.4424\n",
            "\n",
            "Epoch 831: Validation loss decreased (140.442413 --> 139.609253).\n",
            "\t Train_Loss: 32.2642 Val_Loss: 139.6093  BEST VAL Loss: 139.6093\n",
            "\n",
            "Epoch 832: Validation loss decreased (139.609253 --> 139.306183).\n",
            "\t Train_Loss: 31.9896 Val_Loss: 139.3062  BEST VAL Loss: 139.3062\n",
            "\n",
            "Epoch 833: Validation loss decreased (139.306183 --> 138.883896).\n",
            "\t Train_Loss: 31.8522 Val_Loss: 138.8839  BEST VAL Loss: 138.8839\n",
            "\n",
            "Epoch 834: Validation loss decreased (138.883896 --> 138.599991).\n",
            "\t Train_Loss: 31.7917 Val_Loss: 138.6000  BEST VAL Loss: 138.6000\n",
            "\n",
            "Epoch 835: Validation loss decreased (138.599991 --> 138.354721).\n",
            "\t Train_Loss: 31.6486 Val_Loss: 138.3547  BEST VAL Loss: 138.3547\n",
            "\n",
            "Epoch 836: Validation loss decreased (138.354721 --> 138.105652).\n",
            "\t Train_Loss: 31.5741 Val_Loss: 138.1057  BEST VAL Loss: 138.1057\n",
            "\n",
            "Epoch 837: Validation loss decreased (138.105652 --> 137.950485).\n",
            "\t Train_Loss: 31.4513 Val_Loss: 137.9505  BEST VAL Loss: 137.9505\n",
            "\n",
            "Epoch 838: Validation loss decreased (137.950485 --> 137.739029).\n",
            "\t Train_Loss: 31.3576 Val_Loss: 137.7390  BEST VAL Loss: 137.7390\n",
            "\n",
            "Epoch 839: Validation loss decreased (137.739029 --> 137.354050).\n",
            "\t Train_Loss: 31.2532 Val_Loss: 137.3540  BEST VAL Loss: 137.3540\n",
            "\n",
            "Epoch 840: Validation loss decreased (137.354050 --> 136.899719).\n",
            "\t Train_Loss: 31.1505 Val_Loss: 136.8997  BEST VAL Loss: 136.8997\n",
            "\n",
            "Epoch 841: Validation loss decreased (136.899719 --> 136.536209).\n",
            "\t Train_Loss: 31.0509 Val_Loss: 136.5362  BEST VAL Loss: 136.5362\n",
            "\n",
            "Epoch 842: Validation loss decreased (136.536209 --> 136.247543).\n",
            "\t Train_Loss: 30.9476 Val_Loss: 136.2475  BEST VAL Loss: 136.2475\n",
            "\n",
            "Epoch 843: Validation loss decreased (136.247543 --> 136.047318).\n",
            "\t Train_Loss: 30.8501 Val_Loss: 136.0473  BEST VAL Loss: 136.0473\n",
            "\n",
            "Epoch 844: Validation loss decreased (136.047318 --> 135.717651).\n",
            "\t Train_Loss: 30.7452 Val_Loss: 135.7177  BEST VAL Loss: 135.7177\n",
            "\n",
            "Epoch 845: Validation loss decreased (135.717651 --> 135.288559).\n",
            "\t Train_Loss: 30.6516 Val_Loss: 135.2886  BEST VAL Loss: 135.2886\n",
            "\n",
            "Epoch 846: Validation loss decreased (135.288559 --> 135.035309).\n",
            "\t Train_Loss: 30.5464 Val_Loss: 135.0353  BEST VAL Loss: 135.0353\n",
            "\n",
            "Epoch 847: Validation loss decreased (135.035309 --> 134.906860).\n",
            "\t Train_Loss: 30.4556 Val_Loss: 134.9069  BEST VAL Loss: 134.9069\n",
            "\n",
            "Epoch 848: Validation loss decreased (134.906860 --> 134.611893).\n",
            "\t Train_Loss: 30.3491 Val_Loss: 134.6119  BEST VAL Loss: 134.6119\n",
            "\n",
            "Epoch 849: Validation loss decreased (134.611893 --> 134.144882).\n",
            "\t Train_Loss: 30.2603 Val_Loss: 134.1449  BEST VAL Loss: 134.1449\n",
            "\n",
            "Epoch 850: Validation loss decreased (134.144882 --> 133.796005).\n",
            "\t Train_Loss: 30.1540 Val_Loss: 133.7960  BEST VAL Loss: 133.7960\n",
            "\n",
            "Epoch 851: Validation loss decreased (133.796005 --> 133.590134).\n",
            "\t Train_Loss: 30.0678 Val_Loss: 133.5901  BEST VAL Loss: 133.5901\n",
            "\n",
            "Epoch 852: Validation loss decreased (133.590134 --> 133.416031).\n",
            "\t Train_Loss: 29.9615 Val_Loss: 133.4160  BEST VAL Loss: 133.4160\n",
            "\n",
            "Epoch 853: Validation loss decreased (133.416031 --> 133.083679).\n",
            "\t Train_Loss: 29.8739 Val_Loss: 133.0837  BEST VAL Loss: 133.0837\n",
            "\n",
            "Epoch 854: Validation loss decreased (133.083679 --> 132.666534).\n",
            "\t Train_Loss: 29.7727 Val_Loss: 132.6665  BEST VAL Loss: 132.6665\n",
            "\n",
            "Epoch 855: Validation loss decreased (132.666534 --> 132.370300).\n",
            "\t Train_Loss: 29.6808 Val_Loss: 132.3703  BEST VAL Loss: 132.3703\n",
            "\n",
            "Epoch 856: Validation loss decreased (132.370300 --> 132.189407).\n",
            "\t Train_Loss: 29.5849 Val_Loss: 132.1894  BEST VAL Loss: 132.1894\n",
            "\n",
            "Epoch 857: Validation loss decreased (132.189407 --> 131.902695).\n",
            "\t Train_Loss: 29.4893 Val_Loss: 131.9027  BEST VAL Loss: 131.9027\n",
            "\n",
            "Epoch 858: Validation loss decreased (131.902695 --> 131.483109).\n",
            "\t Train_Loss: 29.3966 Val_Loss: 131.4831  BEST VAL Loss: 131.4831\n",
            "\n",
            "Epoch 859: Validation loss decreased (131.483109 --> 131.127548).\n",
            "\t Train_Loss: 29.2994 Val_Loss: 131.1275  BEST VAL Loss: 131.1275\n",
            "\n",
            "Epoch 860: Validation loss decreased (131.127548 --> 130.905685).\n",
            "\t Train_Loss: 29.2084 Val_Loss: 130.9057  BEST VAL Loss: 130.9057\n",
            "\n",
            "Epoch 861: Validation loss decreased (130.905685 --> 130.728516).\n",
            "\t Train_Loss: 29.1108 Val_Loss: 130.7285  BEST VAL Loss: 130.7285\n",
            "\n",
            "Epoch 862: Validation loss decreased (130.728516 --> 130.445221).\n",
            "\t Train_Loss: 29.0191 Val_Loss: 130.4452  BEST VAL Loss: 130.4452\n",
            "\n",
            "Epoch 863: Validation loss decreased (130.445221 --> 130.076904).\n",
            "\t Train_Loss: 28.9232 Val_Loss: 130.0769  BEST VAL Loss: 130.0769\n",
            "\n",
            "Epoch 864: Validation loss decreased (130.076904 --> 129.777466).\n",
            "\t Train_Loss: 28.8299 Val_Loss: 129.7775  BEST VAL Loss: 129.7775\n",
            "\n",
            "Epoch 865: Validation loss decreased (129.777466 --> 129.571167).\n",
            "\t Train_Loss: 28.7357 Val_Loss: 129.5712  BEST VAL Loss: 129.5712\n",
            "\n",
            "Epoch 866: Validation loss decreased (129.571167 --> 129.312866).\n",
            "\t Train_Loss: 28.6415 Val_Loss: 129.3129  BEST VAL Loss: 129.3129\n",
            "\n",
            "Epoch 867: Validation loss decreased (129.312866 --> 128.952682).\n",
            "\t Train_Loss: 28.5482 Val_Loss: 128.9527  BEST VAL Loss: 128.9527\n",
            "\n",
            "Epoch 868: Validation loss decreased (128.952682 --> 128.611893).\n",
            "\t Train_Loss: 28.4540 Val_Loss: 128.6119  BEST VAL Loss: 128.6119\n",
            "\n",
            "Epoch 869: Validation loss decreased (128.611893 --> 128.378464).\n",
            "\t Train_Loss: 28.3614 Val_Loss: 128.3785  BEST VAL Loss: 128.3785\n",
            "\n",
            "Epoch 870: Validation loss decreased (128.378464 --> 128.188766).\n",
            "\t Train_Loss: 28.2676 Val_Loss: 128.1888  BEST VAL Loss: 128.1888\n",
            "\n",
            "Epoch 871: Validation loss decreased (128.188766 --> 127.915039).\n",
            "\t Train_Loss: 28.1753 Val_Loss: 127.9150  BEST VAL Loss: 127.9150\n",
            "\n",
            "Epoch 872: Validation loss decreased (127.915039 --> 127.579430).\n",
            "\t Train_Loss: 28.0827 Val_Loss: 127.5794  BEST VAL Loss: 127.5794\n",
            "\n",
            "Epoch 873: Validation loss decreased (127.579430 --> 127.317993).\n",
            "\t Train_Loss: 27.9904 Val_Loss: 127.3180  BEST VAL Loss: 127.3180\n",
            "\n",
            "Epoch 874: Validation loss decreased (127.317993 --> 127.125832).\n",
            "\t Train_Loss: 27.8988 Val_Loss: 127.1258  BEST VAL Loss: 127.1258\n",
            "\n",
            "Epoch 875: Validation loss decreased (127.125832 --> 126.867508).\n",
            "\t Train_Loss: 27.8069 Val_Loss: 126.8675  BEST VAL Loss: 126.8675\n",
            "\n",
            "Epoch 876: Validation loss decreased (126.867508 --> 126.533226).\n",
            "\t Train_Loss: 27.7160 Val_Loss: 126.5332  BEST VAL Loss: 126.5332\n",
            "\n",
            "Epoch 877: Validation loss decreased (126.533226 --> 126.249077).\n",
            "\t Train_Loss: 27.6245 Val_Loss: 126.2491  BEST VAL Loss: 126.2491\n",
            "\n",
            "Epoch 878: Validation loss decreased (126.249077 --> 126.044518).\n",
            "\t Train_Loss: 27.5343 Val_Loss: 126.0445  BEST VAL Loss: 126.0445\n",
            "\n",
            "Epoch 879: Validation loss decreased (126.044518 --> 125.813774).\n",
            "\t Train_Loss: 27.4433 Val_Loss: 125.8138  BEST VAL Loss: 125.8138\n",
            "\n",
            "Epoch 880: Validation loss decreased (125.813774 --> 125.505547).\n",
            "\t Train_Loss: 27.3536 Val_Loss: 125.5055  BEST VAL Loss: 125.5055\n",
            "\n",
            "Epoch 881: Validation loss decreased (125.505547 --> 125.216026).\n",
            "\t Train_Loss: 27.2632 Val_Loss: 125.2160  BEST VAL Loss: 125.2160\n",
            "\n",
            "Epoch 882: Validation loss decreased (125.216026 --> 125.004639).\n",
            "\t Train_Loss: 27.1737 Val_Loss: 125.0046  BEST VAL Loss: 125.0046\n",
            "\n",
            "Epoch 883: Validation loss decreased (125.004639 --> 124.784477).\n",
            "\t Train_Loss: 27.0838 Val_Loss: 124.7845  BEST VAL Loss: 124.7845\n",
            "\n",
            "Epoch 884: Validation loss decreased (124.784477 --> 124.497391).\n",
            "\t Train_Loss: 26.9945 Val_Loss: 124.4974  BEST VAL Loss: 124.4974\n",
            "\n",
            "Epoch 885: Validation loss decreased (124.497391 --> 124.226395).\n",
            "\t Train_Loss: 26.9048 Val_Loss: 124.2264  BEST VAL Loss: 124.2264\n",
            "\n",
            "Epoch 886: Validation loss decreased (124.226395 --> 124.017776).\n",
            "\t Train_Loss: 26.8154 Val_Loss: 124.0178  BEST VAL Loss: 124.0178\n",
            "\n",
            "Epoch 887: Validation loss decreased (124.017776 --> 123.796852).\n",
            "\t Train_Loss: 26.7259 Val_Loss: 123.7969  BEST VAL Loss: 123.7969\n",
            "\n",
            "Epoch 888: Validation loss decreased (123.796852 --> 123.524414).\n",
            "\t Train_Loss: 26.6363 Val_Loss: 123.5244  BEST VAL Loss: 123.5244\n",
            "\n",
            "Epoch 889: Validation loss decreased (123.524414 --> 123.259644).\n",
            "\t Train_Loss: 26.5465 Val_Loss: 123.2596  BEST VAL Loss: 123.2596\n",
            "\n",
            "Epoch 890: Validation loss decreased (123.259644 --> 123.032669).\n",
            "\t Train_Loss: 26.4565 Val_Loss: 123.0327  BEST VAL Loss: 123.0327\n",
            "\n",
            "Epoch 891: Validation loss decreased (123.032669 --> 122.802490).\n",
            "\t Train_Loss: 26.3666 Val_Loss: 122.8025  BEST VAL Loss: 122.8025\n",
            "\n",
            "Epoch 892: Validation loss decreased (122.802490 --> 122.553650).\n",
            "\t Train_Loss: 26.2762 Val_Loss: 122.5536  BEST VAL Loss: 122.5536\n",
            "\n",
            "Epoch 893: Validation loss decreased (122.553650 --> 122.320946).\n",
            "\t Train_Loss: 26.1856 Val_Loss: 122.3209  BEST VAL Loss: 122.3209\n",
            "\n",
            "Epoch 894: Validation loss decreased (122.320946 --> 122.100334).\n",
            "\t Train_Loss: 26.0943 Val_Loss: 122.1003  BEST VAL Loss: 122.1003\n",
            "\n",
            "Epoch 895: Validation loss decreased (122.100334 --> 121.860062).\n",
            "\t Train_Loss: 26.0031 Val_Loss: 121.8601  BEST VAL Loss: 121.8601\n",
            "\n",
            "Epoch 896: Validation loss decreased (121.860062 --> 121.625267).\n",
            "\t Train_Loss: 25.9113 Val_Loss: 121.6253  BEST VAL Loss: 121.6253\n",
            "\n",
            "Epoch 897: Validation loss decreased (121.625267 --> 121.396194).\n",
            "\t Train_Loss: 25.8193 Val_Loss: 121.3962  BEST VAL Loss: 121.3962\n",
            "\n",
            "Epoch 898: Validation loss decreased (121.396194 --> 121.129349).\n",
            "\t Train_Loss: 25.7269 Val_Loss: 121.1293  BEST VAL Loss: 121.1293\n",
            "\n",
            "Epoch 899: Validation loss decreased (121.129349 --> 120.876945).\n",
            "\t Train_Loss: 25.6342 Val_Loss: 120.8769  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 900: Validation loss did not decrease\n",
            "\t Train_Loss: 25.5412 Val_Loss: 218.4196  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 901: Validation loss did not decrease\n",
            "\t Train_Loss: 66.2979 Val_Loss: 189.4868  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 902: Validation loss did not decrease\n",
            "\t Train_Loss: 45.7121 Val_Loss: 191.0067  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 903: Validation loss did not decrease\n",
            "\t Train_Loss: 54.7097 Val_Loss: 188.2741  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 904: Validation loss did not decrease\n",
            "\t Train_Loss: 55.0736 Val_Loss: 182.8259  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 905: Validation loss did not decrease\n",
            "\t Train_Loss: 47.1790 Val_Loss: 184.7496  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 906: Validation loss did not decrease\n",
            "\t Train_Loss: 44.3220 Val_Loss: 189.7388  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 907: Validation loss did not decrease\n",
            "\t Train_Loss: 51.0524 Val_Loss: 189.0811  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 908: Validation loss did not decrease\n",
            "\t Train_Loss: 50.5243 Val_Loss: 183.7431  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 909: Validation loss did not decrease\n",
            "\t Train_Loss: 44.7489 Val_Loss: 179.1493  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 910: Validation loss did not decrease\n",
            "\t Train_Loss: 44.7179 Val_Loss: 176.9985  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 911: Validation loss did not decrease\n",
            "\t Train_Loss: 47.1733 Val_Loss: 177.3815  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 912: Validation loss did not decrease\n",
            "\t Train_Loss: 46.4142 Val_Loss: 180.8835  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 913: Validation loss did not decrease\n",
            "\t Train_Loss: 43.6529 Val_Loss: 184.6518  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 914: Validation loss did not decrease\n",
            "\t Train_Loss: 42.9672 Val_Loss: 186.7859  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 915: Validation loss did not decrease\n",
            "\t Train_Loss: 44.4682 Val_Loss: 185.4651  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 916: Validation loss did not decrease\n",
            "\t Train_Loss: 44.2351 Val_Loss: 180.8813  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 917: Validation loss did not decrease\n",
            "\t Train_Loss: 42.0288 Val_Loss: 177.1427  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 918: Validation loss did not decrease\n",
            "\t Train_Loss: 41.2843 Val_Loss: 176.1189  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 919: Validation loss did not decrease\n",
            "\t Train_Loss: 42.2178 Val_Loss: 174.8241  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 920: Validation loss did not decrease\n",
            "\t Train_Loss: 42.0591 Val_Loss: 173.8093  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 921: Validation loss did not decrease\n",
            "\t Train_Loss: 40.6958 Val_Loss: 174.6536  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 922: Validation loss did not decrease\n",
            "\t Train_Loss: 40.1864 Val_Loss: 175.9142  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 923: Validation loss did not decrease\n",
            "\t Train_Loss: 40.8023 Val_Loss: 175.1479  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 924: Validation loss did not decrease\n",
            "\t Train_Loss: 40.2091 Val_Loss: 173.6933  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 925: Validation loss did not decrease\n",
            "\t Train_Loss: 39.2548 Val_Loss: 172.9425  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 926: Validation loss did not decrease\n",
            "\t Train_Loss: 39.0714 Val_Loss: 172.5930  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 927: Validation loss did not decrease\n",
            "\t Train_Loss: 39.2076 Val_Loss: 172.4128  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 928: Validation loss did not decrease\n",
            "\t Train_Loss: 38.9321 Val_Loss: 172.1073  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 929: Validation loss did not decrease\n",
            "\t Train_Loss: 38.3660 Val_Loss: 171.6810  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 930: Validation loss did not decrease\n",
            "\t Train_Loss: 38.1940 Val_Loss: 171.0072  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 931: Validation loss did not decrease\n",
            "\t Train_Loss: 38.3088 Val_Loss: 169.7266  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 932: Validation loss did not decrease\n",
            "\t Train_Loss: 38.0587 Val_Loss: 168.0284  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 933: Validation loss did not decrease\n",
            "\t Train_Loss: 37.5711 Val_Loss: 166.7508  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 934: Validation loss did not decrease\n",
            "\t Train_Loss: 37.4168 Val_Loss: 166.1508  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 935: Validation loss did not decrease\n",
            "\t Train_Loss: 37.5159 Val_Loss: 165.3878  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 936: Validation loss did not decrease\n",
            "\t Train_Loss: 37.2136 Val_Loss: 164.7464  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 937: Validation loss did not decrease\n",
            "\t Train_Loss: 36.8186 Val_Loss: 164.5750  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 938: Validation loss did not decrease\n",
            "\t Train_Loss: 36.7186 Val_Loss: 164.4314  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 939: Validation loss did not decrease\n",
            "\t Train_Loss: 36.6217 Val_Loss: 164.0925  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 940: Validation loss did not decrease\n",
            "\t Train_Loss: 36.3600 Val_Loss: 163.6252  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 941: Validation loss did not decrease\n",
            "\t Train_Loss: 36.0869 Val_Loss: 163.1493  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 942: Validation loss did not decrease\n",
            "\t Train_Loss: 35.9544 Val_Loss: 162.6783  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 943: Validation loss did not decrease\n",
            "\t Train_Loss: 35.9028 Val_Loss: 162.0022  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 944: Validation loss did not decrease\n",
            "\t Train_Loss: 35.6683 Val_Loss: 161.3692  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 945: Validation loss did not decrease\n",
            "\t Train_Loss: 35.4440 Val_Loss: 160.7764  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 946: Validation loss did not decrease\n",
            "\t Train_Loss: 35.3376 Val_Loss: 160.0738  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 947: Validation loss did not decrease\n",
            "\t Train_Loss: 35.2163 Val_Loss: 159.3111  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 948: Validation loss did not decrease\n",
            "\t Train_Loss: 35.0354 Val_Loss: 158.5896  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 949: Validation loss did not decrease\n",
            "\t Train_Loss: 34.8684 Val_Loss: 157.9530  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 950: Validation loss did not decrease\n",
            "\t Train_Loss: 34.7488 Val_Loss: 157.4158  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 951: Validation loss did not decrease\n",
            "\t Train_Loss: 34.6151 Val_Loss: 157.0252  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 952: Validation loss did not decrease\n",
            "\t Train_Loss: 34.4457 Val_Loss: 156.7788  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 953: Validation loss did not decrease\n",
            "\t Train_Loss: 34.3006 Val_Loss: 156.5217  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 954: Validation loss did not decrease\n",
            "\t Train_Loss: 34.1723 Val_Loss: 156.1282  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 955: Validation loss did not decrease\n",
            "\t Train_Loss: 34.0187 Val_Loss: 155.6255  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 956: Validation loss did not decrease\n",
            "\t Train_Loss: 33.8676 Val_Loss: 155.0692  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 957: Validation loss did not decrease\n",
            "\t Train_Loss: 33.7485 Val_Loss: 154.4581  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 958: Validation loss did not decrease\n",
            "\t Train_Loss: 33.6208 Val_Loss: 153.8379  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 959: Validation loss did not decrease\n",
            "\t Train_Loss: 33.4724 Val_Loss: 153.2811  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 960: Validation loss did not decrease\n",
            "\t Train_Loss: 33.3412 Val_Loss: 152.7620  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 961: Validation loss did not decrease\n",
            "\t Train_Loss: 33.2223 Val_Loss: 152.2248  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 962: Validation loss did not decrease\n",
            "\t Train_Loss: 33.0884 Val_Loss: 151.6957  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 963: Validation loss did not decrease\n",
            "\t Train_Loss: 32.9518 Val_Loss: 151.2313  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 964: Validation loss did not decrease\n",
            "\t Train_Loss: 32.8346 Val_Loss: 150.7927  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 965: Validation loss did not decrease\n",
            "\t Train_Loss: 32.7225 Val_Loss: 150.3059  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 966: Validation loss did not decrease\n",
            "\t Train_Loss: 32.5936 Val_Loss: 149.8016  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 967: Validation loss did not decrease\n",
            "\t Train_Loss: 32.4646 Val_Loss: 149.3253  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 968: Validation loss did not decrease\n",
            "\t Train_Loss: 32.3487 Val_Loss: 148.8599  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 969: Validation loss did not decrease\n",
            "\t Train_Loss: 32.2322 Val_Loss: 148.3963  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 970: Validation loss did not decrease\n",
            "\t Train_Loss: 32.1108 Val_Loss: 147.9583  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 971: Validation loss did not decrease\n",
            "\t Train_Loss: 31.9936 Val_Loss: 147.5527  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 972: Validation loss did not decrease\n",
            "\t Train_Loss: 31.8807 Val_Loss: 147.1631  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 973: Validation loss did not decrease\n",
            "\t Train_Loss: 31.7653 Val_Loss: 146.7703  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 974: Validation loss did not decrease\n",
            "\t Train_Loss: 31.6490 Val_Loss: 146.3507  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 975: Validation loss did not decrease\n",
            "\t Train_Loss: 31.5362 Val_Loss: 145.8872  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 976: Validation loss did not decrease\n",
            "\t Train_Loss: 31.4246 Val_Loss: 145.3974  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 977: Validation loss did not decrease\n",
            "\t Train_Loss: 31.3141 Val_Loss: 144.9265  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 978: Validation loss did not decrease\n",
            "\t Train_Loss: 31.2058 Val_Loss: 144.5058  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 979: Validation loss did not decrease\n",
            "\t Train_Loss: 31.0969 Val_Loss: 144.1259  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 980: Validation loss did not decrease\n",
            "\t Train_Loss: 30.9873 Val_Loss: 143.7502  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 981: Validation loss did not decrease\n",
            "\t Train_Loss: 30.8811 Val_Loss: 143.3346  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 982: Validation loss did not decrease\n",
            "\t Train_Loss: 30.7759 Val_Loss: 142.9157  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 983: Validation loss did not decrease\n",
            "\t Train_Loss: 30.6705 Val_Loss: 142.5107  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 984: Validation loss did not decrease\n",
            "\t Train_Loss: 30.5662 Val_Loss: 142.1099  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 985: Validation loss did not decrease\n",
            "\t Train_Loss: 30.4642 Val_Loss: 141.7135  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 986: Validation loss did not decrease\n",
            "\t Train_Loss: 30.3614 Val_Loss: 141.3494  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 987: Validation loss did not decrease\n",
            "\t Train_Loss: 30.2594 Val_Loss: 141.0078  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 988: Validation loss did not decrease\n",
            "\t Train_Loss: 30.1590 Val_Loss: 140.6290  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 989: Validation loss did not decrease\n",
            "\t Train_Loss: 30.0583 Val_Loss: 140.2496  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 990: Validation loss did not decrease\n",
            "\t Train_Loss: 29.9598 Val_Loss: 139.8681  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 991: Validation loss did not decrease\n",
            "\t Train_Loss: 29.8613 Val_Loss: 139.4795  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 992: Validation loss did not decrease\n",
            "\t Train_Loss: 29.7626 Val_Loss: 139.0834  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 993: Validation loss did not decrease\n",
            "\t Train_Loss: 29.6652 Val_Loss: 138.6960  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 994: Validation loss did not decrease\n",
            "\t Train_Loss: 29.5693 Val_Loss: 138.3480  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 995: Validation loss did not decrease\n",
            "\t Train_Loss: 29.4718 Val_Loss: 138.0110  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 996: Validation loss did not decrease\n",
            "\t Train_Loss: 29.3756 Val_Loss: 137.6702  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 997: Validation loss did not decrease\n",
            "\t Train_Loss: 29.2797 Val_Loss: 137.3402  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 998: Validation loss did not decrease\n",
            "\t Train_Loss: 29.1822 Val_Loss: 137.0229  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 999: Validation loss did not decrease\n",
            "\t Train_Loss: 29.0835 Val_Loss: 136.6788  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1000: Validation loss did not decrease\n",
            "\t Train_Loss: 28.9811 Val_Loss: 136.3985  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1001: Validation loss did not decrease\n",
            "\t Train_Loss: 28.8710 Val_Loss: 136.1644  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1002: Validation loss did not decrease\n",
            "\t Train_Loss: 28.7545 Val_Loss: 135.9147  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1003: Validation loss did not decrease\n",
            "\t Train_Loss: 28.6382 Val_Loss: 135.7627  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1004: Validation loss did not decrease\n",
            "\t Train_Loss: 28.5293 Val_Loss: 135.3684  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1005: Validation loss did not decrease\n",
            "\t Train_Loss: 28.4247 Val_Loss: 134.8195  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1006: Validation loss did not decrease\n",
            "\t Train_Loss: 28.3223 Val_Loss: 134.4019  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1007: Validation loss did not decrease\n",
            "\t Train_Loss: 28.2272 Val_Loss: 133.7219  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1008: Validation loss did not decrease\n",
            "\t Train_Loss: 28.1564 Val_Loss: 133.6214  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1009: Validation loss did not decrease\n",
            "\t Train_Loss: 28.0163 Val_Loss: 133.4884  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1010: Validation loss did not decrease\n",
            "\t Train_Loss: 27.9018 Val_Loss: 132.8861  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1011: Validation loss did not decrease\n",
            "\t Train_Loss: 27.7575 Val_Loss: 132.2673  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1012: Validation loss did not decrease\n",
            "\t Train_Loss: 27.6587 Val_Loss: 131.7540  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1013: Validation loss did not decrease\n",
            "\t Train_Loss: 27.5291 Val_Loss: 131.5444  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1014: Validation loss did not decrease\n",
            "\t Train_Loss: 27.4404 Val_Loss: 131.0580  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1015: Validation loss did not decrease\n",
            "\t Train_Loss: 27.3249 Val_Loss: 130.7627  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1016: Validation loss did not decrease\n",
            "\t Train_Loss: 27.2062 Val_Loss: 130.5233  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1017: Validation loss did not decrease\n",
            "\t Train_Loss: 27.1224 Val_Loss: 129.9230  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1018: Validation loss did not decrease\n",
            "\t Train_Loss: 27.0074 Val_Loss: 129.5132  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1019: Validation loss did not decrease\n",
            "\t Train_Loss: 26.9010 Val_Loss: 129.1465  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1020: Validation loss did not decrease\n",
            "\t Train_Loss: 26.8209 Val_Loss: 128.7943  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1021: Validation loss did not decrease\n",
            "\t Train_Loss: 26.7122 Val_Loss: 128.7152  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1022: Validation loss did not decrease\n",
            "\t Train_Loss: 26.6097 Val_Loss: 128.3477  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1023: Validation loss did not decrease\n",
            "\t Train_Loss: 26.5150 Val_Loss: 128.0097  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1024: Validation loss did not decrease\n",
            "\t Train_Loss: 26.4048 Val_Loss: 127.5401  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1025: Validation loss did not decrease\n",
            "\t Train_Loss: 26.3031 Val_Loss: 126.9734  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1026: Validation loss did not decrease\n",
            "\t Train_Loss: 26.2147 Val_Loss: 126.9637  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1027: Validation loss did not decrease\n",
            "\t Train_Loss: 26.1574 Val_Loss: 126.1136  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1028: Validation loss did not decrease\n",
            "\t Train_Loss: 26.1694 Val_Loss: 126.5614  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1029: Validation loss did not decrease\n",
            "\t Train_Loss: 26.0607 Val_Loss: 126.2508  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1030: Validation loss did not decrease\n",
            "\t Train_Loss: 25.9827 Val_Loss: 125.2369  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1031: Validation loss did not decrease\n",
            "\t Train_Loss: 25.8299 Val_Loss: 124.6673  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1032: Validation loss did not decrease\n",
            "\t Train_Loss: 25.7542 Val_Loss: 124.9845  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1033: Validation loss did not decrease\n",
            "\t Train_Loss: 25.6745 Val_Loss: 125.0852  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1034: Validation loss did not decrease\n",
            "\t Train_Loss: 25.6201 Val_Loss: 123.6784  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1035: Validation loss did not decrease\n",
            "\t Train_Loss: 25.7460 Val_Loss: 124.0583  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1036: Validation loss did not decrease\n",
            "\t Train_Loss: 25.3611 Val_Loss: 124.5629  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1037: Validation loss did not decrease\n",
            "\t Train_Loss: 25.4959 Val_Loss: 123.1072  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1038: Validation loss did not decrease\n",
            "\t Train_Loss: 25.1691 Val_Loss: 122.1295  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1039: Validation loss did not decrease\n",
            "\t Train_Loss: 25.3914 Val_Loss: 122.7390  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1040: Validation loss did not decrease\n",
            "\t Train_Loss: 25.0607 Val_Loss: 122.8924  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1041: Validation loss did not decrease\n",
            "\t Train_Loss: 25.1547 Val_Loss: 121.2631  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1042: Validation loss did not decrease\n",
            "\t Train_Loss: 24.9815 Val_Loss: 120.9862  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1043: Validation loss did not decrease\n",
            "\t Train_Loss: 24.8578 Val_Loss: 121.5106  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1044: Validation loss did not decrease\n",
            "\t Train_Loss: 24.8230 Val_Loss: 121.6342  BEST VAL Loss: 120.8769\n",
            "\n",
            "Epoch 1045: Validation loss decreased (120.876945 --> 120.254448).\n",
            "\t Train_Loss: 24.8672 Val_Loss: 120.2544  BEST VAL Loss: 120.2544\n",
            "\n",
            "Epoch 1046: Validation loss decreased (120.254448 --> 119.634109).\n",
            "\t Train_Loss: 24.6151 Val_Loss: 119.6341  BEST VAL Loss: 119.6341\n",
            "\n",
            "Epoch 1047: Validation loss did not decrease\n",
            "\t Train_Loss: 24.9538 Val_Loss: 120.5517  BEST VAL Loss: 119.6341\n",
            "\n",
            "Epoch 1048: Validation loss did not decrease\n",
            "\t Train_Loss: 24.5073 Val_Loss: 121.3408  BEST VAL Loss: 119.6341\n",
            "\n",
            "Epoch 1049: Validation loss decreased (119.634109 --> 119.169487).\n",
            "\t Train_Loss: 24.8994 Val_Loss: 119.1695  BEST VAL Loss: 119.1695\n",
            "\n",
            "Epoch 1050: Validation loss decreased (119.169487 --> 118.446655).\n",
            "\t Train_Loss: 24.3743 Val_Loss: 118.4467  BEST VAL Loss: 118.4467\n",
            "\n",
            "Epoch 1051: Validation loss did not decrease\n",
            "\t Train_Loss: 25.0478 Val_Loss: 118.8754  BEST VAL Loss: 118.4467\n",
            "\n",
            "Epoch 1052: Validation loss did not decrease\n",
            "\t Train_Loss: 24.1509 Val_Loss: 120.7920  BEST VAL Loss: 118.4467\n",
            "\n",
            "Epoch 1053: Validation loss decreased (118.446655 --> 117.973015).\n",
            "\t Train_Loss: 25.2050 Val_Loss: 117.9730  BEST VAL Loss: 117.9730\n",
            "\n",
            "Epoch 1054: Validation loss decreased (117.973015 --> 117.249512).\n",
            "\t Train_Loss: 24.0130 Val_Loss: 117.2495  BEST VAL Loss: 117.2495\n",
            "\n",
            "Epoch 1055: Validation loss did not decrease\n",
            "\t Train_Loss: 24.5163 Val_Loss: 117.2563  BEST VAL Loss: 117.2495\n",
            "\n",
            "Epoch 1056: Validation loss did not decrease\n",
            "\t Train_Loss: 23.8163 Val_Loss: 118.8167  BEST VAL Loss: 117.2495\n",
            "\n",
            "Epoch 1057: Validation loss decreased (117.249512 --> 116.201736).\n",
            "\t Train_Loss: 24.7715 Val_Loss: 116.2017  BEST VAL Loss: 116.2017\n",
            "\n",
            "Epoch 1058: Validation loss decreased (116.201736 --> 115.407898).\n",
            "\t Train_Loss: 23.6490 Val_Loss: 115.4079  BEST VAL Loss: 115.4079\n",
            "\n",
            "Epoch 1059: Validation loss did not decrease\n",
            "\t Train_Loss: 24.1709 Val_Loss: 115.8171  BEST VAL Loss: 115.4079\n",
            "\n",
            "Epoch 1060: Validation loss did not decrease\n",
            "\t Train_Loss: 23.4917 Val_Loss: 117.5912  BEST VAL Loss: 115.4079\n",
            "\n",
            "Epoch 1061: Validation loss did not decrease\n",
            "\t Train_Loss: 24.3300 Val_Loss: 115.7036  BEST VAL Loss: 115.4079\n",
            "\n",
            "Epoch 1062: Validation loss decreased (115.407898 --> 114.789726).\n",
            "\t Train_Loss: 23.3328 Val_Loss: 114.7897  BEST VAL Loss: 114.7897\n",
            "\n",
            "Epoch 1063: Validation loss decreased (114.789726 --> 114.038612).\n",
            "\t Train_Loss: 23.5645 Val_Loss: 114.0386  BEST VAL Loss: 114.0386\n",
            "\n",
            "Epoch 1064: Validation loss did not decrease\n",
            "\t Train_Loss: 23.8501 Val_Loss: 114.8739  BEST VAL Loss: 114.0386\n",
            "\n",
            "Epoch 1065: Validation loss did not decrease\n",
            "\t Train_Loss: 23.2892 Val_Loss: 116.7510  BEST VAL Loss: 114.0386\n",
            "\n",
            "Epoch 1066: Validation loss did not decrease\n",
            "\t Train_Loss: 24.3299 Val_Loss: 114.5642  BEST VAL Loss: 114.0386\n",
            "\n",
            "Epoch 1067: Validation loss did not decrease\n",
            "\t Train_Loss: 23.1565 Val_Loss: 114.0857  BEST VAL Loss: 114.0386\n",
            "\n",
            "Epoch 1068: Validation loss decreased (114.038612 --> 113.042229).\n",
            "\t Train_Loss: 23.5797 Val_Loss: 113.0422  BEST VAL Loss: 113.0422\n",
            "\n",
            "Epoch 1069: Validation loss decreased (113.042229 --> 112.494225).\n",
            "\t Train_Loss: 23.2801 Val_Loss: 112.4942  BEST VAL Loss: 112.4942\n",
            "\n",
            "Epoch 1070: Validation loss did not decrease\n",
            "\t Train_Loss: 23.0535 Val_Loss: 113.2084  BEST VAL Loss: 112.4942\n",
            "\n",
            "Epoch 1071: Validation loss did not decrease\n",
            "\t Train_Loss: 23.1840 Val_Loss: 113.1202  BEST VAL Loss: 112.4942\n",
            "\n",
            "Epoch 1072: Validation loss did not decrease\n",
            "\t Train_Loss: 22.8988 Val_Loss: 113.1276  BEST VAL Loss: 112.4942\n",
            "\n",
            "Epoch 1073: Validation loss did not decrease\n",
            "\t Train_Loss: 22.7642 Val_Loss: 113.6137  BEST VAL Loss: 112.4942\n",
            "\n",
            "Epoch 1074: Validation loss did not decrease\n",
            "\t Train_Loss: 22.8888 Val_Loss: 112.7556  BEST VAL Loss: 112.4942\n",
            "\n",
            "Epoch 1075: Validation loss decreased (112.494225 --> 111.266724).\n",
            "\t Train_Loss: 22.5249 Val_Loss: 111.2667  BEST VAL Loss: 111.2667\n",
            "\n",
            "Epoch 1076: Validation loss decreased (111.266724 --> 110.706299).\n",
            "\t Train_Loss: 22.3575 Val_Loss: 110.7063  BEST VAL Loss: 110.7063\n",
            "\n",
            "Epoch 1077: Validation loss decreased (110.706299 --> 110.627541).\n",
            "\t Train_Loss: 22.5819 Val_Loss: 110.6275  BEST VAL Loss: 110.6275\n",
            "\n",
            "Epoch 1078: Validation loss did not decrease\n",
            "\t Train_Loss: 22.3308 Val_Loss: 110.6837  BEST VAL Loss: 110.6275\n",
            "\n",
            "Epoch 1079: Validation loss did not decrease\n",
            "\t Train_Loss: 22.1054 Val_Loss: 111.1724  BEST VAL Loss: 110.6275\n",
            "\n",
            "Epoch 1080: Validation loss did not decrease\n",
            "\t Train_Loss: 22.2376 Val_Loss: 111.0215  BEST VAL Loss: 110.6275\n",
            "\n",
            "Epoch 1081: Validation loss decreased (110.627541 --> 110.356911).\n",
            "\t Train_Loss: 22.0978 Val_Loss: 110.3569  BEST VAL Loss: 110.3569\n",
            "\n",
            "Epoch 1082: Validation loss decreased (110.356911 --> 109.467308).\n",
            "\t Train_Loss: 21.9507 Val_Loss: 109.4673  BEST VAL Loss: 109.4673\n",
            "\n",
            "Epoch 1083: Validation loss decreased (109.467308 --> 108.678299).\n",
            "\t Train_Loss: 21.8816 Val_Loss: 108.6783  BEST VAL Loss: 108.6783\n",
            "\n",
            "Epoch 1084: Validation loss did not decrease\n",
            "\t Train_Loss: 21.9476 Val_Loss: 109.1388  BEST VAL Loss: 108.6783\n",
            "\n",
            "Epoch 1085: Validation loss did not decrease\n",
            "\t Train_Loss: 21.6958 Val_Loss: 109.8481  BEST VAL Loss: 108.6783\n",
            "\n",
            "Epoch 1086: Validation loss did not decrease\n",
            "\t Train_Loss: 21.6895 Val_Loss: 109.2632  BEST VAL Loss: 108.6783\n",
            "\n",
            "Epoch 1087: Validation loss did not decrease\n",
            "\t Train_Loss: 21.6112 Val_Loss: 108.8303  BEST VAL Loss: 108.6783\n",
            "\n",
            "Epoch 1088: Validation loss decreased (108.678299 --> 108.651566).\n",
            "\t Train_Loss: 21.5317 Val_Loss: 108.6516  BEST VAL Loss: 108.6516\n",
            "\n",
            "Epoch 1089: Validation loss decreased (108.651566 --> 108.253738).\n",
            "\t Train_Loss: 21.4542 Val_Loss: 108.2537  BEST VAL Loss: 108.2537\n",
            "\n",
            "Epoch 1090: Validation loss decreased (108.253738 --> 107.547379).\n",
            "\t Train_Loss: 21.4179 Val_Loss: 107.5474  BEST VAL Loss: 107.5474\n",
            "\n",
            "Epoch 1091: Validation loss did not decrease\n",
            "\t Train_Loss: 21.3405 Val_Loss: 107.7339  BEST VAL Loss: 107.5474\n",
            "\n",
            "Epoch 1092: Validation loss did not decrease\n",
            "\t Train_Loss: 21.2215 Val_Loss: 107.9706  BEST VAL Loss: 107.5474\n",
            "\n",
            "Epoch 1093: Validation loss decreased (107.547379 --> 107.351982).\n",
            "\t Train_Loss: 21.2077 Val_Loss: 107.3520  BEST VAL Loss: 107.3520\n",
            "\n",
            "Epoch 1094: Validation loss decreased (107.351982 --> 106.822044).\n",
            "\t Train_Loss: 21.1327 Val_Loss: 106.8220  BEST VAL Loss: 106.8220\n",
            "\n",
            "Epoch 1095: Validation loss decreased (106.822044 --> 106.653748).\n",
            "\t Train_Loss: 21.0462 Val_Loss: 106.6537  BEST VAL Loss: 106.6537\n",
            "\n",
            "Epoch 1096: Validation loss decreased (106.653748 --> 106.310768).\n",
            "\t Train_Loss: 20.9909 Val_Loss: 106.3108  BEST VAL Loss: 106.3108\n",
            "\n",
            "Epoch 1097: Validation loss decreased (106.310768 --> 105.952942).\n",
            "\t Train_Loss: 20.9377 Val_Loss: 105.9529  BEST VAL Loss: 105.9529\n",
            "\n",
            "Epoch 1098: Validation loss did not decrease\n",
            "\t Train_Loss: 20.8686 Val_Loss: 106.1554  BEST VAL Loss: 105.9529\n",
            "\n",
            "Epoch 1099: Validation loss did not decrease\n",
            "\t Train_Loss: 20.7828 Val_Loss: 106.3803  BEST VAL Loss: 105.9529\n",
            "\n",
            "Epoch 1100: Validation loss decreased (105.952942 --> 105.927650).\n",
            "\t Train_Loss: 20.7513 Val_Loss: 105.9277  BEST VAL Loss: 105.9277\n",
            "\n",
            "Epoch 1101: Validation loss decreased (105.927650 --> 105.361916).\n",
            "\t Train_Loss: 20.6753 Val_Loss: 105.3619  BEST VAL Loss: 105.3619\n",
            "\n",
            "Epoch 1102: Validation loss decreased (105.361916 --> 105.062454).\n",
            "\t Train_Loss: 20.6120 Val_Loss: 105.0625  BEST VAL Loss: 105.0625\n",
            "\n",
            "Epoch 1103: Validation loss decreased (105.062454 --> 104.795128).\n",
            "\t Train_Loss: 20.5469 Val_Loss: 104.7951  BEST VAL Loss: 104.7951\n",
            "\n",
            "Epoch 1104: Validation loss decreased (104.795128 --> 104.471947).\n",
            "\t Train_Loss: 20.4979 Val_Loss: 104.4719  BEST VAL Loss: 104.4719\n",
            "\n",
            "Epoch 1105: Validation loss did not decrease\n",
            "\t Train_Loss: 20.4284 Val_Loss: 104.4858  BEST VAL Loss: 104.4719\n",
            "\n",
            "Epoch 1106: Validation loss did not decrease\n",
            "\t Train_Loss: 20.3665 Val_Loss: 104.5703  BEST VAL Loss: 104.4719\n",
            "\n",
            "Epoch 1107: Validation loss decreased (104.471947 --> 104.149635).\n",
            "\t Train_Loss: 20.3172 Val_Loss: 104.1496  BEST VAL Loss: 104.1496\n",
            "\n",
            "Epoch 1108: Validation loss decreased (104.149635 --> 103.692894).\n",
            "\t Train_Loss: 20.2483 Val_Loss: 103.6929  BEST VAL Loss: 103.6929\n",
            "\n",
            "Epoch 1109: Validation loss decreased (103.692894 --> 103.471779).\n",
            "\t Train_Loss: 20.1931 Val_Loss: 103.4718  BEST VAL Loss: 103.4718\n",
            "\n",
            "Epoch 1110: Validation loss decreased (103.471779 --> 103.277519).\n",
            "\t Train_Loss: 20.1350 Val_Loss: 103.2775  BEST VAL Loss: 103.2775\n",
            "\n",
            "Epoch 1111: Validation loss decreased (103.277519 --> 103.030083).\n",
            "\t Train_Loss: 20.0793 Val_Loss: 103.0301  BEST VAL Loss: 103.0301\n",
            "\n",
            "Epoch 1112: Validation loss decreased (103.030083 --> 102.980896).\n",
            "\t Train_Loss: 20.0158 Val_Loss: 102.9809  BEST VAL Loss: 102.9809\n",
            "\n",
            "Epoch 1113: Validation loss decreased (102.980896 --> 102.938110).\n",
            "\t Train_Loss: 19.9592 Val_Loss: 102.9381  BEST VAL Loss: 102.9381\n",
            "\n",
            "Epoch 1114: Validation loss decreased (102.938110 --> 102.516602).\n",
            "\t Train_Loss: 19.9084 Val_Loss: 102.5166  BEST VAL Loss: 102.5166\n",
            "\n",
            "Epoch 1115: Validation loss decreased (102.516602 --> 102.128990).\n",
            "\t Train_Loss: 19.8457 Val_Loss: 102.1290  BEST VAL Loss: 102.1290\n",
            "\n",
            "Epoch 1116: Validation loss decreased (102.128990 --> 101.908966).\n",
            "\t Train_Loss: 19.7886 Val_Loss: 101.9090  BEST VAL Loss: 101.9090\n",
            "\n",
            "Epoch 1117: Validation loss decreased (101.908966 --> 101.666466).\n",
            "\t Train_Loss: 19.7354 Val_Loss: 101.6665  BEST VAL Loss: 101.6665\n",
            "\n",
            "Epoch 1118: Validation loss decreased (101.666466 --> 101.475990).\n",
            "\t Train_Loss: 19.6783 Val_Loss: 101.4760  BEST VAL Loss: 101.4760\n",
            "\n",
            "Epoch 1119: Validation loss decreased (101.475990 --> 101.424530).\n",
            "\t Train_Loss: 19.6226 Val_Loss: 101.4245  BEST VAL Loss: 101.4245\n",
            "\n",
            "Epoch 1120: Validation loss decreased (101.424530 --> 101.262527).\n",
            "\t Train_Loss: 19.5662 Val_Loss: 101.2625  BEST VAL Loss: 101.2625\n",
            "\n",
            "Epoch 1121: Validation loss decreased (101.262527 --> 100.890465).\n",
            "\t Train_Loss: 19.5128 Val_Loss: 100.8905  BEST VAL Loss: 100.8905\n",
            "\n",
            "Epoch 1122: Validation loss decreased (100.890465 --> 100.606560).\n",
            "\t Train_Loss: 19.4571 Val_Loss: 100.6066  BEST VAL Loss: 100.6066\n",
            "\n",
            "Epoch 1123: Validation loss decreased (100.606560 --> 100.396904).\n",
            "\t Train_Loss: 19.4014 Val_Loss: 100.3969  BEST VAL Loss: 100.3969\n",
            "\n",
            "Epoch 1124: Validation loss decreased (100.396904 --> 100.134735).\n",
            "\t Train_Loss: 19.3494 Val_Loss: 100.1347  BEST VAL Loss: 100.1347\n",
            "\n",
            "Epoch 1125: Validation loss decreased (100.134735 --> 99.977684).\n",
            "\t Train_Loss: 19.2933 Val_Loss: 99.9777  BEST VAL Loss: 99.9777\n",
            "\n",
            "Epoch 1126: Validation loss decreased (99.977684 --> 99.899467).\n",
            "\t Train_Loss: 19.2393 Val_Loss: 99.8995  BEST VAL Loss: 99.8995\n",
            "\n",
            "Epoch 1127: Validation loss decreased (99.899467 --> 99.669830).\n",
            "\t Train_Loss: 19.1865 Val_Loss: 99.6698  BEST VAL Loss: 99.6698\n",
            "\n",
            "Epoch 1128: Validation loss decreased (99.669830 --> 99.366844).\n",
            "\t Train_Loss: 19.1327 Val_Loss: 99.3668  BEST VAL Loss: 99.3668\n",
            "\n",
            "Epoch 1129: Validation loss decreased (99.366844 --> 99.124870).\n",
            "\t Train_Loss: 19.0795 Val_Loss: 99.1249  BEST VAL Loss: 99.1249\n",
            "\n",
            "Epoch 1130: Validation loss decreased (99.124870 --> 98.861771).\n",
            "\t Train_Loss: 19.0258 Val_Loss: 98.8618  BEST VAL Loss: 98.8618\n",
            "\n",
            "Epoch 1131: Validation loss decreased (98.861771 --> 98.613358).\n",
            "\t Train_Loss: 18.9737 Val_Loss: 98.6134  BEST VAL Loss: 98.6134\n",
            "\n",
            "Epoch 1132: Validation loss decreased (98.613358 --> 98.486855).\n",
            "\t Train_Loss: 18.9214 Val_Loss: 98.4869  BEST VAL Loss: 98.4869\n",
            "\n",
            "Epoch 1133: Validation loss decreased (98.486855 --> 98.348381).\n",
            "\t Train_Loss: 18.8682 Val_Loss: 98.3484  BEST VAL Loss: 98.3484\n",
            "\n",
            "Epoch 1134: Validation loss decreased (98.348381 --> 98.109039).\n",
            "\t Train_Loss: 18.8164 Val_Loss: 98.1090  BEST VAL Loss: 98.1090\n",
            "\n",
            "Epoch 1135: Validation loss decreased (98.109039 --> 97.886818).\n",
            "\t Train_Loss: 18.7646 Val_Loss: 97.8868  BEST VAL Loss: 97.8868\n",
            "\n",
            "Epoch 1136: Validation loss decreased (97.886818 --> 97.659935).\n",
            "\t Train_Loss: 18.7124 Val_Loss: 97.6599  BEST VAL Loss: 97.6599\n",
            "\n",
            "Epoch 1137: Validation loss decreased (97.659935 --> 97.384544).\n",
            "\t Train_Loss: 18.6612 Val_Loss: 97.3845  BEST VAL Loss: 97.3845\n",
            "\n",
            "Epoch 1138: Validation loss decreased (97.384544 --> 97.171967).\n",
            "\t Train_Loss: 18.6096 Val_Loss: 97.1720  BEST VAL Loss: 97.1720\n",
            "\n",
            "Epoch 1139: Validation loss decreased (97.171967 --> 97.025215).\n",
            "\t Train_Loss: 18.5584 Val_Loss: 97.0252  BEST VAL Loss: 97.0252\n",
            "\n",
            "Epoch 1140: Validation loss decreased (97.025215 --> 96.830208).\n",
            "\t Train_Loss: 18.5073 Val_Loss: 96.8302  BEST VAL Loss: 96.8302\n",
            "\n",
            "Epoch 1141: Validation loss decreased (96.830208 --> 96.618736).\n",
            "\t Train_Loss: 18.4564 Val_Loss: 96.6187  BEST VAL Loss: 96.6187\n",
            "\n",
            "Epoch 1142: Validation loss decreased (96.618736 --> 96.423233).\n",
            "\t Train_Loss: 18.4059 Val_Loss: 96.4232  BEST VAL Loss: 96.4232\n",
            "\n",
            "Epoch 1143: Validation loss decreased (96.423233 --> 96.179337).\n",
            "\t Train_Loss: 18.3552 Val_Loss: 96.1793  BEST VAL Loss: 96.1793\n",
            "\n",
            "Epoch 1144: Validation loss decreased (96.179337 --> 95.939590).\n",
            "\t Train_Loss: 18.3047 Val_Loss: 95.9396  BEST VAL Loss: 95.9396\n",
            "\n",
            "Epoch 1145: Validation loss decreased (95.939590 --> 95.761772).\n",
            "\t Train_Loss: 18.2547 Val_Loss: 95.7618  BEST VAL Loss: 95.7618\n",
            "\n",
            "Epoch 1146: Validation loss decreased (95.761772 --> 95.580788).\n",
            "\t Train_Loss: 18.2045 Val_Loss: 95.5808  BEST VAL Loss: 95.5808\n",
            "\n",
            "Epoch 1147: Validation loss decreased (95.580788 --> 95.379997).\n",
            "\t Train_Loss: 18.1545 Val_Loss: 95.3800  BEST VAL Loss: 95.3800\n",
            "\n",
            "Epoch 1148: Validation loss decreased (95.379997 --> 95.198448).\n",
            "\t Train_Loss: 18.1048 Val_Loss: 95.1984  BEST VAL Loss: 95.1984\n",
            "\n",
            "Epoch 1149: Validation loss decreased (95.198448 --> 94.987724).\n",
            "\t Train_Loss: 18.0550 Val_Loss: 94.9877  BEST VAL Loss: 94.9877\n",
            "\n",
            "Epoch 1150: Validation loss decreased (94.987724 --> 94.741295).\n",
            "\t Train_Loss: 18.0055 Val_Loss: 94.7413  BEST VAL Loss: 94.7413\n",
            "\n",
            "Epoch 1151: Validation loss decreased (94.741295 --> 94.532516).\n",
            "\t Train_Loss: 17.9561 Val_Loss: 94.5325  BEST VAL Loss: 94.5325\n",
            "\n",
            "Epoch 1152: Validation loss decreased (94.532516 --> 94.345604).\n",
            "\t Train_Loss: 17.9067 Val_Loss: 94.3456  BEST VAL Loss: 94.3456\n",
            "\n",
            "Epoch 1153: Validation loss decreased (94.345604 --> 94.150307).\n",
            "\t Train_Loss: 17.8575 Val_Loss: 94.1503  BEST VAL Loss: 94.1503\n",
            "\n",
            "Epoch 1154: Validation loss decreased (94.150307 --> 93.977066).\n",
            "\t Train_Loss: 17.8082 Val_Loss: 93.9771  BEST VAL Loss: 93.9771\n",
            "\n",
            "Epoch 1155: Validation loss decreased (93.977066 --> 93.800575).\n",
            "\t Train_Loss: 17.7591 Val_Loss: 93.8006  BEST VAL Loss: 93.8006\n",
            "\n",
            "Epoch 1156: Validation loss decreased (93.800575 --> 93.585030).\n",
            "\t Train_Loss: 17.7101 Val_Loss: 93.5850  BEST VAL Loss: 93.5850\n",
            "\n",
            "Epoch 1157: Validation loss decreased (93.585030 --> 93.373405).\n",
            "\t Train_Loss: 17.6610 Val_Loss: 93.3734  BEST VAL Loss: 93.3734\n",
            "\n",
            "Epoch 1158: Validation loss decreased (93.373405 --> 93.174995).\n",
            "\t Train_Loss: 17.6120 Val_Loss: 93.1750  BEST VAL Loss: 93.1750\n",
            "\n",
            "Epoch 1159: Validation loss decreased (93.174995 --> 92.969780).\n",
            "\t Train_Loss: 17.5630 Val_Loss: 92.9698  BEST VAL Loss: 92.9698\n",
            "\n",
            "Epoch 1160: Validation loss decreased (92.969780 --> 92.787239).\n",
            "\t Train_Loss: 17.5139 Val_Loss: 92.7872  BEST VAL Loss: 92.7872\n",
            "\n",
            "Epoch 1161: Validation loss decreased (92.787239 --> 92.622589).\n",
            "\t Train_Loss: 17.4647 Val_Loss: 92.6226  BEST VAL Loss: 92.6226\n",
            "\n",
            "Epoch 1162: Validation loss decreased (92.622589 --> 92.435501).\n",
            "\t Train_Loss: 17.4154 Val_Loss: 92.4355  BEST VAL Loss: 92.4355\n",
            "\n",
            "Epoch 1163: Validation loss decreased (92.435501 --> 92.242554).\n",
            "\t Train_Loss: 17.3660 Val_Loss: 92.2426  BEST VAL Loss: 92.2426\n",
            "\n",
            "Epoch 1164: Validation loss decreased (92.242554 --> 92.052872).\n",
            "\t Train_Loss: 17.3165 Val_Loss: 92.0529  BEST VAL Loss: 92.0529\n",
            "\n",
            "Epoch 1165: Validation loss decreased (92.052872 --> 91.847809).\n",
            "\t Train_Loss: 17.2667 Val_Loss: 91.8478  BEST VAL Loss: 91.8478\n",
            "\n",
            "Epoch 1166: Validation loss decreased (91.847809 --> 91.651146).\n",
            "\t Train_Loss: 17.2166 Val_Loss: 91.6511  BEST VAL Loss: 91.6511\n",
            "\n",
            "Epoch 1167: Validation loss decreased (91.651146 --> 91.473412).\n",
            "\t Train_Loss: 17.1664 Val_Loss: 91.4734  BEST VAL Loss: 91.4734\n",
            "\n",
            "Epoch 1168: Validation loss decreased (91.473412 --> 91.287689).\n",
            "\t Train_Loss: 17.1159 Val_Loss: 91.2877  BEST VAL Loss: 91.2877\n",
            "\n",
            "Epoch 1169: Validation loss decreased (91.287689 --> 91.100830).\n",
            "\t Train_Loss: 17.0650 Val_Loss: 91.1008  BEST VAL Loss: 91.1008\n",
            "\n",
            "Epoch 1170: Validation loss decreased (91.100830 --> 90.919266).\n",
            "\t Train_Loss: 17.0140 Val_Loss: 90.9193  BEST VAL Loss: 90.9193\n",
            "\n",
            "Epoch 1171: Validation loss decreased (90.919266 --> 90.723251).\n",
            "\t Train_Loss: 16.9626 Val_Loss: 90.7233  BEST VAL Loss: 90.7233\n",
            "\n",
            "Epoch 1172: Validation loss decreased (90.723251 --> 90.526810).\n",
            "\t Train_Loss: 16.9111 Val_Loss: 90.5268  BEST VAL Loss: 90.5268\n",
            "\n",
            "Epoch 1173: Validation loss decreased (90.526810 --> 90.344254).\n",
            "\t Train_Loss: 16.8593 Val_Loss: 90.3443  BEST VAL Loss: 90.3443\n",
            "\n",
            "Epoch 1174: Validation loss decreased (90.344254 --> 90.159508).\n",
            "\t Train_Loss: 16.8073 Val_Loss: 90.1595  BEST VAL Loss: 90.1595\n",
            "\n",
            "Epoch 1175: Validation loss decreased (90.159508 --> 89.975952).\n",
            "\t Train_Loss: 16.7551 Val_Loss: 89.9760  BEST VAL Loss: 89.9760\n",
            "\n",
            "Epoch 1176: Validation loss decreased (89.975952 --> 89.800804).\n",
            "\t Train_Loss: 16.7028 Val_Loss: 89.8008  BEST VAL Loss: 89.8008\n",
            "\n",
            "Epoch 1177: Validation loss decreased (89.800804 --> 89.620621).\n",
            "\t Train_Loss: 16.6504 Val_Loss: 89.6206  BEST VAL Loss: 89.6206\n",
            "\n",
            "Epoch 1178: Validation loss decreased (89.620621 --> 89.432701).\n",
            "\t Train_Loss: 16.5980 Val_Loss: 89.4327  BEST VAL Loss: 89.4327\n",
            "\n",
            "Epoch 1179: Validation loss decreased (89.432701 --> 89.260048).\n",
            "\t Train_Loss: 16.5476 Val_Loss: 89.2600  BEST VAL Loss: 89.2600\n",
            "\n",
            "Epoch 1180: Validation loss decreased (89.260048 --> 89.029053).\n",
            "\t Train_Loss: 16.5255 Val_Loss: 89.0291  BEST VAL Loss: 89.0291\n",
            "\n",
            "Epoch 1181: Validation loss decreased (89.029053 --> 88.925682).\n",
            "\t Train_Loss: 16.5859 Val_Loss: 88.9257  BEST VAL Loss: 88.9257\n",
            "\n",
            "Epoch 1182: Validation loss did not decrease\n",
            "\t Train_Loss: 16.5602 Val_Loss: 89.0218  BEST VAL Loss: 88.9257\n",
            "\n",
            "Epoch 1183: Validation loss did not decrease\n",
            "\t Train_Loss: 16.4968 Val_Loss: 89.0727  BEST VAL Loss: 88.9257\n",
            "\n",
            "Epoch 1184: Validation loss decreased (88.925682 --> 88.852341).\n",
            "\t Train_Loss: 16.4226 Val_Loss: 88.8523  BEST VAL Loss: 88.8523\n",
            "\n",
            "Epoch 1185: Validation loss decreased (88.852341 --> 88.432343).\n",
            "\t Train_Loss: 16.2942 Val_Loss: 88.4323  BEST VAL Loss: 88.4323\n",
            "\n",
            "Epoch 1186: Validation loss decreased (88.432343 --> 86.963585).\n",
            "\t Train_Loss: 17.1848 Val_Loss: 86.9636  BEST VAL Loss: 86.9636\n",
            "\n",
            "Epoch 1187: Validation loss did not decrease\n",
            "\t Train_Loss: 16.8760 Val_Loss: 87.1356  BEST VAL Loss: 86.9636\n",
            "\n",
            "Epoch 1188: Validation loss did not decrease\n",
            "\t Train_Loss: 17.4776 Val_Loss: 87.7207  BEST VAL Loss: 86.9636\n",
            "\n",
            "Epoch 1189: Validation loss did not decrease\n",
            "\t Train_Loss: 17.0991 Val_Loss: 88.4419  BEST VAL Loss: 86.9636\n",
            "\n",
            "Epoch 1190: Validation loss did not decrease\n",
            "\t Train_Loss: 16.8168 Val_Loss: 90.9362  BEST VAL Loss: 86.9636\n",
            "\n",
            "Epoch 1191: Validation loss did not decrease\n",
            "\t Train_Loss: 16.9739 Val_Loss: 90.4979  BEST VAL Loss: 86.9636\n",
            "\n",
            "Epoch 1192: Validation loss did not decrease\n",
            "\t Train_Loss: 17.0497 Val_Loss: 90.3161  BEST VAL Loss: 86.9636\n",
            "\n",
            "Epoch 1193: Validation loss did not decrease\n",
            "\t Train_Loss: 16.8537 Val_Loss: 89.2979  BEST VAL Loss: 86.9636\n",
            "\n",
            "Epoch 1194: Validation loss did not decrease\n",
            "\t Train_Loss: 16.5831 Val_Loss: 87.8982  BEST VAL Loss: 86.9636\n",
            "\n",
            "Epoch 1195: Validation loss did not decrease\n",
            "\t Train_Loss: 16.4333 Val_Loss: 87.4047  BEST VAL Loss: 86.9636\n",
            "\n",
            "Epoch 1196: Validation loss decreased (86.963585 --> 86.807243).\n",
            "\t Train_Loss: 16.3048 Val_Loss: 86.8072  BEST VAL Loss: 86.8072\n",
            "\n",
            "Epoch 1197: Validation loss decreased (86.807243 --> 86.592331).\n",
            "\t Train_Loss: 16.1212 Val_Loss: 86.5923  BEST VAL Loss: 86.5923\n",
            "\n",
            "Epoch 1198: Validation loss did not decrease\n",
            "\t Train_Loss: 16.0470 Val_Loss: 87.3017  BEST VAL Loss: 86.5923\n",
            "\n",
            "Epoch 1199: Validation loss did not decrease\n",
            "\t Train_Loss: 16.0868 Val_Loss: 87.2564  BEST VAL Loss: 86.5923\n",
            "\n",
            "Epoch 1200: Validation loss did not decrease\n",
            "\t Train_Loss: 16.0894 Val_Loss: 86.8423  BEST VAL Loss: 86.5923\n",
            "\n",
            "Epoch 1201: Validation loss decreased (86.592331 --> 86.307060).\n",
            "\t Train_Loss: 15.9717 Val_Loss: 86.3071  BEST VAL Loss: 86.3071\n",
            "\n",
            "Epoch 1202: Validation loss decreased (86.307060 --> 85.647179).\n",
            "\t Train_Loss: 15.7792 Val_Loss: 85.6472  BEST VAL Loss: 85.6472\n",
            "\n",
            "Epoch 1203: Validation loss decreased (85.647179 --> 85.156067).\n",
            "\t Train_Loss: 15.6806 Val_Loss: 85.1561  BEST VAL Loss: 85.1561\n",
            "\n",
            "Epoch 1204: Validation loss decreased (85.156067 --> 85.147842).\n",
            "\t Train_Loss: 15.7106 Val_Loss: 85.1478  BEST VAL Loss: 85.1478\n",
            "\n",
            "Epoch 1205: Validation loss did not decrease\n",
            "\t Train_Loss: 15.6871 Val_Loss: 85.2373  BEST VAL Loss: 85.1478\n",
            "\n",
            "Epoch 1206: Validation loss decreased (85.147842 --> 85.121376).\n",
            "\t Train_Loss: 15.5484 Val_Loss: 85.1214  BEST VAL Loss: 85.1214\n",
            "\n",
            "Epoch 1207: Validation loss did not decrease\n",
            "\t Train_Loss: 15.4290 Val_Loss: 85.2716  BEST VAL Loss: 85.1214\n",
            "\n",
            "Epoch 1208: Validation loss decreased (85.121376 --> 84.850548).\n",
            "\t Train_Loss: 15.3990 Val_Loss: 84.8505  BEST VAL Loss: 84.8505\n",
            "\n",
            "Epoch 1209: Validation loss decreased (84.850548 --> 84.386948).\n",
            "\t Train_Loss: 15.3715 Val_Loss: 84.3869  BEST VAL Loss: 84.3869\n",
            "\n",
            "Epoch 1210: Validation loss decreased (84.386948 --> 84.272949).\n",
            "\t Train_Loss: 15.3066 Val_Loss: 84.2729  BEST VAL Loss: 84.2729\n",
            "\n",
            "Epoch 1211: Validation loss decreased (84.272949 --> 84.052856).\n",
            "\t Train_Loss: 15.2319 Val_Loss: 84.0529  BEST VAL Loss: 84.0529\n",
            "\n",
            "Epoch 1212: Validation loss did not decrease\n",
            "\t Train_Loss: 15.1794 Val_Loss: 84.1670  BEST VAL Loss: 84.0529\n",
            "\n",
            "Epoch 1213: Validation loss did not decrease\n",
            "\t Train_Loss: 15.1263 Val_Loss: 84.4174  BEST VAL Loss: 84.0529\n",
            "\n",
            "Epoch 1214: Validation loss did not decrease\n",
            "\t Train_Loss: 15.0602 Val_Loss: 84.2793  BEST VAL Loss: 84.0529\n",
            "\n",
            "Epoch 1215: Validation loss did not decrease\n",
            "\t Train_Loss: 15.0052 Val_Loss: 84.0904  BEST VAL Loss: 84.0529\n",
            "\n",
            "Epoch 1216: Validation loss decreased (84.052856 --> 83.699944).\n",
            "\t Train_Loss: 14.9646 Val_Loss: 83.6999  BEST VAL Loss: 83.6999\n",
            "\n",
            "Epoch 1217: Validation loss decreased (83.699944 --> 83.078331).\n",
            "\t Train_Loss: 14.9129 Val_Loss: 83.0783  BEST VAL Loss: 83.0783\n",
            "\n",
            "Epoch 1218: Validation loss decreased (83.078331 --> 82.655777).\n",
            "\t Train_Loss: 14.8506 Val_Loss: 82.6558  BEST VAL Loss: 82.6558\n",
            "\n",
            "Epoch 1219: Validation loss decreased (82.655777 --> 82.489128).\n",
            "\t Train_Loss: 14.8049 Val_Loss: 82.4891  BEST VAL Loss: 82.4891\n",
            "\n",
            "Epoch 1220: Validation loss decreased (82.489128 --> 82.175117).\n",
            "\t Train_Loss: 14.7807 Val_Loss: 82.1751  BEST VAL Loss: 82.1751\n",
            "\n",
            "Epoch 1221: Validation loss did not decrease\n",
            "\t Train_Loss: 14.7376 Val_Loss: 82.1782  BEST VAL Loss: 82.1751\n",
            "\n",
            "Epoch 1222: Validation loss did not decrease\n",
            "\t Train_Loss: 14.6695 Val_Loss: 82.2551  BEST VAL Loss: 82.1751\n",
            "\n",
            "Epoch 1223: Validation loss decreased (82.175117 --> 82.132370).\n",
            "\t Train_Loss: 14.6139 Val_Loss: 82.1324  BEST VAL Loss: 82.1324\n",
            "\n",
            "Epoch 1224: Validation loss did not decrease\n",
            "\t Train_Loss: 14.5779 Val_Loss: 82.1455  BEST VAL Loss: 82.1324\n",
            "\n",
            "Epoch 1225: Validation loss decreased (82.132370 --> 82.084229).\n",
            "\t Train_Loss: 14.5381 Val_Loss: 82.0842  BEST VAL Loss: 82.0842\n",
            "\n",
            "Epoch 1226: Validation loss decreased (82.084229 --> 81.796005).\n",
            "\t Train_Loss: 14.4943 Val_Loss: 81.7960  BEST VAL Loss: 81.7960\n",
            "\n",
            "Epoch 1227: Validation loss decreased (81.796005 --> 81.685791).\n",
            "\t Train_Loss: 14.4562 Val_Loss: 81.6858  BEST VAL Loss: 81.6858\n",
            "\n",
            "Epoch 1228: Validation loss decreased (81.685791 --> 81.398849).\n",
            "\t Train_Loss: 14.4148 Val_Loss: 81.3988  BEST VAL Loss: 81.3988\n",
            "\n",
            "Epoch 1229: Validation loss decreased (81.398849 --> 81.108635).\n",
            "\t Train_Loss: 14.3637 Val_Loss: 81.1086  BEST VAL Loss: 81.1086\n",
            "\n",
            "Epoch 1230: Validation loss decreased (81.108635 --> 80.976639).\n",
            "\t Train_Loss: 14.3206 Val_Loss: 80.9766  BEST VAL Loss: 80.9766\n",
            "\n",
            "Epoch 1231: Validation loss decreased (80.976639 --> 80.743736).\n",
            "\t Train_Loss: 14.2867 Val_Loss: 80.7437  BEST VAL Loss: 80.7437\n",
            "\n",
            "Epoch 1232: Validation loss decreased (80.743736 --> 80.565849).\n",
            "\t Train_Loss: 14.2469 Val_Loss: 80.5658  BEST VAL Loss: 80.5658\n",
            "\n",
            "Epoch 1233: Validation loss decreased (80.565849 --> 80.481064).\n",
            "\t Train_Loss: 14.2023 Val_Loss: 80.4811  BEST VAL Loss: 80.4811\n",
            "\n",
            "Epoch 1234: Validation loss decreased (80.481064 --> 80.241371).\n",
            "\t Train_Loss: 14.1630 Val_Loss: 80.2414  BEST VAL Loss: 80.2414\n",
            "\n",
            "Epoch 1235: Validation loss decreased (80.241371 --> 80.072105).\n",
            "\t Train_Loss: 14.1260 Val_Loss: 80.0721  BEST VAL Loss: 80.0721\n",
            "\n",
            "Epoch 1236: Validation loss decreased (80.072105 --> 79.903412).\n",
            "\t Train_Loss: 14.0856 Val_Loss: 79.9034  BEST VAL Loss: 79.9034\n",
            "\n",
            "Epoch 1237: Validation loss decreased (79.903412 --> 79.654015).\n",
            "\t Train_Loss: 14.0474 Val_Loss: 79.6540  BEST VAL Loss: 79.6540\n",
            "\n",
            "Epoch 1238: Validation loss decreased (79.654015 --> 79.546951).\n",
            "\t Train_Loss: 14.0119 Val_Loss: 79.5470  BEST VAL Loss: 79.5470\n",
            "\n",
            "Epoch 1239: Validation loss decreased (79.546951 --> 79.403053).\n",
            "\t Train_Loss: 13.9734 Val_Loss: 79.4031  BEST VAL Loss: 79.4031\n",
            "\n",
            "Epoch 1240: Validation loss decreased (79.403053 --> 79.218315).\n",
            "\t Train_Loss: 13.9354 Val_Loss: 79.2183  BEST VAL Loss: 79.2183\n",
            "\n",
            "Epoch 1241: Validation loss decreased (79.218315 --> 79.139160).\n",
            "\t Train_Loss: 13.9000 Val_Loss: 79.1392  BEST VAL Loss: 79.1392\n",
            "\n",
            "Epoch 1242: Validation loss decreased (79.139160 --> 78.948662).\n",
            "\t Train_Loss: 13.8630 Val_Loss: 78.9487  BEST VAL Loss: 78.9487\n",
            "\n",
            "Epoch 1243: Validation loss decreased (78.948662 --> 78.842003).\n",
            "\t Train_Loss: 13.8253 Val_Loss: 78.8420  BEST VAL Loss: 78.8420\n",
            "\n",
            "Epoch 1244: Validation loss decreased (78.842003 --> 78.768684).\n",
            "\t Train_Loss: 13.7900 Val_Loss: 78.7687  BEST VAL Loss: 78.7687\n",
            "\n",
            "Epoch 1245: Validation loss decreased (78.768684 --> 78.621017).\n",
            "\t Train_Loss: 13.7557 Val_Loss: 78.6210  BEST VAL Loss: 78.6210\n",
            "\n",
            "Epoch 1246: Validation loss decreased (78.621017 --> 78.527168).\n",
            "\t Train_Loss: 13.7208 Val_Loss: 78.5272  BEST VAL Loss: 78.5272\n",
            "\n",
            "Epoch 1247: Validation loss decreased (78.527168 --> 78.347000).\n",
            "\t Train_Loss: 13.6860 Val_Loss: 78.3470  BEST VAL Loss: 78.3470\n",
            "\n",
            "Epoch 1248: Validation loss decreased (78.347000 --> 78.111160).\n",
            "\t Train_Loss: 13.6512 Val_Loss: 78.1112  BEST VAL Loss: 78.1112\n",
            "\n",
            "Epoch 1249: Validation loss decreased (78.111160 --> 77.935974).\n",
            "\t Train_Loss: 13.6160 Val_Loss: 77.9360  BEST VAL Loss: 77.9360\n",
            "\n",
            "Epoch 1250: Validation loss decreased (77.935974 --> 77.700752).\n",
            "\t Train_Loss: 13.5817 Val_Loss: 77.7008  BEST VAL Loss: 77.7008\n",
            "\n",
            "Epoch 1251: Validation loss decreased (77.700752 --> 77.573288).\n",
            "\t Train_Loss: 13.5485 Val_Loss: 77.5733  BEST VAL Loss: 77.5733\n",
            "\n",
            "Epoch 1252: Validation loss decreased (77.573288 --> 77.471413).\n",
            "\t Train_Loss: 13.5150 Val_Loss: 77.4714  BEST VAL Loss: 77.4714\n",
            "\n",
            "Epoch 1253: Validation loss decreased (77.471413 --> 77.346474).\n",
            "\t Train_Loss: 13.4813 Val_Loss: 77.3465  BEST VAL Loss: 77.3465\n",
            "\n",
            "Epoch 1254: Validation loss decreased (77.346474 --> 77.275833).\n",
            "\t Train_Loss: 13.4484 Val_Loss: 77.2758  BEST VAL Loss: 77.2758\n",
            "\n",
            "Epoch 1255: Validation loss decreased (77.275833 --> 77.105560).\n",
            "\t Train_Loss: 13.4155 Val_Loss: 77.1056  BEST VAL Loss: 77.1056\n",
            "\n",
            "Epoch 1256: Validation loss decreased (77.105560 --> 76.972755).\n",
            "\t Train_Loss: 13.3826 Val_Loss: 76.9728  BEST VAL Loss: 76.9728\n",
            "\n",
            "Epoch 1257: Validation loss decreased (76.972755 --> 76.826195).\n",
            "\t Train_Loss: 13.3502 Val_Loss: 76.8262  BEST VAL Loss: 76.8262\n",
            "\n",
            "Epoch 1258: Validation loss decreased (76.826195 --> 76.662987).\n",
            "\t Train_Loss: 13.3178 Val_Loss: 76.6630  BEST VAL Loss: 76.6630\n",
            "\n",
            "Epoch 1259: Validation loss decreased (76.662987 --> 76.558487).\n",
            "\t Train_Loss: 13.2854 Val_Loss: 76.5585  BEST VAL Loss: 76.5585\n",
            "\n",
            "Epoch 1260: Validation loss decreased (76.558487 --> 76.390205).\n",
            "\t Train_Loss: 13.2537 Val_Loss: 76.3902  BEST VAL Loss: 76.3902\n",
            "\n",
            "Epoch 1261: Validation loss decreased (76.390205 --> 76.265350).\n",
            "\t Train_Loss: 13.2222 Val_Loss: 76.2654  BEST VAL Loss: 76.2654\n",
            "\n",
            "Epoch 1262: Validation loss decreased (76.265350 --> 76.120262).\n",
            "\t Train_Loss: 13.1906 Val_Loss: 76.1203  BEST VAL Loss: 76.1203\n",
            "\n",
            "Epoch 1263: Validation loss decreased (76.120262 --> 75.980301).\n",
            "\t Train_Loss: 13.1592 Val_Loss: 75.9803  BEST VAL Loss: 75.9803\n",
            "\n",
            "Epoch 1264: Validation loss decreased (75.980301 --> 75.886780).\n",
            "\t Train_Loss: 13.1279 Val_Loss: 75.8868  BEST VAL Loss: 75.8868\n",
            "\n",
            "Epoch 1265: Validation loss decreased (75.886780 --> 75.750366).\n",
            "\t Train_Loss: 13.0968 Val_Loss: 75.7504  BEST VAL Loss: 75.7504\n",
            "\n",
            "Epoch 1266: Validation loss decreased (75.750366 --> 75.649567).\n",
            "\t Train_Loss: 13.0661 Val_Loss: 75.6496  BEST VAL Loss: 75.6496\n",
            "\n",
            "Epoch 1267: Validation loss decreased (75.649567 --> 75.493393).\n",
            "\t Train_Loss: 13.0356 Val_Loss: 75.4934  BEST VAL Loss: 75.4934\n",
            "\n",
            "Epoch 1268: Validation loss decreased (75.493393 --> 75.343094).\n",
            "\t Train_Loss: 13.0050 Val_Loss: 75.3431  BEST VAL Loss: 75.3431\n",
            "\n",
            "Epoch 1269: Validation loss decreased (75.343094 --> 75.196892).\n",
            "\t Train_Loss: 12.9746 Val_Loss: 75.1969  BEST VAL Loss: 75.1969\n",
            "\n",
            "Epoch 1270: Validation loss decreased (75.196892 --> 75.040306).\n",
            "\t Train_Loss: 12.9444 Val_Loss: 75.0403  BEST VAL Loss: 75.0403\n",
            "\n",
            "Epoch 1271: Validation loss decreased (75.040306 --> 74.929420).\n",
            "\t Train_Loss: 12.9143 Val_Loss: 74.9294  BEST VAL Loss: 74.9294\n",
            "\n",
            "Epoch 1272: Validation loss decreased (74.929420 --> 74.782249).\n",
            "\t Train_Loss: 12.8845 Val_Loss: 74.7822  BEST VAL Loss: 74.7822\n",
            "\n",
            "Epoch 1273: Validation loss decreased (74.782249 --> 74.676941).\n",
            "\t Train_Loss: 12.8549 Val_Loss: 74.6769  BEST VAL Loss: 74.6769\n",
            "\n",
            "Epoch 1274: Validation loss decreased (74.676941 --> 74.540810).\n",
            "\t Train_Loss: 12.8252 Val_Loss: 74.5408  BEST VAL Loss: 74.5408\n",
            "\n",
            "Epoch 1275: Validation loss decreased (74.540810 --> 74.436501).\n",
            "\t Train_Loss: 12.7957 Val_Loss: 74.4365  BEST VAL Loss: 74.4365\n",
            "\n",
            "Epoch 1276: Validation loss decreased (74.436501 --> 74.325127).\n",
            "\t Train_Loss: 12.7664 Val_Loss: 74.3251  BEST VAL Loss: 74.3251\n",
            "\n",
            "Epoch 1277: Validation loss decreased (74.325127 --> 74.209145).\n",
            "\t Train_Loss: 12.7372 Val_Loss: 74.2091  BEST VAL Loss: 74.2091\n",
            "\n",
            "Epoch 1278: Validation loss decreased (74.209145 --> 74.094971).\n",
            "\t Train_Loss: 12.7081 Val_Loss: 74.0950  BEST VAL Loss: 74.0950\n",
            "\n",
            "Epoch 1279: Validation loss decreased (74.094971 --> 73.947403).\n",
            "\t Train_Loss: 12.6792 Val_Loss: 73.9474  BEST VAL Loss: 73.9474\n",
            "\n",
            "Epoch 1280: Validation loss decreased (73.947403 --> 73.825890).\n",
            "\t Train_Loss: 12.6504 Val_Loss: 73.8259  BEST VAL Loss: 73.8259\n",
            "\n",
            "Epoch 1281: Validation loss decreased (73.825890 --> 73.678352).\n",
            "\t Train_Loss: 12.6217 Val_Loss: 73.6784  BEST VAL Loss: 73.6784\n",
            "\n",
            "Epoch 1282: Validation loss decreased (73.678352 --> 73.574478).\n",
            "\t Train_Loss: 12.5930 Val_Loss: 73.5745  BEST VAL Loss: 73.5745\n",
            "\n",
            "Epoch 1283: Validation loss decreased (73.574478 --> 73.436974).\n",
            "\t Train_Loss: 12.5638 Val_Loss: 73.4370  BEST VAL Loss: 73.4370\n",
            "\n",
            "Epoch 1284: Validation loss decreased (73.436974 --> 73.287895).\n",
            "\t Train_Loss: 12.5297 Val_Loss: 73.2879  BEST VAL Loss: 73.2879\n",
            "\n",
            "Epoch 1285: Validation loss did not decrease\n",
            "\t Train_Loss: 12.6020 Val_Loss: 74.7554  BEST VAL Loss: 73.2879\n",
            "\n",
            "Epoch 1286: Validation loss decreased (73.287895 --> 71.433571).\n",
            "\t Train_Loss: 12.9784 Val_Loss: 71.4336  BEST VAL Loss: 71.4336\n",
            "\n",
            "Epoch 1287: Validation loss did not decrease\n",
            "\t Train_Loss: 13.6842 Val_Loss: 75.5205  BEST VAL Loss: 71.4336\n",
            "\n",
            "Epoch 1288: Validation loss did not decrease\n",
            "\t Train_Loss: 13.5036 Val_Loss: 73.3384  BEST VAL Loss: 71.4336\n",
            "\n",
            "Epoch 1289: Validation loss did not decrease\n",
            "\t Train_Loss: 12.4630 Val_Loss: 71.4371  BEST VAL Loss: 71.4336\n",
            "\n",
            "Epoch 1290: Validation loss did not decrease\n",
            "\t Train_Loss: 13.6323 Val_Loss: 77.0635  BEST VAL Loss: 71.4336\n",
            "\n",
            "Epoch 1291: Validation loss did not decrease\n",
            "\t Train_Loss: 13.5463 Val_Loss: 72.3495  BEST VAL Loss: 71.4336\n",
            "\n",
            "Epoch 1292: Validation loss decreased (71.433571 --> 70.830559).\n",
            "\t Train_Loss: 12.4410 Val_Loss: 70.8306  BEST VAL Loss: 70.8306\n",
            "\n",
            "Epoch 1293: Validation loss did not decrease\n",
            "\t Train_Loss: 13.1849 Val_Loss: 73.5118  BEST VAL Loss: 70.8306\n",
            "\n",
            "Epoch 1294: Validation loss did not decrease\n",
            "\t Train_Loss: 12.9099 Val_Loss: 73.1527  BEST VAL Loss: 70.8306\n",
            "\n",
            "Epoch 1295: Validation loss did not decrease\n",
            "\t Train_Loss: 12.5594 Val_Loss: 70.9847  BEST VAL Loss: 70.8306\n",
            "\n",
            "Epoch 1296: Validation loss did not decrease\n",
            "\t Train_Loss: 13.0992 Val_Loss: 73.7070  BEST VAL Loss: 70.8306\n",
            "\n",
            "Epoch 1297: Validation loss did not decrease\n",
            "\t Train_Loss: 12.4403 Val_Loss: 74.3531  BEST VAL Loss: 70.8306\n",
            "\n",
            "Epoch 1298: Validation loss did not decrease\n",
            "\t Train_Loss: 12.6350 Val_Loss: 70.8827  BEST VAL Loss: 70.8306\n",
            "\n",
            "Epoch 1299: Validation loss decreased (70.830559 --> 70.679726).\n",
            "\t Train_Loss: 12.4351 Val_Loss: 70.6797  BEST VAL Loss: 70.6797\n",
            "\n",
            "Epoch 1300: Validation loss did not decrease\n",
            "\t Train_Loss: 12.3406 Val_Loss: 72.1584  BEST VAL Loss: 70.6797\n",
            "\n",
            "Epoch 1301: Validation loss did not decrease\n",
            "\t Train_Loss: 12.5000 Val_Loss: 71.2571  BEST VAL Loss: 70.6797\n",
            "\n",
            "Epoch 1302: Validation loss decreased (70.679726 --> 70.232185).\n",
            "\t Train_Loss: 12.1501 Val_Loss: 70.2322  BEST VAL Loss: 70.2322\n",
            "\n",
            "Epoch 1303: Validation loss did not decrease\n",
            "\t Train_Loss: 12.4261 Val_Loss: 72.2750  BEST VAL Loss: 70.2322\n",
            "\n",
            "Epoch 1304: Validation loss did not decrease\n",
            "\t Train_Loss: 12.1750 Val_Loss: 72.2451  BEST VAL Loss: 70.2322\n",
            "\n",
            "Epoch 1305: Validation loss decreased (70.232185 --> 69.994255).\n",
            "\t Train_Loss: 12.1991 Val_Loss: 69.9943  BEST VAL Loss: 69.9943\n",
            "\n",
            "Epoch 1306: Validation loss did not decrease\n",
            "\t Train_Loss: 12.1977 Val_Loss: 70.2239  BEST VAL Loss: 69.9943\n",
            "\n",
            "Epoch 1307: Validation loss did not decrease\n",
            "\t Train_Loss: 11.9986 Val_Loss: 71.4637  BEST VAL Loss: 69.9943\n",
            "\n",
            "Epoch 1308: Validation loss decreased (69.994255 --> 69.958244).\n",
            "\t Train_Loss: 12.1442 Val_Loss: 69.9582  BEST VAL Loss: 69.9582\n",
            "\n",
            "Epoch 1309: Validation loss decreased (69.958244 --> 69.802261).\n",
            "\t Train_Loss: 11.9820 Val_Loss: 69.8023  BEST VAL Loss: 69.8023\n",
            "\n",
            "Epoch 1310: Validation loss did not decrease\n",
            "\t Train_Loss: 11.9686 Val_Loss: 71.3376  BEST VAL Loss: 69.8023\n",
            "\n",
            "Epoch 1311: Validation loss decreased (69.802261 --> 69.651169).\n",
            "\t Train_Loss: 12.0208 Val_Loss: 69.6512  BEST VAL Loss: 69.6512\n",
            "\n",
            "Epoch 1312: Validation loss decreased (69.651169 --> 69.353432).\n",
            "\t Train_Loss: 11.8650 Val_Loss: 69.3534  BEST VAL Loss: 69.3534\n",
            "\n",
            "Epoch 1313: Validation loss did not decrease\n",
            "\t Train_Loss: 11.9000 Val_Loss: 70.4887  BEST VAL Loss: 69.3534\n",
            "\n",
            "Epoch 1314: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8697 Val_Loss: 70.0313  BEST VAL Loss: 69.3534\n",
            "\n",
            "Epoch 1315: Validation loss decreased (69.353432 --> 69.193779).\n",
            "\t Train_Loss: 11.7651 Val_Loss: 69.1938  BEST VAL Loss: 69.1938\n",
            "\n",
            "Epoch 1316: Validation loss did not decrease\n",
            "\t Train_Loss: 11.8395 Val_Loss: 69.9206  BEST VAL Loss: 69.1938\n",
            "\n",
            "Epoch 1317: Validation loss did not decrease\n",
            "\t Train_Loss: 11.7392 Val_Loss: 69.6452  BEST VAL Loss: 69.1938\n",
            "\n",
            "Epoch 1318: Validation loss decreased (69.193779 --> 68.606010).\n",
            "\t Train_Loss: 11.7113 Val_Loss: 68.6060  BEST VAL Loss: 68.6060\n",
            "\n",
            "Epoch 1319: Validation loss did not decrease\n",
            "\t Train_Loss: 11.7286 Val_Loss: 69.0251  BEST VAL Loss: 68.6060\n",
            "\n",
            "Epoch 1320: Validation loss did not decrease\n",
            "\t Train_Loss: 11.6331 Val_Loss: 69.4784  BEST VAL Loss: 68.6060\n",
            "\n",
            "Epoch 1321: Validation loss decreased (68.606010 --> 68.465057).\n",
            "\t Train_Loss: 11.6533 Val_Loss: 68.4651  BEST VAL Loss: 68.4651\n",
            "\n",
            "Epoch 1322: Validation loss did not decrease\n",
            "\t Train_Loss: 11.6206 Val_Loss: 68.5917  BEST VAL Loss: 68.4651\n",
            "\n",
            "Epoch 1323: Validation loss did not decrease\n",
            "\t Train_Loss: 11.5605 Val_Loss: 68.9575  BEST VAL Loss: 68.4651\n",
            "\n",
            "Epoch 1324: Validation loss decreased (68.465057 --> 68.156975).\n",
            "\t Train_Loss: 11.5702 Val_Loss: 68.1570  BEST VAL Loss: 68.1570\n",
            "\n",
            "Epoch 1325: Validation loss decreased (68.156975 --> 68.014984).\n",
            "\t Train_Loss: 11.5138 Val_Loss: 68.0150  BEST VAL Loss: 68.0150\n",
            "\n",
            "Epoch 1326: Validation loss did not decrease\n",
            "\t Train_Loss: 11.4986 Val_Loss: 68.6129  BEST VAL Loss: 68.0150\n",
            "\n",
            "Epoch 1327: Validation loss did not decrease\n",
            "\t Train_Loss: 11.4853 Val_Loss: 68.1397  BEST VAL Loss: 68.0150\n",
            "\n",
            "Epoch 1328: Validation loss decreased (68.014984 --> 67.675392).\n",
            "\t Train_Loss: 11.4314 Val_Loss: 67.6754  BEST VAL Loss: 67.6754\n",
            "\n",
            "Epoch 1329: Validation loss did not decrease\n",
            "\t Train_Loss: 11.4299 Val_Loss: 68.0765  BEST VAL Loss: 67.6754\n",
            "\n",
            "Epoch 1330: Validation loss decreased (67.675392 --> 67.665871).\n",
            "\t Train_Loss: 11.3976 Val_Loss: 67.6659  BEST VAL Loss: 67.6659\n",
            "\n",
            "Epoch 1331: Validation loss decreased (67.665871 --> 67.155571).\n",
            "\t Train_Loss: 11.3617 Val_Loss: 67.1556  BEST VAL Loss: 67.1556\n",
            "\n",
            "Epoch 1332: Validation loss did not decrease\n",
            "\t Train_Loss: 11.3576 Val_Loss: 67.6116  BEST VAL Loss: 67.1556\n",
            "\n",
            "Epoch 1333: Validation loss did not decrease\n",
            "\t Train_Loss: 11.3169 Val_Loss: 67.5869  BEST VAL Loss: 67.1556\n",
            "\n",
            "Epoch 1334: Validation loss decreased (67.155571 --> 67.129608).\n",
            "\t Train_Loss: 11.2907 Val_Loss: 67.1296  BEST VAL Loss: 67.1296\n",
            "\n",
            "Epoch 1335: Validation loss did not decrease\n",
            "\t Train_Loss: 11.2814 Val_Loss: 67.4183  BEST VAL Loss: 67.1296\n",
            "\n",
            "Epoch 1336: Validation loss did not decrease\n",
            "\t Train_Loss: 11.2433 Val_Loss: 67.3029  BEST VAL Loss: 67.1296\n",
            "\n",
            "Epoch 1337: Validation loss decreased (67.129608 --> 66.767853).\n",
            "\t Train_Loss: 11.2220 Val_Loss: 66.7679  BEST VAL Loss: 66.7679\n",
            "\n",
            "Epoch 1338: Validation loss did not decrease\n",
            "\t Train_Loss: 11.2047 Val_Loss: 66.8948  BEST VAL Loss: 66.7679\n",
            "\n",
            "Epoch 1339: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1714 Val_Loss: 66.9580  BEST VAL Loss: 66.7679\n",
            "\n",
            "Epoch 1340: Validation loss decreased (66.767853 --> 66.520264).\n",
            "\t Train_Loss: 11.1543 Val_Loss: 66.5203  BEST VAL Loss: 66.5203\n",
            "\n",
            "Epoch 1341: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1309 Val_Loss: 66.5442  BEST VAL Loss: 66.5203\n",
            "\n",
            "Epoch 1342: Validation loss did not decrease\n",
            "\t Train_Loss: 11.1020 Val_Loss: 66.5960  BEST VAL Loss: 66.5203\n",
            "\n",
            "Epoch 1343: Validation loss decreased (66.520264 --> 66.148613).\n",
            "\t Train_Loss: 11.0857 Val_Loss: 66.1486  BEST VAL Loss: 66.1486\n",
            "\n",
            "Epoch 1344: Validation loss decreased (66.148613 --> 66.108231).\n",
            "\t Train_Loss: 11.0610 Val_Loss: 66.1082  BEST VAL Loss: 66.1082\n",
            "\n",
            "Epoch 1345: Validation loss did not decrease\n",
            "\t Train_Loss: 11.0353 Val_Loss: 66.2341  BEST VAL Loss: 66.1082\n",
            "\n",
            "Epoch 1346: Validation loss decreased (66.108231 --> 65.911461).\n",
            "\t Train_Loss: 11.0169 Val_Loss: 65.9115  BEST VAL Loss: 65.9115\n",
            "\n",
            "Epoch 1347: Validation loss decreased (65.911461 --> 65.833954).\n",
            "\t Train_Loss: 10.9919 Val_Loss: 65.8340  BEST VAL Loss: 65.8340\n",
            "\n",
            "Epoch 1348: Validation loss did not decrease\n",
            "\t Train_Loss: 10.9692 Val_Loss: 65.9248  BEST VAL Loss: 65.8340\n",
            "\n",
            "Epoch 1349: Validation loss decreased (65.833954 --> 65.620827).\n",
            "\t Train_Loss: 10.9497 Val_Loss: 65.6208  BEST VAL Loss: 65.6208\n",
            "\n",
            "Epoch 1350: Validation loss decreased (65.620827 --> 65.505547).\n",
            "\t Train_Loss: 10.9249 Val_Loss: 65.5055  BEST VAL Loss: 65.5055\n",
            "\n",
            "Epoch 1351: Validation loss did not decrease\n",
            "\t Train_Loss: 10.9035 Val_Loss: 65.6092  BEST VAL Loss: 65.5055\n",
            "\n",
            "Epoch 1352: Validation loss decreased (65.505547 --> 65.351738).\n",
            "\t Train_Loss: 10.8833 Val_Loss: 65.3517  BEST VAL Loss: 65.3517\n",
            "\n",
            "Epoch 1353: Validation loss decreased (65.351738 --> 65.189041).\n",
            "\t Train_Loss: 10.8595 Val_Loss: 65.1890  BEST VAL Loss: 65.1890\n",
            "\n",
            "Epoch 1354: Validation loss did not decrease\n",
            "\t Train_Loss: 10.8387 Val_Loss: 65.2267  BEST VAL Loss: 65.1890\n",
            "\n",
            "Epoch 1355: Validation loss decreased (65.189041 --> 64.986710).\n",
            "\t Train_Loss: 10.8180 Val_Loss: 64.9867  BEST VAL Loss: 64.9867\n",
            "\n",
            "Epoch 1356: Validation loss decreased (64.986710 --> 64.845451).\n",
            "\t Train_Loss: 10.7951 Val_Loss: 64.8455  BEST VAL Loss: 64.8455\n",
            "\n",
            "Epoch 1357: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7747 Val_Loss: 64.9156  BEST VAL Loss: 64.8455\n",
            "\n",
            "Epoch 1358: Validation loss decreased (64.845451 --> 64.732681).\n",
            "\t Train_Loss: 10.7537 Val_Loss: 64.7327  BEST VAL Loss: 64.7327\n",
            "\n",
            "Epoch 1359: Validation loss decreased (64.732681 --> 64.568230).\n",
            "\t Train_Loss: 10.7315 Val_Loss: 64.5682  BEST VAL Loss: 64.5682\n",
            "\n",
            "Epoch 1360: Validation loss did not decrease\n",
            "\t Train_Loss: 10.7113 Val_Loss: 64.5728  BEST VAL Loss: 64.5682\n",
            "\n",
            "Epoch 1361: Validation loss decreased (64.568230 --> 64.379715).\n",
            "\t Train_Loss: 10.6905 Val_Loss: 64.3797  BEST VAL Loss: 64.3797\n",
            "\n",
            "Epoch 1362: Validation loss decreased (64.379715 --> 64.222054).\n",
            "\t Train_Loss: 10.6688 Val_Loss: 64.2221  BEST VAL Loss: 64.2221\n",
            "\n",
            "Epoch 1363: Validation loss did not decrease\n",
            "\t Train_Loss: 10.6486 Val_Loss: 64.2446  BEST VAL Loss: 64.2221\n",
            "\n",
            "Epoch 1364: Validation loss decreased (64.222054 --> 64.087982).\n",
            "\t Train_Loss: 10.6280 Val_Loss: 64.0880  BEST VAL Loss: 64.0880\n",
            "\n",
            "Epoch 1365: Validation loss decreased (64.087982 --> 63.928822).\n",
            "\t Train_Loss: 10.6069 Val_Loss: 63.9288  BEST VAL Loss: 63.9288\n",
            "\n",
            "Epoch 1366: Validation loss decreased (63.928822 --> 63.912830).\n",
            "\t Train_Loss: 10.5868 Val_Loss: 63.9128  BEST VAL Loss: 63.9128\n",
            "\n",
            "Epoch 1367: Validation loss decreased (63.912830 --> 63.755604).\n",
            "\t Train_Loss: 10.5663 Val_Loss: 63.7556  BEST VAL Loss: 63.7556\n",
            "\n",
            "Epoch 1368: Validation loss decreased (63.755604 --> 63.620392).\n",
            "\t Train_Loss: 10.5456 Val_Loss: 63.6204  BEST VAL Loss: 63.6204\n",
            "\n",
            "Epoch 1369: Validation loss did not decrease\n",
            "\t Train_Loss: 10.5257 Val_Loss: 63.6224  BEST VAL Loss: 63.6204\n",
            "\n",
            "Epoch 1370: Validation loss decreased (63.620392 --> 63.491787).\n",
            "\t Train_Loss: 10.5054 Val_Loss: 63.4918  BEST VAL Loss: 63.4918\n",
            "\n",
            "Epoch 1371: Validation loss decreased (63.491787 --> 63.348888).\n",
            "\t Train_Loss: 10.4849 Val_Loss: 63.3489  BEST VAL Loss: 63.3489\n",
            "\n",
            "Epoch 1372: Validation loss decreased (63.348888 --> 63.310955).\n",
            "\t Train_Loss: 10.4651 Val_Loss: 63.3110  BEST VAL Loss: 63.3110\n",
            "\n",
            "Epoch 1373: Validation loss decreased (63.310955 --> 63.166027).\n",
            "\t Train_Loss: 10.4451 Val_Loss: 63.1660  BEST VAL Loss: 63.1660\n",
            "\n",
            "Epoch 1374: Validation loss decreased (63.166027 --> 63.035389).\n",
            "\t Train_Loss: 10.4249 Val_Loss: 63.0354  BEST VAL Loss: 63.0354\n",
            "\n",
            "Epoch 1375: Validation loss decreased (63.035389 --> 63.010742).\n",
            "\t Train_Loss: 10.4052 Val_Loss: 63.0107  BEST VAL Loss: 63.0107\n",
            "\n",
            "Epoch 1376: Validation loss decreased (63.010742 --> 62.884583).\n",
            "\t Train_Loss: 10.3854 Val_Loss: 62.8846  BEST VAL Loss: 62.8846\n",
            "\n",
            "Epoch 1377: Validation loss decreased (62.884583 --> 62.758057).\n",
            "\t Train_Loss: 10.3655 Val_Loss: 62.7581  BEST VAL Loss: 62.7581\n",
            "\n",
            "Epoch 1378: Validation loss decreased (62.758057 --> 62.708508).\n",
            "\t Train_Loss: 10.3459 Val_Loss: 62.7085  BEST VAL Loss: 62.7085\n",
            "\n",
            "Epoch 1379: Validation loss decreased (62.708508 --> 62.571838).\n",
            "\t Train_Loss: 10.3262 Val_Loss: 62.5718  BEST VAL Loss: 62.5718\n",
            "\n",
            "Epoch 1380: Validation loss decreased (62.571838 --> 62.456959).\n",
            "\t Train_Loss: 10.3065 Val_Loss: 62.4570  BEST VAL Loss: 62.4570\n",
            "\n",
            "Epoch 1381: Validation loss decreased (62.456959 --> 62.412457).\n",
            "\t Train_Loss: 10.2870 Val_Loss: 62.4125  BEST VAL Loss: 62.4125\n",
            "\n",
            "Epoch 1382: Validation loss decreased (62.412457 --> 62.285854).\n",
            "\t Train_Loss: 10.2676 Val_Loss: 62.2859  BEST VAL Loss: 62.2859\n",
            "\n",
            "Epoch 1383: Validation loss decreased (62.285854 --> 62.175472).\n",
            "\t Train_Loss: 10.2480 Val_Loss: 62.1755  BEST VAL Loss: 62.1755\n",
            "\n",
            "Epoch 1384: Validation loss decreased (62.175472 --> 62.115150).\n",
            "\t Train_Loss: 10.2286 Val_Loss: 62.1152  BEST VAL Loss: 62.1152\n",
            "\n",
            "Epoch 1385: Validation loss decreased (62.115150 --> 61.984802).\n",
            "\t Train_Loss: 10.2093 Val_Loss: 61.9848  BEST VAL Loss: 61.9848\n",
            "\n",
            "Epoch 1386: Validation loss decreased (61.984802 --> 61.886379).\n",
            "\t Train_Loss: 10.1899 Val_Loss: 61.8864  BEST VAL Loss: 61.8864\n",
            "\n",
            "Epoch 1387: Validation loss decreased (61.886379 --> 61.825806).\n",
            "\t Train_Loss: 10.1706 Val_Loss: 61.8258  BEST VAL Loss: 61.8258\n",
            "\n",
            "Epoch 1388: Validation loss decreased (61.825806 --> 61.697662).\n",
            "\t Train_Loss: 10.1514 Val_Loss: 61.6977  BEST VAL Loss: 61.6977\n",
            "\n",
            "Epoch 1389: Validation loss decreased (61.697662 --> 61.602428).\n",
            "\t Train_Loss: 10.1321 Val_Loss: 61.6024  BEST VAL Loss: 61.6024\n",
            "\n",
            "Epoch 1390: Validation loss decreased (61.602428 --> 61.529449).\n",
            "\t Train_Loss: 10.1129 Val_Loss: 61.5294  BEST VAL Loss: 61.5294\n",
            "\n",
            "Epoch 1391: Validation loss decreased (61.529449 --> 61.405361).\n",
            "\t Train_Loss: 10.0938 Val_Loss: 61.4054  BEST VAL Loss: 61.4054\n",
            "\n",
            "Epoch 1392: Validation loss decreased (61.405361 --> 61.324806).\n",
            "\t Train_Loss: 10.0746 Val_Loss: 61.3248  BEST VAL Loss: 61.3248\n",
            "\n",
            "Epoch 1393: Validation loss decreased (61.324806 --> 61.248562).\n",
            "\t Train_Loss: 10.0554 Val_Loss: 61.2486  BEST VAL Loss: 61.2486\n",
            "\n",
            "Epoch 1394: Validation loss decreased (61.248562 --> 61.124645).\n",
            "\t Train_Loss: 10.0364 Val_Loss: 61.1246  BEST VAL Loss: 61.1246\n",
            "\n",
            "Epoch 1395: Validation loss decreased (61.124645 --> 61.043373).\n",
            "\t Train_Loss: 10.0173 Val_Loss: 61.0434  BEST VAL Loss: 61.0434\n",
            "\n",
            "Epoch 1396: Validation loss decreased (61.043373 --> 60.953468).\n",
            "\t Train_Loss: 9.9981 Val_Loss: 60.9535  BEST VAL Loss: 60.9535\n",
            "\n",
            "Epoch 1397: Validation loss decreased (60.953468 --> 60.840107).\n",
            "\t Train_Loss: 9.9791 Val_Loss: 60.8401  BEST VAL Loss: 60.8401\n",
            "\n",
            "Epoch 1398: Validation loss decreased (60.840107 --> 60.773327).\n",
            "\t Train_Loss: 9.9600 Val_Loss: 60.7733  BEST VAL Loss: 60.7733\n",
            "\n",
            "Epoch 1399: Validation loss decreased (60.773327 --> 60.679737).\n",
            "\t Train_Loss: 9.9409 Val_Loss: 60.6797  BEST VAL Loss: 60.6797\n",
            "\n",
            "Epoch 1400: Validation loss decreased (60.679737 --> 60.574738).\n",
            "\t Train_Loss: 9.9218 Val_Loss: 60.5747  BEST VAL Loss: 60.5747\n",
            "\n",
            "Epoch 1401: Validation loss decreased (60.574738 --> 60.502716).\n",
            "\t Train_Loss: 9.9027 Val_Loss: 60.5027  BEST VAL Loss: 60.5027\n",
            "\n",
            "Epoch 1402: Validation loss decreased (60.502716 --> 60.396389).\n",
            "\t Train_Loss: 9.8836 Val_Loss: 60.3964  BEST VAL Loss: 60.3964\n",
            "\n",
            "Epoch 1403: Validation loss decreased (60.396389 --> 60.306000).\n",
            "\t Train_Loss: 9.8645 Val_Loss: 60.3060  BEST VAL Loss: 60.3060\n",
            "\n",
            "Epoch 1404: Validation loss decreased (60.306000 --> 60.231262).\n",
            "\t Train_Loss: 9.8453 Val_Loss: 60.2313  BEST VAL Loss: 60.2313\n",
            "\n",
            "Epoch 1405: Validation loss decreased (60.231262 --> 60.122162).\n",
            "\t Train_Loss: 9.8261 Val_Loss: 60.1222  BEST VAL Loss: 60.1222\n",
            "\n",
            "Epoch 1406: Validation loss decreased (60.122162 --> 60.042450).\n",
            "\t Train_Loss: 9.8069 Val_Loss: 60.0424  BEST VAL Loss: 60.0424\n",
            "\n",
            "Epoch 1407: Validation loss decreased (60.042450 --> 59.951550).\n",
            "\t Train_Loss: 9.7877 Val_Loss: 59.9515  BEST VAL Loss: 59.9515\n",
            "\n",
            "Epoch 1408: Validation loss decreased (59.951550 --> 59.851746).\n",
            "\t Train_Loss: 9.7684 Val_Loss: 59.8517  BEST VAL Loss: 59.8517\n",
            "\n",
            "Epoch 1409: Validation loss decreased (59.851746 --> 59.781384).\n",
            "\t Train_Loss: 9.7490 Val_Loss: 59.7814  BEST VAL Loss: 59.7814\n",
            "\n",
            "Epoch 1410: Validation loss decreased (59.781384 --> 59.678993).\n",
            "\t Train_Loss: 9.7296 Val_Loss: 59.6790  BEST VAL Loss: 59.6790\n",
            "\n",
            "Epoch 1411: Validation loss decreased (59.678993 --> 59.596249).\n",
            "\t Train_Loss: 9.7102 Val_Loss: 59.5962  BEST VAL Loss: 59.5962\n",
            "\n",
            "Epoch 1412: Validation loss decreased (59.596249 --> 59.507168).\n",
            "\t Train_Loss: 9.6907 Val_Loss: 59.5072  BEST VAL Loss: 59.5072\n",
            "\n",
            "Epoch 1413: Validation loss decreased (59.507168 --> 59.407360).\n",
            "\t Train_Loss: 9.6711 Val_Loss: 59.4074  BEST VAL Loss: 59.4074\n",
            "\n",
            "Epoch 1414: Validation loss decreased (59.407360 --> 59.334438).\n",
            "\t Train_Loss: 9.6515 Val_Loss: 59.3344  BEST VAL Loss: 59.3344\n",
            "\n",
            "Epoch 1415: Validation loss decreased (59.334438 --> 59.229919).\n",
            "\t Train_Loss: 9.6317 Val_Loss: 59.2299  BEST VAL Loss: 59.2299\n",
            "\n",
            "Epoch 1416: Validation loss decreased (59.229919 --> 59.158443).\n",
            "\t Train_Loss: 9.6119 Val_Loss: 59.1584  BEST VAL Loss: 59.1584\n",
            "\n",
            "Epoch 1417: Validation loss decreased (59.158443 --> 59.053139).\n",
            "\t Train_Loss: 9.5920 Val_Loss: 59.0531  BEST VAL Loss: 59.0531\n",
            "\n",
            "Epoch 1418: Validation loss decreased (59.053139 --> 58.978577).\n",
            "\t Train_Loss: 9.5719 Val_Loss: 58.9786  BEST VAL Loss: 58.9786\n",
            "\n",
            "Epoch 1419: Validation loss decreased (58.978577 --> 58.875832).\n",
            "\t Train_Loss: 9.5517 Val_Loss: 58.8758  BEST VAL Loss: 58.8758\n",
            "\n",
            "Epoch 1420: Validation loss decreased (58.875832 --> 58.804913).\n",
            "\t Train_Loss: 9.5312 Val_Loss: 58.8049  BEST VAL Loss: 58.8049\n",
            "\n",
            "Epoch 1421: Validation loss decreased (58.804913 --> 58.695057).\n",
            "\t Train_Loss: 9.5105 Val_Loss: 58.6951  BEST VAL Loss: 58.6951\n",
            "\n",
            "Epoch 1422: Validation loss decreased (58.695057 --> 58.638592).\n",
            "\t Train_Loss: 9.4893 Val_Loss: 58.6386  BEST VAL Loss: 58.6386\n",
            "\n",
            "Epoch 1423: Validation loss decreased (58.638592 --> 58.496655).\n",
            "\t Train_Loss: 9.4676 Val_Loss: 58.4967  BEST VAL Loss: 58.4967\n",
            "\n",
            "Epoch 1424: Validation loss did not decrease\n",
            "\t Train_Loss: 9.4454 Val_Loss: 58.5048  BEST VAL Loss: 58.4967\n",
            "\n",
            "Epoch 1425: Validation loss decreased (58.496655 --> 58.217251).\n",
            "\t Train_Loss: 9.4228 Val_Loss: 58.2173  BEST VAL Loss: 58.2173\n",
            "\n",
            "Epoch 1426: Validation loss did not decrease\n",
            "\t Train_Loss: 9.4014 Val_Loss: 58.4959  BEST VAL Loss: 58.2173\n",
            "\n",
            "Epoch 1427: Validation loss decreased (58.217251 --> 57.537624).\n",
            "\t Train_Loss: 9.3877 Val_Loss: 57.5376  BEST VAL Loss: 57.5376\n",
            "\n",
            "Epoch 1428: Validation loss did not decrease\n",
            "\t Train_Loss: 9.4172 Val_Loss: 59.4132  BEST VAL Loss: 57.5376\n",
            "\n",
            "Epoch 1429: Validation loss decreased (57.537624 --> 56.001740).\n",
            "\t Train_Loss: 9.5634 Val_Loss: 56.0017  BEST VAL Loss: 56.0017\n",
            "\n",
            "Epoch 1430: Validation loss did not decrease\n",
            "\t Train_Loss: 10.2805 Val_Loss: 60.1523  BEST VAL Loss: 56.0017\n",
            "\n",
            "Epoch 1431: Validation loss did not decrease\n",
            "\t Train_Loss: 10.2831 Val_Loss: 56.6919  BEST VAL Loss: 56.0017\n",
            "\n",
            "Epoch 1432: Validation loss did not decrease\n",
            "\t Train_Loss: 9.5449 Val_Loss: 56.4301  BEST VAL Loss: 56.0017\n",
            "\n",
            "Epoch 1433: Validation loss did not decrease\n",
            "\t Train_Loss: 9.9063 Val_Loss: 61.9239  BEST VAL Loss: 56.0017\n",
            "\n",
            "Epoch 1434: Validation loss did not decrease\n",
            "\t Train_Loss: 10.4619 Val_Loss: 56.4757  BEST VAL Loss: 56.0017\n",
            "\n",
            "Epoch 1435: Validation loss decreased (56.001740 --> 55.231739).\n",
            "\t Train_Loss: 9.4925 Val_Loss: 55.2317  BEST VAL Loss: 55.2317\n",
            "\n",
            "Epoch 1436: Validation loss did not decrease\n",
            "\t Train_Loss: 10.2113 Val_Loss: 59.3091  BEST VAL Loss: 55.2317\n",
            "\n",
            "Epoch 1437: Validation loss did not decrease\n",
            "\t Train_Loss: 10.3825 Val_Loss: 57.4716  BEST VAL Loss: 55.2317\n",
            "\n",
            "Epoch 1438: Validation loss did not decrease\n",
            "\t Train_Loss: 9.3716 Val_Loss: 56.2394  BEST VAL Loss: 55.2317\n",
            "\n",
            "Epoch 1439: Validation loss did not decrease\n",
            "\t Train_Loss: 10.2611 Val_Loss: 60.3336  BEST VAL Loss: 55.2317\n",
            "\n",
            "Epoch 1440: Validation loss did not decrease\n",
            "\t Train_Loss: 10.0632 Val_Loss: 57.7824  BEST VAL Loss: 55.2317\n",
            "\n",
            "Epoch 1441: Validation loss decreased (55.231739 --> 54.932522).\n",
            "\t Train_Loss: 9.6888 Val_Loss: 54.9325  BEST VAL Loss: 54.9325\n",
            "\n",
            "Epoch 1442: Validation loss did not decrease\n",
            "\t Train_Loss: 9.9496 Val_Loss: 56.8910  BEST VAL Loss: 54.9325\n",
            "\n",
            "Epoch 1443: Validation loss did not decrease\n",
            "\t Train_Loss: 9.3413 Val_Loss: 59.0894  BEST VAL Loss: 54.9325\n",
            "\n",
            "Epoch 1444: Validation loss did not decrease\n",
            "\t Train_Loss: 9.8603 Val_Loss: 55.2992  BEST VAL Loss: 54.9325\n",
            "\n",
            "Epoch 1445: Validation loss did not decrease\n",
            "\t Train_Loss: 10.0219 Val_Loss: 56.5622  BEST VAL Loss: 54.9325\n",
            "\n",
            "Epoch 1446: Validation loss did not decrease\n",
            "\t Train_Loss: 9.1730 Val_Loss: 58.5076  BEST VAL Loss: 54.9325\n",
            "\n",
            "Epoch 1447: Validation loss did not decrease\n",
            "\t Train_Loss: 9.6799 Val_Loss: 55.3448  BEST VAL Loss: 54.9325\n",
            "\n",
            "Epoch 1448: Validation loss did not decrease\n",
            "\t Train_Loss: 9.4523 Val_Loss: 55.2332  BEST VAL Loss: 54.9325\n",
            "\n",
            "Epoch 1449: Validation loss did not decrease\n",
            "\t Train_Loss: 9.4742 Val_Loss: 58.5985  BEST VAL Loss: 54.9325\n",
            "\n",
            "Epoch 1450: Validation loss did not decrease\n",
            "\t Train_Loss: 9.4217 Val_Loss: 56.4393  BEST VAL Loss: 54.9325\n",
            "\n",
            "Epoch 1451: Validation loss decreased (54.932522 --> 54.805500).\n",
            "\t Train_Loss: 9.1012 Val_Loss: 54.8055  BEST VAL Loss: 54.8055\n",
            "\n",
            "Epoch 1452: Validation loss did not decrease\n",
            "\t Train_Loss: 9.2749 Val_Loss: 55.5239  BEST VAL Loss: 54.8055\n",
            "\n",
            "Epoch 1453: Validation loss did not decrease\n",
            "\t Train_Loss: 9.1386 Val_Loss: 56.4670  BEST VAL Loss: 54.8055\n",
            "\n",
            "Epoch 1454: Validation loss did not decrease\n",
            "\t Train_Loss: 9.1170 Val_Loss: 55.8261  BEST VAL Loss: 54.8055\n",
            "\n",
            "Epoch 1455: Validation loss did not decrease\n",
            "\t Train_Loss: 9.1395 Val_Loss: 56.5038  BEST VAL Loss: 54.8055\n",
            "\n",
            "Epoch 1456: Validation loss did not decrease\n",
            "\t Train_Loss: 8.9720 Val_Loss: 56.4829  BEST VAL Loss: 54.8055\n",
            "\n",
            "Epoch 1457: Validation loss did not decrease\n",
            "\t Train_Loss: 9.0339 Val_Loss: 55.8397  BEST VAL Loss: 54.8055\n",
            "\n",
            "Epoch 1458: Validation loss did not decrease\n",
            "\t Train_Loss: 9.0336 Val_Loss: 56.1141  BEST VAL Loss: 54.8055\n",
            "\n",
            "Epoch 1459: Validation loss did not decrease\n",
            "\t Train_Loss: 8.9489 Val_Loss: 55.8905  BEST VAL Loss: 54.8055\n",
            "\n",
            "Epoch 1460: Validation loss did not decrease\n",
            "\t Train_Loss: 8.9352 Val_Loss: 55.1922  BEST VAL Loss: 54.8055\n",
            "\n",
            "Epoch 1461: Validation loss did not decrease\n",
            "\t Train_Loss: 8.9280 Val_Loss: 55.2105  BEST VAL Loss: 54.8055\n",
            "\n",
            "Epoch 1462: Validation loss did not decrease\n",
            "\t Train_Loss: 8.8736 Val_Loss: 55.4759  BEST VAL Loss: 54.8055\n",
            "\n",
            "Epoch 1463: Validation loss did not decrease\n",
            "\t Train_Loss: 8.8569 Val_Loss: 55.4577  BEST VAL Loss: 54.8055\n",
            "\n",
            "Epoch 1464: Validation loss did not decrease\n",
            "\t Train_Loss: 8.8461 Val_Loss: 55.7131  BEST VAL Loss: 54.8055\n",
            "\n",
            "Epoch 1465: Validation loss did not decrease\n",
            "\t Train_Loss: 8.8122 Val_Loss: 55.4923  BEST VAL Loss: 54.8055\n",
            "\n",
            "Epoch 1466: Validation loss did not decrease\n",
            "\t Train_Loss: 8.7894 Val_Loss: 55.1841  BEST VAL Loss: 54.8055\n",
            "\n",
            "Epoch 1467: Validation loss did not decrease\n",
            "\t Train_Loss: 8.7530 Val_Loss: 55.3028  BEST VAL Loss: 54.8055\n",
            "\n",
            "Epoch 1468: Validation loss did not decrease\n",
            "\t Train_Loss: 8.7206 Val_Loss: 55.1621  BEST VAL Loss: 54.8055\n",
            "\n",
            "Epoch 1469: Validation loss did not decrease\n",
            "\t Train_Loss: 8.7227 Val_Loss: 55.0367  BEST VAL Loss: 54.8055\n",
            "\n",
            "Epoch 1470: Validation loss did not decrease\n",
            "\t Train_Loss: 8.7010 Val_Loss: 55.2602  BEST VAL Loss: 54.8055\n",
            "\n",
            "Epoch 1471: Validation loss did not decrease\n",
            "\t Train_Loss: 8.6682 Val_Loss: 54.8655  BEST VAL Loss: 54.8055\n",
            "\n",
            "Epoch 1472: Validation loss did not decrease\n",
            "\t Train_Loss: 8.6474 Val_Loss: 54.8443  BEST VAL Loss: 54.8055\n",
            "\n",
            "Epoch 1473: Validation loss decreased (54.805500 --> 54.783546).\n",
            "\t Train_Loss: 8.6301 Val_Loss: 54.7835  BEST VAL Loss: 54.7835\n",
            "\n",
            "Epoch 1474: Validation loss did not decrease\n",
            "\t Train_Loss: 8.6147 Val_Loss: 54.8159  BEST VAL Loss: 54.7835\n",
            "\n",
            "Epoch 1475: Validation loss decreased (54.783546 --> 54.632973).\n",
            "\t Train_Loss: 8.5837 Val_Loss: 54.6330  BEST VAL Loss: 54.6330\n",
            "\n",
            "Epoch 1476: Validation loss decreased (54.632973 --> 54.193188).\n",
            "\t Train_Loss: 8.5634 Val_Loss: 54.1932  BEST VAL Loss: 54.1932\n",
            "\n",
            "Epoch 1477: Validation loss decreased (54.193188 --> 53.992054).\n",
            "\t Train_Loss: 8.5460 Val_Loss: 53.9921  BEST VAL Loss: 53.9921\n",
            "\n",
            "Epoch 1478: Validation loss did not decrease\n",
            "\t Train_Loss: 8.5361 Val_Loss: 54.0682  BEST VAL Loss: 53.9921\n",
            "\n",
            "Epoch 1479: Validation loss did not decrease\n",
            "\t Train_Loss: 8.5066 Val_Loss: 54.3358  BEST VAL Loss: 53.9921\n",
            "\n",
            "Epoch 1480: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4893 Val_Loss: 54.3738  BEST VAL Loss: 53.9921\n",
            "\n",
            "Epoch 1481: Validation loss decreased (53.992054 --> 53.952572).\n",
            "\t Train_Loss: 8.4727 Val_Loss: 53.9526  BEST VAL Loss: 53.9526\n",
            "\n",
            "Epoch 1482: Validation loss decreased (53.952572 --> 53.902687).\n",
            "\t Train_Loss: 8.4549 Val_Loss: 53.9027  BEST VAL Loss: 53.9027\n",
            "\n",
            "Epoch 1483: Validation loss decreased (53.902687 --> 53.731281).\n",
            "\t Train_Loss: 8.4375 Val_Loss: 53.7313  BEST VAL Loss: 53.7313\n",
            "\n",
            "Epoch 1484: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4151 Val_Loss: 54.0027  BEST VAL Loss: 53.7313\n",
            "\n",
            "Epoch 1485: Validation loss decreased (53.731281 --> 53.435696).\n",
            "\t Train_Loss: 8.4033 Val_Loss: 53.4357  BEST VAL Loss: 53.4357\n",
            "\n",
            "Epoch 1486: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3910 Val_Loss: 53.8697  BEST VAL Loss: 53.4357\n",
            "\n",
            "Epoch 1487: Validation loss decreased (53.435696 --> 52.814053).\n",
            "\t Train_Loss: 8.4206 Val_Loss: 52.8141  BEST VAL Loss: 52.8141\n",
            "\n",
            "Epoch 1488: Validation loss did not decrease\n",
            "\t Train_Loss: 8.5158 Val_Loss: 55.2224  BEST VAL Loss: 52.8141\n",
            "\n",
            "Epoch 1489: Validation loss decreased (52.814053 --> 52.056934).\n",
            "\t Train_Loss: 8.7321 Val_Loss: 52.0569  BEST VAL Loss: 52.0569\n",
            "\n",
            "Epoch 1490: Validation loss did not decrease\n",
            "\t Train_Loss: 8.5811 Val_Loss: 53.4633  BEST VAL Loss: 52.0569\n",
            "\n",
            "Epoch 1491: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3359 Val_Loss: 54.1008  BEST VAL Loss: 52.0569\n",
            "\n",
            "Epoch 1492: Validation loss decreased (52.056934 --> 51.972157).\n",
            "\t Train_Loss: 8.3872 Val_Loss: 51.9722  BEST VAL Loss: 51.9722\n",
            "\n",
            "Epoch 1493: Validation loss did not decrease\n",
            "\t Train_Loss: 8.5939 Val_Loss: 54.2769  BEST VAL Loss: 51.9722\n",
            "\n",
            "Epoch 1494: Validation loss decreased (51.972157 --> 51.633095).\n",
            "\t Train_Loss: 8.7204 Val_Loss: 51.6331  BEST VAL Loss: 51.6331\n",
            "\n",
            "Epoch 1495: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4190 Val_Loss: 51.9774  BEST VAL Loss: 51.6331\n",
            "\n",
            "Epoch 1496: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3535 Val_Loss: 55.4763  BEST VAL Loss: 51.6331\n",
            "\n",
            "Epoch 1497: Validation loss decreased (51.633095 --> 51.313210).\n",
            "\t Train_Loss: 8.7085 Val_Loss: 51.3132  BEST VAL Loss: 51.3132\n",
            "\n",
            "Epoch 1498: Validation loss did not decrease\n",
            "\t Train_Loss: 8.6858 Val_Loss: 53.0242  BEST VAL Loss: 51.3132\n",
            "\n",
            "Epoch 1499: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4124 Val_Loss: 52.2577  BEST VAL Loss: 51.3132\n",
            "\n",
            "Epoch 1500: Validation loss decreased (51.313210 --> 51.209126).\n",
            "\t Train_Loss: 8.2613 Val_Loss: 51.2091  BEST VAL Loss: 51.2091\n",
            "\n",
            "Epoch 1501: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4710 Val_Loss: 54.4032  BEST VAL Loss: 51.2091\n",
            "\n",
            "Epoch 1502: Validation loss decreased (51.209126 --> 51.102688).\n",
            "\t Train_Loss: 8.5902 Val_Loss: 51.1027  BEST VAL Loss: 51.1027\n",
            "\n",
            "Epoch 1503: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2971 Val_Loss: 51.6251  BEST VAL Loss: 51.1027\n",
            "\n",
            "Epoch 1504: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1483 Val_Loss: 53.1742  BEST VAL Loss: 51.1027\n",
            "\n",
            "Epoch 1505: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2624 Val_Loss: 51.1996  BEST VAL Loss: 51.1027\n",
            "\n",
            "Epoch 1506: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3264 Val_Loss: 53.3553  BEST VAL Loss: 51.1027\n",
            "\n",
            "Epoch 1507: Validation loss decreased (51.102688 --> 50.914749).\n",
            "\t Train_Loss: 8.3386 Val_Loss: 50.9147  BEST VAL Loss: 50.9147\n",
            "\n",
            "Epoch 1508: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1588 Val_Loss: 51.5252  BEST VAL Loss: 50.9147\n",
            "\n",
            "Epoch 1509: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0535 Val_Loss: 52.8838  BEST VAL Loss: 50.9147\n",
            "\n",
            "Epoch 1510: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1358 Val_Loss: 50.9428  BEST VAL Loss: 50.9147\n",
            "\n",
            "Epoch 1511: Validation loss did not decrease\n",
            "\t Train_Loss: 8.2160 Val_Loss: 52.9996  BEST VAL Loss: 50.9147\n",
            "\n",
            "Epoch 1512: Validation loss decreased (50.914749 --> 50.462662).\n",
            "\t Train_Loss: 8.3382 Val_Loss: 50.4627  BEST VAL Loss: 50.4627\n",
            "\n",
            "Epoch 1513: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1263 Val_Loss: 51.1097  BEST VAL Loss: 50.4627\n",
            "\n",
            "Epoch 1514: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0007 Val_Loss: 53.2312  BEST VAL Loss: 50.4627\n",
            "\n",
            "Epoch 1515: Validation loss decreased (50.462662 --> 50.330196).\n",
            "\t Train_Loss: 8.1907 Val_Loss: 50.3302  BEST VAL Loss: 50.3302\n",
            "\n",
            "Epoch 1516: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3491 Val_Loss: 52.7054  BEST VAL Loss: 50.3302\n",
            "\n",
            "Epoch 1517: Validation loss did not decrease\n",
            "\t Train_Loss: 8.3894 Val_Loss: 50.4014  BEST VAL Loss: 50.3302\n",
            "\n",
            "Epoch 1518: Validation loss decreased (50.330196 --> 49.991035).\n",
            "\t Train_Loss: 8.0712 Val_Loss: 49.9910  BEST VAL Loss: 49.9910\n",
            "\n",
            "Epoch 1519: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1994 Val_Loss: 53.7208  BEST VAL Loss: 49.9910\n",
            "\n",
            "Epoch 1520: Validation loss did not decrease\n",
            "\t Train_Loss: 8.4093 Val_Loss: 50.0227  BEST VAL Loss: 49.9910\n",
            "\n",
            "Epoch 1521: Validation loss did not decrease\n",
            "\t Train_Loss: 8.1133 Val_Loss: 50.6908  BEST VAL Loss: 49.9910\n",
            "\n",
            "Epoch 1522: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8912 Val_Loss: 51.4513  BEST VAL Loss: 49.9910\n",
            "\n",
            "Epoch 1523: Validation loss decreased (49.991035 --> 49.980938).\n",
            "\t Train_Loss: 7.9716 Val_Loss: 49.9809  BEST VAL Loss: 49.9809\n",
            "\n",
            "Epoch 1524: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9951 Val_Loss: 51.6013  BEST VAL Loss: 49.9809\n",
            "\n",
            "Epoch 1525: Validation loss did not decrease\n",
            "\t Train_Loss: 7.9265 Val_Loss: 50.2055  BEST VAL Loss: 49.9809\n",
            "\n",
            "Epoch 1526: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8536 Val_Loss: 50.6244  BEST VAL Loss: 49.9809\n",
            "\n",
            "Epoch 1527: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7906 Val_Loss: 50.9288  BEST VAL Loss: 49.9809\n",
            "\n",
            "Epoch 1528: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8006 Val_Loss: 50.2201  BEST VAL Loss: 49.9809\n",
            "\n",
            "Epoch 1529: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7829 Val_Loss: 51.0227  BEST VAL Loss: 49.9809\n",
            "\n",
            "Epoch 1530: Validation loss decreased (49.980938 --> 49.876293).\n",
            "\t Train_Loss: 7.7976 Val_Loss: 49.8763  BEST VAL Loss: 49.8763\n",
            "\n",
            "Epoch 1531: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8096 Val_Loss: 51.6632  BEST VAL Loss: 49.8763\n",
            "\n",
            "Epoch 1532: Validation loss decreased (49.876293 --> 49.623840).\n",
            "\t Train_Loss: 7.8897 Val_Loss: 49.6238  BEST VAL Loss: 49.6238\n",
            "\n",
            "Epoch 1533: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8261 Val_Loss: 50.3945  BEST VAL Loss: 49.6238\n",
            "\n",
            "Epoch 1534: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7006 Val_Loss: 50.4302  BEST VAL Loss: 49.6238\n",
            "\n",
            "Epoch 1535: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6997 Val_Loss: 50.1737  BEST VAL Loss: 49.6238\n",
            "\n",
            "Epoch 1536: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6784 Val_Loss: 51.3347  BEST VAL Loss: 49.6238\n",
            "\n",
            "Epoch 1537: Validation loss decreased (49.623840 --> 49.420174).\n",
            "\t Train_Loss: 7.7549 Val_Loss: 49.4202  BEST VAL Loss: 49.4202\n",
            "\n",
            "Epoch 1538: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7272 Val_Loss: 50.4763  BEST VAL Loss: 49.4202\n",
            "\n",
            "Epoch 1539: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6477 Val_Loss: 50.0061  BEST VAL Loss: 49.4202\n",
            "\n",
            "Epoch 1540: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6264 Val_Loss: 50.8384  BEST VAL Loss: 49.4202\n",
            "\n",
            "Epoch 1541: Validation loss decreased (49.420174 --> 49.360775).\n",
            "\t Train_Loss: 7.6152 Val_Loss: 49.3608  BEST VAL Loss: 49.3608\n",
            "\n",
            "Epoch 1542: Validation loss did not decrease\n",
            "\t Train_Loss: 7.6131 Val_Loss: 50.0284  BEST VAL Loss: 49.3608\n",
            "\n",
            "Epoch 1543: Validation loss decreased (49.360775 --> 49.347935).\n",
            "\t Train_Loss: 7.5962 Val_Loss: 49.3479  BEST VAL Loss: 49.3479\n",
            "\n",
            "Epoch 1544: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5457 Val_Loss: 50.2214  BEST VAL Loss: 49.3479\n",
            "\n",
            "Epoch 1545: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5270 Val_Loss: 49.5455  BEST VAL Loss: 49.3479\n",
            "\n",
            "Epoch 1546: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5126 Val_Loss: 49.8104  BEST VAL Loss: 49.3479\n",
            "\n",
            "Epoch 1547: Validation loss decreased (49.347935 --> 48.830097).\n",
            "\t Train_Loss: 7.5099 Val_Loss: 48.8301  BEST VAL Loss: 48.8301\n",
            "\n",
            "Epoch 1548: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5088 Val_Loss: 50.1355  BEST VAL Loss: 48.8301\n",
            "\n",
            "Epoch 1549: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5010 Val_Loss: 48.9722  BEST VAL Loss: 48.8301\n",
            "\n",
            "Epoch 1550: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5328 Val_Loss: 50.5135  BEST VAL Loss: 48.8301\n",
            "\n",
            "Epoch 1551: Validation loss decreased (48.830097 --> 48.094532).\n",
            "\t Train_Loss: 7.6136 Val_Loss: 48.0945  BEST VAL Loss: 48.0945\n",
            "\n",
            "Epoch 1552: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5518 Val_Loss: 49.3356  BEST VAL Loss: 48.0945\n",
            "\n",
            "Epoch 1553: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4289 Val_Loss: 49.2671  BEST VAL Loss: 48.0945\n",
            "\n",
            "Epoch 1554: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3936 Val_Loss: 48.6507  BEST VAL Loss: 48.0945\n",
            "\n",
            "Epoch 1555: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4255 Val_Loss: 49.9990  BEST VAL Loss: 48.0945\n",
            "\n",
            "Epoch 1556: Validation loss decreased (48.094532 --> 47.569901).\n",
            "\t Train_Loss: 7.5771 Val_Loss: 47.5699  BEST VAL Loss: 47.5699\n",
            "\n",
            "Epoch 1557: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5687 Val_Loss: 49.3647  BEST VAL Loss: 47.5699\n",
            "\n",
            "Epoch 1558: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4042 Val_Loss: 49.0268  BEST VAL Loss: 47.5699\n",
            "\n",
            "Epoch 1559: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3329 Val_Loss: 48.1821  BEST VAL Loss: 47.5699\n",
            "\n",
            "Epoch 1560: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4123 Val_Loss: 50.3178  BEST VAL Loss: 47.5699\n",
            "\n",
            "Epoch 1561: Validation loss decreased (47.569901 --> 47.194710).\n",
            "\t Train_Loss: 7.6874 Val_Loss: 47.1947  BEST VAL Loss: 47.1947\n",
            "\n",
            "Epoch 1562: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5976 Val_Loss: 48.3689  BEST VAL Loss: 47.1947\n",
            "\n",
            "Epoch 1563: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3131 Val_Loss: 49.4003  BEST VAL Loss: 47.1947\n",
            "\n",
            "Epoch 1564: Validation loss decreased (47.194710 --> 46.974186).\n",
            "\t Train_Loss: 7.3951 Val_Loss: 46.9742  BEST VAL Loss: 46.9742\n",
            "\n",
            "Epoch 1565: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8759 Val_Loss: 51.0948  BEST VAL Loss: 46.9742\n",
            "\n",
            "Epoch 1566: Validation loss did not decrease\n",
            "\t Train_Loss: 8.0541 Val_Loss: 47.3816  BEST VAL Loss: 46.9742\n",
            "\n",
            "Epoch 1567: Validation loss decreased (46.974186 --> 46.614288).\n",
            "\t Train_Loss: 7.6252 Val_Loss: 46.6143  BEST VAL Loss: 46.6143\n",
            "\n",
            "Epoch 1568: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8753 Val_Loss: 50.4169  BEST VAL Loss: 46.6143\n",
            "\n",
            "Epoch 1569: Validation loss did not decrease\n",
            "\t Train_Loss: 7.7988 Val_Loss: 47.3054  BEST VAL Loss: 46.6143\n",
            "\n",
            "Epoch 1570: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3309 Val_Loss: 46.6617  BEST VAL Loss: 46.6143\n",
            "\n",
            "Epoch 1571: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4844 Val_Loss: 50.9910  BEST VAL Loss: 46.6143\n",
            "\n",
            "Epoch 1572: Validation loss did not decrease\n",
            "\t Train_Loss: 7.8853 Val_Loss: 46.8986  BEST VAL Loss: 46.6143\n",
            "\n",
            "Epoch 1573: Validation loss decreased (46.614288 --> 46.287556).\n",
            "\t Train_Loss: 7.5940 Val_Loss: 46.2876  BEST VAL Loss: 46.2876\n",
            "\n",
            "Epoch 1574: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5862 Val_Loss: 49.2008  BEST VAL Loss: 46.2876\n",
            "\n",
            "Epoch 1575: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5579 Val_Loss: 47.2072  BEST VAL Loss: 46.2876\n",
            "\n",
            "Epoch 1576: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3369 Val_Loss: 47.1027  BEST VAL Loss: 46.2876\n",
            "\n",
            "Epoch 1577: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2363 Val_Loss: 49.0282  BEST VAL Loss: 46.2876\n",
            "\n",
            "Epoch 1578: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3808 Val_Loss: 46.7117  BEST VAL Loss: 46.2876\n",
            "\n",
            "Epoch 1579: Validation loss decreased (46.287556 --> 46.232876).\n",
            "\t Train_Loss: 7.3405 Val_Loss: 46.2329  BEST VAL Loss: 46.2329\n",
            "\n",
            "Epoch 1580: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2917 Val_Loss: 48.0751  BEST VAL Loss: 46.2329\n",
            "\n",
            "Epoch 1581: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2418 Val_Loss: 46.4602  BEST VAL Loss: 46.2329\n",
            "\n",
            "Epoch 1582: Validation loss did not decrease\n",
            "\t Train_Loss: 7.4036 Val_Loss: 48.9394  BEST VAL Loss: 46.2329\n",
            "\n",
            "Epoch 1583: Validation loss decreased (46.232876 --> 46.060818).\n",
            "\t Train_Loss: 7.3947 Val_Loss: 46.0608  BEST VAL Loss: 46.0608\n",
            "\n",
            "Epoch 1584: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3741 Val_Loss: 46.2558  BEST VAL Loss: 46.0608\n",
            "\n",
            "Epoch 1585: Validation loss did not decrease\n",
            "\t Train_Loss: 7.3198 Val_Loss: 48.4216  BEST VAL Loss: 46.0608\n",
            "\n",
            "Epoch 1586: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2931 Val_Loss: 46.3368  BEST VAL Loss: 46.0608\n",
            "\n",
            "Epoch 1587: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1072 Val_Loss: 46.8409  BEST VAL Loss: 46.0608\n",
            "\n",
            "Epoch 1588: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0816 Val_Loss: 47.1186  BEST VAL Loss: 46.0608\n",
            "\n",
            "Epoch 1589: Validation loss decreased (46.060818 --> 45.802246).\n",
            "\t Train_Loss: 7.0296 Val_Loss: 45.8022  BEST VAL Loss: 45.8022\n",
            "\n",
            "Epoch 1590: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1150 Val_Loss: 46.9308  BEST VAL Loss: 45.8022\n",
            "\n",
            "Epoch 1591: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0344 Val_Loss: 46.8955  BEST VAL Loss: 45.8022\n",
            "\n",
            "Epoch 1592: Validation loss decreased (45.802246 --> 45.673153).\n",
            "\t Train_Loss: 6.9665 Val_Loss: 45.6732  BEST VAL Loss: 45.6732\n",
            "\n",
            "Epoch 1593: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1516 Val_Loss: 48.5289  BEST VAL Loss: 45.6732\n",
            "\n",
            "Epoch 1594: Validation loss decreased (45.673153 --> 44.618332).\n",
            "\t Train_Loss: 7.5755 Val_Loss: 44.6183  BEST VAL Loss: 44.6183\n",
            "\n",
            "Epoch 1595: Validation loss did not decrease\n",
            "\t Train_Loss: 7.5215 Val_Loss: 46.9376  BEST VAL Loss: 44.6183\n",
            "\n",
            "Epoch 1596: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1027 Val_Loss: 48.5062  BEST VAL Loss: 44.6183\n",
            "\n",
            "Epoch 1597: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2663 Val_Loss: 44.9772  BEST VAL Loss: 44.6183\n",
            "\n",
            "Epoch 1598: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2932 Val_Loss: 47.2563  BEST VAL Loss: 44.6183\n",
            "\n",
            "Epoch 1599: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1607 Val_Loss: 45.8217  BEST VAL Loss: 44.6183\n",
            "\n",
            "Epoch 1600: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9413 Val_Loss: 45.1368  BEST VAL Loss: 44.6183\n",
            "\n",
            "Epoch 1601: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0730 Val_Loss: 48.7108  BEST VAL Loss: 44.6183\n",
            "\n",
            "Epoch 1602: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2182 Val_Loss: 46.1954  BEST VAL Loss: 44.6183\n",
            "\n",
            "Epoch 1603: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9151 Val_Loss: 44.8619  BEST VAL Loss: 44.6183\n",
            "\n",
            "Epoch 1604: Validation loss did not decrease\n",
            "\t Train_Loss: 7.0631 Val_Loss: 47.6191  BEST VAL Loss: 44.6183\n",
            "\n",
            "Epoch 1605: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1668 Val_Loss: 45.6443  BEST VAL Loss: 44.6183\n",
            "\n",
            "Epoch 1606: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8429 Val_Loss: 45.4558  BEST VAL Loss: 44.6183\n",
            "\n",
            "Epoch 1607: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9206 Val_Loss: 48.0113  BEST VAL Loss: 44.6183\n",
            "\n",
            "Epoch 1608: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1076 Val_Loss: 45.3850  BEST VAL Loss: 44.6183\n",
            "\n",
            "Epoch 1609: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8517 Val_Loss: 44.6470  BEST VAL Loss: 44.6183\n",
            "\n",
            "Epoch 1610: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9679 Val_Loss: 48.0830  BEST VAL Loss: 44.6183\n",
            "\n",
            "Epoch 1611: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2345 Val_Loss: 45.0395  BEST VAL Loss: 44.6183\n",
            "\n",
            "Epoch 1612: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8396 Val_Loss: 45.0067  BEST VAL Loss: 44.6183\n",
            "\n",
            "Epoch 1613: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8121 Val_Loss: 47.0877  BEST VAL Loss: 44.6183\n",
            "\n",
            "Epoch 1614: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9829 Val_Loss: 44.9263  BEST VAL Loss: 44.6183\n",
            "\n",
            "Epoch 1615: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7653 Val_Loss: 45.0097  BEST VAL Loss: 44.6183\n",
            "\n",
            "Epoch 1616: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7440 Val_Loss: 46.8103  BEST VAL Loss: 44.6183\n",
            "\n",
            "Epoch 1617: Validation loss decreased (44.618332 --> 44.468491).\n",
            "\t Train_Loss: 6.9165 Val_Loss: 44.4685  BEST VAL Loss: 44.4685\n",
            "\n",
            "Epoch 1618: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8169 Val_Loss: 45.2203  BEST VAL Loss: 44.4685\n",
            "\n",
            "Epoch 1619: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6964 Val_Loss: 45.9234  BEST VAL Loss: 44.4685\n",
            "\n",
            "Epoch 1620: Validation loss decreased (44.468491 --> 44.405430).\n",
            "\t Train_Loss: 6.7397 Val_Loss: 44.4054  BEST VAL Loss: 44.4054\n",
            "\n",
            "Epoch 1621: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8438 Val_Loss: 46.5402  BEST VAL Loss: 44.4054\n",
            "\n",
            "Epoch 1622: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9136 Val_Loss: 44.6120  BEST VAL Loss: 44.4054\n",
            "\n",
            "Epoch 1623: Validation loss decreased (44.405430 --> 44.125393).\n",
            "\t Train_Loss: 6.7371 Val_Loss: 44.1254  BEST VAL Loss: 44.1254\n",
            "\n",
            "Epoch 1624: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7745 Val_Loss: 46.8251  BEST VAL Loss: 44.1254\n",
            "\n",
            "Epoch 1625: Validation loss decreased (44.125393 --> 44.029602).\n",
            "\t Train_Loss: 6.9855 Val_Loss: 44.0296  BEST VAL Loss: 44.0296\n",
            "\n",
            "Epoch 1626: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8134 Val_Loss: 45.4254  BEST VAL Loss: 44.0296\n",
            "\n",
            "Epoch 1627: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7002 Val_Loss: 45.3326  BEST VAL Loss: 44.0296\n",
            "\n",
            "Epoch 1628: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6984 Val_Loss: 44.1824  BEST VAL Loss: 44.0296\n",
            "\n",
            "Epoch 1629: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6959 Val_Loss: 45.3922  BEST VAL Loss: 44.0296\n",
            "\n",
            "Epoch 1630: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6533 Val_Loss: 44.4220  BEST VAL Loss: 44.0296\n",
            "\n",
            "Epoch 1631: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6141 Val_Loss: 44.3873  BEST VAL Loss: 44.0296\n",
            "\n",
            "Epoch 1632: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5720 Val_Loss: 44.7432  BEST VAL Loss: 44.0296\n",
            "\n",
            "Epoch 1633: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5872 Val_Loss: 44.5349  BEST VAL Loss: 44.0296\n",
            "\n",
            "Epoch 1634: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5603 Val_Loss: 44.4999  BEST VAL Loss: 44.0296\n",
            "\n",
            "Epoch 1635: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5353 Val_Loss: 44.1508  BEST VAL Loss: 44.0296\n",
            "\n",
            "Epoch 1636: Validation loss decreased (44.029602 --> 43.934780).\n",
            "\t Train_Loss: 6.5407 Val_Loss: 43.9348  BEST VAL Loss: 43.9348\n",
            "\n",
            "Epoch 1637: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5164 Val_Loss: 44.1709  BEST VAL Loss: 43.9348\n",
            "\n",
            "Epoch 1638: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5109 Val_Loss: 44.3422  BEST VAL Loss: 43.9348\n",
            "\n",
            "Epoch 1639: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4989 Val_Loss: 44.3272  BEST VAL Loss: 43.9348\n",
            "\n",
            "Epoch 1640: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4811 Val_Loss: 43.9825  BEST VAL Loss: 43.9348\n",
            "\n",
            "Epoch 1641: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4757 Val_Loss: 44.0358  BEST VAL Loss: 43.9348\n",
            "\n",
            "Epoch 1642: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4647 Val_Loss: 44.0770  BEST VAL Loss: 43.9348\n",
            "\n",
            "Epoch 1643: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4530 Val_Loss: 44.5521  BEST VAL Loss: 43.9348\n",
            "\n",
            "Epoch 1644: Validation loss decreased (43.934780 --> 43.866138).\n",
            "\t Train_Loss: 6.4522 Val_Loss: 43.8661  BEST VAL Loss: 43.8661\n",
            "\n",
            "Epoch 1645: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4483 Val_Loss: 44.5043  BEST VAL Loss: 43.8661\n",
            "\n",
            "Epoch 1646: Validation loss decreased (43.866138 --> 43.055164).\n",
            "\t Train_Loss: 6.5092 Val_Loss: 43.0552  BEST VAL Loss: 43.0552\n",
            "\n",
            "Epoch 1647: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5958 Val_Loss: 45.8878  BEST VAL Loss: 43.0552\n",
            "\n",
            "Epoch 1648: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7933 Val_Loss: 43.1470  BEST VAL Loss: 43.0552\n",
            "\n",
            "Epoch 1649: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6537 Val_Loss: 44.7534  BEST VAL Loss: 43.0552\n",
            "\n",
            "Epoch 1650: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5049 Val_Loss: 44.0437  BEST VAL Loss: 43.0552\n",
            "\n",
            "Epoch 1651: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4109 Val_Loss: 43.2838  BEST VAL Loss: 43.0552\n",
            "\n",
            "Epoch 1652: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5306 Val_Loss: 46.1004  BEST VAL Loss: 43.0552\n",
            "\n",
            "Epoch 1653: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7652 Val_Loss: 43.2147  BEST VAL Loss: 43.0552\n",
            "\n",
            "Epoch 1654: Validation loss decreased (43.055164 --> 42.982571).\n",
            "\t Train_Loss: 6.5744 Val_Loss: 42.9826  BEST VAL Loss: 42.9826\n",
            "\n",
            "Epoch 1655: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4821 Val_Loss: 45.3488  BEST VAL Loss: 42.9826\n",
            "\n",
            "Epoch 1656: Validation loss decreased (42.982571 --> 42.423290).\n",
            "\t Train_Loss: 6.7270 Val_Loss: 42.4233  BEST VAL Loss: 42.4233\n",
            "\n",
            "Epoch 1657: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7983 Val_Loss: 45.5531  BEST VAL Loss: 42.4233\n",
            "\n",
            "Epoch 1658: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7722 Val_Loss: 44.2541  BEST VAL Loss: 42.4233\n",
            "\n",
            "Epoch 1659: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7204 Val_Loss: 42.8076  BEST VAL Loss: 42.4233\n",
            "\n",
            "Epoch 1660: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8442 Val_Loss: 44.0869  BEST VAL Loss: 42.4233\n",
            "\n",
            "Epoch 1661: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4814 Val_Loss: 43.0075  BEST VAL Loss: 42.4233\n",
            "\n",
            "Epoch 1662: Validation loss decreased (42.423290 --> 42.266872).\n",
            "\t Train_Loss: 6.5315 Val_Loss: 42.2669  BEST VAL Loss: 42.2669\n",
            "\n",
            "Epoch 1663: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5035 Val_Loss: 44.2320  BEST VAL Loss: 42.2669\n",
            "\n",
            "Epoch 1664: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5491 Val_Loss: 43.3562  BEST VAL Loss: 42.2669\n",
            "\n",
            "Epoch 1665: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5844 Val_Loss: 42.7570  BEST VAL Loss: 42.2669\n",
            "\n",
            "Epoch 1666: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6144 Val_Loss: 43.6616  BEST VAL Loss: 42.2669\n",
            "\n",
            "Epoch 1667: Validation loss decreased (42.266872 --> 42.197605).\n",
            "\t Train_Loss: 6.4343 Val_Loss: 42.1976  BEST VAL Loss: 42.1976\n",
            "\n",
            "Epoch 1668: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3927 Val_Loss: 42.2167  BEST VAL Loss: 42.1976\n",
            "\n",
            "Epoch 1669: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4263 Val_Loss: 44.1083  BEST VAL Loss: 42.1976\n",
            "\n",
            "Epoch 1670: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4418 Val_Loss: 42.7864  BEST VAL Loss: 42.1976\n",
            "\n",
            "Epoch 1671: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5209 Val_Loss: 42.9958  BEST VAL Loss: 42.1976\n",
            "\n",
            "Epoch 1672: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3909 Val_Loss: 43.3592  BEST VAL Loss: 42.1976\n",
            "\n",
            "Epoch 1673: Validation loss decreased (42.197605 --> 41.461590).\n",
            "\t Train_Loss: 6.3347 Val_Loss: 41.4616  BEST VAL Loss: 41.4616\n",
            "\n",
            "Epoch 1674: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5730 Val_Loss: 44.7311  BEST VAL Loss: 41.4616\n",
            "\n",
            "Epoch 1675: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7465 Val_Loss: 42.1617  BEST VAL Loss: 41.4616\n",
            "\n",
            "Epoch 1676: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5608 Val_Loss: 42.2082  BEST VAL Loss: 41.4616\n",
            "\n",
            "Epoch 1677: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5757 Val_Loss: 45.0767  BEST VAL Loss: 41.4616\n",
            "\n",
            "Epoch 1678: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6519 Val_Loss: 41.8243  BEST VAL Loss: 41.4616\n",
            "\n",
            "Epoch 1679: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2846 Val_Loss: 41.5283  BEST VAL Loss: 41.4616\n",
            "\n",
            "Epoch 1680: Validation loss did not decrease\n",
            "\t Train_Loss: 6.3781 Val_Loss: 44.8323  BEST VAL Loss: 41.4616\n",
            "\n",
            "Epoch 1681: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6314 Val_Loss: 42.2549  BEST VAL Loss: 41.4616\n",
            "\n",
            "Epoch 1682: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4503 Val_Loss: 42.0326  BEST VAL Loss: 41.4616\n",
            "\n",
            "Epoch 1683: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5073 Val_Loss: 44.5273  BEST VAL Loss: 41.4616\n",
            "\n",
            "Epoch 1684: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4931 Val_Loss: 41.8198  BEST VAL Loss: 41.4616\n",
            "\n",
            "Epoch 1685: Validation loss decreased (41.461590 --> 40.934349).\n",
            "\t Train_Loss: 6.1907 Val_Loss: 40.9343  BEST VAL Loss: 40.9343\n",
            "\n",
            "Epoch 1686: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4729 Val_Loss: 45.0590  BEST VAL Loss: 40.9343\n",
            "\n",
            "Epoch 1687: Validation loss did not decrease\n",
            "\t Train_Loss: 6.8142 Val_Loss: 41.9453  BEST VAL Loss: 40.9343\n",
            "\n",
            "Epoch 1688: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4322 Val_Loss: 41.2819  BEST VAL Loss: 40.9343\n",
            "\n",
            "Epoch 1689: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6519 Val_Loss: 44.1277  BEST VAL Loss: 40.9343\n",
            "\n",
            "Epoch 1690: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5065 Val_Loss: 41.7751  BEST VAL Loss: 40.9343\n",
            "\n",
            "Epoch 1691: Validation loss decreased (40.934349 --> 40.101845).\n",
            "\t Train_Loss: 6.1443 Val_Loss: 40.1018  BEST VAL Loss: 40.1018\n",
            "\n",
            "Epoch 1692: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9290 Val_Loss: 45.9958  BEST VAL Loss: 40.1018\n",
            "\n",
            "Epoch 1693: Validation loss did not decrease\n",
            "\t Train_Loss: 7.1520 Val_Loss: 43.0997  BEST VAL Loss: 40.1018\n",
            "\n",
            "Epoch 1694: Validation loss did not decrease\n",
            "\t Train_Loss: 6.6160 Val_Loss: 41.2020  BEST VAL Loss: 40.1018\n",
            "\n",
            "Epoch 1695: Validation loss did not decrease\n",
            "\t Train_Loss: 7.2413 Val_Loss: 42.5107  BEST VAL Loss: 40.1018\n",
            "\n",
            "Epoch 1696: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5138 Val_Loss: 43.7565  BEST VAL Loss: 40.1018\n",
            "\n",
            "Epoch 1697: Validation loss decreased (40.101845 --> 39.874100).\n",
            "\t Train_Loss: 6.8108 Val_Loss: 39.8741  BEST VAL Loss: 39.8741\n",
            "\n",
            "Epoch 1698: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4157 Val_Loss: 40.4829  BEST VAL Loss: 39.8741\n",
            "\n",
            "Epoch 1699: Validation loss did not decrease\n",
            "\t Train_Loss: 6.5558 Val_Loss: 45.0885  BEST VAL Loss: 39.8741\n",
            "\n",
            "Epoch 1700: Validation loss did not decrease\n",
            "\t Train_Loss: 6.9599 Val_Loss: 41.7366  BEST VAL Loss: 39.8741\n",
            "\n",
            "Epoch 1701: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2921 Val_Loss: 40.2106  BEST VAL Loss: 39.8741\n",
            "\n",
            "Epoch 1702: Validation loss did not decrease\n",
            "\t Train_Loss: 6.7510 Val_Loss: 42.0477  BEST VAL Loss: 39.8741\n",
            "\n",
            "Epoch 1703: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2725 Val_Loss: 42.8741  BEST VAL Loss: 39.8741\n",
            "\n",
            "Epoch 1704: Validation loss decreased (39.874100 --> 39.524677).\n",
            "\t Train_Loss: 6.5466 Val_Loss: 39.5247  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1705: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4024 Val_Loss: 41.0509  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1706: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0674 Val_Loss: 43.4334  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1707: Validation loss did not decrease\n",
            "\t Train_Loss: 6.4172 Val_Loss: 41.0969  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1708: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2107 Val_Loss: 40.8053  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1709: Validation loss did not decrease\n",
            "\t Train_Loss: 6.1074 Val_Loss: 42.3652  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1710: Validation loss did not decrease\n",
            "\t Train_Loss: 6.2559 Val_Loss: 40.4490  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1711: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0589 Val_Loss: 40.0001  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1712: Validation loss did not decrease\n",
            "\t Train_Loss: 6.1099 Val_Loss: 42.5222  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1713: Validation loss did not decrease\n",
            "\t Train_Loss: 6.1415 Val_Loss: 41.5664  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1714: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0070 Val_Loss: 40.6645  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1715: Validation loss did not decrease\n",
            "\t Train_Loss: 6.1210 Val_Loss: 42.2824  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1716: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0534 Val_Loss: 40.9572  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1717: Validation loss did not decrease\n",
            "\t Train_Loss: 5.9353 Val_Loss: 39.8136  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1718: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0835 Val_Loss: 41.7380  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1719: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0096 Val_Loss: 41.3630  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1720: Validation loss did not decrease\n",
            "\t Train_Loss: 5.9007 Val_Loss: 40.8528  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1721: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0287 Val_Loss: 42.4290  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1722: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0293 Val_Loss: 40.6260  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1723: Validation loss did not decrease\n",
            "\t Train_Loss: 5.8587 Val_Loss: 39.8094  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1724: Validation loss did not decrease\n",
            "\t Train_Loss: 5.9593 Val_Loss: 41.4935  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1725: Validation loss did not decrease\n",
            "\t Train_Loss: 6.0054 Val_Loss: 40.4636  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1726: Validation loss did not decrease\n",
            "\t Train_Loss: 5.8380 Val_Loss: 40.5665  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1727: Validation loss did not decrease\n",
            "\t Train_Loss: 5.8676 Val_Loss: 41.8269  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1728: Validation loss did not decrease\n",
            "\t Train_Loss: 5.9315 Val_Loss: 40.2060  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1729: Validation loss did not decrease\n",
            "\t Train_Loss: 5.8137 Val_Loss: 39.9251  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1730: Validation loss did not decrease\n",
            "\t Train_Loss: 5.8135 Val_Loss: 40.9106  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1731: Validation loss did not decrease\n",
            "\t Train_Loss: 5.8791 Val_Loss: 40.0284  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1732: Validation loss did not decrease\n",
            "\t Train_Loss: 5.8020 Val_Loss: 40.5746  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1733: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7637 Val_Loss: 41.2380  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1734: Validation loss did not decrease\n",
            "\t Train_Loss: 5.8078 Val_Loss: 39.9499  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1735: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7843 Val_Loss: 40.1973  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1736: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7370 Val_Loss: 40.1151  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1737: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7410 Val_Loss: 39.6396  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1738: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7483 Val_Loss: 40.6009  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1739: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7279 Val_Loss: 40.2700  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1740: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6997 Val_Loss: 39.8769  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1741: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6996 Val_Loss: 40.2510  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1742: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7106 Val_Loss: 39.5287  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1743: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6871 Val_Loss: 39.8408  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1744: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6584 Val_Loss: 40.1847  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1745: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6605 Val_Loss: 39.7354  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1746: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6653 Val_Loss: 40.1181  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1747: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6491 Val_Loss: 39.6147  BEST VAL Loss: 39.5247\n",
            "\n",
            "Epoch 1748: Validation loss decreased (39.524677 --> 39.470390).\n",
            "\t Train_Loss: 5.6269 Val_Loss: 39.4704  BEST VAL Loss: 39.4704\n",
            "\n",
            "Epoch 1749: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6235 Val_Loss: 39.8744  BEST VAL Loss: 39.4704\n",
            "\n",
            "Epoch 1750: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6240 Val_Loss: 39.4905  BEST VAL Loss: 39.4704\n",
            "\n",
            "Epoch 1751: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6158 Val_Loss: 39.8633  BEST VAL Loss: 39.4704\n",
            "\n",
            "Epoch 1752: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5990 Val_Loss: 39.6461  BEST VAL Loss: 39.4704\n",
            "\n",
            "Epoch 1753: Validation loss decreased (39.470390 --> 39.313881).\n",
            "\t Train_Loss: 5.5847 Val_Loss: 39.3139  BEST VAL Loss: 39.3139\n",
            "\n",
            "Epoch 1754: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5834 Val_Loss: 39.5884  BEST VAL Loss: 39.3139\n",
            "\n",
            "Epoch 1755: Validation loss decreased (39.313881 --> 39.160736).\n",
            "\t Train_Loss: 5.5846 Val_Loss: 39.1607  BEST VAL Loss: 39.1607\n",
            "\n",
            "Epoch 1756: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5736 Val_Loss: 39.5159  BEST VAL Loss: 39.1607\n",
            "\n",
            "Epoch 1757: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5542 Val_Loss: 39.4662  BEST VAL Loss: 39.1607\n",
            "\n",
            "Epoch 1758: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5465 Val_Loss: 39.2114  BEST VAL Loss: 39.1607\n",
            "\n",
            "Epoch 1759: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5418 Val_Loss: 39.4369  BEST VAL Loss: 39.1607\n",
            "\n",
            "Epoch 1760: Validation loss decreased (39.160736 --> 38.963737).\n",
            "\t Train_Loss: 5.5421 Val_Loss: 38.9637  BEST VAL Loss: 38.9637\n",
            "\n",
            "Epoch 1761: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5326 Val_Loss: 39.2850  BEST VAL Loss: 38.9637\n",
            "\n",
            "Epoch 1762: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5173 Val_Loss: 39.1659  BEST VAL Loss: 38.9637\n",
            "\n",
            "Epoch 1763: Validation loss did not decrease\n",
            "\t Train_Loss: 5.5052 Val_Loss: 39.1620  BEST VAL Loss: 38.9637\n",
            "\n",
            "Epoch 1764: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4983 Val_Loss: 39.2089  BEST VAL Loss: 38.9637\n",
            "\n",
            "Epoch 1765: Validation loss decreased (38.963737 --> 38.873409).\n",
            "\t Train_Loss: 5.4917 Val_Loss: 38.8734  BEST VAL Loss: 38.8734\n",
            "\n",
            "Epoch 1766: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4871 Val_Loss: 39.0886  BEST VAL Loss: 38.8734\n",
            "\n",
            "Epoch 1767: Validation loss decreased (38.873409 --> 38.806374).\n",
            "\t Train_Loss: 5.4808 Val_Loss: 38.8064  BEST VAL Loss: 38.8064\n",
            "\n",
            "Epoch 1768: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4728 Val_Loss: 39.0910  BEST VAL Loss: 38.8064\n",
            "\n",
            "Epoch 1769: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4630 Val_Loss: 38.8532  BEST VAL Loss: 38.8064\n",
            "\n",
            "Epoch 1770: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4533 Val_Loss: 38.9139  BEST VAL Loss: 38.8064\n",
            "\n",
            "Epoch 1771: Validation loss decreased (38.806374 --> 38.754349).\n",
            "\t Train_Loss: 5.4438 Val_Loss: 38.7543  BEST VAL Loss: 38.7543\n",
            "\n",
            "Epoch 1772: Validation loss decreased (38.754349 --> 38.723660).\n",
            "\t Train_Loss: 5.4357 Val_Loss: 38.7237  BEST VAL Loss: 38.7237\n",
            "\n",
            "Epoch 1773: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4278 Val_Loss: 38.7683  BEST VAL Loss: 38.7237\n",
            "\n",
            "Epoch 1774: Validation loss decreased (38.723660 --> 38.675007).\n",
            "\t Train_Loss: 5.4208 Val_Loss: 38.6750  BEST VAL Loss: 38.6750\n",
            "\n",
            "Epoch 1775: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4140 Val_Loss: 38.7933  BEST VAL Loss: 38.6750\n",
            "\n",
            "Epoch 1776: Validation loss decreased (38.675007 --> 38.524769).\n",
            "\t Train_Loss: 5.4084 Val_Loss: 38.5248  BEST VAL Loss: 38.5248\n",
            "\n",
            "Epoch 1777: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4016 Val_Loss: 38.6924  BEST VAL Loss: 38.5248\n",
            "\n",
            "Epoch 1778: Validation loss decreased (38.524769 --> 38.381428).\n",
            "\t Train_Loss: 5.3966 Val_Loss: 38.3814  BEST VAL Loss: 38.3814\n",
            "\n",
            "Epoch 1779: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3927 Val_Loss: 38.7471  BEST VAL Loss: 38.3814\n",
            "\n",
            "Epoch 1780: Validation loss decreased (38.381428 --> 38.280762).\n",
            "\t Train_Loss: 5.3912 Val_Loss: 38.2808  BEST VAL Loss: 38.2808\n",
            "\n",
            "Epoch 1781: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3896 Val_Loss: 38.7890  BEST VAL Loss: 38.2808\n",
            "\n",
            "Epoch 1782: Validation loss decreased (38.280762 --> 38.082729).\n",
            "\t Train_Loss: 5.3944 Val_Loss: 38.0827  BEST VAL Loss: 38.0827\n",
            "\n",
            "Epoch 1783: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4030 Val_Loss: 38.8435  BEST VAL Loss: 38.0827\n",
            "\n",
            "Epoch 1784: Validation loss decreased (38.082729 --> 37.939224).\n",
            "\t Train_Loss: 5.4159 Val_Loss: 37.9392  BEST VAL Loss: 37.9392\n",
            "\n",
            "Epoch 1785: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4123 Val_Loss: 38.8265  BEST VAL Loss: 37.9392\n",
            "\n",
            "Epoch 1786: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4032 Val_Loss: 37.9983  BEST VAL Loss: 37.9392\n",
            "\n",
            "Epoch 1787: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3768 Val_Loss: 38.5460  BEST VAL Loss: 37.9392\n",
            "\n",
            "Epoch 1788: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3426 Val_Loss: 38.1195  BEST VAL Loss: 37.9392\n",
            "\n",
            "Epoch 1789: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3188 Val_Loss: 38.1811  BEST VAL Loss: 37.9392\n",
            "\n",
            "Epoch 1790: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3069 Val_Loss: 38.2716  BEST VAL Loss: 37.9392\n",
            "\n",
            "Epoch 1791: Validation loss decreased (37.939224 --> 37.926182).\n",
            "\t Train_Loss: 5.3046 Val_Loss: 37.9262  BEST VAL Loss: 37.9262\n",
            "\n",
            "Epoch 1792: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3078 Val_Loss: 38.3547  BEST VAL Loss: 37.9262\n",
            "\n",
            "Epoch 1793: Validation loss decreased (37.926182 --> 37.675388).\n",
            "\t Train_Loss: 5.3172 Val_Loss: 37.6754  BEST VAL Loss: 37.6754\n",
            "\n",
            "Epoch 1794: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3368 Val_Loss: 38.4337  BEST VAL Loss: 37.6754\n",
            "\n",
            "Epoch 1795: Validation loss decreased (37.675388 --> 37.466755).\n",
            "\t Train_Loss: 5.3550 Val_Loss: 37.4668  BEST VAL Loss: 37.4668\n",
            "\n",
            "Epoch 1796: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3646 Val_Loss: 38.4051  BEST VAL Loss: 37.4668\n",
            "\n",
            "Epoch 1797: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3580 Val_Loss: 37.4832  BEST VAL Loss: 37.4668\n",
            "\n",
            "Epoch 1798: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3325 Val_Loss: 38.1488  BEST VAL Loss: 37.4668\n",
            "\n",
            "Epoch 1799: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2848 Val_Loss: 37.6652  BEST VAL Loss: 37.4668\n",
            "\n",
            "Epoch 1800: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2510 Val_Loss: 37.8189  BEST VAL Loss: 37.4668\n",
            "\n",
            "Epoch 1801: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2348 Val_Loss: 37.8973  BEST VAL Loss: 37.4668\n",
            "\n",
            "Epoch 1802: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2320 Val_Loss: 37.5777  BEST VAL Loss: 37.4668\n",
            "\n",
            "Epoch 1803: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2379 Val_Loss: 38.0003  BEST VAL Loss: 37.4668\n",
            "\n",
            "Epoch 1804: Validation loss decreased (37.466755 --> 37.377411).\n",
            "\t Train_Loss: 5.2498 Val_Loss: 37.3774  BEST VAL Loss: 37.3774\n",
            "\n",
            "Epoch 1805: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2771 Val_Loss: 38.1102  BEST VAL Loss: 37.3774\n",
            "\n",
            "Epoch 1806: Validation loss decreased (37.377411 --> 37.211712).\n",
            "\t Train_Loss: 5.2804 Val_Loss: 37.2117  BEST VAL Loss: 37.2117\n",
            "\n",
            "Epoch 1807: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2853 Val_Loss: 37.9924  BEST VAL Loss: 37.2117\n",
            "\n",
            "Epoch 1808: Validation loss decreased (37.211712 --> 37.172710).\n",
            "\t Train_Loss: 5.2750 Val_Loss: 37.1727  BEST VAL Loss: 37.1727\n",
            "\n",
            "Epoch 1809: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2667 Val_Loss: 37.7636  BEST VAL Loss: 37.1727\n",
            "\n",
            "Epoch 1810: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2167 Val_Loss: 37.2989  BEST VAL Loss: 37.1727\n",
            "\n",
            "Epoch 1811: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1807 Val_Loss: 37.4402  BEST VAL Loss: 37.1727\n",
            "\n",
            "Epoch 1812: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1700 Val_Loss: 37.5756  BEST VAL Loss: 37.1727\n",
            "\n",
            "Epoch 1813: Validation loss decreased (37.172710 --> 37.144218).\n",
            "\t Train_Loss: 5.1661 Val_Loss: 37.1442  BEST VAL Loss: 37.1442\n",
            "\n",
            "Epoch 1814: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1741 Val_Loss: 37.5385  BEST VAL Loss: 37.1442\n",
            "\n",
            "Epoch 1815: Validation loss decreased (37.144218 --> 37.051895).\n",
            "\t Train_Loss: 5.1903 Val_Loss: 37.0519  BEST VAL Loss: 37.0519\n",
            "\n",
            "Epoch 1816: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2534 Val_Loss: 37.9170  BEST VAL Loss: 37.0519\n",
            "\n",
            "Epoch 1817: Validation loss decreased (37.051895 --> 36.749706).\n",
            "\t Train_Loss: 5.2548 Val_Loss: 36.7497  BEST VAL Loss: 36.7497\n",
            "\n",
            "Epoch 1818: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2005 Val_Loss: 37.4419  BEST VAL Loss: 36.7497\n",
            "\n",
            "Epoch 1819: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1800 Val_Loss: 37.0612  BEST VAL Loss: 36.7497\n",
            "\n",
            "Epoch 1820: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2066 Val_Loss: 37.3442  BEST VAL Loss: 36.7497\n",
            "\n",
            "Epoch 1821: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1205 Val_Loss: 37.0442  BEST VAL Loss: 36.7497\n",
            "\n",
            "Epoch 1822: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1585 Val_Loss: 36.9170  BEST VAL Loss: 36.7497\n",
            "\n",
            "Epoch 1823: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2082 Val_Loss: 37.8057  BEST VAL Loss: 36.7497\n",
            "\n",
            "Epoch 1824: Validation loss decreased (36.749706 --> 36.483837).\n",
            "\t Train_Loss: 5.2568 Val_Loss: 36.4838  BEST VAL Loss: 36.4838\n",
            "\n",
            "Epoch 1825: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1355 Val_Loss: 36.6622  BEST VAL Loss: 36.4838\n",
            "\n",
            "Epoch 1826: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1036 Val_Loss: 37.3350  BEST VAL Loss: 36.4838\n",
            "\n",
            "Epoch 1827: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1315 Val_Loss: 36.8190  BEST VAL Loss: 36.4838\n",
            "\n",
            "Epoch 1828: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1102 Val_Loss: 37.0215  BEST VAL Loss: 36.4838\n",
            "\n",
            "Epoch 1829: Validation loss decreased (36.483837 --> 36.311981).\n",
            "\t Train_Loss: 5.1579 Val_Loss: 36.3120  BEST VAL Loss: 36.3120\n",
            "\n",
            "Epoch 1830: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2465 Val_Loss: 37.4312  BEST VAL Loss: 36.3120\n",
            "\n",
            "Epoch 1831: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2723 Val_Loss: 36.3142  BEST VAL Loss: 36.3120\n",
            "\n",
            "Epoch 1832: Validation loss decreased (36.311981 --> 35.993015).\n",
            "\t Train_Loss: 5.0743 Val_Loss: 35.9930  BEST VAL Loss: 35.9930\n",
            "\n",
            "Epoch 1833: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2239 Val_Loss: 38.9225  BEST VAL Loss: 35.9930\n",
            "\n",
            "Epoch 1834: Validation loss did not decrease\n",
            "\t Train_Loss: 5.8445 Val_Loss: 36.6465  BEST VAL Loss: 35.9930\n",
            "\n",
            "Epoch 1835: Validation loss did not decrease\n",
            "\t Train_Loss: 5.7103 Val_Loss: 36.2259  BEST VAL Loss: 35.9930\n",
            "\n",
            "Epoch 1836: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4267 Val_Loss: 38.8886  BEST VAL Loss: 35.9930\n",
            "\n",
            "Epoch 1837: Validation loss decreased (35.993015 --> 35.679611).\n",
            "\t Train_Loss: 6.3232 Val_Loss: 35.6796  BEST VAL Loss: 35.6796\n",
            "\n",
            "Epoch 1838: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6926 Val_Loss: 37.5940  BEST VAL Loss: 35.6796\n",
            "\n",
            "Epoch 1839: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4206 Val_Loss: 38.7549  BEST VAL Loss: 35.6796\n",
            "\n",
            "Epoch 1840: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6516 Val_Loss: 36.2010  BEST VAL Loss: 35.6796\n",
            "\n",
            "Epoch 1841: Validation loss decreased (35.679611 --> 34.658154).\n",
            "\t Train_Loss: 5.2044 Val_Loss: 34.6582  BEST VAL Loss: 34.6582\n",
            "\n",
            "Epoch 1842: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6893 Val_Loss: 39.1208  BEST VAL Loss: 34.6582\n",
            "\n",
            "Epoch 1843: Validation loss did not decrease\n",
            "\t Train_Loss: 5.9225 Val_Loss: 37.6161  BEST VAL Loss: 34.6582\n",
            "\n",
            "Epoch 1844: Validation loss did not decrease\n",
            "\t Train_Loss: 5.6662 Val_Loss: 36.2811  BEST VAL Loss: 34.6582\n",
            "\n",
            "Epoch 1845: Validation loss did not decrease\n",
            "\t Train_Loss: 5.9757 Val_Loss: 36.0846  BEST VAL Loss: 34.6582\n",
            "\n",
            "Epoch 1846: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2087 Val_Loss: 37.5682  BEST VAL Loss: 34.6582\n",
            "\n",
            "Epoch 1847: Validation loss decreased (34.658154 --> 34.385506).\n",
            "\t Train_Loss: 5.9942 Val_Loss: 34.3855  BEST VAL Loss: 34.3855\n",
            "\n",
            "Epoch 1848: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4562 Val_Loss: 35.2664  BEST VAL Loss: 34.3855\n",
            "\n",
            "Epoch 1849: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2478 Val_Loss: 37.0438  BEST VAL Loss: 34.3855\n",
            "\n",
            "Epoch 1850: Validation loss did not decrease\n",
            "\t Train_Loss: 5.4772 Val_Loss: 36.8449  BEST VAL Loss: 34.3855\n",
            "\n",
            "Epoch 1851: Validation loss did not decrease\n",
            "\t Train_Loss: 5.3655 Val_Loss: 35.2893  BEST VAL Loss: 34.3855\n",
            "\n",
            "Epoch 1852: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1900 Val_Loss: 34.9411  BEST VAL Loss: 34.3855\n",
            "\n",
            "Epoch 1853: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2757 Val_Loss: 35.7566  BEST VAL Loss: 34.3855\n",
            "\n",
            "Epoch 1854: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2667 Val_Loss: 35.1018  BEST VAL Loss: 34.3855\n",
            "\n",
            "Epoch 1855: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2755 Val_Loss: 35.4074  BEST VAL Loss: 34.3855\n",
            "\n",
            "Epoch 1856: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2998 Val_Loss: 36.2724  BEST VAL Loss: 34.3855\n",
            "\n",
            "Epoch 1857: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1334 Val_Loss: 35.7529  BEST VAL Loss: 34.3855\n",
            "\n",
            "Epoch 1858: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0934 Val_Loss: 35.4026  BEST VAL Loss: 34.3855\n",
            "\n",
            "Epoch 1859: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1705 Val_Loss: 37.0825  BEST VAL Loss: 34.3855\n",
            "\n",
            "Epoch 1860: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2208 Val_Loss: 36.5032  BEST VAL Loss: 34.3855\n",
            "\n",
            "Epoch 1861: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1721 Val_Loss: 34.7617  BEST VAL Loss: 34.3855\n",
            "\n",
            "Epoch 1862: Validation loss did not decrease\n",
            "\t Train_Loss: 5.2045 Val_Loss: 35.7627  BEST VAL Loss: 34.3855\n",
            "\n",
            "Epoch 1863: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0059 Val_Loss: 36.7823  BEST VAL Loss: 34.3855\n",
            "\n",
            "Epoch 1864: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0639 Val_Loss: 36.4356  BEST VAL Loss: 34.3855\n",
            "\n",
            "Epoch 1865: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1460 Val_Loss: 36.2140  BEST VAL Loss: 34.3855\n",
            "\n",
            "Epoch 1866: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0087 Val_Loss: 35.6869  BEST VAL Loss: 34.3855\n",
            "\n",
            "Epoch 1867: Validation loss decreased (34.385506 --> 34.271568).\n",
            "\t Train_Loss: 5.0741 Val_Loss: 34.2716  BEST VAL Loss: 34.2716\n",
            "\n",
            "Epoch 1868: Validation loss did not decrease\n",
            "\t Train_Loss: 5.1474 Val_Loss: 35.3710  BEST VAL Loss: 34.2716\n",
            "\n",
            "Epoch 1869: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9610 Val_Loss: 36.0965  BEST VAL Loss: 34.2716\n",
            "\n",
            "Epoch 1870: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9868 Val_Loss: 35.8299  BEST VAL Loss: 34.2716\n",
            "\n",
            "Epoch 1871: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0164 Val_Loss: 35.3766  BEST VAL Loss: 34.2716\n",
            "\n",
            "Epoch 1872: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9204 Val_Loss: 34.8668  BEST VAL Loss: 34.2716\n",
            "\n",
            "Epoch 1873: Validation loss decreased (34.271568 --> 34.147228).\n",
            "\t Train_Loss: 4.9513 Val_Loss: 34.1472  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1874: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9853 Val_Loss: 35.0546  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1875: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9333 Val_Loss: 35.4584  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1876: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9078 Val_Loss: 34.9911  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1877: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9577 Val_Loss: 35.8200  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1878: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9947 Val_Loss: 35.0496  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1879: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9695 Val_Loss: 34.9523  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1880: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8857 Val_Loss: 35.2398  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1881: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9678 Val_Loss: 34.6750  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1882: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9381 Val_Loss: 35.5773  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1883: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8786 Val_Loss: 35.5733  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1884: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8798 Val_Loss: 34.6352  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1885: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8890 Val_Loss: 34.7729  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1886: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8407 Val_Loss: 35.0217  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1887: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8370 Val_Loss: 34.8730  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1888: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8672 Val_Loss: 34.9180  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1889: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8112 Val_Loss: 34.7238  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1890: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8206 Val_Loss: 34.3855  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1891: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8401 Val_Loss: 34.8634  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1892: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8234 Val_Loss: 34.6412  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1893: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7938 Val_Loss: 34.2834  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1894: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8150 Val_Loss: 34.7669  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1895: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8343 Val_Loss: 34.5996  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1896: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8242 Val_Loss: 34.7470  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1897: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7745 Val_Loss: 34.7284  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1898: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8061 Val_Loss: 34.2753  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1899: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8564 Val_Loss: 35.1859  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1900: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8671 Val_Loss: 34.6803  BEST VAL Loss: 34.1472\n",
            "\n",
            "Epoch 1901: Validation loss decreased (34.147228 --> 34.091354).\n",
            "\t Train_Loss: 4.7811 Val_Loss: 34.0914  BEST VAL Loss: 34.0914\n",
            "\n",
            "Epoch 1902: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8138 Val_Loss: 34.8726  BEST VAL Loss: 34.0914\n",
            "\n",
            "Epoch 1903: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8603 Val_Loss: 34.6436  BEST VAL Loss: 34.0914\n",
            "\n",
            "Epoch 1904: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8041 Val_Loss: 34.6321  BEST VAL Loss: 34.0914\n",
            "\n",
            "Epoch 1905: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7591 Val_Loss: 34.4789  BEST VAL Loss: 34.0914\n",
            "\n",
            "Epoch 1906: Validation loss decreased (34.091354 --> 33.786613).\n",
            "\t Train_Loss: 4.8225 Val_Loss: 33.7866  BEST VAL Loss: 33.7866\n",
            "\n",
            "Epoch 1907: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8155 Val_Loss: 34.6496  BEST VAL Loss: 33.7866\n",
            "\n",
            "Epoch 1908: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7450 Val_Loss: 34.8575  BEST VAL Loss: 33.7866\n",
            "\n",
            "Epoch 1909: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7645 Val_Loss: 34.0479  BEST VAL Loss: 33.7866\n",
            "\n",
            "Epoch 1910: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7742 Val_Loss: 34.1835  BEST VAL Loss: 33.7866\n",
            "\n",
            "Epoch 1911: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7556 Val_Loss: 34.0399  BEST VAL Loss: 33.7866\n",
            "\n",
            "Epoch 1912: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7161 Val_Loss: 34.0520  BEST VAL Loss: 33.7866\n",
            "\n",
            "Epoch 1913: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7184 Val_Loss: 34.1334  BEST VAL Loss: 33.7866\n",
            "\n",
            "Epoch 1914: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7274 Val_Loss: 33.8775  BEST VAL Loss: 33.7866\n",
            "\n",
            "Epoch 1915: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7305 Val_Loss: 34.3828  BEST VAL Loss: 33.7866\n",
            "\n",
            "Epoch 1916: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7129 Val_Loss: 34.1456  BEST VAL Loss: 33.7866\n",
            "\n",
            "Epoch 1917: Validation loss decreased (33.786613 --> 33.767292).\n",
            "\t Train_Loss: 4.6907 Val_Loss: 33.7673  BEST VAL Loss: 33.7673\n",
            "\n",
            "Epoch 1918: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6951 Val_Loss: 34.0801  BEST VAL Loss: 33.7673\n",
            "\n",
            "Epoch 1919: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7010 Val_Loss: 34.2226  BEST VAL Loss: 33.7673\n",
            "\n",
            "Epoch 1920: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7002 Val_Loss: 34.3157  BEST VAL Loss: 33.7673\n",
            "\n",
            "Epoch 1921: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6731 Val_Loss: 33.9906  BEST VAL Loss: 33.7673\n",
            "\n",
            "Epoch 1922: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6771 Val_Loss: 33.7862  BEST VAL Loss: 33.7673\n",
            "\n",
            "Epoch 1923: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6907 Val_Loss: 34.2939  BEST VAL Loss: 33.7673\n",
            "\n",
            "Epoch 1924: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7000 Val_Loss: 33.9606  BEST VAL Loss: 33.7673\n",
            "\n",
            "Epoch 1925: Validation loss decreased (33.767292 --> 33.674801).\n",
            "\t Train_Loss: 4.6627 Val_Loss: 33.6748  BEST VAL Loss: 33.6748\n",
            "\n",
            "Epoch 1926: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6619 Val_Loss: 34.1033  BEST VAL Loss: 33.6748\n",
            "\n",
            "Epoch 1927: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6773 Val_Loss: 34.0070  BEST VAL Loss: 33.6748\n",
            "\n",
            "Epoch 1928: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7058 Val_Loss: 34.0304  BEST VAL Loss: 33.6748\n",
            "\n",
            "Epoch 1929: Validation loss decreased (33.674801 --> 33.621014).\n",
            "\t Train_Loss: 4.6497 Val_Loss: 33.6210  BEST VAL Loss: 33.6210\n",
            "\n",
            "Epoch 1930: Validation loss decreased (33.621014 --> 33.589344).\n",
            "\t Train_Loss: 4.6462 Val_Loss: 33.5893  BEST VAL Loss: 33.5893\n",
            "\n",
            "Epoch 1931: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6713 Val_Loss: 34.3043  BEST VAL Loss: 33.5893\n",
            "\n",
            "Epoch 1932: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7105 Val_Loss: 33.7550  BEST VAL Loss: 33.5893\n",
            "\n",
            "Epoch 1933: Validation loss decreased (33.589344 --> 33.494465).\n",
            "\t Train_Loss: 4.6595 Val_Loss: 33.4945  BEST VAL Loss: 33.4945\n",
            "\n",
            "Epoch 1934: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6281 Val_Loss: 33.8094  BEST VAL Loss: 33.4945\n",
            "\n",
            "Epoch 1935: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6477 Val_Loss: 33.7824  BEST VAL Loss: 33.4945\n",
            "\n",
            "Epoch 1936: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6961 Val_Loss: 33.9940  BEST VAL Loss: 33.4945\n",
            "\n",
            "Epoch 1937: Validation loss decreased (33.494465 --> 33.436199).\n",
            "\t Train_Loss: 4.6377 Val_Loss: 33.4362  BEST VAL Loss: 33.4362\n",
            "\n",
            "Epoch 1938: Validation loss decreased (33.436199 --> 33.308014).\n",
            "\t Train_Loss: 4.6144 Val_Loss: 33.3080  BEST VAL Loss: 33.3080\n",
            "\n",
            "Epoch 1939: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6548 Val_Loss: 34.1953  BEST VAL Loss: 33.3080\n",
            "\n",
            "Epoch 1940: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7126 Val_Loss: 33.6516  BEST VAL Loss: 33.3080\n",
            "\n",
            "Epoch 1941: Validation loss decreased (33.308014 --> 33.239479).\n",
            "\t Train_Loss: 4.6533 Val_Loss: 33.2395  BEST VAL Loss: 33.2395\n",
            "\n",
            "Epoch 1942: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6035 Val_Loss: 33.6588  BEST VAL Loss: 33.2395\n",
            "\n",
            "Epoch 1943: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6762 Val_Loss: 33.5666  BEST VAL Loss: 33.2395\n",
            "\n",
            "Epoch 1944: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7403 Val_Loss: 34.0466  BEST VAL Loss: 33.2395\n",
            "\n",
            "Epoch 1945: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6381 Val_Loss: 33.3861  BEST VAL Loss: 33.2395\n",
            "\n",
            "Epoch 1946: Validation loss decreased (33.239479 --> 32.686954).\n",
            "\t Train_Loss: 4.6290 Val_Loss: 32.6870  BEST VAL Loss: 32.6870\n",
            "\n",
            "Epoch 1947: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8177 Val_Loss: 34.8119  BEST VAL Loss: 32.6870\n",
            "\n",
            "Epoch 1948: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0360 Val_Loss: 33.8633  BEST VAL Loss: 32.6870\n",
            "\n",
            "Epoch 1949: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8068 Val_Loss: 32.7555  BEST VAL Loss: 32.6870\n",
            "\n",
            "Epoch 1950: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7881 Val_Loss: 34.2688  BEST VAL Loss: 32.6870\n",
            "\n",
            "Epoch 1951: Validation loss decreased (32.686954 --> 32.615791).\n",
            "\t Train_Loss: 5.1750 Val_Loss: 32.6158  BEST VAL Loss: 32.6158\n",
            "\n",
            "Epoch 1952: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6961 Val_Loss: 33.5476  BEST VAL Loss: 32.6158\n",
            "\n",
            "Epoch 1953: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7393 Val_Loss: 34.5268  BEST VAL Loss: 32.6158\n",
            "\n",
            "Epoch 1954: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8050 Val_Loss: 33.2686  BEST VAL Loss: 32.6158\n",
            "\n",
            "Epoch 1955: Validation loss decreased (32.615791 --> 32.039536).\n",
            "\t Train_Loss: 4.5988 Val_Loss: 32.0395  BEST VAL Loss: 32.0395\n",
            "\n",
            "Epoch 1956: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9109 Val_Loss: 34.3656  BEST VAL Loss: 32.0395\n",
            "\n",
            "Epoch 1957: Validation loss did not decrease\n",
            "\t Train_Loss: 5.0163 Val_Loss: 33.8819  BEST VAL Loss: 32.0395\n",
            "\n",
            "Epoch 1958: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8511 Val_Loss: 32.7230  BEST VAL Loss: 32.0395\n",
            "\n",
            "Epoch 1959: Validation loss did not decrease\n",
            "\t Train_Loss: 4.9188 Val_Loss: 32.8593  BEST VAL Loss: 32.0395\n",
            "\n",
            "Epoch 1960: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7083 Val_Loss: 32.4244  BEST VAL Loss: 32.0395\n",
            "\n",
            "Epoch 1961: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7738 Val_Loss: 32.2242  BEST VAL Loss: 32.0395\n",
            "\n",
            "Epoch 1962: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7355 Val_Loss: 33.7616  BEST VAL Loss: 32.0395\n",
            "\n",
            "Epoch 1963: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7319 Val_Loss: 33.9337  BEST VAL Loss: 32.0395\n",
            "\n",
            "Epoch 1964: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7838 Val_Loss: 32.3214  BEST VAL Loss: 32.0395\n",
            "\n",
            "Epoch 1965: Validation loss decreased (32.039536 --> 31.824060).\n",
            "\t Train_Loss: 4.6123 Val_Loss: 31.8241  BEST VAL Loss: 31.8241\n",
            "\n",
            "Epoch 1966: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7037 Val_Loss: 33.2584  BEST VAL Loss: 31.8241\n",
            "\n",
            "Epoch 1967: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8906 Val_Loss: 32.5684  BEST VAL Loss: 31.8241\n",
            "\n",
            "Epoch 1968: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8703 Val_Loss: 32.8687  BEST VAL Loss: 31.8241\n",
            "\n",
            "Epoch 1969: Validation loss did not decrease\n",
            "\t Train_Loss: 4.7707 Val_Loss: 33.3837  BEST VAL Loss: 31.8241\n",
            "\n",
            "Epoch 1970: Validation loss did not decrease\n",
            "\t Train_Loss: 4.8267 Val_Loss: 31.9679  BEST VAL Loss: 31.8241\n",
            "\n",
            "Epoch 1971: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6418 Val_Loss: 32.4275  BEST VAL Loss: 31.8241\n",
            "\n",
            "Epoch 1972: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5786 Val_Loss: 33.8435  BEST VAL Loss: 31.8241\n",
            "\n",
            "Epoch 1973: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6619 Val_Loss: 33.4450  BEST VAL Loss: 31.8241\n",
            "\n",
            "Epoch 1974: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5847 Val_Loss: 32.3774  BEST VAL Loss: 31.8241\n",
            "\n",
            "Epoch 1975: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5809 Val_Loss: 32.3600  BEST VAL Loss: 31.8241\n",
            "\n",
            "Epoch 1976: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5984 Val_Loss: 32.2468  BEST VAL Loss: 31.8241\n",
            "\n",
            "Epoch 1977: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5199 Val_Loss: 32.8707  BEST VAL Loss: 31.8241\n",
            "\n",
            "Epoch 1978: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5635 Val_Loss: 33.4263  BEST VAL Loss: 31.8241\n",
            "\n",
            "Epoch 1979: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5517 Val_Loss: 32.6993  BEST VAL Loss: 31.8241\n",
            "\n",
            "Epoch 1980: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4921 Val_Loss: 31.8450  BEST VAL Loss: 31.8241\n",
            "\n",
            "Epoch 1981: Validation loss did not decrease\n",
            "\t Train_Loss: 4.6063 Val_Loss: 32.9879  BEST VAL Loss: 31.8241\n",
            "\n",
            "Epoch 1982: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5854 Val_Loss: 33.0092  BEST VAL Loss: 31.8241\n",
            "\n",
            "Epoch 1983: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5557 Val_Loss: 32.4543  BEST VAL Loss: 31.8241\n",
            "\n",
            "Epoch 1984: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5318 Val_Loss: 32.4352  BEST VAL Loss: 31.8241\n",
            "\n",
            "Epoch 1985: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5746 Val_Loss: 32.0125  BEST VAL Loss: 31.8241\n",
            "\n",
            "Epoch 1986: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5185 Val_Loss: 32.7501  BEST VAL Loss: 31.8241\n",
            "\n",
            "Epoch 1987: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4746 Val_Loss: 33.1151  BEST VAL Loss: 31.8241\n",
            "\n",
            "Epoch 1988: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5248 Val_Loss: 32.3402  BEST VAL Loss: 31.8241\n",
            "\n",
            "Epoch 1989: Validation loss decreased (31.824060 --> 31.685553).\n",
            "\t Train_Loss: 4.4556 Val_Loss: 31.6856  BEST VAL Loss: 31.6856\n",
            "\n",
            "Epoch 1990: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5003 Val_Loss: 32.2382  BEST VAL Loss: 31.6856\n",
            "\n",
            "Epoch 1991: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4914 Val_Loss: 32.4950  BEST VAL Loss: 31.6856\n",
            "\n",
            "Epoch 1992: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4934 Val_Loss: 32.6269  BEST VAL Loss: 31.6856\n",
            "\n",
            "Epoch 1993: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4579 Val_Loss: 32.2993  BEST VAL Loss: 31.6856\n",
            "\n",
            "Epoch 1994: Validation loss decreased (31.685553 --> 31.645237).\n",
            "\t Train_Loss: 4.4772 Val_Loss: 31.6452  BEST VAL Loss: 31.6452\n",
            "\n",
            "Epoch 1995: Validation loss did not decrease\n",
            "\t Train_Loss: 4.5178 Val_Loss: 32.5918  BEST VAL Loss: 31.6452\n",
            "\n",
            "Epoch 1996: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4714 Val_Loss: 32.7047  BEST VAL Loss: 31.6452\n",
            "\n",
            "Epoch 1997: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4687 Val_Loss: 32.0861  BEST VAL Loss: 31.6452\n",
            "\n",
            "Epoch 1998: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4513 Val_Loss: 31.9529  BEST VAL Loss: 31.6452\n",
            "\n",
            "Epoch 1999: Validation loss did not decrease\n",
            "\t Train_Loss: 4.4694 Val_Loss: 31.8600  BEST VAL Loss: 31.6452\n",
            "\n"
          ]
        }
      ],
      "source": [
        "LSTM_best_model, train_losses, val_losses = trainer(LSTM_model, X_train, y_train, X_val, y_val, optimizer, criterion, n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "rOoQ6lrfFYQo",
        "outputId": "524e9b96-0509-47b1-ea61-6a37caa624d5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(train_losses, label = 'Train_loss')\n",
        "plt.plot(val_losses, label = 'validation_loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVxb1IHaYwmU"
      },
      "source": [
        "---\n",
        "### 6.4 Evaluate model on validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "gGzInAaoDBWy"
      },
      "outputs": [],
      "source": [
        "val_predict_LSTM = LSTM_best_model(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "0plcD3u8DKqa",
        "outputId": "c005d444-b1ce-4ae9-aead-3000a03a40e9"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1400x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(df[-len(y_val):].index, y_val.cpu(), label=\"actual\", marker=\"o\")\n",
        "plt.plot(df[-len(y_val):].index, val_predict_LSTM.detach().cpu(), label=\"predicted\", marker=\"o\")\n",
        "plt.title(\"Electric production IP prediction by LSTM RNN model\", fontsize=25)\n",
        "plt.ylabel(\"ylabel\")\n",
        "plt.legend(title_fontsize=14, fontsize=13, fancybox=True, shadow=True, frameon=True)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WfqFMJLan26"
      },
      "source": [
        "---\n",
        "---\n",
        "## 7 Comaprison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "R7E2YreFari1",
        "outputId": "6b367c2b-6d79-4adc-d46c-4361e8136e30"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1400x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(df[-len(y_val):].index, y_val.cpu(), label=\"actual\", marker=\"o\")\n",
        "plt.plot(df[-len(y_val):].index, val_predict_RNN.detach().cpu(), label=\"predicted_RNN\", marker=\"o\")\n",
        "plt.plot(df[-len(y_val):].index, val_predict_GRU.detach().cpu(), label=\"predicted_GRU\", marker=\"o\")\n",
        "plt.plot(df[-len(y_val):].index, val_predict_LSTM.detach().cpu(), label=\"predicted_LSTM\", marker=\"o\")\n",
        "plt.title(\"Electric production IP prediction\", fontsize=25)\n",
        "plt.ylabel(\"ylabel\")\n",
        "plt.legend(title_fontsize=14, fontsize=13, fancybox=True, shadow=True, frameon=True)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McCOcrqqhXkt"
      },
      "source": [
        "\n",
        "<b>1-Rank these architectures based on their performance?\n",
        "\n",
        "2-Why are they ranked in this order?\n",
        "\n",
        "3-Run the notebook again with look_back = 15.\n",
        "write about the difference in the comparison plot and the possible cause for that difference.</b>\n",
        "\n",
        "<font color='#73FF73'><b>Your answer:</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, RNN was the worst in performance and after that we have GRU and after that ther is LSTM.</br>\n",
        "Because RNN's have simple structure, they suffer from vanishing gradient problem. LSTMs are designed to address the vanishing gradient problem by using a special type of memory cell that can store information over long periods of time. LSTMs have gates that control how information flows into, out of, and within the memory cell. These gates allow LSTMs to selectively remember and forget information, which is important for processing long sequences. GRUs are a simplified version of LSTMs that are easier to train and run. GRUs do not have as many gates as LSTMs, and they do not have a separate memory cell. However, they are still able to effectively process long sequences.</br>\n",
        "If we change the look_back and make it less, the model will be more likely to overfit and miss the long-term relations. And we can see that tthe model performance will be poorer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### I did this homework with Ali Falahati"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "r7FFJaRgaqC1",
        "prWan6z7eKEp",
        "u78V0kKvjp8f",
        "nIfT0qVkOTzG",
        "uwmm5DhEBl8d",
        "5Ay_sL8ZBq1R",
        "utvDPP4kXEI-",
        "fkIg9lbrXiKZ",
        "-SlbDcQ-XuYx",
        "xIIm3f8yX9XF",
        "5Jcdsct5Y9a8",
        "mDMYeUTH5Ki7",
        "6WfqFMJLan26"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
